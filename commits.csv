repo,sha,message
pytorch/pytorch,76bcc87277fcd5030ee844cc783a9e1524012423,"fix TIMM mobilevit_s complier issue for dynamic CPU path (#100230)

For TIMM ```mobilevit``` dynamic path, there has a compiler issue(```
python -m torch.backends.xeon.run_cpu --node_id 0 benchmarks/dynamo/timm_models.py --performance --float32 -dcpu -n2 --inductor --no-skip --dashboard --only mobilevit_s --inference --dynamic-shapes```
):

```
/tmp/torchinductor_xiaobing/xy/cxyslqzcsxkco4ieph7t63kn5q74ka35ak75lwfon32nlalxmru5.cpp:29:130: error: invalid operands of types ‘long int’ and ‘double’ to binary ‘operator%’
   29 |                             auto tmp0 = in_ptr0[static_cast<long>((((((-1L) + ks1) / 8L)*(((-1L) + ks1) / 8L))*((((2L*((i2 / 1L) % (std::ceil((1.0/2.0) + ((1.0/2.0)*(((-1L) + ks1)
```

There has a modulo for ```long % double```, this PR will convert inputs to long before do this operation.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/100230
Approved by: https://github.com/jansel"
pytorch/pytorch,d7fa7fa8cfa086b9358c929cbf02e24f8cbe01a1,"Introduce fast path in the CPU equal op

Differential Revision: D45282119nnPull Request resolved: https://github.com/pytorch/pytorch/pull/100024"
pytorch/pytorch,628a8df1c9de02fa7a332c1cbea615db08a01c4e,"[c10d] Comment out ddp_hook_with_optimizer_parity tests (#100215)

This is a mirror PR of D45339293

Summary:
These tests cause the following errors internally with unknown reason:
```
AttributeError: type object 'TestDistBackendWithSpawn' has no attribute 'test_ddp_hook_with_optimizer_parity_adam'
AttributeError: type object 'TestDistBackendWithSpawn' has no attribute 'test_ddp_hook_with_optimizer_parity_adamw'
AttributeError: type object 'TestDistBackendWithSpawn' has no attribute 'test_ddp_hook_with_optimizer_parity_sgd'
```
Commenting these tests out to unblock other PRs.

Test Plan: Sandcastle

Pull Request resolved: https://github.com/pytorch/pytorch/pull/100215
Approved by: https://github.com/wz337, https://github.com/fduwjj"
pytorch/pytorch,9609aeefbb0b31ffa801034c2daa3fecc6804707,"Revert ""[c10d] Comment out ddp_hook_with_optimizer_parity tests (#100215)""

This reverts commit ae40a6c7356190ef86b14b10a94a58ca41ca496b.

Reverted https://github.com/pytorch/pytorch/pull/100215 on behalf of https://github.com/huydhn due to Sorry for revert your change, but it breaks lint, please run lintrunner -a torch/testing/_internal/distributed/distributed_test.py to fix the issue then reland it"
pytorch/pytorch,674018903d2d4e54848c85c53052a838e6d2a0e8,"per-Tensor `grad_fn` for in-place foreach functions (#96405)

Generate a `grad_fn` for each (tuple of) `Tensor`(s) of the same index for `_foreach_foo_` and each `grad_fn` is `FooBackward`.

The current status of foreach functions' backward support for the record:
- out-place: Implemented, but no optimized implementations like their forward path
- in-place: not implemented. I think this check https://github.com/pytorch/pytorch/blob/7eaaefafb3ce0e4a0a9e1eb647e340711973ec12/torchgen/api/autograd.py#L309-L311 is partly responsible but the difference of signature between out-place and in-place (see https://github.com/pytorch/pytorch/pull/96405#discussion_r1154690940) would prevent in-place from using out-place versions (the logic is around https://github.com/pytorch/pytorch/blob/7eaaefafb3ce0e4a0a9e1eb647e340711973ec12/torchgen/api/autograd.py#L495-L500)

```c++
void _foreach_abs_(c10::DispatchKeySet ks, at::TensorList self) {
  auto self_ = unpack(self, ""self"", 0);
  #ifndef NDEBUG
  std::vector<c10::optional<Storage>> self__storage_saved(self_.size());
  for (const Tensor& tensor : self_)
    self__storage_saved.push_back(
      tensor.has_storage() ? c10::optional<Storage>(tensor.storage()) : c10::nullopt);
  std::vector<c10::intrusive_ptr<TensorImpl>> self__impl_saved(self_.size());
  for (size_t i=0; i<self_.size(); i++)
    if (self_[i].defined()) self__impl_saved[i] = self_[i].getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::_foreach_abs_(ks & c10::after_autograd_keyset, self_);
  }
  #ifndef NDEBUG
  for (size_t i=0; i<self_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (self__storage_saved[i].has_value() && !at::impl::tensorlist_has_dispatch(self_))
      AT_ASSERT(self__storage_saved[i].value().is_alias_of(self_[i].storage()));
  }
  for (size_t i=0; i<self_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (self__impl_saved[i] && !at::impl::tensorlist_has_dispatch(self_))
      AT_ASSERT(self__impl_saved[i] == self_[i].getIntrusivePtr());
  }
  #endif
}
```

Related:
- #95431
- #95765 for multiple `grad_fn`s logic

---

Examples: outputs of `_foreach_add_.List`, `_foreach_addcmul_.ScalarList`, and `_foreach_exp`

```c++
void _foreach_addcmul__ScalarList(c10::DispatchKeySet ks, at::TensorList self, at::TensorList tensor1, at::TensorList tensor2, at::ArrayRef<at::Scalar> scalars) {
  auto self_ = unpack(self, ""self"", 0);
  auto tensor1_ = unpack(tensor1, ""tensor1"", 1);
  auto tensor2_ = unpack(tensor2, ""tensor2"", 2);
  auto _any_requires_grad = compute_requires_grad( self, tensor1, tensor2 );

  (void)_any_requires_grad;
  std::vector<c10::optional<at::Tensor>> original_selfs(self.size());
  std::vector<std::shared_ptr<AddcmulBackward0>> grad_fns;
  if (_any_requires_grad) {
    for (const auto& i : c10::irange( self.size() )) {
      const auto ith_requires_grad = compute_requires_grad(self[i], tensor1[i], tensor2[i]);
      check_inplace(self[i], ith_requires_grad);
      grad_fns.push_back([&]() -> std::shared_ptr<AddcmulBackward0> {
          if (!ith_requires_grad) {
              return nullptr;
          } else {
              auto grad_fn = std::shared_ptr<AddcmulBackward0>(new AddcmulBackward0(), deleteNode);
              grad_fn->set_next_edges(collect_next_edges( self[i], tensor1[i], tensor2[i] ));
              return grad_fn;
          }
      }());
    }
    if (!grad_fns.empty()) {

        for (const auto& i : c10::irange(grad_fns.size())) {
            auto grad_fn = grad_fns[i];
            if (grad_fn != nullptr) {
                grad_fn->self_scalar_type = self[i].scalar_type();
                grad_fn->tensor1_scalar_type = tensor1[i].scalar_type();
                if (grad_fn->should_compute_output(1)) {
                  grad_fn->tensor2_ = SavedVariable(tensor2[i], false);
                }
                grad_fn->value = scalars[i];
                if (grad_fn->should_compute_output(2)) {
                  grad_fn->tensor1_ = SavedVariable(tensor1[i], false);
                }
                grad_fn->tensor2_scalar_type = tensor2[i].scalar_type();
            }
        }
    }
  }
  #ifndef NDEBUG
  std::vector<c10::optional<Storage>> self__storage_saved(self_.size());
  for (const Tensor& tensor : self_)
    self__storage_saved.push_back(
      tensor.has_storage() ? c10::optional<Storage>(tensor.storage()) : c10::nullopt);
  std::vector<c10::intrusive_ptr<TensorImpl>> self__impl_saved(self_.size());
  for (size_t i=0; i<self_.size(); i++)
    if (self_[i].defined()) self__impl_saved[i] = self_[i].getIntrusivePtr();
  std::vector<c10::optional<Storage>> tensor1__storage_saved(tensor1_.size());
  for (const Tensor& tensor : tensor1_)
    tensor1__storage_saved.push_back(
      tensor.has_storage() ? c10::optional<Storage>(tensor.storage()) : c10::nullopt);
  std::vector<c10::intrusive_ptr<TensorImpl>> tensor1__impl_saved(tensor1_.size());
  for (size_t i=0; i<tensor1_.size(); i++)
    if (tensor1_[i].defined()) tensor1__impl_saved[i] = tensor1_[i].getIntrusivePtr();
  std::vector<c10::optional<Storage>> tensor2__storage_saved(tensor2_.size());
  for (const Tensor& tensor : tensor2_)
    tensor2__storage_saved.push_back(
      tensor.has_storage() ? c10::optional<Storage>(tensor.storage()) : c10::nullopt);
  std::vector<c10::intrusive_ptr<TensorImpl>> tensor2__impl_saved(tensor2_.size());
  for (size_t i=0; i<tensor2_.size(); i++)
    if (tensor2_[i].defined()) tensor2__impl_saved[i] = tensor2_[i].getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::_foreach_addcmul_(ks & c10::after_autograd_keyset, self_, tensor1_, tensor2_, scalars);
  }
  #ifndef NDEBUG
  for (size_t i=0; i<self_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (self__storage_saved[i].has_value() && !at::impl::tensorlist_has_dispatch(self_))
      TORCH_INTERNAL_ASSERT(self__storage_saved[i].value().is_alias_of(self_[i].storage()));
  }
  for (size_t i=0; i<self_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (self__impl_saved[i] && !at::impl::tensorlist_has_dispatch(self_))
      TORCH_INTERNAL_ASSERT(self__impl_saved[i] == self_[i].getIntrusivePtr());
  }
  for (size_t i=0; i<tensor1_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (tensor1__storage_saved[i].has_value() && !at::impl::tensorlist_has_dispatch(tensor1_))
      TORCH_INTERNAL_ASSERT(tensor1__storage_saved[i].value().is_alias_of(tensor1_[i].storage()));
  }
  for (size_t i=0; i<tensor1_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (tensor1__impl_saved[i] && !at::impl::tensorlist_has_dispatch(tensor1_))
      TORCH_INTERNAL_ASSERT(tensor1__impl_saved[i] == tensor1_[i].getIntrusivePtr());
  }
  for (size_t i=0; i<tensor2_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (tensor2__storage_saved[i].has_value() && !at::impl::tensorlist_has_dispatch(tensor2_))
      TORCH_INTERNAL_ASSERT(tensor2__storage_saved[i].value().is_alias_of(tensor2_[i].storage()));
  }
  for (size_t i=0; i<tensor2_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (tensor2__impl_saved[i] && !at::impl::tensorlist_has_dispatch(tensor2_))
      TORCH_INTERNAL_ASSERT(tensor2__impl_saved[i] == tensor2_[i].getIntrusivePtr());
  }
  #endif
  if (!grad_fns.empty()) {
      auto differentiable_outputs = flatten_tensor_args( self );
      TORCH_INTERNAL_ASSERT(differentiable_outputs.size() == grad_fns.size());
      for (const auto& i : c10::irange(grad_fns.size())) {
          auto grad_fn = grad_fns[i];
          if (grad_fn != nullptr) {
              rebase_history(differentiable_outputs[i], grad_fns[i]);
          }
      }
  }
}

```

```c++
void _foreach_add__List(c10::DispatchKeySet ks, at::TensorList self, at::TensorList other, const at::Scalar & alpha) {
  auto self_ = unpack(self, ""self"", 0);
  auto other_ = unpack(other, ""other"", 1);
  auto _any_requires_grad = compute_requires_grad( self, other );

  (void)_any_requires_grad;
  std::vector<c10::optional<at::Tensor>> original_selfs(self.size());
  std::vector<std::shared_ptr<AddBackward0>> grad_fns;
  if (_any_requires_grad) {
    for (const auto& i : c10::irange( self.size() )) {
      const auto ith_requires_grad = compute_requires_grad(self[i], other[i]);
      check_inplace(self[i], ith_requires_grad);
      grad_fns.push_back([&]() -> std::shared_ptr<AddBackward0> {
          if (!ith_requires_grad) {
              return nullptr;
          } else {
              auto grad_fn = std::shared_ptr<AddBackward0>(new AddBackward0(), deleteNode);
              grad_fn->set_next_edges(collect_next_edges( self[i], other[i] ));
              return grad_fn;
          }
      }());
    }
    if (!grad_fns.empty()) {

        for (const auto& i : c10::irange(grad_fns.size())) {
            auto grad_fn = grad_fns[i];
            if (grad_fn != nullptr) {
                grad_fn->other_scalar_type = other[i].scalar_type();
                grad_fn->alpha = alpha;
                grad_fn->self_scalar_type = self[i].scalar_type();
            }
        }
    }
  }
  #ifndef NDEBUG
  std::vector<c10::optional<Storage>> self__storage_saved(self_.size());
  for (const Tensor& tensor : self_)
    self__storage_saved.push_back(
      tensor.has_storage() ? c10::optional<Storage>(tensor.storage()) : c10::nullopt);
  std::vector<c10::intrusive_ptr<TensorImpl>> self__impl_saved(self_.size());
  for (size_t i=0; i<self_.size(); i++)
    if (self_[i].defined()) self__impl_saved[i] = self_[i].getIntrusivePtr();
  std::vector<c10::optional<Storage>> other__storage_saved(other_.size());
  for (const Tensor& tensor : other_)
    other__storage_saved.push_back(
      tensor.has_storage() ? c10::optional<Storage>(tensor.storage()) : c10::nullopt);
  std::vector<c10::intrusive_ptr<TensorImpl>> other__impl_saved(other_.size());
  for (size_t i=0; i<other_.size(); i++)
    if (other_[i].defined()) other__impl_saved[i] = other_[i].getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::_foreach_add_(ks & c10::after_autograd_keyset, self_, other_, alpha);
  }
  #ifndef NDEBUG
  for (size_t i=0; i<self_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (self__storage_saved[i].has_value() && !at::impl::tensorlist_has_dispatch(self_))
      TORCH_INTERNAL_ASSERT(self__storage_saved[i].value().is_alias_of(self_[i].storage()));
  }
  for (size_t i=0; i<self_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (self__impl_saved[i] && !at::impl::tensorlist_has_dispatch(self_))
      TORCH_INTERNAL_ASSERT(self__impl_saved[i] == self_[i].getIntrusivePtr());
  }
  for (size_t i=0; i<other_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (other__storage_saved[i].has_value() && !at::impl::tensorlist_has_dispatch(other_))
      TORCH_INTERNAL_ASSERT(other__storage_saved[i].value().is_alias_of(other_[i].storage()));
  }
  for (size_t i=0; i<other_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (other__impl_saved[i] && !at::impl::tensorlist_has_dispatch(other_))
      TORCH_INTERNAL_ASSERT(other__impl_saved[i] == other_[i].getIntrusivePtr());
  }
  #endif
  if (!grad_fns.empty()) {
      auto differentiable_outputs = flatten_tensor_args( self );
      TORCH_INTERNAL_ASSERT(differentiable_outputs.size() == grad_fns.size());
      for (const auto& i : c10::irange(grad_fns.size())) {
          auto grad_fn = grad_fns[i];
          if (grad_fn != nullptr) {
              rebase_history(differentiable_outputs[i], grad_fns[i]);
          }
      }
  }
}

...

void _foreach_exp_(c10::DispatchKeySet ks, at::TensorList self) {
  auto self_ = unpack(self, ""self"", 0);
  auto _any_requires_grad = compute_requires_grad( self );

  (void)_any_requires_grad;
  std::vector<c10::optional<at::Tensor>> original_selfs(self.size());
  std::vector<std::shared_ptr<ExpBackward0>> grad_fns;
  if (_any_requires_grad) {
    for (const auto& i : c10::irange( self.size() )) {
      const auto ith_requires_grad = compute_requires_grad(self[i]);
      check_inplace(self[i], ith_requires_grad);
      grad_fns.push_back([&]() -> std::shared_ptr<ExpBackward0> {
          if (!ith_requires_grad) {
              return nullptr;
          } else {
              auto grad_fn = std::shared_ptr<ExpBackward0>(new ExpBackward0(), deleteNode);
              grad_fn->set_next_edges(collect_next_edges( self[i] ));
              return grad_fn;
          }
      }());
    }
  }
  #ifndef NDEBUG
  std::vector<c10::optional<Storage>> self__storage_saved(self_.size());
  for (const Tensor& tensor : self_)
    self__storage_saved.push_back(
      tensor.has_storage() ? c10::optional<Storage>(tensor.storage()) : c10::nullopt);
  std::vector<c10::intrusive_ptr<TensorImpl>> self__impl_saved(self_.size());
  for (size_t i=0; i<self_.size(); i++)
    if (self_[i].defined()) self__impl_saved[i] = self_[i].getIntrusivePtr();
  #endif
  {
    at::AutoDispatchBelowAutograd guard;
    at::redispatch::_foreach_exp_(ks & c10::after_autograd_keyset, self_);
  }
  #ifndef NDEBUG
  for (size_t i=0; i<self_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (self__storage_saved[i].has_value() && !at::impl::tensorlist_has_dispatch(self_))
      TORCH_INTERNAL_ASSERT(self__storage_saved[i].value().is_alias_of(self_[i].storage()));
  }
  for (size_t i=0; i<self_.size() && !at::impl::dispatch_mode_enabled(); i++) {
    if (self__impl_saved[i] && !at::impl::tensorlist_has_dispatch(self_))
      TORCH_INTERNAL_ASSERT(self__impl_saved[i] == self_[i].getIntrusivePtr());
  }
  #endif
  if (!grad_fns.empty()) {
      auto differentiable_outputs = flatten_tensor_args( self );
      TORCH_INTERNAL_ASSERT(differentiable_outputs.size() == grad_fns.size());
      for (const auto& i : c10::irange(grad_fns.size())) {
          auto grad_fn = grad_fns[i];
          if (grad_fn != nullptr) {
              rebase_history(differentiable_outputs[i], grad_fns[i]);
          }
      }
  }
  if (!grad_fns.empty()) {

      for (const auto& i : c10::irange(grad_fns.size())) {
          auto grad_fn = grad_fns[i];
          if (grad_fn != nullptr) {
              grad_fn->result_ = SavedVariable(self[i], true, self[i].is_view());
          }
      }
  }
}

```
Pull Request resolved: https://github.com/pytorch/pytorch/pull/96405
Approved by: https://github.com/soulitzer"
pytorch/pytorch,1f4183e275e69b9dde8d6f3a3e44686320495658,"[FSDP] Subtest sharding strategy in test_fsdp_grad_acc.py (#100178)

Let us make the unit test faster.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/100178
Approved by: https://github.com/rohan-varma"
pytorch/pytorch,ae40a6c7356190ef86b14b10a94a58ca41ca496b,"[c10d] Comment out ddp_hook_with_optimizer_parity tests (#100215)

This is a mirror PR of D45339293

Summary:
These tests cause the following errors internally with unknown reason:
```
AttributeError: type object 'TestDistBackendWithSpawn' has no attribute 'test_ddp_hook_with_optimizer_parity_adam'
AttributeError: type object 'TestDistBackendWithSpawn' has no attribute 'test_ddp_hook_with_optimizer_parity_adamw'
AttributeError: type object 'TestDistBackendWithSpawn' has no attribute 'test_ddp_hook_with_optimizer_parity_sgd'
```
Commenting these tests out to unblock other PRs.

Test Plan: Sandcastle

Pull Request resolved: https://github.com/pytorch/pytorch/pull/100215
Approved by: https://github.com/wz337, https://github.com/fduwjj"
pytorch/pytorch,ca1cf434e773856312815d3676c37405c0c6cb77,"Not flatten states when use_orig_param is True and sharding is NO_SHARD (#100189)

When use_orig_param is True and sharding is NO_SHARD, parameters and states are not flattened, so optimizer states should not be flattened as well. The unit test will fail without the fix.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/100189
Approved by: https://github.com/awgu"
pytorch/pytorch,477ca1789c5eaa915a46134863fb6130c8980b6b,"Avoid elementwise dispatch of gradient unscaling/validation ops in `_foreach_non_finite_check_and_unscale_cpu_` (#100108)

Fixes [#82206](https://github.com/pytorch/pytorch/issues/82206)

When executing a `ShardedGradScaler` step in the context of `cpu_offload`, [the function](https://github.com/pytorch/pytorch/blob/ecd2c71871f8bf9a9fa4a4d875609b0922061a6f/torch/distributed/fsdp/sharded_grad_scaler.py#L151-L152) `_foreach_non_finite_check_and_unscale_cpu_` is grindingly slow. This issue is due to the elementwise op dispatching/redispatching/execution that is engendered by the current approach to gradient tensor validation:
https://github.com/pytorch/pytorch/blob/ecd2c71871f8bf9a9fa4a4d875609b0922061a6f/torch/distributed/fsdp/sharded_grad_scaler.py#L159-L163

The subsequent `isinf` and `isnan` checks with associated `any` checks result in unscalable elementwise op dispatches:
https://github.com/pytorch/pytorch/blob/ecd2c71871f8bf9a9fa4a4d875609b0922061a6f/torch/distributed/fsdp/sharded_grad_scaler.py#L173-L181

This inefficency is of course hidden in the current FSDP tests given their (appropriately) trivial parameter dimensionality. In the perf analysis below, the example test configures only the final `Linear(4, 8)` module parameters to require grad, so there are 40 elements to iterate through. However, if one increases the dimensionality to a still-modest 320008 elements (changing the final module to `Linear(40000,8)`), the execution time/cpu cost of the test is dominated by the elementwise op dispatching/redispatching/execution of the `any` validation ops in this function.

To characterize the current behavior, I use a slightly modified version of an existing `ShardedGradScaler` test [^1]. The following modifications to the test are made to allow the analysis:

1. Run just `CUDAInitMode.CUDA_BEFORE` for clarity instead of additional scenarios
2. Increase the final module to `Linear(40000, 8)` (along with modifying the preceding module to make the dimensions work) ,
3. For the cProfile run (but not valgrind or perf) the test runs just a single [`_train_for_several_steps`](https://github.com/pytorch/pytorch/blob/ecd2c71871f8bf9a9fa4a4d875609b0922061a6f/torch/testing/_internal/common_fsdp.py#L926-L934) step per rank (instead of 2 steps)
4. I temporarily reduce `init_scale` further to ensure we don't hit any `infs`, short-circuiting our analysis

### Current behavior

The most relevant call subgraph:
![callgrind_subgraph_elementwise_dispatch](https://user-images.githubusercontent.com/7462936/234656744-b7ca81b2-ce5b-4035-9918-0ad57d3689d3.png)

Note that:

1. Instead of dispatching to the relevant autograd op and then redispatching to the relevant CPU op implementation 8 times per test, (2 train steps x 2 any calls per parameter per step x 2 orig parameters) we (I believe unnecessarily) call the relevant dispatch flow elementwise, so 640016 times! (only 1 node in this trace so 320008 elements/2 X 2 train steps x 2 calls per element per step).
2. Nearly 50% of the relative (inclusive) instruction reads for the entire test in `callgrind` are executed by the `isnan` (320008 execs), `isinf` (320008 execs) and `any` (640016 execs) calls.
3. The `any` pre-dispatch entry point IRs (`torch::autograd::THPVariable_any`) vs actual op implementation IRs (`at::native::structured_any_all_out::impl`) are below to give one a sense of the relative dispatch and op execution cost in an elementwise context[^3].
![THPVariable_any_op_elementwise_dispatch_absolute_IR](https://user-images.githubusercontent.com/7462936/234656886-3c017ee3-8a04-4a7d-bdf8-6c690de42c92.png)
![structured_any_all_out_impl_absolute_IR](https://user-images.githubusercontent.com/7462936/234656915-0b203bb7-bd05-4ceb-a38b-67b0d4862aa7.png)

Using cprofile stats:

```bash
python -c ""import pstats; stats=pstats.Stats('/tmp/fsdp_cprofile_8wa9uw39.stats'); stats.print_stats()""
...
ncalls  tottime  	percall  	cumtime  	percall  filename:lineno(function)
1   	20.159   	20.159   	66.805   	66.805 	 torch/distributed/fsdp/sharded_grad_scaler.py:151(_foreach_non_finite_check_and_unscale_cpu_)
160004  18.427    	0.000   	18.427    	0.000 	 {built-in method torch.isinf}
160004  6.026    	0.000    	6.026    	0.000 	 {built-in method torch.isnan}
```
We see that a single step of the scaler runs for more than a minute. Though there is non-trivial cprofile overhead, we can infer from this that per-element op dispatches/executions are on the order of a 100ns.

On the order of 100 nanoseconds per dispatch is acceptable if we're using typical tensor access patterns, but if we're dispatching each element for each op, obviously everything is going to come to a grinding halt for many practical use cases.

(Given the cost of this function is currently O(n) in the number of gradient elements, feel free to set `TORCH_SHOW_DISPATCH_TRACE=1` if you want to make this function cry :rofl:)

I've attached a flamegraph at the bottom of the PR[^2] that more intuitively demonstrates the manner and extent of resource consumption attributable to this function with just a modest number of gradient elements.

### After the loop refactor in this PR:

The most relevant call subgraph:
![callgrind_subgraph_elementwise_dispatch_fix](https://user-images.githubusercontent.com/7462936/234657001-0a448756-b4ce-468e-9f91-1d21597df057.png)

Note that:

1. Less than 0.4% of the relative (inclusive) instruction reads for the entire test in `callgrind` are executed by the `isnan` (4 execs), `isinf` (4 execs) and `any` (8 execs) calls (versus ~50% and 320008, 320008, 640016 respectively above)
2. The `any` pre-dispatch entry point IRs (`torch::autograd::THPVariable_any`) vs actual op implementation IRs (`at::native::structured_any_all_out::impl`) reflect far less overhead (of secondary importance to item number 1)
![THPVariable_any_op_elementwise_dispatch_absolute_IR_fix](https://user-images.githubusercontent.com/7462936/234659454-b1e262cf-d291-4d44-aff2-e27efe284e9c.png)
![structured_any_all_out_impl_absolute_IR_fix](https://user-images.githubusercontent.com/7462936/234657154-91fa7cb8-e39e-48c7-abf0-cc58f06c0ae1.png)

Using cprofile stats:

```bash
python -c ""import pstats; stats=pstats.Stats('/tmp/fsdp_cprofile_pfap7nwk.stats'); stats.print_stats()""
...
ncalls  tottime  	percall  	cumtime  	percall  	filename:lineno(function)
1    	0.013    	0.013    	0.109    	0.109 		torch/distributed/fsdp/sharded_grad_scaler.py:151(_foreach_non_finite_check_and_unscale_cpu_)
2    	0.022    	0.011    	0.022    	0.011 		{built-in method torch.isinf}
2    	0.018    	0.009    	0.018    	0.009 		{built-in method torch.isnan}
```
We can see our function runtime has dropped from more than a minute to ~100ms.

### Assumptions associated with this loop refactor:

The key assumptions here are:

1. The grads are always on CPU in this function so any MTA-safe constraints ([`can_use_fast_route`](https://github.com/pytorch/pytorch/blob/efc3887ea508b3cfd94603fd8afe4e8cf6dce7b7/aten/src/ATen/native/cuda/AmpKernels.cu#L110-L111) relating to the relevant CUDA kernel path selection, i.e. slower `TensorIterator` gpu kernel vs `multi_tensor_apply_kernel`) do not apply in this context

2. We've already filtered by dtype and device and can assume the presence of a single CPU device. Unless manually creating separate CPU devices with manually set non-default indexes (which I don't think FSDP supports and should be validated prior to this function), device equality should always be `True` for `cpu` type devices so we should just need to check that the current device is of `cpu` type. [^4].

![elementwise_dispatch](https://user-images.githubusercontent.com/7462936/234660413-8c96ef90-7a23-4307-b8ed-c1fbf932f1e9.svg)

[^1]: `TestShardedGradScalerParityWithDDP.test_fsdp_ddp_parity_with_grad_scaler_offload_true_none_mixed_precision_use_orig_params` test in `test/distributed/fsdp/test_fsdp_sharded_grad_scaler.py`
[^2]: Note the native frame stacks for `torch::autograd::THPVariable_isinf`, `torch::autograd::THPVariable_isnan`, `torch::autograd::THPVariable_any` in particular.
[^3]: There's more `TensorIterator` etc. setup overhead further up the stack beyond `structured_any_all_out`, but roughly speaking
[^4]: Device equality is based on [type and index combination](https://github.com/pytorch/pytorch/blob/efc3887ea508b3cfd94603fd8afe4e8cf6dce7b7/c10/core/Device.h#L47-L51), CPU device type is -1 by default (`None` on the python side) and is intended to [always be 0](https://github.com/pytorch/pytorch/blob/cf21240f67a2dd316f3c9e41e3b9d61d4abac07a/c10/core/Device.h#L29) if set explicitly. Though technically, unless in debug mode, this constraint isn't [actually validated](https://github.com/pytorch/pytorch/blob/bb4e9e9124a81859de6ac1ef0c0798643cb9b7a6/c10/core/Device.h#L171-L184), so one can actually manually create separate `cpu` devices with invalid indices. I suspect it's safe to ignore that potential incorrect/unusual configuration in this context but let me know if you'd like to add another `cpu` device equality check.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/100108
Approved by: https://github.com/awgu"
pytorch/pytorch,089b085c32dc3ddc486d46466c84bb2c66c01ed7,"Optimize periodic jobs (#100182)

Split existing 4 hour scheduled into two 8 hour ones
And schedule x86 MacOS test every 8 hours and exclude them from leak
checks
Schedule iOS tests every 8 hours and exclude them from leak-checks as
well

Remove IOS metal job, as it is already tested by ARM64 MPS job as well
as x86 and arm64 vanilla jobs, as they never caught any regressions in
last 60 days, based on data from running the following query on RockSet:
```sql
SELECT started_at,
      DATE_DIFF(
            'MINUTE',
            PARSE_TIMESTAMP_ISO8601(started_at),
            PARSE_TIMESTAMP_ISO8601(completed_at)
        ) as duration,
    conclusion, name, html_url, torchci_classification
  FROM commons.workflow_job
  WHERE
  workflow_name = 'periodic' and
  name like 'ios-12% % build (default, 1, 1, macos-12)' and
  url like 'https://api.github.com/repos/pytorch/pytorch/%'
  and conclusion = 'failure'
  order by started_at desc, run_id;
```

Pull Request resolved: https://github.com/pytorch/pytorch/pull/100182
Approved by: https://github.com/PaliC, https://github.com/huydhn"
pytorch/pytorch,8fe91d16b022954206c378f637cb3de81f8f219a,"Remove CUDA 11.6 note from complex docs (#100118)

Removes note in the complex docs pointing to the CUDA 11.6 wheels introduced in https://github.com/pytorch/pytorch/pull/80363.
Background: this warning was added via https://github.com/pytorch/pytorch/issues/79876 which pointed out a slow compilation time in 11.3. The 11.6 pip wheels were thus recommended but are not build anymore as our current support is 11.7, 11.8 (and 12.1 experimental in nightlies).

The note is confusing users as it doesn't explain why 11.6 is needed.
Reference: https://discuss.pytorch.org/t/complex-numbers-cuda-11-6-documentation-warning/178588/1

Pull Request resolved: https://github.com/pytorch/pytorch/pull/100118
Approved by: https://github.com/msaroufim"
pytorch/pytorch,67e0913de9b400d0f32950f3f03dddd39f44c450,"Add support for serializing real tensor data in after aot minifier (#99834)

The new minifier script looks like this:

```
import torch._dynamo.repro.after_aot
reader = torch._dynamo.repro.after_aot.InputReader(save_dir='/tmp/tmpcsngx39e')
buf0 = reader.storage('e2b39c716c0d4efb9fa57375a3902b9dab666893', 16)
t0 = reader.tensor(buf0, (4,))
args = [t0]
mod = make_fx(Repro(), tracing_mode='real')(*args)
```

The real tensor data is stored in the storages folder of the checkpoint dump directory. If you delete this folder / it is otherwise missing, we will transparently fall back to generating random data like before. The tensors are serialized using content store from #99809, which means each storage is content-addressed and we will automatically deduplicate equivalent data (which is useful if you keep dumping out, e.g., your parameters.) We don't use the tensor serialization capability from content store, instead all of the tensor metadata is stored inline inside the repro script (so that everything is in one file if you lose the checkpointed tensors).

We also add a stable_hash option to content store, where we use a slow SHA-1 sum on the data in CPU side to compute a hash that is stable across systems with the same endianness.

Out of rage, I also added support for Dtype.itemsize property access.

Signed-off-by: Edward Z. Yang <ezyang@meta.com>

Pull Request resolved: https://github.com/pytorch/pytorch/pull/99834
Approved by: https://github.com/voznesenskym"
pytorch/pytorch,100a25d0211e8b223326b1195817867f5b6ffd23,"Basic dynamo support for traceable collectives (#94440)

Make traceable collectives work with torchdynamo,
bypassing problems with tracing the AsyncTensor subclass.

Accept a suboptimal solution for now, and optimize it later.
For now, wait happens immediately, which generally forces an early sync.

Later, find a way either in dynamo or AOT stack to handle
AsyncCollectiveTensor to get the wait in the optimal place.

Note on implementation:
- Dynamo traces 'user-level' fc apis that are designed to behave differently
  in eager vs compiled.  In eager, there will be work-obj registration and
  a wrapper subclass will insert a 'wait' call at the appropriate time.
  In compile/trace mode, wait will be immetiately called, and work obj
  registration is required to be handled by the compile backend at runtime.
- Dynamo needs to trace into some of the helper functions in the 'user-level'
  api, such as '_expand_group' which is essentially a constant transformation.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/94440
Approved by: https://github.com/kumpera"
pytorch/pytorch,dc10004553cc051548897e4a5d84d08f571e2352,"Add asan slow test shard (#99925)

Fixes #ISSUE_NUMBER

Pull Request resolved: https://github.com/pytorch/pytorch/pull/99925
Approved by: https://github.com/huydhn"
pytorch/pytorch,9f0092c4b7fcc9ca44ac082609ed6c586156d00d,"[CI] Replace timm_efficientdet with timm_vision_transformer in smoketest (#100106)

Pull Request resolved: https://github.com/pytorch/pytorch/pull/100106
Approved by: https://github.com/yanboliang"
pytorch/pytorch,3a5427baf45e53f6424668b167fd58b126fd2e37,"Add torch.utils._content_store (#99809)

Implements a simple content-addressable store for storages (with tensors implemented as cheap references on top), enabling incremental serialization of tensors to disk, which I intend to use in the accuracy repro extractor.  Check the comment at the top of torch/utils/_content_store.py for more details on the intended use case.

One major piece of this PR is implementing the content hash for tensors.  For our prospective use case, we may need to repeatedly hash up to 80 GB of tensor data every time we snapshot (and we may snapshot multiple times).  Using a conventional cryptographic hash and hashing each snapshot would likely take on order of minutes, which seemed too slow to me.  So instead, I implemented a crappy hash function that can be run on GPU.  It is at least somewhat theoretically grounded: using random parameters generated by Philox, we use the standard shift-multiply and xor sum universal hash family.  The hash function is a bit dorky though; instead of properly doing 160-bit math, it just runs 32-bit hash five times and cats them together.  By the way, this sets the first precedent for kernel in PyTorch library which MUST be torch.compile'd to be run (in fact, this kernel does not run in eager mode because of the use of xor_sum, which doesn't actually exist in ATen.)

I had to add a few more primitives to inductor, namely randint (over the entire int range) and xor_sum.  Fortunately, these primitives are natively supported by Triton/C++, and so they were very easy to plumb through.  xor_sum is exposed as a prim, while randint special cases on when low/high span the entire 32-bit signed integer range.

Thanks to Jeff Johnson for letting me bounce ideas of him on a Saturday morning lol.

Signed-off-by: Edward Z. Yang <ezyang@meta.com>

Pull Request resolved: https://github.com/pytorch/pytorch/pull/99809
Approved by: https://github.com/voznesenskym"
pytorch/pytorch,45bf3f6216a9f1321df3669976f7e96f9965984a,"Optimized EMA implementation (#94820)

This PR proposes an optimized way to do Exponential Moving Average (EMA), which is faster than the current way using `swa_utils.AveragedModel` described in https://pytorch.org/docs/stable/optim.html#custom-averaging-strategies.

This implementation is asynchronous, and is built as an optimizer wrapper so that the EMA weight update happens without any additional CPU/GPU sync, just after optimizer steps, and with limited code changes.

Example usage:
```
model = Model().to(device)
opt = torch.optim.Adam(model.parameters())

opt = EMAOptimizer(opt, device, 0.9999)

for epoch in range(epochs):
    training_loop(model, opt)

    regular_eval_accuracy = evaluate(model)

    with opt.swap_ema_weights():
        ema_eval_accuracy = evaluate(model)
```

Here are some benchmarks (time per iteration) on various torchvision models:

|model|this PR iteration time                      |swa_utils.AveragedModel iteration time| iteration speedup                                      |
|-----|-----------------------------|-----------------------|---------------------------------------------|
|     |                             |                       |                                             |
|regnet_x_1_6gf|62.73                        |67.998                 |1.08                                         |
|regnet_x_3_2gf|101.75                       |109.422                |1.08                                         |
|regnet_x_400mf|25.13                        |32.005                 |1.27                                         |
|regnet_x_800mf|33.01                        |37.466                 |1.13                                         |
|regnet_x_8gf|128.13                       |134.868                |1.05                                         |
|regnet_y_16gf|252.91                       |261.292                |1.03                                         |
|regnet_y_1_6gf|72.14                        |84.22                  |1.17                                         |
|regnet_y_3_2gf|99.99                        |109.296                |1.09                                         |
|regnet_y_400mf|29.53                        |36.506                 |1.24                                         |
|regnet_y_800mf|37.82                        |43.634                 |1.15                                         |
|regnet_y_8gf|196.63                       |203.317                |1.03                                         |
|resnet101|128.80                       |137.434                |1.07                                         |
|resnet152|182.85                       |196.498                |1.07                                         |
|resnet18|29.06                        |29.975                 |1.03                                         |
|resnet34|50.73                        |53.443                 |1.05                                         |
|resnet50|76.88                        |80.602                 |1.05                                         |
|resnext101_32x8d|277.29                       |280.759                |1.01                                         |
|resnext101_64x4d|269.56                       |281.052                |1.04                                         |
|resnext50_32x4d|100.73                       |101.102                |1.00                                         |
|shufflenet_v2_x0_5|10.56                        |15.419                 |1.46                                         |
|shufflenet_v2_x1_0|13.11                        |18.525                 |1.41                                         |
|shufflenet_v2_x1_5|18.05                        |23.132                 |1.28                                         |
|shufflenet_v2_x2_0|25.04                        |30.008                 |1.20                                         |
|squeezenet1_1|14.26                        |14.325                 |1.00                                         |
|swin_b|264.52                       |274.613                |1.04                                         |
|swin_s|180.66                       |188.914                |1.05                                         |
|swin_t|108.62                       |112.632                |1.04                                         |
|swin_v2_s|220.29                       |231.153                |1.05                                         |
|swin_v2_t|127.27                       |133.586                |1.05                                         |
|vgg11|95.52                        |103.714                |1.09                                         |
|vgg11_bn|106.49                       |120.711                |1.13                                         |
|vgg13|132.94                       |147.063                |1.11                                         |
|vgg13_bn|149.73                       |165.256                |1.10                                         |
|vgg16|158.19                       |172.865                |1.09                                         |
|vgg16_bn|177.04                       |192.888                |1.09                                         |
|vgg19|184.76                       |194.194                |1.05                                         |
|vgg19_bn|203.30                       |213.334                |1.05                                         |
|vit_b_16|217.31                       |219.748                |1.01                                         |
|vit_b_32|69.47                        |75.692                 |1.09                                         |
|vit_l_32|223.20                       |258.487                |1.16                                         |
|wide_resnet101_2|267.38                       |279.836                |1.05                                         |
|wide_resnet50_2|145.06                       |154.918                |1.07                                         |

You can see that in all cases it is faster than using `AveragedModel`. In fact in many cases, adding EMA does not add any overhead since the computation is hidden behind the usual iteration flow.

This is a similar implementation to the one currently in [NVIDIA NeMo](https://github.com/NVIDIA/NeMo).

If the team is interested in merging this, let me know and I'll add some documentation similar to `swa_utils` and tests.

Credits to @szmigacz for the implementation.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/94820
Approved by: https://github.com/janeyx99"
pytorch/pytorch,676a23f4528291e2406834bfa2368b11b1cb58a7,"[RFC] Allow elastic agent to fail fast (#99051)

Summary: Today, on a segfault on a single trainer , we end up keeping the gpu on all ranks blocked for 5 minutes due to elastic agents barrier timeouts

Test Plan: Rely on existing test to validate . Looking to get some feedback on adding UTs

Differential Revision: D44929488

Pull Request resolved: https://github.com/pytorch/pytorch/pull/99051
Approved by: https://github.com/kurman, https://github.com/kiukchung"
pytorch/pytorch,dbc7e919b8b88461cdb540c1c4d8956f2b2e2e16,"add Wmissing-prototypes to clang-tidy (#96805)

   This PR introduces **-Wmissing-prototypes** of clang-tidy to prevent further coding errors such as the one fixed by PR #96714.

<!--
copilot:summary
-->
### <samp>🤖 Generated by Copilot at fd2cf2a</samp>

This pull request makes several internal functions static to improve performance and avoid name clashes. It also fixes some typos, formatting, and missing includes in various files. It adds a new .clang-tidy check to warn about missing prototypes for non-static functions.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/96805
Approved by: https://github.com/malfet, https://github.com/albanD"
pytorch/pytorch,14c3eb7fb6708b7cede6bb49ed05f7b8dee72f25,"[Testing] flip switch, remove slow path assertions (#99101)

Pull Request resolved: https://github.com/pytorch/pytorch/pull/99101
Approved by: https://github.com/bdhirsh"
pytorch/pytorch,e7157bd048c663522f4f4101f470d5fd5240b049,"[inductor] Fix shape padding (#99917)

Summary:
We were using the ""percentiles"" form of triton.testing.do_bench, which
returns a list of like (20th, 50th, 80th) percentile timing; I don't think we
care about that much detail, so let's just use the mean.  I also took the
opportunity to clean up the redundant setting of rep, warmup, and fast_flush.

Test Plan:
```
TORCHBENCH_ATOL=1e-3 TORCHBENCH_RTOL=1e-3 TORCHINDUCTOR_PERMUTE_FUSION=1 TORCHINDUCTOR_SHAPE_PADDING=1 buck2 run mode/opt mode/inplace pytorch/benchmark:run -- ads_dhen_5x --part over --bs 1024 -d cuda -t train --torchdynamo inductor
```

Reviewed By: jiawenliu64

Differential Revision: D45241751

Pull Request resolved: https://github.com/pytorch/pytorch/pull/99917
Approved by: https://github.com/jiawenliu64"
pytorch/pytorch,e51453298bb11ebf8c5c0342952cfb20020522c1,"[ONNX] Improve diagnostics performance (#99936)

Summary
- Do not call `fx_graph_module.print_readable` when recording `fx.GraphModule` function argument diagnostics.
- Cache `inspect.getsourcelines` results.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/99936
Approved by: https://github.com/justinchuby, https://github.com/abock"
pytorch/pytorch,36e1ae677834b0979f94d94e6415f3d837b87cbf,"De-select odd numbered heads from nn.MHA fastpath (#99672)

Summary:
https://github.com/pytorch/pytorch/issues/97128

* Add test for mha num_heads %2 != 0
* Fix test
* Add test for bias false
* show test passes

Test Plan: sandcastle

Differential Revision: D45161767

Pull Request resolved: https://github.com/pytorch/pytorch/pull/99672
Approved by: https://github.com/ngimel"
pytorch/pytorch,3a09aa597744aad3ce0a401b1848a140ff8c1214,"[c10d] Faster coalescing (#98793)

### Description
The PR aims at reducing CPU overhead of context manager style coalescing.

By ""context manager style coalescing"", we mean:
Sync style:
```
with _coalescing_manager():
     for i in range(num_coll):
         dist.all_reduce(tensors[i])
```
Async style:
```
with _coalescing_manager(async_ops=True) as cm:
     for i in range(num_coll):
         dist.all_reduce(tensors[i])
cm.wait()
```
In previous implementation, each collective in the `num_coll` loop actually calls into the C++ backend, accumulating pybind overhead.

In the new implementation, we capture the collectives at Python level, and only fire towards C++ at the exit of the coalescing manager.

### Tests
In current PR, the ""fast path"" only applies to all-reduce.
- Flattened 512M: 16.38 ms, including CPU time 131.21 us
- Old _coalescing_manager 64 x 8M: 22.19 ms, including CPU time 2865 us
- New _coalescing_manager 64 x 8M: 16.93 ms, including CPU time 635 us

Hence a 4x reduction in CPU overhead (dependent on `num_coll`).

Cc @mrshenli @kumpera @wanchaol @fegin
Pull Request resolved: https://github.com/pytorch/pytorch/pull/98793
Approved by: https://github.com/kumpera"
pytorch/pytorch,3009c42e7da0b2bbeeae7538877b6d3f0b53906f,"[CI Testing] Re-enable timm_efficientdet training (#99787)

Fixes #ISSUE_NUMBER

Pull Request resolved: https://github.com/pytorch/pytorch/pull/99787
Approved by: https://github.com/desertfire"
pytorch/pytorch,f0e28b1cb9e0bc30877465b9c804a46461e537db,"Adding the maintainers approved in 2023Q1 Core Maintainers meeting (#98520)

Added Nikita to Core Maintainers
Merged MKLDNN with CPU Performance
Renamed CUDA to GPU Performance
Added Jiong to Compiler and CPU Performance
Added Xiaobing to CPU Performance
Marking Vitaly and Jian Hui as Emeritus
Pull Request resolved: https://github.com/pytorch/pytorch/pull/98520
Approved by: https://github.com/ezyang, https://github.com/soumith, https://github.com/dzhulgakov"
pytorch/pytorch,7d2a18da0b3427fcbe44b461a0aa508194535885,"Enable ruff in lintrunner (#99785)

### This change

- Implements the ruff linter in pytorch lintrunner. It is adapted from https://github.com/justinchuby/lintrunner-adapters/blob/main/lintrunner_adapters/adapters/ruff_linter.py. It does **both linting and fixing**. 🔧
- Migrated all flake8 configs to the ruff config and enabled it for the repo. ✅
- **`ruff` lints the whole repo in under 2s** 🤯

Fixes https://github.com/pytorch/pytorch/issues/94737 Replaces #99280

@huydhn @Skylion007

<!--
copilot:all
-->
### <samp>🤖 Generated by Copilot at 6b982dd</samp>

### Summary
🧹🛠️🎨

<!--
1.  🧹 This emoji represents cleaning or tidying up, which is what `ruff` does by formatting and linting the code. It also suggests improving the code quality and removing unnecessary or redundant code.
2.  🛠️ This emoji represents tools or fixing, which is what `ruff` is as a code formatter and linter. It also suggests enhancing the code functionality and performance, and resolving potential issues or bugs.
3.  🎨 This emoji represents art or creativity, which is what `ruff` allows by providing a consistent and configurable style for the code. It also suggests adding some flair or personality to the code, and making it more readable and enjoyable.
-->
Add `[tool.ruff]` section to `pyproject.toml` to configure `ruff` code formatter and linter. This change aims to improve code quality and consistency with a single tool.

> _`ruff` cleans the code_
> _like a spring breeze in the fields_
> _`pyproject.toml`_

### Walkthrough
*  Configure `ruff` code formatter and linter for the whole project ([link](https://github.com/pytorch/pytorch/pull/99785/files?diff=unified&w=0#diff-50c86b7ed8ac2cf95bd48334961bf0530cdc77b5a56f852c5c61b89d735fd711R22-R79))

Pull Request resolved: https://github.com/pytorch/pytorch/pull/99785
Approved by: https://github.com/malfet, https://github.com/Skylion007"
pytorch/pytorch,6e3cdcad08f2955117cc5757bdb7bcc7387ac355,"Fix flake8 lint errors - part 2 - manual fixes (#99799)

<!--
copilot:all
-->
### <samp>🤖 Generated by Copilot at 8aef78f</samp>

### Summary
📝🚀🛠️

<!--
1.  📝 for modifying the logging format and style
2.  🚀 for improving performance and avoiding unnecessary string creation
3.  🛠️ for fixing flake8 issues
-->
This pull request updates some logging calls to use old-style string formatting with `%s` placeholders instead of f-strings in `torch/_dynamo/logging.py`, `torch/_functorch/compilers.py`, and `torch/fx/passes/pass_manager.py` as part of a logging standardization effort. It also adds a `# noqa: F404` comment to the `import __future__` statement in `torch/overrides.py` to fix a flake8 warning.

> _`log` uses old style_
> _formatting strings with `%s`_
> _logging is faster_

### Walkthrough
*  Standardize logging format and style to use old-style string formatting with `%s` placeholders instead of f-string syntax for performance and consistency ([link](https://github.com/pytorch/pytorch/pull/99799/files?diff=unified&w=0#diff-18807f7fd187b8bc8e69e93722566195b36d5bf269099b415a6f90b552228d6bL55-R55), [link](https://github.com/pytorch/pytorch/pull/99799/files?diff=unified&w=0#diff-fae8a66564055743ec031edb87eb22edeebf7fdebef9d21660d5e6a6252e5222L370-R373), [link](https://github.com/pytorch/pytorch/pull/99799/files?diff=unified&w=0#diff-5f3e37ded032f24e247dcf4a3be4b73ea0cf21382e342631742e5a04550202e1L72-R72))
*  Suppress flake8 warning for `import __future__` statement in `torch/overrides.py` with `# noqa: F404` comment ([link](https://github.com/pytorch/pytorch/pull/99799/files?diff=unified&w=0#diff-4f601fe7f31e875ee4354882c0bb490bc35e51d3d413d058cc5fda3be8ca9f15L23-R23))

Pull Request resolved: https://github.com/pytorch/pytorch/pull/99799
Approved by: https://github.com/Skylion007"
pytorch/pytorch,48d112c431fc988de5473d3992f6c72ec5f02a6b,"Fix fake tracing of cross entropy with label smoothing and weight (#99830)

Fixes #99726
Adds a special path in cross entropy implementation for tensor subclasses, we don't always use it as it requires slightly more memory and is a bit slower.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/99830
Approved by: https://github.com/ezyang"
pytorch/pytorch,9db6920635ea19213099323f22fa41be6e2d6959,"[spmd] Add list handling to data parallel and add foreach tests (#99373)

This PR adds list handling logic to the new DataParallel expansion and
add foreach optimizer tests, currently current testing sgd optimizers
in foreach mode, for both replicate and fully shard

Next step:

Add fused optim tests
Pull Request resolved: https://github.com/pytorch/pytorch/pull/99373
Approved by: https://github.com/mrshenli"
pytorch/pytorch,e9bf94149eec0bdaccdb1317efd3f35c6f60a6c9,"[spmd] Introduce Compile Mode FSDP with DTensor (#99062)

This PR introduces compile mode Data Parallel (FSDP/DDP) using DTensor sharding.

Along with the algorithm, it also introduces a new DataParallelMode so that `compile` API can take it
and apply data parallel. This PR trys to preserve the DTensorExpand
approach first to avoid BC, we shall discuss steps to remove
DTensorExpand.

The data parallel mode uses heuristics to determine node types in the
graphs and assign the corresponding sharding. The detailed algorithm
described in the design doc.

The benefits of this approach:
- Model parameters and optimizer states are all DTensors after  `spmd.compile`, which is necessary for FSDP, and also makes it super easier for checkpointing
- As model parameter/optim states are sharding in a per-parameter approach, it would be able to compose with sophisticated second order optimizer (i.e. Shampoo) in a easier way.
- We leverage the model parameter/grads information to derive data parallel pattern. In this way we don't need to worry about DTensor op coverage anymore! As data parallel is just a special case of DTensor operation.
- Use dtensor_expand might work for DDP but aren't going to work for FSDP as dtensor might choose to allgather activation, which might violate native fsdp algorithm.
- The approach is general enough to support both DDP/FSDP and a mixed mode

Follow ups:
- Add the ""default"" data parallel mode which supports mixing of
replicate/fully shard
- Test more e2e models with more different types of optimizers, etc
- migrate the existing stack from the DTensorExpand mode
- build optimizations on top of this prototype

Differential Revision: [D45174400](https://our.internmc.facebook.com/intern/diff/D45174400)
Pull Request resolved: https://github.com/pytorch/pytorch/pull/99062
Approved by: https://github.com/mrshenli"
pytorch/pytorch,96d3f3dee389112c1a1b209d4abd847872f0efc7,"Discover and run C++ tests with run_test.py (#99559)

This depends on [pytest-cpp](https://github.com/pytest-dev/pytest-cpp) to discover and run C++ tests with pytest. C++ tests are built under `${WORKSPACE}/build/bin` directory and copied to the test job under the same path.

* To expose them to `run_test`, I choose to use the mock path prefix `cpp`, for example `build/bin/c10_Array_test` would be named as `cpp/c10_Array_test` and the `python test/run_test.py --cpp -i cpp/c10_Array_test` would run the test in the same way as other Python tests.  I could copy them from `build/bin` to `test/cpp`, but it will be mixed with the source code and CMake file.  So this looks easier
* Some executable under `build/bin` are not C++ tests, and they are exclude, for example `build/bin/torch_shm_manager`
* C++ tests need to run with pytest directly as python command doesn't understand it
* The change is gated by the new `--cpp` argument to `run_test.py`, for example `python test/run_test.py --cpp` will run all available C++ tests
* The tests can be run in parallel
* Failing tests can be retried with `--reruns=2` and `--sw`

```
============================= test session starts ==============================
platform darwin -- Python 3.9.15, pytest-7.2.0, pluggy-1.0.0 -- /Users/huydo/miniconda3/envs/py3.9/bin/python3
cachedir: .pytest_cache
hypothesis profile 'default' -> database=DirectoryBasedExampleDatabase('/Users/huydo/Storage/mine/pytorch/test/.hypothesis/examples')
rootdir: /Users/huydo/Storage/mine/pytorch, configfile: pytest.ini
plugins: xdoctest-1.1.0, cpp-2.3.0, rerunfailures-10.3, shard-0.1.2, flakefinder-1.1.0, hypothesis-6.56.4, xdist-3.0.2, repeat-0.9.1
collecting ... collected 3 items / 2 deselected / 1 selected
Running 1 items in this shard: build/bin/scalar_tensor_test::TestScalarTensor.TestScalarTensorMPS
stepwise: skipping 2 already passed items.

../build/bin/scalar_tensor_test::TestScalarTensor::TestScalarTensorMPS RERUN [100%]
../build/bin/scalar_tensor_test::TestScalarTensor::TestScalarTensorMPS RERUN [100%]
../build/bin/scalar_tensor_test::TestScalarTensor::TestScalarTensorMPS FAILED [100%]
```

* `--import-slow-tests` and `--import-disabled-tests` won't work for now and that's ok to have it as a future task.

I also add `pytest-cpp==2.3.0` to Linux Docker, MacOS, and Windows.

### Testing

Build PyTorch and run `python test/run_test.py --cpp` on my laptop.  CI change would come later in a separate PR.  Also running `python test/run_test.py --help` now shows all C++ test discovered under `build/bin`
Pull Request resolved: https://github.com/pytorch/pytorch/pull/99559
Approved by: https://github.com/clee2000"
pytorch/pytorch,7876c503b7afa7223574479ff60fe1d5441d0717,"[FSDP][optim_state_dict] Consolidate rank0_only load logic (#99647)

Follow up https://github.com/pytorch/pytorch/pull/99624, this PR consolidate the logic of `use_orig_params=False` with `use_orig_params=True` to use the same logic to load optimizer checkpoint when rank0_only is True.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/99647
Approved by: https://github.com/wz337"
pytorch/pytorch,dd07dab1c76de154a878a35b6692e3ddbeb4dd7e,"[FSDP][optim_state_dict] Support rank0_only when use_orig_params is on (#99624)

This PR makes `use_orig_params=True` case support rank0_only loading for optim state_dict. The implementation is different from `use_orig_params=False`. The `use_orig_params=False` implementation first flatten the parameters on rank0 and then broadcast the states while this implementation broadcast the state when doing the flattening. The implementation is slower as it broadcast the original parameters instead of the flattened ones. However, the implementation introduced by this PR is simpler. As loading is usually happen once per training life, the performance difference can be ignored. In next PR, we will consolidate the implementations in favor of the simpleness.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/99624
Approved by: https://github.com/wz337"
pytorch/pytorch,400075f7339d1f404ac31f4f776670677bf5d938,"[stacktraces] Keep addr2line procs around (#99670)

This PR caches the addr -> Frame information across calls to symbolize,
and also keeps the addr2line symbolizing processes around once requested.

This makes calls to symbolize frames that have been seen before nearly instant,
and makes lookup of address in libraries that have already been loaded by
addr2line faster.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/99670
Approved by: https://github.com/ezyang"
pytorch/pytorch,b96bb2f1a645da0b591e492f5311c53bcd8bf530,"[spmd] Introduce ParallelMode and add DTensorExpandMode (#98452)

This PR introduces a ParallelMode interface to define how to do
SPMD expansion and optimize the captured graph. This would be
beneifical for different parallelisms to expand differently
and apply different optimization passes

Put DTensorExpandMode as the first parallel mode that does the
existing dtensor_expand functionality.

Differential Revision: [D45174399](https://our.internmc.facebook.com/intern/diff/D45174399)
Pull Request resolved: https://github.com/pytorch/pytorch/pull/98452
Approved by: https://github.com/mrshenli"
pytorch/pytorch,9861ec9785b53e71c9affd7d268ef7073eb1c446,"Revert ""[c10d] Faster coalescing (#98793)""

This reverts commit db456ab83da6a505dcebc128903d5ee4fc2d5712.

Reverted https://github.com/pytorch/pytorch/pull/98793 on behalf of https://github.com/DanilBaibak due to Break internal build"
pytorch/pytorch,daff0408860e5a1017965f74c889a0c1c28e4c73,"[inductor] skip triton.Config that spills (#99385)

TLDR, I did a quick study of register spill in max-autotune and coordesc descent tuning. The conclusion is for the pointwise/reduction kernels, register spill is rare in inductor (which means the configs we consider are relatively reasonable), but it indeed happens sometimes. TBH, this PR does not gonna help reducing compilation time for max-autotune/coordinate descent tuning much because register spilling is very rare. But this PR only contains 2 lines of significant code change, so I guess it's still good to merge it considering ROI and code complexity.

# Register Spill in Max-Autotuner
I ran command
```
rm -rf /tmp/torchinductor_shunting_tmp && time TORCHINDUCTOR_MAX_AUTOTUNE_POINTWISE=1 TORCHINDUCTOR_CACHE_DIR=/tmp/torchinductor_shunting_tmp python -u benchmarks/dynamo/torchbench.py --backend inductor --amp --performance --dashboard --only ${MODEL} --disable-cudagraphs --training 2>&1 | tee /tmp/mylog
```
and then analyze the log.
$ cat /tmp/mylog | grep 'nspill' | wc -l

will show the total number of triton.Config's we benchmark;

$ cat /tmp/mylog  | grep 'nspill' | grep -v 'nspill 0'

will show the number of triton.Config's that spill registers.

Checked 5 models
- hf_Bert 0 spills
- resnet50: 2 out of 199 triton.Config's spill. For the 2 configs that spill, they are suboptimal according to the log: https://gist.github.com/shunting314/7ea30a9dafad7156919a99df5feba0ee
- timm_vision_transformer: 2/77 spills. The spilled configs are again sub-optimal: https://gist.github.com/shunting314/a48cbcfb14a07c0b84555e2cf7154852
- BERT_pytorch: 0/123 spills
- timm_resnest 0/255 spills

# Register Spill in Coordinate Descent Tuner
I ran command
```
rm -rf /tmp/torchinductor_shunting_tmp && time TORCHINDUCTOR_MAX_AUTOTUNE_POINTWISE=1 TORCHINDUCTOR_CACHE_DIR=/tmp/torchinductor_shunting_tmp TORCHINDUCTOR_COORDINATE_DESCENT_TUNING=1 TORCHINDUCTOR_PERSISTENT_REDUCTIONS=0  python -u benchmarks/dynamo/torchbench.py --backend inductor --amp --performance --dashboard --only ${MODEL} --disable-cudagraphs --training 2>&1 | tee /tmp/mylog

```

and then analyze the log.

$ cat /tmp/mylog | grep COORDESC | wc -l
shows the total number of configs considered by the coordinate descent tuner

$ cat /tmp/mylog | grep COORDESC | grep -v 'nspill 0'
shows the ones that spill.

Checked 3 models
- hf_Bert (log https://gist.github.com/shunting314/bd943887e77609c7c8b323fe3f554c85 )
0/525 spills
- resnet50: 0/783 spills
- timm_vision_transformer: 2/380 (log https://gist.github.com/shunting314/6231f06c1398e0cddb2a96bf52389c78 )
the 2 spilled one are sub-optimal

# Ignore Spilled Config

With this PR, I run test tests for timm_vision_transformer and  can see all 4 spilled configs (2 for max-autotune and 2 for coordinate descent tuner according to the study above) are skipped for benchmarking:
```
[2023-04-18 00:03:37,291] torch._inductor.triton_heuristics: [DEBUG] Skip config XBLOCK: 16, YBLOCK: 512, num_warps: 8, num_stages: 1 because of register spilling: 6
[2023-04-18 00:04:50,523] torch._inductor.triton_heuristics: [DEBUG] Skip config XBLOCK: 64, RBLOCK: 64, num_warps: 8, num_stages: 1 because of register spilling: 626
[2023-04-18 00:04:50,523] torch._inductor.triton_heuristics: [DEBUG] Skip config XBLOCK: 8, RBLOCK: 512, num_warps: 8, num_stages: 1 because of register spilling: 778
[2023-04-18 00:05:47,170] torch._inductor.triton_heuristics: [DEBUG] Skip config XBLOCK: 1, num_warps: 2, num_stages: 1 because of register spilling: 4
```

Pull Request resolved: https://github.com/pytorch/pytorch/pull/99385
Approved by: https://github.com/jansel"
pytorch/pytorch,716ba6851e5f03cb092f7b1b90775eed429685d0,"Make testing._internal.common_utils safe to import (#99659)

In edge cases in CI, SLOW_TESTS_FILE is defined but does not point to an existing file.

Guessing this is due to a test case that opens a subprocses and cwd's but doesn't clean its env.

We shouldn't make importing common_utils fail, so issue a warning and proceed.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/99659
Approved by: https://github.com/ezyang, https://github.com/malfet"
pytorch/pytorch,971df458db49d3b19bb7374731095792916eca2d,"Reland of ""Python binding to set/get CUDA rng state offset"" (#99565)

Why?
* To reduce the latency of hot path in https://github.com/pytorch/pytorch/pull/97377

Concern - I had to add `set_offset` in all instances of `GeneratorImpl`. I don't know if there is a better way.

~~~~
import torch
torch.cuda.manual_seed(123)
print(torch.cuda.get_rng_state())
torch.cuda.set_rng_state_offset(40)
print(torch.cuda.get_rng_state())

tensor([123,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,
          0,   0], dtype=torch.uint8)
tensor([123,   0,   0,   0,   0,   0,   0,   0,  40,   0,   0,   0,   0,   0,
          0,   0], dtype=torch.uint8)
~~~~

Reland of https://github.com/pytorch/pytorch/pull/98965

(cherry picked from commit 8214fe07e8a200e0fe9ca4264bb6fca985c4911e)

Pull Request resolved: https://github.com/pytorch/pytorch/pull/99565
Approved by: https://github.com/anijain2305"
pytorch/pytorch,3af467eff49e4cf34896165d687c3e7b9c383a4c,"inductor: support sqrt for dynamic shape (#99514)

When running TIMM ```convit_base``` dynamic shape case, there is always has AssertionError, see https://github.com/pytorch/pytorch/issues/97877.

A simple reproduce code is:
```
import torch
import torch._dynamo
import torch._dynamo.config as config

config.dynamic_shapes=True
torch._dynamo.config.assume_static_by_default=False
class Model(torch.nn.Module):
    def __init__(self):
        super(Model, self).__init__()

    def forward(self, x):
        B, N, C = x.shape
        return self.get_rel_indices(N)

    def get_rel_indices(self, num_patches: int) -> torch.Tensor:
        img_size = int(num_patches ** .5)
        #rel_indices = torch.zeros(1, num_patches, num_patches, 3)
        ind = torch.arange(img_size)
        return ind

model = Model().eval()
opt_model = torch._dynamo.optimize('inductor')(model)

x = torch.randn(8, 8, 8)
ref = model(x)
with torch.no_grad():
    for i in range(3):
        out = opt_model(x)

```

After this code, the generated code will be like this:
```

kernel_cpp_0 = async_compile.cpp('''
#include ""/tmp/torchinductor_xiaobing/x5/cx5442c6dcuxsrrlnqi476yzjlgc6g53ukppuaettiyp6dszhmr4.h""
extern ""C"" void kernel(long* out_ptr0,
                       const long ks0)
{
    {
        #pragma GCC ivdep
        for(long i0=static_cast<long>(0L); i0<static_cast<long>(std::floor(std::sqrt(ks0))); i0+=static_cast<long>(1L))
        {
            auto tmp0 = static_cast<long>(i0);
            out_ptr0[static_cast<long>(i0)] = tmp0;
        }
    }
}
''')
```

Pull Request resolved: https://github.com/pytorch/pytorch/pull/99514
Approved by: https://github.com/jansel, https://github.com/jgong5"
pytorch/pytorch,6026caed1e83f502e56c6c660a874f3f5f547b07,"Make HAS_CPU boolean lazy, speed up import time (#99537)

Signed-off-by: Edward Z. Yang <ezyang@meta.com>
Pull Request resolved: https://github.com/pytorch/pytorch/pull/99537
Approved by: https://github.com/bertmaher, https://github.com/albanD"
pytorch/pytorch,db456ab83da6a505dcebc128903d5ee4fc2d5712,"[c10d] Faster coalescing (#98793)

### Description
The PR aims at reducing CPU overhead of context manager style coalescing.

By ""context manager style coalescing"", we mean:
Sync style:
```
with _coalescing_manager():
     for i in range(num_coll):
         dist.all_reduce(tensors[i])
```
Async style:
```
with _coalescing_manager(async_ops=True) as cm:
     for i in range(num_coll):
         dist.all_reduce(tensors[i])
cm.wait()
```
In previous implementation, each collective in the `num_coll` loop actually calls into the C++ backend, accumulating pybind overhead.

In the new implementation, we capture the collectives at Python level, and only fire towards C++ at the exit of the coalescing manager.

### Tests
In current PR, the ""fast path"" only applies to all-reduce.
- Flattened 512M: 16.38 ms, including CPU time 131.21 us
- Old _coalescing_manager 64 x 8M: 22.19 ms, including CPU time 2865 us
- New _coalescing_manager 64 x 8M: 16.93 ms, including CPU time 635 us

Hence a 4x reduction in CPU overhead (dependent on `num_coll`).

Cc @mrshenli @kumpera @wanchaol @fegin
Pull Request resolved: https://github.com/pytorch/pytorch/pull/98793
Approved by: https://github.com/kumpera"
pytorch/pytorch,e21f648cde785941db6678063cd9e3107862f107,"improve mkldnn matmul performance when one input is contiguous tensor but the strides is not default contiguous strides (#99511)

giving the following case:
```
import torch

a= torch.empty_strided([64, 1, 33], [33, 3, 1], dtype=torch.bfloat16).fill_(1)
b = torch.randn(64, 33, 256).to(dtype = torch.bfloat16)
y = torch.ops.aten.bmm(a, b)
```
```a``` is a contiguous tensor, but the strides are not defaulted contiguous strides ([33, 33, 1]), onednn matmul always running a non-optimized path:
```
onednn_verbose,exec,cpu,matmul,gemm:jit,undef,src_bf16::blocked:abc:f0 wei_bf16::blocked:abc:f0 dst_bf16::blocked:abc:f0,attr-scratchpad:user ,,64x1x33:64x33x256:64x1x256,7.28711
```
This PR will convert the inputs' stride to deafult contiguous stride before calling onednn to running an optimization path:
```
onednn_verbose,exec,cpu,matmul,brg:avx512_core_amx_bf16,undef,src_bf16::blocked:abc:f0 wei_bf16::blocked:abc:f0 dst_bf16::blocked:abc:f0,attr-scratchpad:user ,,64x1x33:64x33x256:64x1x256,3.06396
```

Pull Request resolved: https://github.com/pytorch/pytorch/pull/99511
Approved by: https://github.com/mingfeima, https://github.com/jgong5"
pytorch/pytorch,c67c16bcd2007cce35440a0c95b163ede2b73b61,"Switch calling convention back to real tensors (#99320)

Months ago, in order to get dynamic shapes working through to Dynamo backends, we changed the calling convention to pass fake tensors rather than real tensors as example inputs to backends. The motivation at the time was, well, backends shouldn't really be peeking at the real tensors when they are doing compilation, and so it would make more sense to hide the real tensors from backends. But there were a bunch of problems:

* This interacted poorly with our accuracy minifier design: accuracy minifier needs access to the real inputs in order to run the model and figure out what happens!
* The TensorRT backend required real inputs and we never figured out how to fix it.
* In practice, all the backends needed to detect if they were passed real tensors, and fakeify them anyway (certainly AOTAutograd does this)
* Parameters and inputs are treated non-uniformly: parameters had to be passed as real tensors, because CUDA graphs requires knowing what the actual tensors are

Furthermore, there were some more problems discovered after the fact:

* Backends may want to optimize on aspects of tensors which you cannot tell without having real tensors; e.g., alignment of the data pointer

So, this PR decides that changing the calling convention was a bad idea, and switches back to passing real tensors. There is a problem though: AOTAutograd will perform fakeification, which means that in practice backends are still going to end up with fake tensors in the end anyway. I want to change this, but this will require some work with bdhirsh's upcoming AOTAutograd export refactor.

Signed-off-by: Edward Z. Yang <ezyang@meta.com>

Pull Request resolved: https://github.com/pytorch/pytorch/pull/99320
Approved by: https://github.com/voznesenskym"
pytorch/pytorch,41d79695907cd4105b8e7167cf8a57ba48e1f079,"[SPMD] Upstream iter_move_grads_and_optimizers (#98785)

This PR upstreams `iter_move_grads_and_optimizer` which delay some of the gradients and the corresponding optimizer to the next iteration. D44512863(credit to @lessw2020 ) is the internal implementation, which is only good for the old _SPMD expansion.  This PR changes the implmentation to use the new APIs.

Differential Revision: [D44836486](https://our.internmc.facebook.com/intern/diff/D44836486/)
Pull Request resolved: https://github.com/pytorch/pytorch/pull/98785
Approved by: https://github.com/mrshenli"
pytorch/pytorch,48463f687a77744a66ab521984f26483923a82ed,"refactor macro with AMP (#99285)

Fixes #ISSUE_NUMBER
as the tiltle, optimize the macro with AMP and put the macro in `.hpp` file, so that we can use it for custom device.  @bdhirsh  @albanD
as we talked at this discuss, optimize the macros so that we can add a new macro for other devide, and move these macros to `.hpp` so that we can include these macros with custom device to configure the ops.
https://dev-discuss.pytorch.org/t/improve-the-extension-with-privateuse1-for-custom-device/1196/7

Pull Request resolved: https://github.com/pytorch/pytorch/pull/99285
Approved by: https://github.com/albanD, https://github.com/bdhirsh"
pytorch/pytorch,52ecc3274b1c16fcca3a3d89bd261dbc6513d6ed,"[inductor] coordinate descent tuning upon max-autotune (#97203)

Command to run max autotune baseline:
```
TORCHINDUCTOR_MAX_AUTOTUNE=1 time python benchmarks/dynamo/torchbench.py --backend inductor --amp --performance --only ${MODEL_NAME} --training --batch-size-file $(realpath benchmarks/dynamo/torchbench_models_list.txt)
```

Command to do coordinate descent autotuning:
```
TORCHINDUCTOR_COORDINATE_DESCENT_TUNING=1 TORCHINDUCTOR_CACHE_DIR=/tmp/torchinductor_shunting_coordesc TORCHINDUCTOR_PERSISTENT_REDUCTIONS=0 TORCHINDUCTOR_MAX_AUTOTUNE=1 time python benchmarks/dynamo/torchbench.py --backend inductor --amp --performance --only ${MODEL_NAME} --training --batch-size-file $(realpath benchmarks/dynamo/torchbench_models_list.txt)
```

Explanation of the envvars show up on the command:
```
- TORCHINDUCTOR_COORDINATE_DESCENT_TUNING=1 : enable coordinate descent tuning
- TORCHINDUCTOR_PERSISTENT_REDUCTIONS=0 : disable persistent reduction. Need do this so we can tune RBLOCK for reductions
- TORCHINDUCTOR_MAX_AUTOTUNE=1: enable max autotune
- TORCHINDUCTOR_CACHE_DIR=/tmp/torchinductor_shunting_coordesc : use a separate cache dir for coordinate descent tuning. Optional.
```

Here are my experiments results for around 40 torchbench models: https://docs.google.com/spreadsheets/d/1G7i2whIf8Yu-HhN_WovNxwcE-iFDSAw4x3NK4uL4XhI/edit#gid=0

Some highlights
- We improve 2.2% further upon max-autotune on average (geomean)
- timm_resnest benefits most from coordinate descent tuning. There is 1.07x speedup
- We have descent speedup on transformer models
  - BERT_pytorch:  1.056x
  - timm_vision_transformer: 1.04x
  - hf_Bert: 1.030x
- For resnet models, it looks like we have less gain as model get larger. My guess is larger model spend more time on mm/conv, so our tuning for pointwise/reduction helps less
  - resnet18: 1.021x
  - resnet50: 1.014x
  - resnet152: 1.005x

This kind of coordinate descent autotuning can give us 'upper bound' of the gain we can get for tuning configs for pointwise/reduction. On the other hand, by spot checking, we roughly double the compilation time compared to max-autotune. Next steps can be
- we disable persistent reduction in coordinate descent autotune (it's still enabled in baseline) so we can tune RBLOCK for reduction. We can also try to use autotune to pick persistent reduction or not.
- pick good config without benchmarking (e.g. Natalia mentioned checking register spill)
- try the idea on matmul so we know what's the potential there.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/97203
Approved by: https://github.com/ngimel"
pytorch/pytorch,840431fa59a7125eea22a2f1aa278c7faff91205,"Fix test/test_proxy_tensor (#99415)

test_proxy_tensor fails when run by itself (python test/test_proxy_tensor.py -v),
but not when all of the tests are run together.

The cause is that torch._dynamo isn't imported in
torch/fx/experimenta/proxy_tensor.py but it is using functions from the
torch._dynamo package.

The fix in this PR is to add the import statements. In the future we can
consider always importing torch._dynamo on `import torch` or moving the
import to the top of the file, but there are some serious circular
dependencies to be worked out.

NB: an import in the middle of the file makes the function a bit slow
the first time the import happens but all subsequent calls are fast.

Test Plan:
- python test/test_proxy_tensor.py
Pull Request resolved: https://github.com/pytorch/pytorch/pull/99415
Approved by: https://github.com/soulitzer"
pytorch/pytorch,60c8a75a7efa6c62622b6c1e2533b73d3042a819,"[EASY] Turn on slow path assertions but only on first run (#98945)

We should at least run the assertions on the first run, because it helps find workspace allocations like in cublas.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/98945
Approved by: https://github.com/ezyang, https://github.com/jansel"
pytorch/pytorch,472f46635ed0188b81710e824212172ebdd390fc,"Cache output tensors on execution (#98944)

Caches output tensors for the common case when the output Tensor storage is unaliased for all graph outputs in all paths. For these persisted tensors we adjust the liveness tracking by also checking that the output tensor does not have an additional python reference.

I limit cached output tensors to be unaliased. If a descendent node discovers it has an alias of a prior output, then the aliased output will no longer be persisted in the ancestor.

The large majority of tensors are unaliased, and preserving aliased output tensors would add significant additional complexity with marginal gains. For instance, when do checkpointing and re-recordings, we need to remove the persisted tensors otherwise it would prevent memory from being reclaimed. If a single persisted tensor was present in multiple paths then that would create an inter-path dependence which adds complexity. Additionally, each further caching of the output would affect the reference count of the other caches, and that reference count would also need to be adjusted depending on if a node was checkpointed.

Still need to do a complete a run but for the models I tried makes the performance extremely close between trees and non trees impl.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/98944
Approved by: https://github.com/jansel, https://github.com/ngimel"
pytorch/pytorch,46b937719065d35c6118fa5a5152268d30e6e0b3,"[CI] Collect inductor max-autotune performance every Sunday (#99387)

Pull Request resolved: https://github.com/pytorch/pytorch/pull/99387
Approved by: https://github.com/malfet, https://github.com/huydhn"
pytorch/pytorch,8214fe07e8a200e0fe9ca4264bb6fca985c4911e,"Python binding to set/get CUDA rng state offset (#98965)

Why?
* To reduce the latency of hot path in https://github.com/pytorch/pytorch/pull/97377

Concern - I had to add `set_offset` in all instances of `GeneratorImpl`. I don't know if there is a better way.

~~~~
import torch
torch.cuda.manual_seed(123)
print(torch.cuda.get_rng_state())
torch.cuda.set_rng_state_offset(40)
print(torch.cuda.get_rng_state())

tensor([123,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,
          0,   0], dtype=torch.uint8)
tensor([123,   0,   0,   0,   0,   0,   0,   0,  40,   0,   0,   0,   0,   0,
          0,   0], dtype=torch.uint8)
~~~

Pull Request resolved: https://github.com/pytorch/pytorch/pull/98965
Approved by: https://github.com/kulinseth, https://github.com/ezyang"
pytorch/pytorch,5b692fd819f1428fc070c3ec3a0cde5d4b83dd03,"Fix bug in check required output size in _as_strided_scatter_meta (#98483)

Original Issue from #92670

pytest ./generated/test_XuyangBai_PointDSC.py -k test_004

==> RuntimeError: as_strided_scatter: sizes [4], strides [85], storage offset 256 and itemsize 4 requiring a storage size of 2048 are out of bounds for storage of size 1024

Repro:

```
class Model(nn.Module):
    def __init__(self):
        super(Model, self).__init__()

    def forward(self, x):
        x[1].fill_diagonal_(0)   # this check size failed

device = torch.device(""cpu"")
model = Model()
model.to(device)

torch._dynamo.reset()
compiled_model = torch._dynamo.optimize(""inductor"")(model)

arg = [torch.rand([4, 1, 1])]
compiled_model(*arg)

```
The error was raised at the checking required size in as_strided_scatter.

https://github.com/pytorch/pytorch/blob/master/torch/_prims/__init__.py#L1818

In the case of input is a tensor with storage offset(a view), when compute input's storage length, should also take input's base tensor's size/stride/offset into account instead of compare it with number of element of input.

This diff fix the bug and add test.

Fixes #ISSUE_NUMBER

Pull Request resolved: https://github.com/pytorch/pytorch/pull/98483
Approved by: https://github.com/ngimel"
pytorch/pytorch,780922c24ec931000cb6a67eeebd2b2288eeb7df,"Switch calling convention back to real tensors (#99320)

Months ago, in order to get dynamic shapes working through to Dynamo backends, we changed the calling convention to pass fake tensors rather than real tensors as example inputs to backends. The motivation at the time was, well, backends shouldn't really be peeking at the real tensors when they are doing compilation, and so it would make more sense to hide the real tensors from backends. But there were a bunch of problems:

* This interacted poorly with our accuracy minifier design: accuracy minifier needs access to the real inputs in order to run the model and figure out what happens!
* The TensorRT backend required real inputs and we never figured out how to fix it.
* In practice, all the backends needed to detect if they were passed real tensors, and fakeify them anyway (certainly AOTAutograd does this)
* Parameters and inputs are treated non-uniformly: parameters had to be passed as real tensors, because CUDA graphs requires knowing what the actual tensors are

Furthermore, there were some more problems discovered after the fact:

* Backends may want to optimize on aspects of tensors which you cannot tell without having real tensors; e.g., alignment of the data pointer

So, this PR decides that changing the calling convention was a bad idea, and switches back to passing real tensors. There is a problem though: AOTAutograd will perform fakeification, which means that in practice backends are still going to end up with fake tensors in the end anyway. I want to change this, but this will require some work with bdhirsh's upcoming AOTAutograd export refactor.

Signed-off-by: Edward Z. Yang <ezyang@meta.com>

Pull Request resolved: https://github.com/pytorch/pytorch/pull/99320
Approved by: https://github.com/voznesenskym"
pytorch/pytorch,436edc5ac3de4c4e677ed136473bafe72002cc93,"[ONNX] Retire 'DynamoOptimizeExporter' (#99202)

<!--
copilot:all
-->
### <samp>🤖 Generated by Copilot at f2ccd03</samp>

### Summary
🗑️📝🛠️

<!--
1.  🗑️ - This emoji represents the removal of unused or unnecessary code, such as the class `DynamoOptimizeExporter` and some imports and decorators.
2.  📝 - This emoji represents the improvement of code readability and consistency, such as replacing `skip_fx_exporters` with `xfail` and using more descriptive names for the FX exporters.
3.  🛠️ - This emoji represents the simplification and refactoring of the code, such as removing some FX exporters and reducing the number of arguments and conditions in the tests.
-->
Removed unused code and simplified test logic for FX to ONNX conversion. This involved removing `skip_fx_exporters` and `DynamoOptimizeExporter`, and using `xfail` instead of `skip_fx_exporters` in `pytorch_test_common.py` and `test_fx_to_onnx_with_onnxruntime.py`.

> _Some FX exporters were not in use_
> _So they were removed without excuse_
> _The tests were updated_
> _With `xfail` annotated_
> _To make the ONNX logic more smooth_

### Walkthrough
*  Remove unused imports of `Mapping`, `Type`, and `exporter` from `test/onnx/pytorch_test_common.py` ([link](https://github.com/pytorch/pytorch/pull/99202/files?diff=unified&w=0#diff-26ce853445bf331686abb33393ee166726923ce36aa2a8de98ac7a2e3bc5a6d8L9-R9), [link](https://github.com/pytorch/pytorch/pull/99202/files?diff=unified&w=0#diff-26ce853445bf331686abb33393ee166726923ce36aa2a8de98ac7a2e3bc5a6d8L16-R16))
*  Replace custom `skip_fx_exporters` function with standard `xfail` decorator in `test/onnx/pytorch_test_common.py` and `test/onnx/test_fx_to_onnx_with_onnxruntime.py` to simplify test skipping logic and mark tests as expected to fail ([link](https://github.com/pytorch/pytorch/pull/99202/files?diff=unified&w=0#diff-26ce853445bf331686abb33393ee166726923ce36aa2a8de98ac7a2e3bc5a6d8L209-R220), [link](https://github.com/pytorch/pytorch/pull/99202/files?diff=unified&w=0#diff-c8fa56eefd7f98fb4f9739d57df57f02ede77e28528133736010a6d06651ebcbL319-R288), [link](https://github.com/pytorch/pytorch/pull/99202/files?diff=unified&w=0#diff-c8fa56eefd7f98fb4f9739d57df57f02ede77e28528133736010a6d06651ebcbL375-R343), [link](https://github.com/pytorch/pytorch/pull/99202/files?diff=unified&w=0#diff-c8fa56eefd7f98fb4f9739d57df57f02ede77e28528133736010a6d06651ebcbL619-R563), [link](https://github.com/pytorch/pytorch/pull/99202/files?diff=unified&w=0#diff-c8fa56eefd7f98fb4f9739d57df57f02ede77e28528133736010a6d06651ebcbL721-R656), [link](https://github.com/pytorch/pytorch/pull/99202/files?diff=unified&w=0#diff-c8fa56eefd7f98fb4f9739d57df57f02ede77e28528133736010a6d06651ebcbL788-R718))
*  Remove unused `DynamoOptimizeExporter` class from `torch/onnx/_internal/fx/dynamo_exporter.py` and remove references to it in `test/onnx/test_fx_to_onnx_with_onnxruntime.py` to simplify FX exporter logic and remove unused code ([link](https://github.com/pytorch/pytorch/pull/99202/files?diff=unified&w=0#diff-3ecf10bc5f6eb95a19441118cb947bd44766dc5eb9b26346f922759bb0f8c9f2L16-L85), [link](https://github.com/pytorch/pytorch/pull/99202/files?diff=unified&w=0#diff-c8fa56eefd7f98fb4f9739d57df57f02ede77e28528133736010a6d06651ebcbL37-R37), [link](https://github.com/pytorch/pytorch/pull/99202/files?diff=unified&w=0#diff-c8fa56eefd7f98fb4f9739d57df57f02ede77e28528133736010a6d06651ebcbL411-L415), [link](https://github.com/pytorch/pytorch/pull/99202/files?diff=unified&w=0#diff-c8fa56eefd7f98fb4f9739d57df57f02ede77e28528133736010a6d06651ebcbL452-L454))
*  Remove unused variables and parameters related to different FX exporters in `test/onnx/test_fx_to_onnx_with_onnxruntime.py` and use `torch.onnx.dynamo_export` directly to simplify code ([link](https://github.com/pytorch/pytorch/pull/99202/files?diff=unified&w=0#diff-c8fa56eefd7f98fb4f9739d57df57f02ede77e28528133736010a6d06651ebcbL50-R47), [link](https://github.com/pytorch/pytorch/pull/99202/files?diff=unified&w=0#diff-c8fa56eefd7f98fb4f9739d57df57f02ede77e28528133736010a6d06651ebcbL191-R188), [link](https://github.com/pytorch/pytorch/pull/99202/files?diff=unified&w=0#diff-c8fa56eefd7f98fb4f9739d57df57f02ede77e28528133736010a6d06651ebcbL245-R224), [link](https://github.com/pytorch/pytorch/pull/99202/files?diff=unified&w=0#diff-c8fa56eefd7f98fb4f9739d57df57f02ede77e28528133736010a6d06651ebcbL265-R237), [link](https://github.com/pytorch/pytorch/pull/99202/files?diff=unified&w=0#diff-c8fa56eefd7f98fb4f9739d57df57f02ede77e28528133736010a6d06651ebcbL279), [link](https://github.com/pytorch/pytorch/pull/99202/files?diff=unified&w=0#diff-c8fa56eefd7f98fb4f9739d57df57f02ede77e28528133736010a6d06651ebcbL296))
*  Replace `skip` decorators with `xfail` decorators in `test/onnx/test_fx_to_onnx_with_onnxruntime.py` to mark tests as expected to fail instead of skipping them unconditionally ([link](https://github.com/pytorch/pytorch/pull/99202/files?diff=unified&w=0#diff-c8fa56eefd7f98fb4f9739d57df57f02ede77e28528133736010a6d06651ebcbL524-R471), [link](https://github.com/pytorch/pytorch/pull/99202/files?diff=unified&w=0#diff-c8fa56eefd7f98fb4f9739d57df57f02ede77e28528133736010a6d06651ebcbL665-R600), [link](https://github.com/pytorch/pytorch/pull/99202/files?diff=unified&w=0#diff-c8fa56eefd7f98fb4f9739d57df57f02ede77e28528133736010a6d06651ebcbL748-R675), [link](https://github.com/pytorch/pytorch/pull/99202/files?diff=unified&w=0#diff-c8fa56eefd7f98fb4f9739d57df57f02ede77e28528133736010a6d06651ebcbL767-R696))
*  Replace `skip_fx_exporters` decorator with `skip_dynamic_fx_test` decorator in `test/onnx/test_fx_to_onnx_with_onnxruntime.py` to skip tests only for dynamic shapes instead of a specific FX exporter ([link](https://github.com/pytorch/pytorch/pull/99202/files?diff=unified&w=0#diff-c8fa56eefd7f98fb4f9739d57df57f02ede77e28528133736010a6d06651ebcbL591-R541), [link](https://github.com/pytorch/pytorch/pull/99202/files?diff=unified&w=0#diff-c8fa56eefd7f98fb4f9739d57df57f02ede77e28528133736010a6d06651ebcbL831-R761))

Pull Request resolved: https://github.com/pytorch/pytorch/pull/99202
Approved by: https://github.com/abock"
pytorch/pytorch,01fdcbdcc952e0f60167c6a12100a404d7c7bd29,"[FSDP][optim_state_dict][Easy] Temporarily disable rank0_only=True for use_orig_paramscaseEas (#99354)

Summary: We have not made use_orig_params=True support rank0_only optimizer_state_dict.

Test Plan: CI

Reviewed By: wz337

Differential Revision: D45054041

Pull Request resolved: https://github.com/pytorch/pytorch/pull/99354
Approved by: https://github.com/wz337"
pytorch/pytorch,bdaf32261f71e9d3e1033b223e3a094a708b9e69,"[FSDP] Ensure that customized non tensor optimizer state can be saved (#99214)

The current logic does not actually handle all different non-tensor optimizer states correctly. This PR fixes the issue and adds a test.

This PR will solve https://github.com/pytorch/pytorch/issues/99079

Differential Revision: [D45021331](https://our.internmc.facebook.com/intern/diff/D45021331/)
Pull Request resolved: https://github.com/pytorch/pytorch/pull/99214
Approved by: https://github.com/awgu, https://github.com/awaelchli"
pytorch/pytorch,dede0bb0659dfa595fea67faadd4a8dcc30350d6,"[NCCL] Use OptionalCUDAGuard in ProcessGroupNCCL::WorkNCCL::synchronizeInternal (#98895)

Using `CUDAGuard` does redundant `set_device` in the following loop:
```C++
{
    for (auto& device : devices_) {
      at::cuda::CUDAGuard gpuGuard(device); // set device
      // ...
      // ~gpuGuard() sets original device
    }
    // ...
}
```
It would be more efficient to use `OptionalCUDAGuard` as follows:
```C++
{
    at::cuda::OptionalCUDAGuard gpuGuard;
    for (auto& device : devices_) {
      gpuGuard.set_index(device.index()); // set device
      // ...
    }
    // ...
    // ~gpuGuard() sets original device
}
```

Pull Request resolved: https://github.com/pytorch/pytorch/pull/98895
Approved by: https://github.com/mrshenli"
pytorch/pytorch,148d49260aa29ba6d4c8354a019f64a27503d5b8,"[SPMD] Implement split_fused_optimizer to split one fused_optimizer node to two (#98784)

Several optimization passes requires the ability to split the fused_optimizer.  This PR adds the API to support the use cases.

Differential Revision: [D44806450](https://our.internmc.facebook.com/intern/diff/D44806450/)
Pull Request resolved: https://github.com/pytorch/pytorch/pull/98784
Approved by: https://github.com/mrshenli"
pytorch/pytorch,31f311a816c026bbfca622d6121d6a7fab44260d,"[PT2E][Quantization] Refactor Quantizer and QNNPACKQuantizer (#99063)

This diff renames quantization spec/config and operator config. It moves these
datastructures to base quantizer.
Base quantizer API now has get_supported_operators that returns list of
patterns that a quantizer quantizes.
There are two choices being debated for how to convey to user what a particular
quantizer will quantize.

1. Modules. We just convey what nn.Modules will be quantized. Of course that
does not mean that equivalent functional variants wont be quantized, however
for simplifity we just use nn.Module. If certain ops are quatnzied in fused
manner then that will considered internal details. Pros and cons of this
approach
pros:
  - Simple. Only nn Modules are listed.
  - User does not have to see fusion patterns.
Cons:
  - confusing perhaps because it is not clear if supported = nn.Conv2d also
    means that the quantizer supported functional.conv2d
  - Hiding fusion pattern means user has no say in not fusing. Meaning if
    conv2d + relu is fused and user configures to quantize only conv, quantizer
    will also quantize the following relu as if conv2d + relu are fused.

2. Patterns. Be explicit about what is supported and enumerate all possible
compbinations.
Pros:
  - it is very clear what quantizer will do. no surprises.
Cons:
  - It is not simple to parse.
  - It can be argued taht fusion is internal detail of the quantizer. So some
    quantizer implementation may chose to expose fusion patterns, while others
    may not and may not even provide any configurability.

One option is to move set_supported_operators/modules out of base quantizer and
let each quantizer define its own way of communicating what is supported. Issue
with this is that when we want to ""Compose"" multiple quantizers there is no way
for user to define the order of composition if user does not know what a
quantizer supports. For exampl quantizer A may quantizer conv + relu while B
only conv, but B's implementation is fast. In that case you may compose (B, A)
such B quantizes conv and A quantizes relu. Not knowning what A
and B support, makes such composition harder

Differential Revision: [D44895547](https://our.internmc.facebook.com/intern/diff/D44895547/)

**NOTE FOR REVIEWERS**: This PR has internal Meta-specific changes or comments, please review them on [Phabricator](https://our.internmc.facebook.com/intern/diff/D44895547/)!
Pull Request resolved: https://github.com/pytorch/pytorch/pull/99063
Approved by: https://github.com/jerryzh168"
pytorch/pytorch,157c869026bd0aa866e0138d5ed57d09966863fc,"Enable FSDP ``use_orig_params=True`` mixed precision training when some ranks have no (non-zero sized) parameter shards (#99175)

Fixes #99174

## Enable FSDP ``use_orig_params=True`` mixed precision training when some ranks have no (non-zero sized) parameter shards

### The issue

Now that ``use_orig_params=True`` allows non-uniform ``requires_grad`` (:tada: :rocket: thanks @awgu!!!) with [#98221](https://github.com/pytorch/pytorch/pull/98221), there will be circumstances wherein some ranks have no (non-zero sized) local shards of the original parameters (and hence no associated gradients).

### Use Cases
For a simple Transformer case, imagine a user wraps all encoder layers in separate FSDP instances but allows the classifier head to be wrapped in the same FSDP instance as the relatively large embeddings layers. While this is a sub-optimal wrapping strategy for most use-cases, I believe it is expected to be supported (full precision training works in that context).

I originally encountered this issue while extending a package I maintain, leveraging the relaxed ``requires_grad`` contstraint to simplify multi-phase scheduled fine-tuning FSDP configuration, so a [concrete example is there](https://finetuning-scheduler.readthedocs.io/en/latest/advanced/fsdp_scheduled_fine_tuning.html#basic-scheduled-fine-tuning-with-fsdp).

### Reproduction and Remediation
Currently, ``ShardedGradScaler`` does not accommodate these situations, failing to initialize ``optimizer_state[""found_inf_per_device""]`` when ``unscale_`` is called.

In this PR, I extend the existing ``ShardedGradScaler`` tests with an ``use_orig_params=True`` dimension added to the parameterization and test scenarios wherein one rank possesses no (non-zero sized) parameter shards.

The relevant issue can be reproduced with the tests I'm adding in this PR. The current (pre-PR) execution of these tests fail in ``use_orig_params=True`` mode with this error:

```python
./test_fsdp_sharded_grad_scaler.py::TestShardedGradScalerParityWithDDP::test_fsdp_ddp_parity_with_grad_scaler_offload_false_none_mixed_precision_use_orig_params Failed with Error: Process 0 exited with error code 10 and exception:
Traceback (most recent call last):
  File ""/home/speediedan/repos/pytorch/torch/testing/_internal/common_distributed.py"", line 657, in run_test
    getattr(self, test_name)()
  File ""/home/speediedan/repos/pytorch/torch/testing/_internal/common_distributed.py"", line 543, in wrapper
    fn()
  File ""/home/speediedan/repos/pytorch/torch/testing/_internal/common_utils.py"", line 259, in instantiated_test
    test(self, **param_kwargs)
  File ""/home/speediedan/repos/pytorch/torch/testing/_internal/common_distributed.py"", line 174, in wrapper
    return func(*args, **kwargs)
  File ""/home/speediedan/repos/pytorch/test/distributed/fsdp/test_fsdp_sharded_grad_scaler.py"", line 187, in test_fsdp_ddp_parity_with_grad_scaler
    self._test_fsdp_parity(
  File ""/home/speediedan/repos/pytorch/torch/testing/_internal/common_fsdp.py"", line 1152, in _test_fsdp_parity
    fsdp_loss = self._train_for_several_steps(
  File ""/home/speediedan/repos/pytorch/torch/testing/_internal/common_fsdp.py"", line 1016, in _train_for_several_steps
    sharded_grad_scaler.step(optim)
  File ""/home/speediedan/repos/pytorch/torch/distributed/fsdp/sharded_grad_scaler.py"", line 291, in step
    return super().step(optimizer, *args, **kwargs)
  File ""/home/speediedan/repos/pytorch/torch/cuda/amp/grad_scaler.py"", line 368, in step
    assert len(optimizer_state[""found_inf_per_device""]) > 0, ""No inf checks were recorded for this optimizer.""
AssertionError: No inf checks were recorded for this optimizer.
```

A few implementation notes/considerations and questions:

1. Rather than just initialize  ``per_device_found_inf``, one could disable the grad scalar altogether for relevant ranks, altering ``unscale_`` to reduce with a subgroup or some rank mask construct to avoid the ``all_reduce`` s in ``distributed/fsdp/sharded_grad_scaler.py:unscale_()`` from hanging. Given that users may subsequently add parameter groups to an optimizer that would require re-enabling the scaler and the complexity associated with maintaining a separate mask construct or process subgroup, I thought this implementation was cleaner.
2. I extended ``_train_for_several_steps`` and ``_test_fsdp_parity`` in ``/torch/testing/_internal/common_fsdp.py`` with the ability to configure ``sharded_grad_scaler_kwargs`` for future testing flexibility.
3. Should the user be warned that no parameter shards were associated with a given rank? My initial thought is that this should be considered an implementation detail, part of supporting ``use_orig_params`` with heterogeneous ``requires_grad``, and therefore should be transparently handled by PyTorch. Should a DEBUG level message be added? If so, likely further upstream rather than at the scaler step level.
4. Rather than extend the existing ``ShardedGradScaler`` tests with an ``use_orig_params=True`` dimension added to the parameterization, let me know if you prefer that I instead narrow the scope of the new testing to a single additional test, e.g.:
	```python
	# from typing import Optional
	from typing import Optional, List
	# ...
	# use_orig_params = [""enable_use_orig_params"", None]
	use_orig_params: List[Optional[str]] = [None]
	# ...
	configs = list(itertools.product(cpu_offload_config, sharding_strategy_config, mixed_precision, use_orig_params))
	configs.append((CPUOffload(offload_params=False), None, ""enable_mixed_precision"", ""enable_use_orig_params""))
	```
Thanks as always to the PyTorch distributed team for your astonishingly impressive and valuable contributions to the open-source ML engineering community!

Pull Request resolved: https://github.com/pytorch/pytorch/pull/99175
Approved by: https://github.com/awgu"
pytorch/pytorch,3c4622c0ec3528449f834c4ba1fe44c662bc406e,"Patch failing slow-test logic for inductor-dynamic (#99182)

Fixes #98954

But.. I'm not sure what the right fix is
Pull Request resolved: https://github.com/pytorch/pytorch/pull/99182
Approved by: https://github.com/huydhn"
pytorch/pytorch,606ce5b653d9967581d9a02930a150b4a1794421,"[ONNX] Introduce Input/Ouptut adapter; Switch to 'DynamoExporter' (#98421)

Summary
* Introduce input/output adapter. Due to design differences, input/output format
between PyTorch model and exported ONNX model are often not the same. E.g., `None`
inputs are allowed for PyTorch model, but are not supported by ONNX. Nested constructs
of tensors are allowed for PyTorch model, but only flattened tensors are supported by ONNX,
etc. The new input/output adapter is exported with the model. Providing an interface to
automatically convert and validate inputs/outputs format.
* As suggested by #98251,
provide extension for unwrapping user defined python classes for `dynamo.export` based
exporter. Unblock huggingface models.
* Re-wire tests to run through `DynamoExporter` w/ `dynamo_export` api. Kept
`DynamoOptimizeExporter` in the tests for now for coverage of this change.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/98421
Approved by: https://github.com/justinchuby, https://github.com/titaiwangms, https://github.com/thiagocrepaldi"
pytorch/pytorch,bd07f8d2e03d2d53865584d04945c9f36d4be019,"DDP forward support custom stream accelerated copy. (#98723)

At present, DDP forward uses `_get_stream` to get a stream,which is cudaStream.
If the custom module already registered to torch, I can use `getattr` to get it and it's stream. Then, the custom stream is used to copy the tensor.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/98723
Approved by: https://github.com/ezyang"
pytorch/pytorch,1e78a2edccb1e19654310b6d27f692c0c314c093,"Make summarize_perf.py work with perf-compare (#99095)

[perf-compare](https://github.com/pytorch/pytorch/actions/workflows/inductor-perf-compare.yml) has a different structure than that of the nightlies.
For these files, the script now generates:

```
# cuda float32 training performance results
## Geometric mean speedup
            huggingface    timm_models    torchbench
--------  -------------  -------------  ------------
inductor           1.46            1.4          1.17

## Mean compilation time
            huggingface    timm_models    torchbench
--------  -------------  -------------  ------------
inductor          57.85          97.63         60.18

## Peak memory compression ratio
            huggingface    timm_models    torchbench
--------  -------------  -------------  ------------
inductor           1.06           1.01          0.83
```

Pull Request resolved: https://github.com/pytorch/pytorch/pull/99095
Approved by: https://github.com/ezyang"
pytorch/pytorch,35c6547f02739dd74dee8e07b08ec2d39036de56,"Adds 3D attn_mask support to merge_masks() for Multihead Attention fast path (#98991)

Fixes #97409

Adds support for 3D attn_mask by always expanding attn_mask to 4D as per https://github.com/pytorch/pytorch/pull/98375#issuecomment-1499504721

Pull Request resolved: https://github.com/pytorch/pytorch/pull/98991
Approved by: https://github.com/jbschlosser"
pytorch/pytorch,bba20908311d70493a00c9b0ac961273204ba4c5,"Enable fused optimizer for DP (#98270)

Differential Revision: [D42714482](https://our.internmc.facebook.com/intern/diff/D42714482/)

**NOTE FOR REVIEWERS**: This PR has internal Meta-specific changes or comments, please review them on [Phabricator](https://our.internmc.facebook.com/intern/diff/D42714482/)!
Pull Request resolved: https://github.com/pytorch/pytorch/pull/98270
Approved by: https://github.com/awgu"
pytorch/pytorch,bc8cb62bcba2fe5f6b736e7de1c16176bf5ac91d,"torch.compile benchmark utility (#97699)

I've had many exchanges that look like this https://github.com/rasbt/faster-pytorch-blog/pull/2 so this is an attempt to get make this problem easier

Pull Request resolved: https://github.com/pytorch/pytorch/pull/97699
Approved by: https://github.com/ezyang"
pytorch/pytorch,f3080997e57eb9b5d0fce3ed18d00806e1877e87,"[SPMD] Introduce remove_copy_for_optimizer optimization (#98580)

This PR adds the ability to remove unused `copy_` (`len(node.users) == 0`) that generated by tracing the optimizer.

Differential Revision: [D44761556](https://our.internmc.facebook.com/intern/diff/D44761556/)
Pull Request resolved: https://github.com/pytorch/pytorch/pull/98580
Approved by: https://github.com/mrshenli"
pytorch/pytorch,401320690b4e72b8541e6f1614caed1eb4def6a6,"[SPMD] Add optimizer states and steps to the return (#98579)

This will correctly functionalize the optimizer. Otherwise, there are orphand copy_.

Differential Revision: [D44761512](https://our.internmc.facebook.com/intern/diff/D44761512/)
Pull Request resolved: https://github.com/pytorch/pytorch/pull/98579
Approved by: https://github.com/mrshenli"
pytorch/pytorch,07a1378f521555404973a7921af808f680933032,"[SPMD] Introduce schedule_comm_wait (#98578)

`schedule_comm_wait` delays the wait_tensor ops as late as possible. Note that this optimization currently does not reorder the computation ops. For `foreach` based optimizer, we observe that reordering the computation ops is required to achieve a good performance.

Differential Revision: [D44761487](https://our.internmc.facebook.com/intern/diff/D44761487/)
Pull Request resolved: https://github.com/pytorch/pytorch/pull/98578
Approved by: https://github.com/mrshenli"
pytorch/pytorch,e37986d48f0f45cce4404e5830ca3677f7a4560b,"[memory viz] support larger visualizations (#98865)

When there are > 15000 polygons trace_plot starts to get really slow.
So order the allocations and take the smallest allocations beyond the 15000
limit and put them into a single summarized polygon.
A slider allows this limit to be adjusted.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/98865
Approved by: https://github.com/yf225"
pytorch/pytorch,def50d253401540cfdc6c0fffa444d0ee643cc11,"Create a new unstable workflow for periodic jobs (#98858)

And move ROCm distributed job there as it's very flaky in trunk at the moment.  Also move ROCm slow job to `slow` workflow as it should be.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/98858
Approved by: https://github.com/malfet, https://github.com/ZainRizvi"
pytorch/pytorch,97889fa199cb2043d55bc02ce5a11c00b5175b7b,"simplify indexing expression before trying to determine strides (#98783)

This fixes a few failing cases where we fail to compute stride_hint for an indexing expression with ModularIndexing

When can size_hint error out? It shouldn't happen when we are getting regular size hints for expressions where free vars are in ShapeEnv. But this is not the case when we try to recover strides from indexing expressions (which is what stride_hint is for). Suppose you have an indexing expression that looks like
```
289*d0 + ModularIndexing(7399*d1 + d2, 1, 17) + 17*ModularIndexing(7399*d1 + d2, 17, 17) + 46240*ModularIndexing(7399*d1 + d2, 289, 128)
```
and want to understand its stride wrt to variable `d1`. Let's ignore for a moment that stride for ModularIndexing is not well defined, it'll become negative around modulo divisor value, but even without that, the way we usually compute stride is we substitute `0` and `1` for `d1` and compute difference in indexing expression with those substitutions - this is our stride. But for the expression above, the difference would result in an expression that still has free variable `d2` that we don't have a substitution for.
The fix that this PR makes is it expands stride computation to substitute not only `0` and `1` for the variable we are computing a stride for, but also `0` for other variables in the indexing expression (`support_vars`).
Note that computing strides in `stride_hints` is a performance optimization that we use to reorder dimensions or make split decisions for split reduction. If it fails, it's not a hard error - we may incorrectly apply reordering by it won't affect correctness.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/98783
Approved by: https://github.com/ezyang, https://github.com/voznesenskym"
pytorch/pytorch,5a2de506fcbf77b75f7912d914296de7dd3766ab,"[spmd compile api] run gm_transforms before running the first iteration (#98788)

Summary: The non-transformed graph module contains functionalized optimizer which, in a memory constraint environment, needs to be defunctionalized (via fx transformation or lowering to Inductor) before running the first iteration. Otherwise OOM may occur.

Test Plan: Manually tested.

Reviewed By: mrshenli

Differential Revision: D44843942

Pull Request resolved: https://github.com/pytorch/pytorch/pull/98788
Approved by: https://github.com/mrshenli"
pytorch/pytorch,dda95236c942f9b68120ccc52f7eb29dad5d5fc3,"Add fast path in our type checks and argparser (#98764)

Add fastpath for common use cases in our python arg parsing.
This is using the observation that exact type check is a lot fast (pointer comparison) than subtype check (isintance call). So we make sure to do these before any isinstance check.

This can be pretty significant where `a.view((1, 1, 1, 1))` goes from ~1.13us to 800ns.

Full test:

Tested perf locally with cpu freq locked and script pinned to a single core to reduce jitter.
Benchmark results after doing each change in this PR one by one:
```
[albandes@albandes-fedora-K2202N0104138 test]$ # Original
[albandes@albandes-fedora-K2202N0104138 test]$ taskset 0x1 ipython foo.py
No CUDA runtime is found, using CUDA_HOME='/usr/local/cuda'
Running  a.view(1)
827 ns ± 0.945 ns per loop (mean ± std. dev. of 7 runs, 1,000,000 loops each)
Running  a.view((1, 1))
947 ns ± 1.23 ns per loop (mean ± std. dev. of 7 runs, 1,000,000 loops each)
Running  a.view((1, 1, 1))
1.04 µs ± 0.882 ns per loop (mean ± std. dev. of 7 runs, 1,000,000 loops each)
Running  a.view((1, 1, 1, 1))
1.14 µs ± 1.59 ns per loop (mean ± std. dev. of 7 runs, 1,000,000 loops each)
Running  a.squeeze(0)
797 ns ± 0.955 ns per loop (mean ± std. dev. of 7 runs, 1,000,000 loops each)
Running  a.squeeze((0,))
937 ns ± 1.51 ns per loop (mean ± std. dev. of 7 runs, 1,000,000 loops each)
Running  a.squeeze((0, 1))
1.02 µs ± 3.52 ns per loop (mean ± std. dev. of 7 runs, 1,000,000 loops each)
[albandes@albandes-fedora-K2202N0104138 test]$ taskset 0x1 ipython foo.py
No CUDA runtime is found, using CUDA_HOME='/usr/local/cuda'
Running  a.view(1)
823 ns ± 1.76 ns per loop (mean ± std. dev. of 7 runs, 1,000,000 loops each)
Running  a.view((1, 1))
938 ns ± 1.38 ns per loop (mean ± std. dev. of 7 runs, 1,000,000 loops each)
Running  a.view((1, 1, 1))
1.03 µs ± 0.801 ns per loop (mean ± std. dev. of 7 runs, 1,000,000 loops each)
Running  a.view((1, 1, 1, 1))
1.13 µs ± 0.877 ns per loop (mean ± std. dev. of 7 runs, 1,000,000 loops each)
Running  a.squeeze(0)
768 ns ± 2.27 ns per loop (mean ± std. dev. of 7 runs, 1,000,000 loops each)
Running  a.squeeze((0,))
927 ns ± 0.779 ns per loop (mean ± std. dev. of 7 runs, 1,000,000 loops each)
Running  a.squeeze((0, 1))
1.01 µs ± 1.34 ns per loop (mean ± std. dev. of 7 runs, 1,000,000 loops each)

[albandes@albandes-fedora-K2202N0104138 test]$ # checkLong fastpath
[albandes@albandes-fedora-K2202N0104138 test]$ taskset 0x1 ipython foo.py
No CUDA runtime is found, using CUDA_HOME='/usr/local/cuda'
Running  a.view(1)
801 ns ± 0.982 ns per loop (mean ± std. dev. of 7 runs, 1,000,000 loops each)
Running  a.view((1, 1))
900 ns ± 0.593 ns per loop (mean ± std. dev. of 7 runs, 1,000,000 loops each)
Running  a.view((1, 1, 1))
1 µs ± 1.44 ns per loop (mean ± std. dev. of 7 runs, 1,000,000 loops each)
Running  a.view((1, 1, 1, 1))
1.1 µs ± 1.38 ns per loop (mean ± std. dev. of 7 runs, 1,000,000 loops each)
Running  a.squeeze(0)
782 ns ± 0.968 ns per loop (mean ± std. dev. of 7 runs, 1,000,000 loops each)
Running  a.squeeze((0,))
1.11 µs ± 424 ns per loop (mean ± std. dev. of 7 runs, 1,000,000 loops each)
Running  a.squeeze((0, 1))
1.09 µs ± 54.7 ns per loop (mean ± std. dev. of 7 runs, 1,000,000 loops each)
[albandes@albandes-fedora-K2202N0104138 test]$ taskset 0x1 ipython foo.py
No CUDA runtime is found, using CUDA_HOME='/usr/local/cuda'
Running  a.view(1)
817 ns ± 0.65 ns per loop (mean ± std. dev. of 7 runs, 1,000,000 loops each)
Running  a.view((1, 1))
912 ns ± 0.853 ns per loop (mean ± std. dev. of 7 runs, 1,000,000 loops each)
Running  a.view((1, 1, 1))
1.02 µs ± 8.45 ns per loop (mean ± std. dev. of 7 runs, 1,000,000 loops each)
Running  a.view((1, 1, 1, 1))
1.11 µs ± 2.53 ns per loop (mean ± std. dev. of 7 runs, 1,000,000 loops each)
Running  a.squeeze(0)
781 ns ± 0.942 ns per loop (mean ± std. dev. of 7 runs, 1,000,000 loops each)
Running  a.squeeze((0,))
939 ns ± 1.57 ns per loop (mean ± std. dev. of 7 runs, 1,000,000 loops each)
Running  a.squeeze((0, 1))
1.01 µs ± 0.875 ns per loop (mean ± std. dev. of 7 runs, 1,000,000 loops each)

[albandes@albandes-fedora-K2202N0104138 test]$ # Tensor check fastpath
[albandes@albandes-fedora-K2202N0104138 test]$ taskset 0x1 ipython foo.py
No CUDA runtime is found, using CUDA_HOME='/usr/local/cuda'
Running  a.view(1)
806 ns ± 2.8 ns per loop (mean ± std. dev. of 7 runs, 1,000,000 loops each)
Running  a.view((1, 1))
903 ns ± 1.82 ns per loop (mean ± std. dev. of 7 runs, 1,000,000 loops each)
Running  a.view((1, 1, 1))
1 µs ± 1.21 ns per loop (mean ± std. dev. of 7 runs, 1,000,000 loops each)
Running  a.view((1, 1, 1, 1))
1.1 µs ± 1.17 ns per loop (mean ± std. dev. of 7 runs, 1,000,000 loops each)
Running  a.squeeze(0)
770 ns ± 1.66 ns per loop (mean ± std. dev. of 7 runs, 1,000,000 loops each)
Running  a.squeeze((0,))
931 ns ± 3.36 ns per loop (mean ± std. dev. of 7 runs, 1,000,000 loops each)
Running  a.squeeze((0, 1))
1.02 µs ± 0.983 ns per loop (mean ± std. dev. of 7 runs, 1,000,000 loops each)
[albandes@albandes-fedora-K2202N0104138 test]$ taskset 0x1 ipython foo.py
No CUDA runtime is found, using CUDA_HOME='/usr/local/cuda'
Running  a.view(1)
813 ns ± 2.42 ns per loop (mean ± std. dev. of 7 runs, 1,000,000 loops each)
Running  a.view((1, 1))
915 ns ± 0.868 ns per loop (mean ± std. dev. of 7 runs, 1,000,000 loops each)
Running  a.view((1, 1, 1))
1.02 µs ± 1.09 ns per loop (mean ± std. dev. of 7 runs, 1,000,000 loops each)
Running  a.view((1, 1, 1, 1))
1.11 µs ± 1.15 ns per loop (mean ± std. dev. of 7 runs, 1,000,000 loops each)
Running  a.squeeze(0)
785 ns ± 0.807 ns per loop (mean ± std. dev. of 7 runs, 1,000,000 loops each)
Running  a.squeeze((0,))
941 ns ± 1.02 ns per loop (mean ± std. dev. of 7 runs, 1,000,000 loops each)
Running  a.squeeze((0, 1))
1.02 µs ± 0.857 ns per loop (mean ± std. dev. of 7 runs, 1,000,000 loops each)

[albandes@albandes-fedora-K2202N0104138 test]$ # Fast path number in intlist/symintlist
[albandes@albandes-fedora-K2202N0104138 test]$ taskset 0x1 ipython foo.py
No CUDA runtime is found, using CUDA_HOME='/usr/local/cuda'
Running  a.view(1)
728 ns ± 0.503 ns per loop (mean ± std. dev. of 7 runs, 1,000,000 loops each)
Running  a.view((1, 1))
749 ns ± 0.829 ns per loop (mean ± std. dev. of 7 runs, 1,000,000 loops each)
Running  a.view((1, 1, 1))
771 ns ± 0.727 ns per loop (mean ± std. dev. of 7 runs, 1,000,000 loops each)
Running  a.view((1, 1, 1, 1))
800 ns ± 0.962 ns per loop (mean ± std. dev. of 7 runs, 1,000,000 loops each)
Running  a.squeeze(0)
772 ns ± 0.622 ns per loop (mean ± std. dev. of 7 runs, 1,000,000 loops each)
Running  a.squeeze((0,))
883 ns ± 0.567 ns per loop (mean ± std. dev. of 7 runs, 1,000,000 loops each)
Running  a.squeeze((0, 1))
915 ns ± 0.638 ns per loop (mean ± std. dev. of 7 runs, 1,000,000 loops each)
[albandes@albandes-fedora-K2202N0104138 test]$ taskset 0x1 ipython foo.py
No CUDA runtime is found, using CUDA_HOME='/usr/local/cuda'
Running  a.view(1)
735 ns ± 1.27 ns per loop (mean ± std. dev. of 7 runs, 1,000,000 loops each)
Running  a.view((1, 1))
753 ns ± 2.57 ns per loop (mean ± std. dev. of 7 runs, 1,000,000 loops each)
Running  a.view((1, 1, 1))
774 ns ± 1.38 ns per loop (mean ± std. dev. of 7 runs, 1,000,000 loops each)
Running  a.view((1, 1, 1, 1))
801 ns ± 0.835 ns per loop (mean ± std. dev. of 7 runs, 1,000,000 loops each)
Running  a.squeeze(0)
773 ns ± 0.677 ns per loop (mean ± std. dev. of 7 runs, 1,000,000 loops each)
Running  a.squeeze((0,))
873 ns ± 1.1 ns per loop (mean ± std. dev. of 7 runs, 1,000,000 loops each)
Running  a.squeeze((0, 1))
907 ns ± 0.836 ns per loop (mean ± std. dev. of 7 runs, 1,000,000 loops each)
```

<details>
  <summary>Test script</summary>

```python
import torch
from IPython import get_ipython

a = torch.empty(1)
print(""Running "", ""a.view(1)"")
get_ipython().run_line_magic(""timeit"", ""a.view(1)"")
print(""Running "", ""a.view((1, 1))"")
get_ipython().run_line_magic(""timeit"", ""a.view((1, 1))"")
print(""Running "", ""a.view((1, 1, 1))"")
get_ipython().run_line_magic(""timeit"", ""a.view((1, 1, 1))"")
print(""Running "", ""a.view((1, 1, 1, 1))"")
get_ipython().run_line_magic(""timeit"", ""a.view((1, 1, 1, 1))"")

a = torch.empty(1, 1, 1)
print(""Running "", ""a.squeeze(0)"")
get_ipython().run_line_magic(""timeit"", ""a.squeeze(0)"")
print(""Running "", ""a.squeeze((0,))"")
get_ipython().run_line_magic(""timeit"", ""a.squeeze((0,))"")
print(""Running "", ""a.squeeze((0, 1))"")
get_ipython().run_line_magic(""timeit"", ""a.squeeze((0, 1))"")
```

</details>
Pull Request resolved: https://github.com/pytorch/pytorch/pull/98764
Approved by: https://github.com/ngimel"
pytorch/pytorch,c139df407bf09f4a8908a8c1260855d3184e2b7f,"Skip failing test_torchinductor_codegen_dynamic_shapes tests on CPU (#98621)

This test starts to fail in trunk after https://github.com/pytorch/pytorch/pull/97230.  The original PR missed this because these test are marked as slow and is only run periodically.  Is this ok to skip them like `test_upsample_cat_conv_dynamic_shapes`?

Here is an example failure https://github.com/pytorch/pytorch/actions/runs/4638277468/jobs/8208270657. The following tests are all failing with `Failed to find dynamic for loop variable` error like others in the list.  They are:

* `test_conv2d_binary_dynamic_shapes`.  Fixes https://github.com/pytorch/pytorch/issues/98679
* `test_conv2d_unary_dynamic_shapes`.  Fixes https://github.com/pytorch/pytorch/issues/98680
* `test_conv_bn_fuse_dynamic_shapes`.  Fixes https://github.com/pytorch/pytorch/issues/98681
* `test_conv_transpose2d_unary_dynamic_shapes`.  Fixes https://github.com/pytorch/pytorch/issues/98682

Pull Request resolved: https://github.com/pytorch/pytorch/pull/98621
Approved by: https://github.com/malfet"
pytorch/pytorch,3fcc5ff0d670747a267ad0645f4b2b64ce29234d,"Avoid passing buffers to optimizers during spmd rematerialization (#98714)

Pull Request resolved: https://github.com/pytorch/pytorch/pull/98714
Approved by: https://github.com/fegin"
pytorch/pytorch,95621b3c2e8bc0f1d7f88fb0bcb3e3e56a0ff961,"[aot] fix disable amp for runtime wrapper (#97864)

For the current runtime wrapper in aot, `disable_amp` is always set to True. In fact, we would like to avoid disabling autocast if possible because accessing TLS is slow. In this PR, `disable_amp` depends on whether there is any autocast enabled instead of always being True. Many operators would get an improvement of performance (inductor v.s. eager) with this fix.

Example of operators' 0.8 speedup in torchbench (inductor v.s. eager):
<html xmlns:v=""urn:schemas-microsoft-com:vml""
xmlns:o=""urn:schemas-microsoft-com:office:office""
xmlns:x=""urn:schemas-microsoft-com:office:excel""
xmlns=""http://www.w3.org/TR/REC-html40"">

<head>

<meta name=ProgId content=Excel.Sheet>
<meta name=Generator content=""Microsoft Excel 15"">
<link id=Main-File rel=Main-File
href=""file:///C:/Users/xuanliao/AppData/Local/Temp/msohtmlclip1/01/clip.htm"">
<link rel=File-List
href=""file:///C:/Users/xuanliao/AppData/Local/Temp/msohtmlclip1/01/clip_filelist.xml"">

</head>

<body link=""#0563C1"" vlink=""#954F72"">

  | current | new
-- | -- | --
aten.hardsigmoid.default | 0.709372349 | 0.81414306
aten.tanh.default | 0.715227805 | 0.855556349
aten.add.Scalar | 0.682292123 | 0.860371222
aten.sigmoid_backward.default | 0.688039934 | 0.915606579

</body>

</html>

Pull Request resolved: https://github.com/pytorch/pytorch/pull/97864
Approved by: https://github.com/EikanWang, https://github.com/jansel, https://github.com/jgong5, https://github.com/bdhirsh"
pytorch/pytorch,96fb64a1599f3e1679baa9db16f71c8a854577c7,"Turn off cudagraph trees  (#98709)

There were some recent failures on master, and I think it's fair to defer on turning it on till we get a bit of the Tensor construction overhead down because that shows up a lot in the TB benchmarks.

There may ultimately be an unavoidable tradeoff between memory and performance to some extent but we can get the overhead numbers down a bit first.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/98709
Approved by: https://github.com/Chillee"
pytorch/pytorch,584244460b752a839a73acbbf7d9d406f9244233,"use float as accumulate type for reduce Ops: min, max, minmax on CPU (#96079)

Use float32 as acc type for `min`, `max` and `minmax`, in the function ` vec::reduce_all`, float16 inputs will be accumulated in float32.

The performance benefit basically comes from the vectorization of `Half` https://github.com/pytorch/pytorch/pull/96076

Tested on Intel(R) Xeon(R) Gold 6248 CPU @ 2.50GHz

**single socket**
```
(before)
### using OMP_NUM_THREADS=20
### using numactl --physcpubind=0-19 --membind=0
max: size: torch.Size([64, 128, 1024])  2.071 ms

(after)
### using OMP_NUM_THREADS=20
### using numactl --physcpubind=0-19 --membind=0
max: size: torch.Size([64, 128, 1024])  0.071 ms
```

**single core**
```
(before)
### using OMP_NUM_THREADS=1
### using numactl --physcpubind=0 --membind=0
max: size: torch.Size([64, 128, 1024])  33.488 ms

(after)
### using OMP_NUM_THREADS=1
### using numactl --physcpubind=0 --membind=0
max: size: torch.Size([64, 128, 1024])  0.953 ms
```

Pull Request resolved: https://github.com/pytorch/pytorch/pull/96079
Approved by: https://github.com/jgong5, https://github.com/kit1980"
pytorch/pytorch,3925f6edb2cf4f4ffc2e7a4f2972d9be8f207ad1,"add Half to cat fast path on CPU (#96078)

Extend current fast path on `cat` with `Half`: for non-arithmetic Ops, simply do `Vec::load` and `Vec::store`.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/96078
Approved by: https://github.com/jgong5, https://github.com/ezyang"
pytorch/pytorch,2edfcafd4b7db1cbe1a0a79a8625e54bd2527789,"[inductor] remove RBLOCK from persistent reduction kernel's parameter list (#98653)

This PR resolves comments https://github.com/pytorch/pytorch/pull/97203#discussion_r1160491318 . Send a separate PR since it's easier to test and make sure there is no perf impact.

Tests:
1. python test/inductor/test_torchinductor.py
2. run `python benchmarks/dynamo/torchbench.py --backend inductor --amp --performance --dashboard --only hf_Bert --disable-cudagraphs --training` before and after the change to make sure the perf change is neutral.

Now a persistent reduction kernel in hf_Bert looks like:
```
@persistent_reduction(
    size_hints=[4096, 1024],
    reduction_hint=ReductionHint.INNER,
    filename=__file__,
    meta={'signature': {0: '*fp32', 1: '*i64', 2: '*fp16', 3: '*i64', 4: '*fp16', 5: '*i64', 6: '*fp16', 7: '*fp16', 8: '*fp16', 9: '*fp16', 10: 'i32', 11: 'i32'}, 'device': 0, 'constants': {}, 'mutated_arg_names': ['in_out_ptr0'], 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11), equal_to_1=())]}
)
@triton.jit
def triton_(in_out_ptr0, in_ptr0, in_ptr1, in_ptr2, in_ptr3, in_ptr4, in_ptr5, in_ptr6, in_ptr7, out_ptr2, xnumel, rnumel, XBLOCK : tl.constexpr):
    xnumel = 4096
    rnumel = 768
    RBLOCK: tl.constexpr = 1024
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
```

Pull Request resolved: https://github.com/pytorch/pytorch/pull/98653
Approved by: https://github.com/jansel"
pytorch/pytorch,85a90d9181bb16d29360abd246985eaec8ee8123,"Rename assert options, turn off by default (#98616)

Rename the runtime assert checking options to be more clear. Also turn off the slow path checking, since it is slow enough to significantly affect our compilation time speed in dashboard.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/98616
Approved by: https://github.com/davidberard98, https://github.com/Neilblaze"
pytorch/pytorch,48397cddd78b4499832520d3c335eb9ef6b0f563,"[inductor] Fix benchmark_compiled_module codegen with CppWrapperCodeGen (#98608)

The python function `benchmark_compiled_module` ends up using C++ expression printer to print the size for `rand_strided`, so you get a set e.g. `{2, 17}` instead of a
tuple `(2, 17)`. Here is a complete example from master:

```python
def benchmark_compiled_module(times=10, repeat=10):
    from torch._dynamo.testing import rand_strided
    from torch._inductor.utils import print_performance
    arg0_1 = rand_strided({2, 17}, {17, 1}, device='cpu', dtype=torch.float32)
    arg1_1 = rand_strided({2, 17}, {17, 1}, device='cpu', dtype=torch.uint8)
    return print_performance(lambda: call([arg0_1, arg1_1]), times=times, repeat=repeat)
```

Pull Request resolved: https://github.com/pytorch/pytorch/pull/98608
Approved by: https://github.com/ngimel"
pytorch/pytorch,ebd4c165ffa586491dac51c555b8bffd1f844b5a,"Back out ""`GradScaler` recomputes `optimizer_state[""found_inf_per_device""]` before `optimizer.step` (#97415)"" (#98613)

Summary: This change causes multi-GPU job from XI team to hang after 8K steps.

Differential Revision: D44797248

Pull Request resolved: https://github.com/pytorch/pytorch/pull/98613
Approved by: https://github.com/ngimel"
pytorch/pytorch,8a29afe98a685f9bf6635d6bc8d1b73d4582fd6d,"[RFC] Add warning about object-based collectives for GPU tensors to docs. (#97702)

Using GPU tensors in these collectives have caused SEVs, user
confusion, and slowness in the past. These APIs were only designed to
communicate arbitrary python objects, and GPU tensors should either be copied
to CPU first or use the regular collecitves. Add a warning indicating so.

Differential Revision: [D44435849](https://our.internmc.facebook.com/intern/diff/D44435849/)
Pull Request resolved: https://github.com/pytorch/pytorch/pull/97702
Approved by: https://github.com/kumpera"
pytorch/pytorch,eb5da4df8a64d0777d328cf6e06317a2fee75392,"Speed up LossCTC.cu (#97269)

For these two kernels, `grid.x == 1` is enough. `grid.x > 1` leads to repeated computation.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/97269
Approved by: https://github.com/ngimel, https://github.com/malfet"
pytorch/pytorch,fdb9441e7e7c6f8a41ba7994a40df0563c9f1809,"Stop recursion on trivial replacement (#97903)

Pattern replacement behaves incorrectly when the replacement pattern maps inputs to outputs (such a pattern can be used to replace redundant code). However, current code in `torch.fx.subgraph_rewriter._replace_pattern` causes the list of replacement nodes to include the entire graph before that node, resulting in an exponential slowdown due to recursive calls traversing the entire graph multiple times.

The proposed fix is to add a check in `_replace_pattern` to prevent the call to `get_replacement_nodes`:
```python
        for ret_node in copied_returning_nodes:
            if ret_node in match.placeholder_nodes:
                replacement_nodes.append(ret_node)
            else:
                get_replacement_nodes(ret_node)
```

Fixes #97817

Pull Request resolved: https://github.com/pytorch/pytorch/pull/97903
Approved by: https://github.com/angelayi"
pytorch/pytorch,1bcb88089468a6ebc667bd76256c4dd6f58b7ee3,"Reduce includes of CUDACachingAllocator.h (#97072)

On my machine this goes from > 200 to ~80, making rebuilds faster.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/97072
Approved by: https://github.com/wanchaol"
pytorch/pytorch,5af47dbb23c8ecb9e3ba559926e1054f7c353446,"Add slow workflow to upload test stats workflow (#98447)

I wonder if it would be better to write an exclusion list instead
Pull Request resolved: https://github.com/pytorch/pytorch/pull/98447
Approved by: https://github.com/huydhn"
pytorch/pytorch,37dbd5bf76afc90fdd89890d6d3c0a40af679f7b,"[spmd compile API] add a (temporary) mechanism for overriding input tensors' placements (#98391)

Currently, the compile API assumes all input tensors' shard dimension is the first dimension. dtensor expansion doesn't work when there are input tensors whose shard dimension is not the first dimension.

In addtion, respect non-tensor inputs beyond nn.Module and optim.Optimizers.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/98391
Approved by: https://github.com/mrshenli"
pytorch/pytorch,0830808dde798bd6d700131bda8c82b727a9427e,"[spmd expansion] speed up expansion by ~5x (#98389)

According to profiling, the top two expensive operations in spmd expansion are propagate_op_sharding and make_fx (for every dispatcher op node). This PR makes the following changes to speed up spmd expansion:
- We are unneccessarily doing propagate_op_sharding twice for every op. Remove one.
- When no tensor redistribution is required, we only need to update non-tensor args of the node according to op_schema and avoid building a GraphModule just for the node.

On a DDP use cases + foreach Adam, this change speeds up spmd expansion by ~5x (~10 min -> ~2 min).
Pull Request resolved: https://github.com/pytorch/pytorch/pull/98389
Approved by: https://github.com/mrshenli"
pytorch/pytorch,a05d787eb6e1abd98f5b9e5281102d2d43cf9294,"[inductor] Fix slow tests not being run in CI (#97841)

PyTorch slow tests are run in CI with `PYTORCH_TEST_SKIP_FAST=1` which skips any
test not decorated with `@slowTest`. That means tests marked with
`skipIf(not TEST_WITH_SLOW)` are never run.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/97841
Approved by: https://github.com/jansel"
pytorch/pytorch,51a978fe7bbeaaa2f22056543a7fc2eeb17e5fbb,"Set number of threads to be 1 for ARM (#97482) (#98267)

Summary:
In highly multi-threaded environment, using # of threads to be matching hardware_concurrency leads to high contention. x86 path actually ends up using different path (MKL path), which results in using 1 thread for x86 as well.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/98267
Approved by: https://github.com/malfet"
pytorch/pytorch,0eab3ab51ec3e83bd9961b167bfbdbab7fc064e6,"[pt2][inductor] add `triton.__verison__` as cache key, update cache layout (#98010)

Summary:
* change caching to have `system` and `cache` components, where `system` servers as an identifier for that machine's performance. similar to original method of having GPU type and CUDA version be cache keys, and now also includes Triton version. `cache` is similar to the original cache type, but now without GPU name or CUDA version

```
{
    ""system"": {
        ""device"": ""NVIDIA PG509-210"",
        ""version"": {
            ""cuda"": ""11.4.0"",
            ""triton"": ""2.1.0""
        },
        ""hash"": ""e7cfb8786d2e1366b3df564bcb2f957d07545e98bf20c98d33a43b6ee80a91e0""
    },
    ""cache"": {
        ""bias_addmm"": {
            ""[('cuda', 'torch.float32', 2048, 160, 0, 1, 0), ('cuda', 'torch.float32', 2048, 1140, 228148, 1, 206080), ('cuda', 'torch.float32', 1140, 160, 1, 1140, 0)]"": {
                ""bias_addmm-alpha=1-beta=1-c73frtshmeth2spjun3zc4l2q7ck43wl356pnlmsmxgmzbfsz7ef"": 0.03654399886727333,
                ""addmm-alpha=1-beta=1-c4xxd3iocu4yt6z4udrlqnumays7q6mfnfd3qprh4fxgsvyhqdkf"": 0.03564799949526787,
                ""triton_mm-ACC_TYPE='tl.float32'-ALLOW_TF32=True-BLOCK_K=32-BLOCK_M=64-BLOCK_N=64-EVEN_K=False-GROUP_M=8-num_stages=2-num_warps=4-cxgwpjkimm4azwffrfuqniwncnv4h5bxrpo4od4an4bstnh7qrqh"": 0.04927999898791313,
                ""triton_mm-ACC_TYPE='tl.float32'-ALLOW_TF32=True-BLOCK_K=32-BLOCK_M=64-BLOCK_N=128-EVEN_K=False-GROUP_M=8-num_stages=3-num_warps=4-cqlirysniekkuuvc4ue33dr4gpfzsb5e4bexarrsnsyei4slxvcz"": 0.03651199862360954,
                ""triton_mm-ACC_TYPE='tl.float32'-ALLOW_TF32=True-BLOCK_K=32-BLOCK_M=128-BLOCK_N=64-EVEN_K=False-GROUP_M=8-num_stages=3-num_warps=4-cww5uss3k4d3ei2c4lx63pudyzxdwl3ieibhxcrue4zg424eqrnu"": 0.03580800071358681,
                ""triton_mm-ACC_TYPE='tl.float32'-ALLOW_TF32=True-BLOCK_K=32-BLOCK_M=64-BLOCK_N=128-EVEN_K=False-GROUP_M=8-num_stages=4-num_warps=8-cqcla5edxdm3n6rrkmjehexsudravx6lpphfo5zazldpo3rzpqc4"": 0.03558399900794029,
                ""triton_mm-ACC_TYPE='tl.float32'-ALLOW_TF32=True-BLOCK_K=32-BLOCK_M=128-BLOCK_N=64-EVEN_K=False-GROUP_M=8-num_stages=4-num_warps=8-c7gdf2snt4bjlnuzdy3px4pyq3lbsdh4jp6jaie7lq6mdxccy6nl"": 0.03455999866127968,
                ""triton_mm-ACC_TYPE='tl.float32'-ALLOW_TF32=True-BLOCK_K=32-BLOCK_M=64-BLOCK_N=32-EVEN_K=False-GROUP_M=8-num_stages=5-num_warps=8-cjhcy4scxgy4lxbhjiinvxl3bbrqya63jilcckx2ltsg3mpzxyqr"": 0.036288000643253326,
                ""triton_mm-ACC_TYPE='tl.float32'-ALLOW_TF32=True-BLOCK_K=32-BLOCK_M=32-BLOCK_N=64-EVEN_K=False-GROUP_M=8-num_stages=5-num_warps=8-cu32a5vsbaln3t55jm2y6xhwgyggejmoatyakcm2huvxofw2zzva"": 0.0398080013692379,
                ""triton_mm-ACC_TYPE='tl.float32'-ALLOW_TF32=True-BLOCK_K=32-BLOCK_M=128-BLOCK_N=128-EVEN_K=False-GROUP_M=8-num_stages=2-num_warps=8-croberh4l55jxlrlgkttigtebsnmosycc5rdtbtn3lp3bpovgz4a"": 0.0732479989528656,
                ""triton_mm-ACC_TYPE='tl.float32'-ALLOW_TF32=True-BLOCK_K=64-BLOCK_M=64-BLOCK_N=64-EVEN_K=False-GROUP_M=8-num_stages=3-num_warps=8-c6oxgunysrqpiwwoinylb3sb2hzvx66yhehma64drqvmz52h3r5t"": 0.0306560005992651,
                ""triton_mm-ACC_TYPE='tl.float32'-ALLOW_TF32=True-BLOCK_K=128-BLOCK_M=32-BLOCK_N=32-EVEN_K=False-GROUP_M=8-num_stages=2-num_warps=4-cdrev5e3zno6z6flmhlbxgd26gkdpurljyhrw3ovx6pftoe62dpf"": 0.04800000041723251,
                ""triton_mm-ACC_TYPE='tl.float32'-ALLOW_TF32=True-BLOCK_K=16-BLOCK_M=64-BLOCK_N=64-EVEN_K=False-GROUP_M=8-num_stages=2-num_warps=4-ce3ofrgngrwuo45hw5wqlzztium7gfkf4n5x25gwu4d6ygkea4bs"": 0.0751039981842041,
                ""triton_mm-ACC_TYPE='tl.float32'-ALLOW_TF32=True-BLOCK_K=16-BLOCK_M=32-BLOCK_N=32-EVEN_K=False-GROUP_M=8-num_stages=1-num_warps=2-cfkz2smezre4x7hyhc2kbeawhqup6qpwzgiavrai2ghe5ghouvn4"": 0.07401599735021591
            },
            ...,
        },
        ...,
    }
}

```

Test Plan:
MAST no global: sw-966772723-OfflineTraining_df2509b8
MAST global: sw-966766969-OfflineTraining_19df7c20

Differential Revision: D44550100

Pull Request resolved: https://github.com/pytorch/pytorch/pull/98010
Approved by: https://github.com/jansel"
pytorch/pytorch,558e5a240e0c2a5e9a0d5350fe6c25b029efca6a,"Introduce torch.onnx.dynamo_export API (#97920)

This is the first phase of the new ONNX exporter API for exporting from TorchDynamo and FX, and represents the beginning of a new era for exporting ONNX from PyTorch.

The API here is a starting point upon which we will layer more capability and expressiveness in subsequent phases. This first phase introduces the following into `torch.onnx`:

```python
dynamo_export(
    model: torch.nn.Module,
    /,
    *model_args,
    export_options: Optional[ExportOptions] = None,
    **model_kwargs,
) -> ExportOutput:
    ...

class ExportOptions:
    opset_version: Optional[int] = None
    dynamic_shapes: Optional[bool] = None
    logger: Optional[logging.Logger] = None

class ExportOutputSerializer(Protocol):
    def serialize(
        self,
        export_output: ExportOutput,
        destination: io.BufferedIOBase,
    ) -> None:
        ...

class ExportOutput:
    model_proto: onnx.ModelProto

    def save(
        self,
        destination: Union[str, io.BufferedIOBase],
        *,
        serializer: Optional[ExportOutputSerializer] = None,
    ) -> None:
        ...
```

In addition to the API in the first commit on this PR, we have a few experiments for exporting Dynamo and FX to ONNX that this PR rationalizes through the new Exporter API and adjusts tests to use the new API.

- A base `FXGraphModuleExporter` exporter from which all derive:
  - `DynamoExportExporter`: uses dynamo.export to acquire FX graph
  - `DynamoOptimizeExporter`: uses dynamo.optimize to acquire FX graph
  - `FXSymbolicTraceExporter`: uses FX symbolic tracing

The `dynamo_export` API currently uses `DynamoOptimizeExporter`.

### Next Steps (subsequent PRs):

* Combine `DynamoExportExporter` and `DynamoOptimizeExporter` into a single `DynamoExporter`.
* Make it easy to test `FXSymbolicTraceExporter` through the same API; eventually `FXSymbolicTraceExporter` goes away entirely when the Dynamo approach works for large models. We want to keep `FXSymbolicTraceExporter` around for now for experimenting and internal use.
* Parameterize (on `ExportOptions`) and consolidate Dynamo exporter tests.
  - This PR intentionally leaves the existing tests unchanged as much as possible except for the necessary plumbing.
* Subsequent API phases:
  - Diagnostics
  - Registry, dispatcher, and Custom Ops
  - Passes
  - Dynamic shapes

Fixes #94774

Pull Request resolved: https://github.com/pytorch/pytorch/pull/97920
Approved by: https://github.com/justinchuby, https://github.com/titaiwangms, https://github.com/thiagocrepaldi, https://github.com/shubhambhokare1"
pytorch/pytorch,950431c334d29ab4f93c1779d6b00ab0a4012721,"extract out a caffe2 macros library (#98156)

Slowly carving out the minimal caffe2 dependencies to build PyTorch.

Differential Revision: [D44609764](https://our.internmc.facebook.com/intern/diff/D44609764/)

**NOTE FOR REVIEWERS**: This PR has internal Meta-specific changes or comments, please review them on [Phabricator](https://our.internmc.facebook.com/intern/diff/D44609764/)!
Pull Request resolved: https://github.com/pytorch/pytorch/pull/98156
Approved by: https://github.com/ezyang, https://github.com/PaliC"
pytorch/pytorch,86505c692f666ac3a672d9b7dc2de894d1647452,"Disable inductor/test_minifier on ASAN (#98263)

This is to mitigate the timeout issue on ASAN https://github.com/pytorch/pytorch/issues/98262.  This test is slow on ASAN and it seems to cause problem to correctly compute the number of shards needed to run it.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/98263
Approved by: https://github.com/kit1980, https://github.com/malfet"
pytorch/pytorch,13f169c9da2625859b823f4b8ba08821f9853773,"Per Channel in brack-propagation function (#97475)

Summary:
Supporting Per Channel quantization in the gradient computation function.

One workaround that I have added here is
Current QNNPACK is not designed to process [transposed weight](https://fb.workplace.com/groups/pytorch.edge.users/permalink/1283737025829921/)
Here we are simply replacing Per Channel to Per Tensor to compute a gradient (Some slow learning curve or WER degradation might be expected - We don't know, nothing is guaranteed)

Test Plan:
You can create your own synthetic model,
FP32 layer -> INT8 layer with Per Channel and see if loss is decreasing

Differential Revision: D43898794

Pull Request resolved: https://github.com/pytorch/pytorch/pull/97475
Approved by: https://github.com/weiwangmeta"
pytorch/pytorch,095c129bd32918092dfe08878df7e658041e28e8,"[CI] Add inference run for the performance dashboard (#98174)

Summary: Remove fp32 training performance run and trade for amp inference
performance run.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/98174
Approved by: https://github.com/huydhn"
pytorch/pytorch,ced5c89b6fbe827a538b7ada96b2f9a5989871c7,"add explicit vectorization for Half dtype on CPU (#96076)

This patch is part of half float performance optimization on CPU:
* add specification for dtype `Half` in `Vectorized<>` under both avx256 and avx512.
* add specification for dtype `Half` in functional utils, e.g. `vec::map_reduce<>()`, which uses float32 as accumulate type.

Also add a helper struct `vec_hold_type<scalar_t>`, since Vectorized<Half>::value_type is pointing to its underlying storage type which is `uint16_t`, leading to error if the kernel uses `Vec::value_type`.

Half uses the same logic as BFloat16 in the Vectorized<>, each half vector is mapped to 2x float vectors for computation.

Notice that this patch modified the cmake files by adding **-mf16c** on AVX2 build, from https://gcc.gnu.org/onlinedocs/gcc/x86-Options.html, we can see that all the hardware platforms that support **avx2** already have **f16c**

Pull Request resolved: https://github.com/pytorch/pytorch/pull/96076
Approved by: https://github.com/malfet"
pytorch/pytorch,c99895ca6f98eab834611e007de772534fd57fb9,"Move pull and trunk slow tests to periodic (#98040)

I notice that we are running some slow tests for CPU and `sm86` on pull and trunk.  They take much longer to run than other shards (1.5x to 2x longer).  I propose that we move them to periodic instead. Thoughts?

The correlation between them are:

* `linux-bionic-cuda11.7-py3.10-gcc7-sm86 / test (slow)` and `linux-bionic-cuda11.7-py3.10-gcc7-sm86 / test (default)` is 0.93
* `linux-bionic-py3.8-clang9-slow / test (slow)` and `linux-bionic-py3.8-clang9 / test (default)` is 0.98

<!--
copilot:summary
-->
### <samp>🤖 Generated by Copilot at db56750</samp>

This pull request updates the `.github/workflows` files to optimize the testing workflows for PyTorch. It adds new periodic workflows for more platforms and configurations, and removes some redundant or slow workflows from the pull and trunk workflows.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/98040
Approved by: https://github.com/malfet"
pytorch/pytorch,8e5f491623885294f819090752f3388717166590,"[Inductor] simplify CPP backend Tile2D code and support non-contiguous load/store (#97626)

Remove `CppTile2DTailKernel` and `CppTile2DKernelChecker` and reuse `CppVecKernel` and `CppVecKernelChecker` for them. Add vectorization with fallback for load/store in CppVecKernel for the non-contiguous load/store needed by `CppTile2DTailKernel`.

This PR also adds a functional support for transposed copy of bfloat16 data types. Better performance requires vectorized intrinsics implemented for at::vec::transpose_mxn. cc @soumith @voznesenskym @penguinwu @anijain2305 @EikanWang @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @Xia-Weiwen @wenzhe-nrv @jiayisunx @peterbell10 @desertfire

Pull Request resolved: https://github.com/pytorch/pytorch/pull/97626
Approved by: https://github.com/jansel"
pytorch/pytorch,2ab18a23e1c5ef9a5a0b2da82fc5eaeda969ea9f,"Update ideep submodule (#97430)

### Description

This PR is to update ideep submodule for the following two aspects:

1. At inductor side, we are supporting dynamic shape path for packed linear, which we hopes the packed weight of linear doesn't depend on the input shapes and still can get a better a performance using a packed weight got from a dummy input shapes. However the current ideep has a accuracy issue for this case. This updating will fix the issue. 
2. Add an extra arg is_channels_last for deconv to notify ideep whether to go channels last or not because the memory format checks of ideep (e.g. is_nhwc(), is_ndhwc()) is not 100% identical to suggest_memory_format() from pytorch.

### Performance Benchmark

Use TorchBench test in ICX with 40 cores
Intel OpenMP & tcmalloc were preloaded
![image](https://user-images.githubusercontent.com/61222868/229072474-193513ba-6727-4451-91ff-0d57e016736f.png)

Pull Request resolved: https://github.com/pytorch/pytorch/pull/97430
Approved by: https://github.com/jgong5"
pytorch/pytorch,2630144786e906b40abbe017294d404bcfe3c6ae,"Call to mkldnn_matmul from aten::addmm on AArch64 (#91763)

We have noticed that on BERT_pytorch in torchbenchmark majority of time is spent in running GEMM in aten:addmm. At the moment this calls into BLAS routine, but on AArch64 it will be faster if it calls into mkldnn_matmul. Performance wise compared to build with OpenBLAS it runs faster 1.2x faster on 16 cores with batch size of 8 on Graviton3, while if fast math mode (mkldnn_matmul exposes through oneDNN and Arm Compute Library option to run GEMM with FP32 inputs using BBF16 operations) is enabled then it is 2.3x

Fixes #ISSUE_NUMBER

Pull Request resolved: https://github.com/pytorch/pytorch/pull/91763
Approved by: https://github.com/jgong5, https://github.com/ngimel, https://github.com/malfet"
pytorch/pytorch,d03799f9a50eefe3ad82b01d7c586d7b765f41b7,"optimize the AMP func name in custom_device_mod (#98052)

Fixes #ISSUE_NUMBER
1、optimize the func name of AMP in custom device module，use `torch.foo.set_autocast_enable` instead of `torch.foo.set_autocast_foo_enable`.
2、In AMP with custom device，use `custom_device_mod.set_autocast_enable` instead of `getattr(custom_device_mod,  ""set_autocast_enable""`, because we have check that `custom_device_mod` hasattr `set_autocast_enable` before.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/98052
Approved by: https://github.com/bdhirsh"
pytorch/pytorch,4e26ad786d51b61003d48c247c976beb2a05de1b,"fix load_sharded_optimizer_state_dict error on multi node (#98063)

Fixes #95892

This PR fixes the placement error in ChunkShardingSpec when training with multi nodes. 'rank:{global_rank}/cuda:{local_rank}' should be used but 'rank:{global_rank}/cuda:{global_rank}' is used so this would result in a CUDA error: invalid device ordinal.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/98063
Approved by: https://github.com/kumpera"
pytorch/pytorch,91ad5984d8bde4b384a5a46668932cb44a2b616b,"Add script to summarize performance from CI performance run (#97977)

Signed-off-by: Edward Z. Yang <ezyang@meta.com>

Pull Request resolved: https://github.com/pytorch/pytorch/pull/97977
Approved by: https://github.com/wconstab"
pytorch/pytorch,c218309f88ef2fb9b23fd842ae00f6ae2304543a,"[dynamo] profiler.record_function on all dynamo_timed functions (#96495)

**Summary**: profiler.record_function inserts an event into the chrome trace generated by the pytorch profiler. This PR adds record_function everywhere that @dynamo_timed is annotated.

dynamo_timed and the CLI viewer torch._dynamo.utils.compile_times() are already useful on their own; but for identifying _when_ these get called, it's nice to be able to view in the profiler chrome trace.

Why not just turn on python stack traces in the profiler to get this information? Dynamo compilation is implemented in python and therefore produces a huge amount of events when it records compilation steps. The resulting trace files are often too large to load in chrome://tracing, and they take a long time to generate. Additionally, the stack traces are deep enough that they are often hard to read. This approach produces much more readable traces with lower overhead.

**Tests**:
- Added in test/dynamo/test_profiler.py. Verified in https://github.com/pytorch/pytorch/actions/runs/4559322864/jobs/8043307798?pr=96495 that the tests are actually running.
- Performance run with `ciflow/inductor-perf-compare` shows no noticeable change in compilation time or speedup numbers. Geomean speedup changes from 1.275 -> 1.277. Geomean compilation times change from 54.2s -> 53.8s. That's likely just due to noise. All individual benchmark numbers regressed by no more than 5% between the two runs; and we see improvements of around the same magnitude, suggesting this is, again, just noise. For meta employees, you can see the results in a google sheets here: https://docs.google.com/spreadsheets/d/1Ki69XvcgxcA3ZnqC5n_jav5KiD4u7Wojlad3VTnIdlk/edit?usp=sharing

**Example**:

Run this:

```python
import torch

def gn(x):
    return x.sin().cos()

def fn(x, y):
    return x.sin() * y.cos()

x, y = [torch.rand((2, 2), device='cuda') for _ in range(2)]

# just to clear out any lazy initialization
with torch.profiler.profile() as prof:
    torch.compile(gn)(x)

with torch.profiler.profile() as prof:
    torch.compile(fn)(x, y)

prof.export_chrome_trace(""./dynamo_timed_profile.json"")
```

and we can see that the resulting trace shows important dynamo steps, even when python tracing is turned off.

<img width=""867"" alt=""Screenshot 2023-03-29 at 7 26 15 PM"" src=""https://user-images.githubusercontent.com/5067123/228712263-8ae67ab9-1a52-4765-a9c2-7c5cf0abe2f5.png"">

Pull Request resolved: https://github.com/pytorch/pytorch/pull/96495
Approved by: https://github.com/ngimel, https://github.com/mlazos"
pytorch/pytorch,ca135ed6b57f44776cfedf6fd5a5b28eeed98e85,"[PyTorch] Optimize TupleType::annotation_str_impl for small tuples (#97910)

In general, we can't profitably gather an array of all the elements' annotation strings so that we can reserve the final string because we'd have to heap-allocate that array. If we do it as a fast path for small tuples (which Tuple itself sets precedent for!), we can stack-allocate the array of annotation strings and make it profitable.

Differential Revision: [D44519675](https://our.internmc.facebook.com/intern/diff/D44519675/)
Pull Request resolved: https://github.com/pytorch/pytorch/pull/97910
Approved by: https://github.com/suo, https://github.com/Skylion007"
pytorch/pytorch,97fc8ea5f4cb9970c1b8a7cae3fe85c06fe261e3,"Run the benchmark suite with dynamic batch only (#97912)

Symbolic shapes compile time on full CI with inductor is horribly long (even though our aot_eager local runs seemed to suggest that the added latency was only 10s per model.) To patch over the problem for now, run the benchmark suite with dynamic batch only.  This should absolve a lot of sins.

Signed-off-by: Edward Z. Yang <ezyang@meta.com>

Pull Request resolved: https://github.com/pytorch/pytorch/pull/97912
Approved by: https://github.com/janeyx99, https://github.com/desertfire"
pytorch/pytorch,19162083f8831be87be01bb84f186310cad1d348,"Improved perfs for vectorized bilinear interpolate cpu uint8 RGB-case (channels last) (#96848)

## Description

- Based on https://github.com/pytorch/pytorch/pull/96651
  - Improved perfs for vectorized **bilinear** interpolate uint8 RGB-case, **channels last**
    - unified RGB and RGBA processing code such that RGB input is not copied into RGBA
  - Performances are more close to Pillow-SIMD (labeled as `Pillow (9.0.0.post1)` in the results)
  - RGBA case perfs are the same after refactoring (see Source link below)
- Fixed mem pointer alignment, added more comments (reviews from #96651)

## Results

- `Pillow (9.0.0.post1)` == Pillow-SIMD

```
[-------------------------------------------------------------------------------------------------- Resize -------------------------------------------------------------------------------------------------]
                                                                                 |  Pillow (9.0.0.post1)  |  torch (2.1.0a0+gitd6e220c) PR  |  torch (2.1.0a0+git2b75955) nightly  |  Speed-up: PR vs nightly
1 threads: --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
      3 torch.uint8 channels_last bilinear (256, 256) -> (32, 32) aa=True        |    38.674 (+-0.323)    |         57.591 (+-0.244)        |          131.033 (+-1.448)           |      2.275 (+-0.000)
      3 torch.uint8 channels_last bilinear (256, 256) -> (32, 32) aa=False       |                        |         39.471 (+-0.166)        |          113.911 (+-1.736)           |      2.886 (+-0.000)
      3 torch.uint8 channels_last bilinear (256, 256) -> (224, 224) aa=True      |   128.512 (+-1.916)    |        161.592 (+-1.242)        |          299.679 (+-2.099)           |      1.855 (+-0.000)
      3 torch.uint8 channels_last bilinear (256, 256) -> (224, 224) aa=False     |                        |        150.994 (+-1.180)        |          285.331 (+-1.919)           |      1.890 (+-0.000)
      3 torch.uint8 channels_last bilinear (256, 256) -> (320, 320) aa=True      |   180.045 (+-2.223)    |        220.581 (+-1.363)        |          431.057 (+-3.536)           |      1.954 (+-0.000)
      3 torch.uint8 channels_last bilinear (256, 256) -> (320, 320) aa=False     |                        |        219.391 (+-1.409)        |          429.410 (+-3.620)           |      1.957 (+-0.000)
      3 torch.uint8 channels_last bilinear (520, 520) -> (32, 32) aa=True        |   113.911 (+-1.024)    |        129.457 (+-1.295)        |          459.610 (+-13.322)          |      3.550 (+-0.000)
      3 torch.uint8 channels_last bilinear (520, 520) -> (32, 32) aa=False       |                        |         59.800 (+-0.199)        |          400.015 (+-11.815)          |      6.689 (+-0.000)
      3 torch.uint8 channels_last bilinear (520, 520) -> (224, 224) aa=True      |   283.050 (+-2.664)    |        339.143 (+-1.209)        |          683.555 (+-4.466)           |      2.016 (+-0.000)
      3 torch.uint8 channels_last bilinear (520, 520) -> (224, 224) aa=False     |                        |        250.601 (+-1.236)        |          603.545 (+-2.644)           |      2.408 (+-0.000)
      3 torch.uint8 channels_last bilinear (712, 712) -> (32, 32) aa=True        |   186.723 (+-2.213)    |        199.960 (+-1.343)        |          860.867 (+-21.763)          |      4.305 (+-0.000)
      3 torch.uint8 channels_last bilinear (712, 712) -> (32, 32) aa=False       |                        |         79.188 (+-0.261)        |          703.019 (+-25.805)          |      8.878 (+-0.000)
      3 torch.uint8 channels_last bilinear (712, 712) -> (224, 224) aa=True      |   412.353 (+-4.476)    |        462.230 (+-1.983)        |         1101.673 (+-49.299)          |      2.383 (+-0.000)
      3 torch.uint8 channels_last bilinear (712, 712) -> (224, 224) aa=False     |                        |        327.973 (+-1.852)        |          941.062 (+-5.549)           |      2.869 (+-0.000)

      3 torch.uint8 channels_last bilinear (64, 64) -> (224, 224) aa=True        |    61.191 (+-0.926)    |         80.795 (+-0.518)        |          160.853 (+-1.506)           |      1.991 (+-0.000)
      3 torch.uint8 channels_last bilinear (224, 224) -> (270, 268) aa=True      |   134.488 (+-2.129)    |        169.147 (+-1.324)        |          327.343 (+-2.846)           |      1.935 (+-0.000)
      3 torch.uint8 channels_last bilinear (256, 256) -> (1024, 1024) aa=True    |  1037.045 (+-24.982)   |        938.623 (+-9.010)        |         2603.360 (+-20.530)          |      2.774 (+-0.000)
      3 torch.uint8 channels_last bilinear (224, 224) -> (64, 64) aa=True        |    52.792 (+-0.613)    |         73.692 (+-0.264)        |          131.829 (+-1.333)           |      1.789 (+-0.000)
      3 torch.uint8 channels_last bilinear (270, 268) -> (224, 224) aa=True      |   139.596 (+-1.944)    |        173.778 (+-1.039)        |          320.063 (+-2.562)           |      1.842 (+-0.000)
      3 torch.uint8 channels_last bilinear (1024, 1024) -> (256, 256) aa=True    |   690.132 (+-10.946)   |        772.758 (+-2.864)        |         2036.860 (+-36.109)          |      2.636 (+-0.000)
      3 torch.uint8 channels_last bilinear (64, 64) -> (224, 224) aa=False       |                        |         78.747 (+-0.799)        |          158.479 (+-1.702)           |      2.013 (+-0.000)
      3 torch.uint8 channels_last bilinear (224, 224) -> (270, 268) aa=False     |                        |        167.046 (+-1.077)        |          322.104 (+-2.764)           |      1.928 (+-0.000)
      3 torch.uint8 channels_last bilinear (256, 256) -> (1024, 1024) aa=False   |                        |        918.967 (+-5.251)        |         2611.388 (+-29.917)          |      2.842 (+-0.000)
      3 torch.uint8 channels_last bilinear (224, 224) -> (64, 64) aa=False       |                        |         55.336 (+-0.251)        |          113.869 (+-1.243)           |      2.058 (+-0.000)
      3 torch.uint8 channels_last bilinear (270, 268) -> (224, 224) aa=False     |                        |        156.505 (+-1.095)        |          299.861 (+-2.710)           |      1.916 (+-0.000)
      3 torch.uint8 channels_last bilinear (1024, 1024) -> (256, 256) aa=False   |                        |        514.344 (+-1.905)        |         1776.796 (+-19.660)          |      3.454 (+-0.000)

```

Note: There is no perf regression for other case. There some cases (see Source below) with small speed-ups, for the rest it is roughly around 1.0 +/- 0.1 which may be attributed to noisy measurements ...

[Source](https://gist.github.com/vfdev-5/1c0778904a07ce40401306548b9525e8#file-20230329-181023-pr_vs_nightly-speedup-md)

## Context

- https://github.com/pytorch/pytorch/pull/90771

Pull Request resolved: https://github.com/pytorch/pytorch/pull/96848
Approved by: https://github.com/NicolasHug, https://github.com/peterbell10"
pytorch/pytorch,379fb476543725f1e4bf9012ae327a0cfb444e92,"[SPMD] Support foreach optimizers with functionalization (#97853)

My first attempt was to apply the same solution as how proxy_tensor.py
handles other inplace ops. However, foreach is different in the way
that it's schema is `native_functions.yaml` does not return anything,
whereas ops like `addcmul_` and `addcdiv_` do return Tensors (Thanks
bdhirsh for teaching me this!). As a result, the proxy output
during tracing does not wrap anything, and hence we cannot correctly
connect it with subsequent operators. Modifying `native_functions.yaml`
is not a preferred solution. After discussing with bdhirsh, the
temporary solution is to do foreach functionalization as a graph
pass for now. Later, when https://github.com/pytorch/pytorch/issues/97852
is addressed, we will switch to default functionalization.

Edit: the latest version follows @bdhirsh 's suggestion on using
`make_fx` `decomposition_table` instead of implementing manual
fx.Graph tranforms to functionalize `_foreach_add_`.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/97853
Approved by: https://github.com/fegin, https://github.com/wanchaol"
pytorch/pytorch,22b723132bf69a320628fa692316b00f166af792,"Update ufmt to v2.1.0 (#97900)

Updates ufmt to the latest version with all the relevant bugfixes and performance improvements.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/97900
Approved by: https://github.com/ezyang, https://github.com/malfet"
pytorch/pytorch,4ae4c6f68ab64ddf04026419b39d6095fe2f0513,"Fix typo when setting FSDP state dict config (#97110)

`get_state_dict_type` in FSDP looks for a key called `_optim_state_dict_config` when getting the optimizer state dict config.  However, `set_state_dict_type` sets the config at a key called `_optimstate_dict_config`.  This looks like a typo.

This fixes the discrepancy, so that when you set the state dict type, it is correctly used.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/97110
Approved by: https://github.com/awgu, https://github.com/fegin"
pytorch/pytorch,f4ac8e00528a1567b3273a41d9a4163763f6b161,"Add dynamo config skip_nnmodule_hook_guards (#97830)

This lets users that are sure they won't use hooks avoid overhead
related to dynamo guards on (assumedly) empty hook dicts on all
nn modules.

Only enable this flag if you are sure you won't change hook-behavior
after compiling.  It is ok to register a hook and then compile, if
you promise never to remove/alter the hook.  It is also ok to
not register a hook and compile, if you never register a hook later.

Note- this is not the best we can do, and hopefully in the future
we can avoid the need for this option following some of these paths
- make guards fast enough to not be an issue when guarding on hook
  dicts
- make a mode where dynamo actually skips tracing __call__ so
  hooks are consistently ignored by compiled programs
- use nnmodule versioning so hook changes can be guarded without
  explicit hook dict guards

Pull Request resolved: https://github.com/pytorch/pytorch/pull/97830
Approved by: https://github.com/jansel"
pytorch/pytorch,9e2e345af702377c5af57dbe07fa7ec2b5f47bb1,"[inductor] avoid kernel cache miss because of different arg name (#97755)

We previously use buffer name for the variable containing randomly generated kernel input in the kernel benchmark. This has a big drawback. The same kernel may be used for different buffers. However if we use buffer name as argument name, the kernel source code for different invocation of the kernel will be different. This cause the following downsides:
- compile time will be longer since we can not reused compiled kernel due to cache miss
- this cause inconsistent behavior with TORCHINDUCTOR_BENCHMARK_KERNEL enabled or disabled. We may see more kernels (some are essentially duplicated) in the compiled module if TORCHINDUCTOR_BENCHMARK_KERNEL is enabled.
- this obscure some optimization opportunities. E.g., a kernel spend 6% time is worth looking at. But if the kernel is called 20 times and now it show up as 20 different kernels each spend 0.3% of time, it would be less obvious that we should optimize this kernel.

In this PR, we just use canonical name like `arg_{i}` rather than the buffer name to avoid all the issues above.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/97755
Approved by: https://github.com/jansel"
pytorch/pytorch,597b558c51007c6d9b7751df1a5c893024377d4d,"[BE]: Update flake8 and plugins and fix bugs (#97795)

Update flake8 and flake8-plugins in lintrunner to a modern version. Enables more checks and makes flake8 checks significantly faster. Added a few additional rule ignores that will need to be fixed in the future.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/97795
Approved by: https://github.com/alexsio27444, https://github.com/janeyx99, https://github.com/ezyang"
pytorch/pytorch,662a8cf74db7b16f53ea92010308c4f039fac40a,"[FSDP][8/N] Simplify addr padding internals (#97796)

This is a follow-up to the last PR to greatly simplify the approach. This should be much cleaner.

**Details**
Let `N` denote the number of original parameters flattened into a given flat parameter with `M` extra padding tensors.
- `_numels_with_padding`: length `N + M`
- `_is_padding_mask`: length `N + M`
- `_numels`, `_param_infos`, `_shapes`, `_fqns`, `_param_extensions`: length `N`

`_shard_param_indices` and `_shard_param_offsets` were used to determine (1) if a given original parameter is in the local shard and if so, then (2) what is its offset in the _sharded_ flat parameter, and (3) how many numel are in the _sharded_ flat parameter.

This PR reworks how to achieve (1), (2), and (3) to allow for simplifying the previously mentioned data structures. In particular, it saves one extra tuple `_shard_param_infos: Tuple[_ShardParamInfo, ...]` of length `N` where each `_ShardParamInfo` entry gives exactly the needed info. For example, the offset into the sharded flat parameter is now pre-computed, so we do not need to do `offset = 0; offset += numel_in_shard` over a `for` loop each time now.

For optimizer state dict, `FSDPParamInfo.param_indices` now maps to the indexes with respect to the length `N` data structures, not the length `N + M` ones. The only purpose of `param_indices` is to be able to index into `flat_param._shard_param_infos[i]` to get the contained info to flatten the unsharded original parameter optimizer state and extract the part in the local shard.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/97796
Approved by: https://github.com/rohan-varma"
pytorch/pytorch,79d2a8dd9eb8753ef3e4d7eafee7d09a9ce42df8,"[PyTorch] Second try: use c10::FastMap for memoizing in Pickler (#96688)

These maps don't rely on reference stability, so FastMap should be fine.

First try (#96360) was reverted because it broke internal tests.

Differential Revision: [D43995796](https://our.internmc.facebook.com/intern/diff/D43995796/)

**NOTE FOR REVIEWERS**: This PR has internal Meta-specific changes or comments, please review them on [Phabricator](https://our.internmc.facebook.com/intern/diff/D43995796/)!
Pull Request resolved: https://github.com/pytorch/pytorch/pull/96688
Approved by: https://github.com/malfet"
pytorch/pytorch,b443198966bf06c188c19141d1d9ce9c453d2ce5,"Fix sparse addmv ref impl for non-contig tensors (#97730)

Fix logic in `test_block_addmm` that tested op against itself rather than against dense implementation, by implementing `ref_addvm` function that converts tensor back to dense before multiplying it with vector.

Fix reference implementation by passing stride for vector and result. (Not sure wether it will be more perf efficient to iterate over strided tensor or request a dense copy as MKL implementation does)

Print more verbose error message if values differ.

Fixes https://github.com/pytorch/pytorch/issues/97629 , https://github.com/pytorch/pytorch/issues/97589 ,  https://github.com/pytorch/pytorch/issues/97563

Pull Request resolved: https://github.com/pytorch/pytorch/pull/97730
Approved by: https://github.com/cpuhrsch"
pytorch/pytorch,bb42104fe84218899e29a9878927709a62c2c790,"[DataLoader] Fix  collation logic (#97789)

Similar to #97737, a previous auto-refactor changed how `bytes` are handled during collation, which can potentially lead to performance regression. This PR undoes that.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/97789
Approved by: https://github.com/albanD"
pytorch/pytorch,c0e0fbb6e158f13cb990f17e80b5903ac4407bfe,"inductor: fix _dynamic_reshape_indexer issue when tail index is sym (#97502)

For TIMM **swin_base_patch4_window7_224**  dynamic shape case, there has an error for ```view``` op:

```
  File ""/home/xiaobing/pytorch-offical/torch/_inductor/lowering.py"", line 229, in wrapped
    out = decomp_fn(*args, **kwargs)
  File ""/home/xiaobing/pytorch-offical/torch/_inductor/lowering.py"", line 665, in view
    return TensorBox(View.create(x.data, sizes))
  File ""/home/xiaobing/pytorch-offical/torch/_inductor/ir.py"", line 1325, in create
    reindex = cls.dynamic_reshape_indexer(old_size, new_size)
  File ""/home/xiaobing/pytorch-offical/torch/_inductor/ir.py"", line 1351, in dynamic_reshape_indexer
    reindex2 = cls._dynamic_reshape_indexer(flat, new_size)
  File ""/home/xiaobing/pytorch-offical/torch/_inductor/ir.py"", line 1406, in _dynamic_reshape_indexer
    assert size_new == 1
torch._dynamo.exc.BackendCompilerFailed: backend='inductor' raised:
LoweringException: AssertionError:
  target: aten.view.default
  args[0]: TensorBox(StorageBox(
    Pointwise(
      'cpu',
      torch.float32,
      def inner_fn(index):
          i0, i1, i2, i3 = index
          tmp0 = ops.load(buf37, i3 + 49 * i2 + 2401 * i1 + 9604 * i0)
          tmp1 = ops.load(arg35_1, i3 + 49 * i2)
          tmp2 = ops.load(arg1_1, i1 + 4 * (tmp1))
          tmp3 = tmp0 + tmp2
          return tmp3
      ,
      ranges=[64, 4, 49, 49],
      origins={add_12}
    )
  ))
  args[1]: [64//s3, s3, 4, 49, 49]
```

the target shaps of ```view``` is ```[64//s3, s3, 4, 49, 49]```, and ```Sym(s3)``` is 64, see

```
sym_size_16: Sym(s3) = torch.ops.aten.sym_size(arg34_1, 0)
floordiv_3: Sym(64//s3) = sym_size_13 // sym_size_16
view_33: f32[64//s3, 64//(64//s3), 4, 49, 49] = torch.ops.aten.view.default(add_12, [floordiv_3, sym_size_16, 4, sym_size_14, sym_size_14]);  add_12 = floordiv_3 = sym_size_16 = None
```

For the tail index of the new size is ```Sym(64//s3)```, it is not a number, we shouldn't directly compare it with ```1```.

Currently, I didn't find a simple test case to reproduce it, I just test it for the real model.

```
python -m torch.backends.xeon.run_cpu --core_list 0 --ncores_per_instance 1 benchmarks/dynamo/timm_models.py --performance --float32 -dcpu -n50 --inductor --only swin_base_patch4_window7_224 --batch_size 1 --threads 1
```

Pull Request resolved: https://github.com/pytorch/pytorch/pull/97502
Approved by: https://github.com/jgong5, https://github.com/ezyang"
pytorch/pytorch,d8cc8ffebcf9d93e728734e879685264aa2d22c0,"[DataLoader] Short circuit pin_memory recursion when operating on bytes (#97737)

Slack thread: https://pytorch.slack.com/archives/GEEQ2K4MD/p1679962409906099

I was seeing some massive (~2x) slowdowns on a job after running it on PyTorch 2.0. From some profiling in `py-spy` it looked like the pin_memory thread was doing a lot more work than before. Looking at a trace in `nsys` I saw the thread doing the forward pass having a bunch of `pthread_cond_timedwait` with GIL reacquire calls in it’s call stack, and it seemed like the thread doing the forward pass was getting blocked (waiting for the GIL) by the pin memory thread (which was holding the GIL).

After some debugging I found out the issue. If a `bytes` was passed into `pin_memory`, previously in 1.13 (before https://github.com/pytorch/pytorch/pull/94709) it would short-circuit and return here
https://github.com/pytorch/pytorch/blob/d922c29a22e4bf0fba49526f7536395eb8cd66f4/torch/utils/data/_utils/pin_memory.py#L54-L55
since `bytes` was in `torch._six.string_classes`:
```
>>> from torch._six import string_classes
>>> string_classes
(<class 'str'>, <class 'bytes'>)
>>>
```

However after https://github.com/pytorch/pytorch/pull/94709, if a `bytes` was passed into `pin_memory` it would fall into here instead
https://github.com/pytorch/pytorch/blob/c263bd43e8e8502d4726643bc6fd046f0130ac0e/torch/utils/data/_utils/pin_memory.py#L68-L73
because the previous check is now doing `isinstance(data, str)` instead of `isinstance(data, (str, bytes))`!
https://github.com/pytorch/pytorch/blob/c263bd43e8e8502d4726643bc6fd046f0130ac0e/torch/utils/data/_utils/pin_memory.py#L56-L57

As a result, `pin_memory` gets called recursively for each element in the `bytes` leading to a ton of wasted recursion. This also explains the slowdown / GIL contention I was seeing.

This PR simply changes `isinstance(data, str)` to `isinstance(data, (str, bytes))` to match the behavior before https://github.com/pytorch/pytorch/pull/94709

Pull Request resolved: https://github.com/pytorch/pytorch/pull/97737
Approved by: https://github.com/albanD, https://github.com/NivekT"
pytorch/pytorch,0176fb4cd6b7b82dbdd90160c694f85ae6aa04f6,"Remove fast_nvcc entry in README.md (#97624)

After https://github.com/pytorch/pytorch/pull/96665 landed, fast_nvcc tool is no longer available.
This commit removes the documentation for it so as not to confuse users.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/97624
Approved by: https://github.com/drisspg"
pytorch/pytorch,1c15cd48e213db3fe13f01056021f34fd5326ca7,"[FSDP][7/N] Add alignment padding for `use_orig_params=True` (#97667)

This PR adds intra-`FlatParameter` 16-byte alignment padding to the `use_orig_params=True` code path to avoid clones in TorchInductor.

**Approach**
The `FlatParameter` maintains several data structures about its original parameters. Notably, the data structures `_param_infos`, `_shapes`, `_numels`, and `_fqns` have the same length and index in the same way.

This PR treats alignment padding _like_ an original parameter in that the padding gets flattened into the `FlatParameter`. Therefore, it must be reflected in the aforementioned data structures. However, given the way in which the data structures are used, we choose to do the following if the `i`th tensor flattened into the `FlatParameter` is padding:
- `_numels[i]` is the numel of padding
- `_param_infos[i] == _shapes[i] == _fqns[i] == None`

This choice is because (1) we must record the padding numel to account for it (e.g. for views) and (2) we prefer to preserve the invariant that the data structures index in the same way over avoiding `None` entries.

To ease the burden of other FSDP developers, we separate the parameter flattening logic:
- `_init_flat_param_and_metadata()`: This should be called only once in the `FlatParamHandle` constructor. The `FlatParameter` metadata is assumed to be static thereafter.
- `flatten_tensors()` / `flatten_tensors_into_flat_param()`: These can be used for optimizer and model state dict and can be called after construction time.

This separation allows `_init_flat_param_and_metadata()` to contain the much heavier metadata logic, while keeping the latter methods to be much lighter. The only constraint is that the alignment padding logic must be kept consistent between the two, but this should be worth the simper interface.

**Testing**
- This PR directly modifies the `use_orig_params=True` code path, so all existing tests passing gives good signal.
    - Some existing unit tests had to be adjusted to account for the alignment padding.
- This PR adds two tests in `test_fsdp_flatten_params.py` to explicitly test the sharding metadata with alignment for both parameter full precision and mixed precision since the latter requires possibly more padding elements due to the decreased per-element size.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/97667
Approved by: https://github.com/rohan-varma"
pytorch/pytorch,91cce4c09aef38b1cb5be5d4a9d55845aeff02b3,"Sort: Use cub::WarpMergeSort for small sorts (32 < n <= 128) (#96223)

We currently use `bitonicSortKVInplace` for sorts of size `n <= 32`
but use `radixSortKVInplace` for `32 < n <= 4096`. Bitonic sort is
also unstable, which forces stable sorts fall back to which is up to
4x slower in this small regime.

This PR adds a new kernel `warpMergeSortKVInplace` using
`cub::WarpMergeSort` to implement sorts with `32 < n <= 128` and all
stable sorts with `n < 128`. This results in up to a 2x speedup for
unstable sorts and up to 15x for stable sorts, depending on the input
geometry.

This also doesn't increase the total number of kernels since we are
replacing radix-sorts of size 32 and 128.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/96223
Approved by: https://github.com/ngimel"
pytorch/pytorch,759e527ea1bd79026a68bef46dc854bdb26367ba,"Use internal symbolizer for FBCODE (#97172)

Summary:
addr2line does not work fast on fbcode binaries, so use the
internally symbolize pathway.

Test Plan: sandcastle

Differential Revision: D44227690

Pull Request resolved: https://github.com/pytorch/pytorch/pull/97172
Approved by: https://github.com/eellison"
pytorch/pytorch,38da54e9c9471565812d2be123ee4e9fd6bfdbc0,"Split rnn primitive for inference and training (#96736)

## Description
Currently, both inference and training will use `forward_training` in rnn primitive, which will bring performance downgrade for inference (The performance drop is from rnn primitive and unnecessary creation of `pd` and `workspace`). This PR is to split them into `forward_inference` and `forward_training` seperately.

## Performance
With this fix PR, in RNN-T inference, the throughput reduction is 167 ms, which increases `3.7%` of E2E time.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/96736
Approved by: https://github.com/jgong5"
pytorch/pytorch,233742cb2f5cc645afc3a511185c3ec59924f7de,"Add accuracy tests for traced optimizers (#97577)

Pull Request resolved: https://github.com/pytorch/pytorch/pull/97577
Approved by: https://github.com/yifuwang"
pytorch/pytorch,75fb0b6c9f47cc82cdf41aea710108c42e4f075b,"Enable full train_step tracing and customizable dist graph expansion (#97416)

This commit adds an entry point for full `train_step` tracing and
expansion. Model forward, backwrd, and optimizer step will be included
in one graph. DTensor expansion will be applied on top to insert
collective communications. Users can also provide an `Override`
implementation to skip non-traceable submodules and directly install
submodule logic to the  DTensor-expanded graph by inserting `fx.Nodes`.

Differential Revision: [D44325177](https://our.internmc.facebook.com/intern/diff/D44325177)
Pull Request resolved: https://github.com/pytorch/pytorch/pull/97416
Approved by: https://github.com/yifuwang, https://github.com/wanchaol"
pytorch/pytorch,e67b58105abd0587ec167f5c7951eca46140aa06,"Enable lowering to inductor (#96927)

**Summary**
Enable the lowering path from a quantized 2.0 fx graph into Inductor. The basic usage will be
```
export_module, guards = torchdynamo.export(m, *args)
prepare_module = prepare_pt2e(export_module, *args)
convert_module = convert_pt2e(prepare_module)
ooptimized_module = compile_fx(convert_module, example_inputs)
```
Most of the issues we met previously has already been fixed in PyTorch Master. So in this PR, we mainly do 2 things:
1. Add the basic usage into a UT.
2. Move `handle_dynamo_export_graph` before the fusion passes, otherwise the dynamo_export_graph will hit the fusion passes twice which is un-expected.

**Test Plan**
```
clear && python -m pytest test_quantization.py -k test_inductor_backend_config_conv
```

Pull Request resolved: https://github.com/pytorch/pytorch/pull/96927
Approved by: https://github.com/jgong5, https://github.com/EikanWang, https://github.com/jansel, https://github.com/jerryzh168"
pytorch/pytorch,c757647dd8c198fbcca4d4e6c7cba6f1d12b97e6,"[Better Transformer] make is_causal a hint and force attn_mask to be set on `is_causal=True` in F.MHA (#97214)

Summary:
This fixes an issue raised in [is_causal parameter in torch.nn.TransformerEncoderLayer.forward does not work #96941](https://github.com/pytorch/pytorch/issues/96941) where results computed with is_causal do not properly reflect causal masking.

In PyTorch 2.0, Accelerated PT Transformers added the is_causal parameter to legacy nn.Transformer* and nn.MHA APIs aligned with and intended to engage the is_causal parameter of the new scaled_dot_product_attention (SDPA) operator.

At present is_causal works differently for Transformer* modules, the nn.MHA and F.MHA:
* The nn.Transformer* modules treat is_causal as an optional indicator about the format of attn_mask. This is because some layers (such as the CLIP layer use the attention mask in the layer, and thus the attn_mask was a required feature.)
* Initially, nn.MHA and F.MHA were defined to align with F.SDPA in behavior: a user may specify either the attention mask, or is_causal, but not both.  It seemed to make sense at the time to align SDPA and MHA, esp since there was a larger overlap of parameters which have since changed, e.g., with the removal of need_weights from SDPA. (See below for why this makes sense.)

Unfortunately, this does not work because of how MHA was changed to handle the need_weights parameter.  When need_weights is present, we do not (any more) call SDPA because support for need_weights was removed from SDPA before the release.  The rationale is that need_weights defeats all optimization at the foundation of SDPA performance.  Having the flag might thus mislead users into thinking they get good performance and have them disappointed when they enable a legacy feature of MHA which massively degrades performance.  (They might not think anything of enabling that, because it is on by default in MHA today, which leads to more  issues.)

Since SDPA does not (no longer) support need_weights, we need to pick a separate path which implements attention using a set of discrete operations that allocates a tensor for weights.  Alas, this code path does not have support for is_causal, because attention is implemented as matmul and using the attention mask.  Thus, is_causal has no impact.  (A substantially similar situation arises with how kpm is implemented today because Nested Tensors are not supported by torch.compile() in 2.0)

This problem was masked because all uses of legacy nn.MHA (and F.MHA) come through nn.Transformer* which called self-attention (i.e., nn.MHA) only ever with the attention mask attn_mask, and never with is_causal, a missed optimization opportunit that would have been addressed in a future performance update.

Regrettably, always calling nn.MHA with attn_mask prevented diagnosing of the issue of not having a suitable attention mask when need_weights support was dropped from SDPA and a discrete implementation of attention was added for that scenario, and for the execution path with key_padding_mask.

We have two options to address this issue:

Solution 1: Whenever nn.MHA and F.MHA are executed with is_causal set, we internally create a causal mask at significant expense of allocating a tensor and filling it with a triangular causal matrix.  This increases memory usage, and runtime, for allocating a causal mask.  To add insult to injury, in all current (and likely future) execution scenarios, MHA is called by a model using the nn.Transformer API which already has that matrix and passes it from nn.module to nn.module.  Then the passing in of attn_mask has to be suppressed by nn.TransformerEncoderLayer, only for nn.MHA to immediately allocate the very same tensor again to satisfy the requirement to have an attention mask for the computation. (We expect new use cases to use SDPA directly.)

Solution 2: We align the behavior of nn.MHA and F.MHA with the rest of the existing nn.Transformer API, and require the attention mask to be passed into nn.MHA in addition to is_causal as an optional indicator about the nature of the attention mask rather than as an alternative to attn_mask.  Then, when we choose the code path for processing MHA with need_weights or a key_padding_mask, we have the attn_mask passed down through the nn.Transformer* hierarchy, without the added overhead of allocating an attention mask as in scenario 1.

This PR implements solution 2 which offers better performance and in retrospect aligns MHA better with the rest of the Transformer modules as the definition of SDPA evolved into a more streamlined high-performance operator.  It ostensibly changes how is_causal works, by requiring the attention mask to be specified.  However, as described here, and as shown in the submitted issue, is_causal is not working as intended today, so it requires a change regardless.

In that sense, a change in API does not occur per-se, as the current implementation is not working, and a change has to occur either way to resolve the submitted issue, breaking any use cases that depend on the current implementation.  Checks exist (and more can be added) that flag any scenarios where is_causal is passed as True, but no attention mask is provided, ensuring that there's not quiet change from even the faulty behavior present in 2.0.

As  an upside, the present implementation will improve performance by addressing the passing of the is_causal flag from Transformer modules to MHA, speeding up training for these examples, e.g., finetuning BERT, RoBERTa, XLM-R models.

Differential Revision: D44245725

Pull Request resolved: https://github.com/pytorch/pytorch/pull/97214
Approved by: https://github.com/albanD"
pytorch/pytorch,2b75955c9fae436b7778d91aa6b776f43e72f01c,"[CI] Add missing --cold-start-latency for the dashboard run (#97547)

Pull Request resolved: https://github.com/pytorch/pytorch/pull/97547
Approved by: https://github.com/huydhn"
pytorch/pytorch,b5edf183343c8588bfb4105f7545525b644546be,"`GradScaler` recomputes `optimizer_state[""found_inf_per_device""]` before `optimizer.step` (#97415)

I found a discrepancy between non-fused and fused optimizers, which is to use `optimizer_state[""found_inf""]` or to recompute `found_inf`.

- non fused: https://github.com/pytorch/pytorch/blob/e64ddd1ab9d46cfc921c19269969ffc5cd7d6f6c/torch/cuda/amp/grad_scaler.py#L289
- fused: https://github.com/pytorch/pytorch/blob/e64ddd1ab9d46cfc921c19269969ffc5cd7d6f6c/torch/cuda/amp/grad_scaler.py#L353
    - where `_check_inf_per_device` is https://github.com/pytorch/pytorch/blob/e64ddd1ab9d46cfc921c19269969ffc5cd7d6f6c/torch/cuda/amp/grad_scaler.py#L564-L573

The other way to align the behavior is to use the existing `found_inf` in https://github.com/pytorch/pytorch/blob/e64ddd1ab9d46cfc921c19269969ffc5cd7d6f6c/torch/cuda/amp/grad_scaler.py#L353.

I'd say this PR is for the sake of ""safety"" and the alternative is to keep the existing behavior.
I honestly have no idea if it's expected to double-check the sanity of gradients in `GradScaler.step`.

---

what I've observed in huggingface/transformers T5-base example so far seems like that non-fused optimizers lead to invalid parameters while the fused not.
The cause seems to be that `gradients` become inf/nan before `GradScaler.step(optimizer)` after `GradScaler._unscale_grads_` (more precicely, the call of `torch._amp_foreach_non_finite_check_and_unscale_`) in the script of the issue linked below, i.e. the gradient clipping and/or unscaling lead to inf/nan as these happen after the grad check. See
https://github.com/pytorch/pytorch/blob/788300cc2aa096d8d5c1e7fbfc87e5439a338251/aten/src/ATen/native/cuda/AmpKernels.cu#L165-L174.

Fixes #96755 🙏

Pull Request resolved: https://github.com/pytorch/pytorch/pull/97415
Approved by: https://github.com/ngimel, https://github.com/janeyx99"
pytorch/pytorch,a66625da3bcdf1e262ddf7bc7fedaa070147c78e,"[PyTorch] Optimize DictType::annotation_str_impl (#96498)

stringstream construction is expensive, and we can exactly reserve space for the output string while doing the same number of string copies. (If we wanted to improve performance further, we could introduce annotation_str_out to append the output to a given std::string and thus avoid copying subtype annotation strings. It occurs to me that the existing approach is quadratic in the number of layers of nesting, so we should probably do this!)

Differential Revision: [D43919651](https://our.internmc.facebook.com/intern/diff/D43919651/)
Pull Request resolved: https://github.com/pytorch/pytorch/pull/96498
Approved by: https://github.com/Skylion007"
pytorch/pytorch,000cfeb84841554a5d15b261199f2d1b64fc420c,"[PyTorch] Optimize TupleType::annotation_str_impl (#96497)

stringstream is expensive to create, we used stringstream instead of ostringstream, and we can easily specialize the empty tuple. Also, anybody compiling with C++20 support can move out of the stringstream and it shouldn't hurt people without C++20 support to do so. I would consider specializing the 1-element case as well but I don't have evidence that that's necessary right now.

Differential Revision: [D43882402](https://our.internmc.facebook.com/intern/diff/D43882402/)
Pull Request resolved: https://github.com/pytorch/pytorch/pull/96497
Approved by: https://github.com/Skylion007"
pytorch/pytorch,bb74d04353797e6fba79233725019ee0d8ac4403,"Remove inductor-perf-test-nightly label (#97290)

Remove labels according to Ed's suggestion
""I do NOT think performance dashboard should be label triggered. It easy to put a label on the PR, and then forget about it and keep spamming our limited A100 capacity when you push updates to your PR."" Instead, one can use the ""Run workflow"" option and specify their feature branch in https://github.com/pytorch/pytorch/actions/workflows/inductor-perf-compare.yml

Pull Request resolved: https://github.com/pytorch/pytorch/pull/97290
Approved by: https://github.com/ezyang, https://github.com/desertfire"
pytorch/pytorch,63e1f12b49908ac729ad74ad1690317ea67179d1,"Speedup bincount and histc on CUDA (#97090)

This is to speed up torch.bincount and torch.histc on CUDA.

1. Speed up int64_t gpuAtomicAdd,
2. and optimize the histogram kernel.

# Fixes #96626
After speedup, time cost in #96626 would be

```
... (run 2 times and ignore the first run)
case 1 CPU  0.0003631114959716797 seconds
case 1 CUDA 0.0005860328674316406 seconds
case 2 CPU  0.0013742446899414062 seconds
case 2 CUDA 0.0008623600006103516 seconds
```

Note that in ""*case 1 CUDA*"", the **max** op takes the most time, i.e., https://github.com/pytorch/pytorch/blob/5ee5a164ffeb7b7a167c53009fb8fe5f5bd439d9/aten/src/ATen/native/cuda/SummaryOps.cu#L334-L335, which is not to be optimized in this PR.

# Benchmark

Time is measured on i7-10700 + RTX 3080, Ubuntu 22.04 (in WSL). The baseline is PyTorch 2.0.0+cu117. My dev version of PyTorch is compiled with CUDA 11.8. Each case is measured 15 times to take the median.

## torch.bincount
#elem | nbins | distribution | CPU | PyTorch 2.0.0 | this PR | speedup
-- | -- | -- | -- | -- | -- | --
2**20 | 80 | random.uniform | 0.000834 | 0.005783 | 0.000266 | 21.8x
2**20 | 80 | narrow in 1 bin | 0.001576 | 0.003967 | 0.000563 | 7.0x
2**20 | 500 | random.uniform | 0.000852 | 0.003641 | 0.000334 | 10.9x
2**20 | 500 | narrow in 1% bins | 0.000894 | 0.001878 | 0.000349 | 5.4x
2**20 | 2048 | random.uniform | 0.000891 | 0.000820 | 0.000298 | 2.8x
2**20 | 2048 | narrow in 1% bins | 0.000958 | 1.043251 | 0.000335 | 3,116.6x
2**26 | 80 | random.uniform | 0.067715 | 0.322409 | 0.003032 | 106.3x
2**26 | 80 | narrow in 1 bin | 0.110940 | 0.194644 | 0.017651 | 11.0x
2**26 | 500 | random.uniform | 0.066666 | 0.192302 | 0.002535 | 75.8x
2**26 | 500 | narrow in 1% bins | 0.066130 | 0.092237 | 0.005462 | 16.9x
2**26 | 2048 | random.uniform | 0.066371 | 0.035308 | 0.002476 | 14.3x
2**26 | 2048 | narrow in 1% bins | 0.068453 | 72.122858 | 0.003185 | 22,644.3x

## torch.histc (float32)
#elem | nbins | distribution | CPU | PyTorch 2.0.0 | this PR | speedup
-- | -- | -- | -- | -- | -- | --
2**20 | 80 | random.uniform | 0.001261 | 0.000145 | 9.47E-05 | 1.5x
2**20 | 80 | narrow in 1 bin | 0.001074 | 0.000356 | 0.000311 | 1.1x
2**20 | 500 | random.uniform | 0.001162 | 0.000227 | 9.18E-05 | 2.5x
2**20 | 500 | narrow in 1% bins | 0.001082 | 0.000201 | 0.000152 | 1.3x
2**20 | 2048 | random.uniform | 0.001100 | 0.000203 | 0.000118 | 1.7x
2**20 | 2048 | narrow in 1% bins | 0.001089 | 0.000396 | 0.000107 | 3.7x
2**26 | 80 | random.uniform | 0.064219 | 0.001170 | 0.000786 | 1.5x
2**26 | 80 | narrow in 1 bin | 0.056471 | 0.013283 | 0.011939 | 1.1x
2**26 | 500 | random.uniform | 0.078183 | 0.003411 | 0.000562 | 6.1x
2**26 | 500 | narrow in 1% bins | 0.056711 | 0.002763 | 0.002738 | 1.0x
2**26 | 2048 | random.uniform | 0.059296 | 0.003503 | 0.000533 | 6.6x
2**26 | 2048 | narrow in 1% bins | 0.061754 | 0.015703 | 0.000962 | 16.3x

## torch.histc (int64)
#elem | nbins | distribution | CPU | PyTorch 2.0.0 | this PR | speedup
-- | -- | -- | -- | -- | -- | --
2**20 | 80 | random.uniform | N/A | 0.005614 | 9.47E-05 | 59.3x
2**20 | 80 | narrow in 1 bin | N/A | 0.003799 | 0.000395 | 9.6x
2**20 | 500 | random.uniform | N/A | 0.003665 | 9.58E-05 | 38.2x
2**20 | 500 | narrow in 1% bins | N/A | 0.001760 | 0.000178 | 9.9x
2**20 | 2048 | random.uniform | N/A | 0.000693 | 0.000111 | 6.2x
2**20 | 2048 | narrow in 1% bins | N/A | 1.082904 | 0.000123 | 8,802.4x
2**26 | 80 | random.uniform | N/A | 0.320400 | 0.001145 | 279.9x
2**26 | 80 | narrow in 1 bin | N/A | 0.193668 | 0.015229 | 12.7x
2**26 | 500 | random.uniform | N/A | 0.182897 | 0.000823 | 222.2x
2**26 | 500 | narrow in 1% bins | N/A | 0.089363 | 0.00376 | 23.8x
2**26 | 2048 | random.uniform | N/A | 0.033190 | 0.000832 | 39.9x
2**26 | 2048 | narrow in 1% bins | N/A | 71.721012 | 0.001525 | 47,017.8x

## Banchmark code

Here is the benchmark code:

```python3
import time
import torch

cases = [
    (""bincount    bins=80   wide  "", torch.randint(80, [2**20]),   lambda x: torch.bincount(x, minlength=80)),
    (""bincount    bins=80   narrow"", torch.randint(1, [2**20]),    lambda x: torch.bincount(x, minlength=80)),
    (""bincount    bins=500  wide  "", torch.randint(500, [2**20]),  lambda x: torch.bincount(x, minlength=500)),
    (""bincount    bins=500  narrow"", torch.randint(5, [2**20]),    lambda x: torch.bincount(x, minlength=500)),
    (""bincount    bins=2048 wide  "", torch.randint(2048, [2**20]), lambda x: torch.bincount(x, minlength=2048)),
    (""bincount    bins=2048 narrow"", torch.randint(20, [2**20]),   lambda x: torch.bincount(x, minlength=2048)),
    (""histc_float bins=80   wide  "", torch.rand(2**20),            lambda x: torch.histc(x, bins=80, min=0., max=1.)),
    (""histc_float bins=80   narrow"", torch.rand(2**20)*.01,        lambda x: torch.histc(x, bins=80, min=0., max=1.)),
    (""histc_float bins=500  wide  "", torch.rand(2**20),            lambda x: torch.histc(x, bins=500, min=0., max=1.)),
    (""histc_float bins=500  narrow"", torch.rand(2**20)*.01,        lambda x: torch.histc(x, bins=500, min=0., max=1.)),
    (""histc_float bins=2048 wide  "", torch.rand(2**20),            lambda x: torch.histc(x, bins=2048, min=0., max=1.)),
    (""histc_float bins=2048 narrow"", torch.rand(2**20)*.01,        lambda x: torch.histc(x, bins=2048, min=0., max=1.)),
    (""histc_int   bins=80   wide  "", torch.randint(80, [2**20]),   lambda x: torch.histc(x, bins=80, min=0., max=80.)),
    (""histc_int   bins=80   narrow"", torch.randint(1, [2**20]),    lambda x: torch.histc(x, bins=80, min=0., max=80.)),
    (""histc_int   bins=500  wide  "", torch.randint(500, [2**20]),  lambda x: torch.histc(x, bins=500, min=0., max=500.)),
    (""histc_int   bins=500  narrow"", torch.randint(5, [2**20]),    lambda x: torch.histc(x, bins=500, min=0., max=500.)),
    (""histc_int   bins=2048 wide  "", torch.randint(2048, [2**20]), lambda x: torch.histc(x, bins=2048, min=0., max=2048.)),
    (""histc_int   bins=2048 narrow"", torch.randint(20, [2**20]),   lambda x: torch.histc(x, bins=2048, min=0., max=2048.)),
]

def test(case, device):
    name, x, func = case
    x = x.to(device)
    time_samples = []
    for _ in range(15):
        torch.cuda.synchronize()
        t1 = time.time()
        func(x)
        torch.cuda.synchronize()
        t2 = time.time()
        time_samples.append(t2 - t1)
    median = sorted(time_samples)[len(time_samples) // 2]
    print(device, name, median)

for case in cases:
    test(case, device=""cuda"")

# for case in cases:
#     test(case, device=""cpu"")
```
Pull Request resolved: https://github.com/pytorch/pytorch/pull/97090
Approved by: https://github.com/ngimel"
pytorch/pytorch,5d8c7e7ea47cb6e1faf333430889a804de87536e,"Sort: Use cub::WarpMergeSort for small sorts (32 < n <= 128) (#96223)

We currently use `bitonicSortKVInplace` for sorts of size `n <= 32`
but use `radixSortKVInplace` for `32 < n <= 4096`. Bitonic sort is
also unstable, which forces stable sorts fall back to which is up to
4x slower in this small regime.

This PR adds a new kernel `warpMergeSortKVInplace` using
`cub::WarpMergeSort` to implement sorts with `32 < n <= 128` and all
stable sorts with `n < 128`. This results in up to a 2x speedup for
unstable sorts and up to 15x for stable sorts, depending on the input
geometry.

This also doesn't increase the total number of kernels since we are
replacing radix-sorts of size 32 and 128.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/96223
Approved by: https://github.com/ngimel"
pytorch/pytorch,3b54592050359b7ba16db7009321858976053313,"[PyTorch] Add annotation_str benchmark (#96496)

To be used to evaluate performance of following improvements. Baseline numbers:

https://gist.github.com/swolchok/c8bcb92be1dc6e67c4f7efad498becd5

Differential Revision: [D43919653](https://our.internmc.facebook.com/intern/diff/D43919653/)

**NOTE FOR REVIEWERS**: This PR has internal Meta-specific changes or comments, please review them on [Phabricator](https://our.internmc.facebook.com/intern/diff/D43919653/)!
Pull Request resolved: https://github.com/pytorch/pytorch/pull/96496
Approved by: https://github.com/Skylion007"
pytorch/pytorch,aab34a476f8e25d769046c26ad803867fb92bb24,"inductor(cpu): support mkldnn packed linear to improve bfloat16 performance (#96954)

As title, enable mkldnn packed linear to improve bfloat16 performance.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/96954
Approved by: https://github.com/EikanWang, https://github.com/jgong5, https://github.com/desertfire"
pytorch/pytorch,3282030fa4cce46f1714c570484a81f059a83615,"[inductor] reusing autotuning sub-processes (#97219)

The major cost of doing autotuning in sub process is process creating and initialization. Previously we do that for each benchmark task. This PR reuse a child process as long as it has not crashed yet. This improves compiling time a lot. It's still a bit slower than single process tuning though. Here are the comparison between single process tuning and multi-process tuning:
- if a benchmark task will crash the process, then single process tuning is a no-go
- if a benchmark task works fine, then tuning in child process will be slower. We will try to leveraging multi-GPU to further speed this up.

TLDR for the compilation time: we reduce the 11x slowdown to 1.5x. We'll try to further improve that.

Here are the compilation time comparison:

Single process autotuning:
```
AUTOTUNE ref_mm_plus_mm(2048x64, 64x1536, 2048x64, 64x1536)
  triton_mm_plus_mm_0 0.0276s 100.0%
  triton_mm_plus_mm_6 0.0287s 96.4%
  triton_mm_plus_mm_5 0.0307s 90.0%
  triton_mm_plus_mm_1 0.0317s 87.1%
  triton_mm_plus_mm_7 0.0379s 73.0%
  ref_mm_plus_mm 0.0389s 71.1%
  triton_mm_plus_mm_2 0.0399s 69.2%
  triton_mm_plus_mm_3 0.0410s 67.5%
  triton_mm_plus_mm_4 0.0410s 67.5%
SingleProcess AUTOTUNE takes 9.04686689376831 seconds
```

Naive multi process tuning (not reuse child process): 11x slower than single process autotuning

```
AUTOTUNE ref_mm_plus_mm(2048x64, 64x1536, 2048x64, 64x1536)
  triton_mm_plus_mm_0 0.0287s 100.0%
  triton_mm_plus_mm_6 0.0287s 100.0%
  triton_mm_plus_mm_1 0.0317s 90.3%
  triton_mm_plus_mm_5 0.0317s 90.3%
  triton_mm_plus_mm_7 0.0379s 75.7%
  ref_mm_plus_mm 0.0389s 73.7%
  triton_mm_plus_mm_2 0.0399s 71.8%
  triton_mm_plus_mm_3 0.0399s 71.8%
  triton_mm_plus_mm_4 0.0420s 68.3%
SubProcess AUTOTUNE takes 101.22216320037842 seconds
```

Multi process tuning reusing child process (this PR): 1.5x slower than single process autotuning
```
AUTOTUNE ref_mm_plus_mm(2048x64, 64x1536, 2048x64, 64x1536)
  triton_mm_plus_mm_0 0.0276s 100.0%
  triton_mm_plus_mm_6 0.0287s 96.4%
  triton_mm_plus_mm_5 0.0307s 90.0%
  triton_mm_plus_mm_1 0.0317s 87.1%
  triton_mm_plus_mm_7 0.0379s 73.0%
  ref_mm_plus_mm 0.0389s 71.1%
  triton_mm_plus_mm_2 0.0399s 69.2%
  triton_mm_plus_mm_3 0.0410s 67.5%
  triton_mm_plus_mm_4 0.0410s 67.5%
SubProcess AUTOTUNE takes 13.752070665359497 seconds
```

Pull Request resolved: https://github.com/pytorch/pytorch/pull/97219
Approved by: https://github.com/ngimel"
pytorch/pytorch,244736a5a58d3c27d20f143d257f99d20a24ed73,"Mark ROCm tests as flaky (#97259)

Before https://github.com/pytorch/pytorch/pull/96464, ROCm tests in trunk are already quite flaky https://hud.pytorch.org/reliability/pytorch/pytorch?jobName=trunk%20%2F%20linux-focal-rocm5.4.2-py3.8%20%2F%20test%20(default).

After https://github.com/pytorch/pytorch/pull/96464, there is a new group of flaky failures coming from functorch.  So let's mark the test as flaky to monitor without impacting trunk.

Two flaky tests currently seeing in trunk are:

* https://github.com/pytorch/pytorch/issues/97256
* `functorch/test_memory_efficient_fusion.py` OOM

Pull Request resolved: https://github.com/pytorch/pytorch/pull/97259
Approved by: https://github.com/malfet, https://github.com/zou3519"
pytorch/pytorch,cc0701e5b36874387fbcca1ebb948af0874f3ca0,"[inductor] Move fx-fusion tests to a separate file (#97028)

They're sort of independent of the rest of inductor, and this makes
them a bit easier to find and marginally faster to run.

Differential Revision: [D44168337](https://our.internmc.facebook.com/intern/diff/D44168337/)

**NOTE FOR REVIEWERS**: This PR has internal Meta-specific changes or comments, please review them on [Phabricator](https://our.internmc.facebook.com/intern/diff/D44168337/)!

Pull Request resolved: https://github.com/pytorch/pytorch/pull/97028
Approved by: https://github.com/jansel"
pytorch/pytorch,6b691b99daa32d0f0650b11d8dbb7d3a0059993c,"add amp support for custom backend (#96188)

Fixes #ISSUE_NUMBER
1、add amp support for custom backend
2、optimize the file `backend_registration.py`, and rename it with `custom_backend_registration.py`. And then we would register other funcs for custom backend.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/96188
Approved by: https://github.com/bdhirsh"
pytorch/pytorch,aacbf091dbc3132ee8fe9a89edc1e3bac979612e,"Allow fused optimizers to call _foreach_zero_ in zero_grad (#97159)

Fixes #97032

Pull Request resolved: https://github.com/pytorch/pytorch/pull/97159
Approved by: https://github.com/Skylion007"
pytorch/pytorch,bda9d7ba73fa6f8f1aa01189cc2b03d81601cb9e,"[pytorch][2/3] Pytorch profiler permits CPU events with CUPTI Range profiler mode (#97048)

Summary:
## Motivation
Initial version of CUPTI Range profile was conservative in turning of all other event types in kineto/pytorch profiler.
However, there is value in enabling CPU side activity logging. This let's us correlate the CPU operator -> GPU kernel statistics, helps us analyze flops/other performance metrics at the operator level.

## Details
1. Update pytorch profiler experimental configs parsing to allow setting CPU activities along with range profiler. Only enable on per kernel measurement mode.
1. Fixed Clang tidy issues (added nolint for 2 of them)

Test Plan: Testplan see bottom diff

Differential Revision: D44165079

Pull Request resolved: https://github.com/pytorch/pytorch/pull/97048
Approved by: https://github.com/aaronenyeshi"
pytorch/pytorch,5471621497ea0068bd453d251bf5ec2621e8119f,"[BE] Remove unnecessary dict comprehensions (#97116)

Removes unnecessary dict comprehensions that optimize creation of dicts from iterables

Pull Request resolved: https://github.com/pytorch/pytorch/pull/97116
Approved by: https://github.com/kit1980"
pytorch/pytorch,5ee5a164ffeb7b7a167c53009fb8fe5f5bd439d9,"[aot] disable inference view tracking (#96478)

For inference, we should disable unnecessary view tracking to save time. Most of operators get an improvement of performance (inductor v.s. eager). This PR fix the general regression of operators for inductor.

Example of operators' speedup in torchbench (inductor v.s. eager):
<html xmlns:v=""urn:schemas-microsoft-com:vml""
xmlns:o=""urn:schemas-microsoft-com:office:office""
xmlns:x=""urn:schemas-microsoft-com:office:excel""
xmlns=""http://www.w3.org/TR/REC-html40"">

<head>

<meta name=ProgId content=Excel.Sheet>
<meta name=Generator content=""Microsoft Excel 15"">
<link id=Main-File rel=Main-File
href=""file:///C:/Users/xuanliao/AppData/Local/Temp/msohtmlclip1/01/clip.htm"">
<link rel=File-List
href=""file:///C:/Users/xuanliao/AppData/Local/Temp/msohtmlclip1/01/clip_filelist.xml"">
</head>

<body link=""#0563C1"" vlink=""#954F72"">

  | current | new
-- | -- | --
aten.hardsigmoid.default | [0.6426090814905988, 0.6791992931354925, 0.7046010955095103] | [0.7921782106271767, 0.8919522525991529, 0.9128089963571694]
aten.tanh.default | [0.6135534976747065, 0.7588851221588919, 0.898274076411234] | [0.857534066531159, 1.0524121834821605, 1.2535141671420165]
aten.floor.default | [0.6115868728087821, 0.6115868728087821, 0.6115868728087821] | [0.9472870784346195, 0.9472870784346195, 0.9472870784346195]
aten.exp.default | [0.7784016216625718, 0.9279358274876591, 1.1201178548406794] | [0.5777145055206203, 0.8610140436473923, 1.1850714193498957]
aten.mul_.Tensor | [0.14381872531802153, 0.14638969818507447,   0.14947766446663138] | [0.37695307573466363, 0.3832122689450142, 0.38963470437456904]
aten.hardtanh_.default | [0.49502896822398157, 0.5897512505705527, 0.8052969399847189] | [0.4915338157706071, 0.6098169585316151, 0.8587605051115021]
aten.relu_.default | [0.47776870021339685, 0.54452322796367, 0.6516167164223963] | [0.4764791289773786, 0.5608095328163419, 0.6753350976452626]

</body>

</html>

Pull Request resolved: https://github.com/pytorch/pytorch/pull/96478
Approved by: https://github.com/EikanWang, https://github.com/jansel, https://github.com/jgong5, https://github.com/bdhirsh"
pytorch/pytorch,a5923ab3f38cc320d61c231c5fa8363fb8b6a5c7,"Revert ""[inductor] do benchmark in sub processes for max autotuning (#96410)"" (#97075)

This reverts commit 34256bc73080d7898138c821273b9f31fab777f8.

@kit1980: I'm not sure how best to revert a co-dev PR like https://github.com/pytorch/pytorch/pull/96410#issuecomment-1474704337.  IIRC, Ivan and Eli did a revert PR like this before, so I create one here just in case we need to use it.  If that's the case, please feel free to get this merge to fix trunk.  Otherwise, this can be closed.

@shunting314 If you can do a forward fix faster than this, please help do so.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/97075
Approved by: https://github.com/kit1980"
pytorch/pytorch,34256bc73080d7898138c821273b9f31fab777f8,"[inductor] do benchmark in sub processes for max autotuning (#96410)

This PR implements the support to benchmark max-autotune choices in subprocesses. This way crash like https://github.com/openai/triton/issues/1298 will only abort the autotuning child process but the parent process can continue.

There are a few things to note:
- cuda runtime does not work with fork. So we have to use spawn to create child processes. Check the best practice from pytorch multithreading module: https://pytorch.org/docs/stable/notes/multiprocessing.html
- to run a job in a child process, the multiprocessing module needs to pickle both the target function and arguments and pass them to child process. This is the major complexity of this prototype since there are quite a lot of corner cases making pickle fail.

Here I list the pickle related issues I encountered:
- pickle a StorageBox cause infinite recursion. Error: https://gist.github.com/171e5ab404b7855dee2dfa1d9f093442 . Work around by pickle the inner buffer.
- IRNode store fx.Node's in its origin fields. However, we can not pickle a fx.Node. It fails when with the following error when picking the fx.Node.graph: https://gist.github.com/9c289e895d7091d7ec787c67bc3c0d70. Work around by skip origins when pickling a IRNode.
- jinja Template in TritonTemplateKernel can not be pickled: `TypeError: Template.__new__() missing 1 required positional argument: 'source' `. Workaround by pickle the source rather than jinjia Template. During unpickling, rebuild the jinja template.
- due to how select_algorithm.template_kernels is populated, in child process, it's empty. Work around by passing select_algorithm.template_kernels from parent process to child process directly.
  - There is some change in TritonTemplate.generate to make a TritonTemplateKernel pickle'able. A TritonTemplate is refered to in the closure for a TritonTemplateKernel object.
- We can not pass choice to child process directly because of pickle failure for lambda/local function being used. However cloudpickle can handle lambda. Work around by passing the cloudpickle'd choice object to child process. The child project need to unpickle it explictly.

Test:
```
python test/inductor/test_max_autotune.py -k test_max_autotune_mm_plus_mm
```
This is basically the repro I get from Bert Maher.

Benchmark in sub process is about 4x slower than benchmark in the same process. Without doing any profiling, I feel the time may be cost by starting a new process and doing initialization. Some ~thread~ process pool may help.

```
AUTOTUNE ref_mm_plus_mm(2048x64, 64x1536, 2048x64, 64x1536)
  triton_mm_plus_mm_0 0.0276s 100.0%
  triton_mm_plus_mm_6 0.0287s 96.4%
  triton_mm_plus_mm_5 0.0317s 87.1%
  triton_mm_plus_mm_1 0.0328s 84.4%
  ref_mm_plus_mm 0.0379s 73.0%
  triton_mm_plus_mm_7 0.0379s 73.0%
  triton_mm_plus_mm_2 0.0399s 69.2%
  triton_mm_plus_mm_3 0.0410s 67.5%
  triton_mm_plus_mm_4 0.0410s 67.5%
AUTOTUNE takes 12.001659393310547 seconds

AUTOTUNE ref_mm_plus_mm(2048x64, 64x1536, 2048x64, 64x1536)
  triton_mm_plus_mm_0 0.0276s 100.0%
  triton_mm_plus_mm_6 0.0287s 96.4%
  triton_mm_plus_mm_1 0.0317s 87.1%
  triton_mm_plus_mm_5 0.0317s 87.1%
  ref_mm_plus_mm 0.0379s 73.0%
  triton_mm_plus_mm_7 0.0389s 71.1%
  triton_mm_plus_mm_2 0.0399s 69.2%
  triton_mm_plus_mm_3 0.0410s 67.5%
  triton_mm_plus_mm_4 0.0410s 67.5%
AUTOTUNE takes 51.39659810066223 seconds
```

The feature is disabled by default and can be enabled by setting the following config or envvar:
```
autotune_in_subproc = os.environ.get(""TORCHINDUCTOR_AUTOTUNE_IN_SUBPROC"") == ""1""
```

Differential Revision: [D43996048](https://our.internmc.facebook.com/intern/diff/D43996048)
Pull Request resolved: https://github.com/pytorch/pytorch/pull/96410
Approved by: https://github.com/jansel"
pytorch/pytorch,dd9ade6377ef480ad0c60d4dce2f9cfbfeee98e9,"Remove unnecessary items() call in zero_grad (#97040)

Micro-optimization to zero_grad() which is performance critical
Pull Request resolved: https://github.com/pytorch/pytorch/pull/97040
Approved by: https://github.com/ezyang, https://github.com/albanD"
pytorch/pytorch,98a5cf090d6b460afd1b43e47cd52404e8e07c91,"[SDPA] Remove the chunk_grad from mem-eff attention (#96880)

# Summary

There exists an optimization within the scaled_dot_product_efficieint bacwkard attention path to, under the right conditions, output grad_q, grad_k, grad_v all as aliases of the same storage. This was done to optimize for the hot path where mha does packed linear_projection -> chunk -> (view stuff) -> sdpa. The thought was that chunk-> would be able to ""trivially"" cat inputs to chunk.backward(). However upon closer inspection chunk.backward will call ` cat` irregardless of the inputs so this is not being utilized.

I validated this by profiling on main and then this branch and the traces produced the same both with `split.backward()` calling into cat.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/96880
Approved by: https://github.com/cpuhrsch"
pytorch/pytorch,d4b8ed2b1145c9ac5a569abb4c645894ef79dc1a,"Fail fast when dynamo attempts to add unspecialized int/float as additional graph inputs (#96786)

Summary:
Verified the changes to catch unspecialized int/floats being added as additional graph in D44037548 prior to RP(https://github.com/pytorch/pytorch/pull/95621).

However, with #95621 the issue to be solved originally is no longer valid because int & float in `forward` will always be specialized in export. This RP is to add the assertion anyway *(though not be hit unless there is a regression)* to immediately catch the attempt to add unspecialized int/float to additional graphargs

Test Plan:
Example of the error message would look like:
```
Dynamo attempts to add additional input: value=9.999999747378752e-06, source=NNModuleSource(inner=AttrSource(base=NNModuleSource(inner=AttrSource(base=LocalInputSource(local_name='self', pos=0), member='torch_module')), member='eps'))
```
Passed all export tests
```
Buck UI: https://www.internalfb.com/buck2/fea72653-5549-47e7-a9bf-740eb86a8e26
Test UI: https://www.internalfb.com/intern/testinfra/testrun/8725724422167257
RE: reSessionID-7b3470b1-c293-4c4a-9671-dd0b7a2839b8  Up: 6.0 KiB  Down: 0 B
Jobs completed: 101. Time elapsed: 115.7s.
Tests finished: Pass 98. Fail 0. Fatal 0. Skip 0. 0 builds failed
```

Differential Revision: D44075910

Pull Request resolved: https://github.com/pytorch/pytorch/pull/96786
Approved by: https://github.com/tugsbayasgalan, https://github.com/ezyang"
pytorch/pytorch,92eb9d363a7110a43746cc3e704a393837d5047c,"Decoder native functions join the dead code society (#96025)

Summary: Decoder native joins the dead code society

With the recent introduction of PT2, we no longer need native decoder operators:
1 - full-function SDPA kernels can be used to implement cross-attention efficiently without the (slower) decoder MHA blob.
2 - torch.compile() generates more efficient code across many platforms from the python implementation of decoders than the decoder layer blob by tailoring code to target

Test Plan: github & sandcastle

Differential Revision: D43811808

Pull Request resolved: https://github.com/pytorch/pytorch/pull/96025
Approved by: https://github.com/ezyang, https://github.com/albanD"
pytorch/pytorch,cf732053e4f6b93b0a93006613552cd97f415b80,"nn.EmbeddingBag bound check (#96022)

Summary: Today if we're accessing out of bound embedding rows, it'll either go through or throw IMA. This is not ideal - adding bound checks. This will probably slow things down - need to benchmark it.

Test Plan:
TODO: add some tests

Tried a simple example and it's showing this:
```
aten/src/ATen/native/cuda/EmbeddingBag.cu:143: EmbeddingBag_updateOutputKernel_sum_mean: block: [0,0,0], thread: [0,1,0] Assertion `input[emb] < numRows` failed.
```

Differential Revision: D43810777

Pull Request resolved: https://github.com/pytorch/pytorch/pull/96022
Approved by: https://github.com/cpuhrsch, https://github.com/ngimel"
pytorch/pytorch,8c2341c1b9b2322dc707e9da1bfab7d4ebb232fa,"Remove pytest block list (#96698)

Enables the last few files under pytest.

xdist was causing problems with `profiler/test_profiler` `test_source_multithreaded` due to creating extra threads.  Luckily we don't use it so we can disable it with `-p no:xdist`, but this is incompatible with pytest-rerunfailures==10.2, so upgrade to 10.3.  I'd update the windows ami but idk how.

`dynamo/test_optimizers` and `dynamo/test_repros` both had tests that used skip_if_pytest.  https://github.com/pytorch/pytorch/pull/93251/files suggests that it is due to pytest assertion rewriting, so I added `PYTEST_DONT_REWRITE` to their module docstrings to prevent pytest from rewriting assertions.

Disable test by issue in `dynamo/test_dynamic_shapes` seems sane.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/96698
Approved by: https://github.com/huydhn, https://github.com/malfet"
pytorch/pytorch,71adb32ddc9fbae458726430ac2c4f01dece3285,"[DDP] API to get data parallel parameters (#95097)

Add a private API to retrieve data parallel parameters. This is
useful for example for apply_optimizer_in_backward in the case user wishes to
ensure it is applied only on DDP managed parameters.

Differential Revision: [D43383878](https://our.internmc.facebook.com/intern/diff/D43383878/)
Pull Request resolved: https://github.com/pytorch/pytorch/pull/95097
Approved by: https://github.com/zhaojuanmao, https://github.com/fegin"
pytorch/pytorch,3ce9aac7866e238cd39d2905b863e9987288d30b,"Add environment variable to force flattening of 3D input tensor (#96761)

Adding an environment variable `TORCH_LINEAR_FLATTEN_3D` to force flattening of 3D input tensor even when it is non-contiguous.

Today, the `Linear` op would flatten a 3D input sensor if it is contiguous.

It was found that even for some non-contiguous inputs (esp. with BF16 data type), flattening would also yield higher performance.
For example:
```
x_size = (3072, 1196, 128)
x = torch.rand(x_size, device=""cuda"", dtype=torch.bfloat16)
x = torch.transpose(x, 1, 2)
torch._C._nn.linear(x, weight, bias)
```

Since the detailed auto-tuning is unknown, this PR adds an environment variable for users to make a choice.
(Default value is still 0.)
Pull Request resolved: https://github.com/pytorch/pytorch/pull/96761
Approved by: https://github.com/ngimel"
pytorch/pytorch,54cd4a67d0205d47d8d862b178388a6d3d21369e,"Output peak memory stats from dynamo torchbench perf CI (#95666)

Adds absolute memory usage numbers (in addition to compression ratio) to performance jobs.

Example output:
<img width=""1211"" alt=""image"" src=""https://user-images.githubusercontent.com/4984825/225419950-500908c5-00ce-4711-afa2-c995bf90d35d.png"">

Pull Request resolved: https://github.com/pytorch/pytorch/pull/95666
Approved by: https://github.com/ezyang, https://github.com/williamwen42"
pytorch/pytorch,e9d9151eecd3871aa15c48e3f1fa25c6faee9a3a,"[aot autograd] avoid cloning some inputs unnecessarily when they dont require grad (#96342)

When constructing the joint graph, we normally have to clone any inputs that are mutated, so that we can pass in the original, pre-mutation inputs as leaves to autograd.

Previously, we were doing this for all mutated inputs - but we only need to do it for inputs that require gradients and participate in autograd.

Hopefully this should speed up code like batch norm - I think before this we were unnecessarily cloning the running stats during training.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/96342
Approved by: https://github.com/albanD, https://github.com/ezyang"
pytorch/pytorch,a2694699821be04e6a74760ba754911e714a5486,"aot autograd refactor: make all synthetic base logic layered in a single location (#96235)

This  refactor doesn't significantly change LoC in aot autograd, but I think this nets out to making it clearer (interested in peoples' thoughts).

The idea is that I tried to re-write the part of aot autograd that deals with synthetic bases in a layered way, similar to how Ed wrote the logic for dedup'ing inputs: it happens in one place, and all of the downstream transformation in aot autograd don't have to worry about it.

Specifically, I added a new function `aot_wrapper_synthetic_base`, similar to the existing `aot_wrapper_dedupe`.

The benefit: none of the other code in aot autograd needs to think about synthetic bases (previously, synthetic base code was intertwined in several places).

The downsides: there are two.

(1) `aot_wrapper_synthetic_base()` needs to have its own epilogue. There is one particularly hairy case, where factoring the synthetic base logic to a single location was painful: If you have two inputs that alias each other, where one gets a data mutation, and the other gets a metadata mutation.

Ordinarily, metadata mutations are handled by the runtime epilogue, in `create_runtime_wrapper`. However, now that things are factored this way, the runtime wrapper operates only on synthetic bases instead of operating on the original inputs. For data mutations, it is fine to apply the data mutation to the synthetic base instead of the original input alias. But for metadata mutations, we **need** to apply the metadata mutation directly to the original inputs.

The way that I handled this was by tracking which inputs slot into this specific case (part of a synthetic base, and get metadata mutations), and updateing the flat_fn() that we pass downstream to return these updated inputs as extra outputs. From the perspective of downstream logic, these are real user outputs, that it can treat like any other user outputs. `aot_wrapper_synthetic_base` will know to grab these extra outputs and use them to apply the metadata mutations.

This was pretty annoying, but has the benefit that all of that logic is encapsulated entirely in `aot_wrapper_synthetic_base()`.

(2) input mutations are now performed on the synthetic base instead of the individual aliases.

You can see the original code comment [here](https://github.com/pytorch/pytorch/blob/b0b5f3c6c681896febbd9ff7ad7649b13def345d/torch/_functorch/aot_autograd.py#L1131) for details. We used to do the optimized thing in this case, and now we do the less optimized thing (copying the entire synthetic base, instead of the potentially smaller alias).

To be fair, we had no data showing that this optimization was showing improvements on any models in practice. I also think that the main reason anyone would ever run across this problem is because of a graph break - so if you care about perf, you probably want to avoid the extra graph breaks to begin with. I haven't added any warnings for this, but we probably could depending on what people think.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/96235
Approved by: https://github.com/ezyang"
pytorch/pytorch,7a076b7b934e8ef20a9e225dc946333adfa370cf,"[aot_autograd] only performance functionalization analysis pass once (#95992)

For a while now, we've been re-running our functionalization analysis pass twice - once for get metadata when dedup'ing, and an entire second time during aot_dispatch_base/autograd.

This should also probably speed up compile times pretty noticeably, since we're going from:

(a) inference-only trace case: 3 fw traces -> 2 fw traces
(b) autograd trace case: 2 fw traces + 1 joint trace -> 1 fw trace + 1 joint trace

Pull Request resolved: https://github.com/pytorch/pytorch/pull/95992
Approved by: https://github.com/ezyang"
pytorch/pytorch,3fd24fb608a04db704bcf10f8f5624c8983cc612,"COO intersection: allow external hash + hash reuse in sparse_mask (#94596)

External hash implies more flexibility in the op coverage + performance improvement.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/94596
Approved by: https://github.com/cpuhrsch"
pytorch/pytorch,6110effa86e3b1990540e79ab78fc209528ecce3,"Rework torch.compile docs (#96706)

Chatted with @stas00 on slack and here are some great improvements he suggested to the compile docs

- [x] Rename `dynamo` folder to `compile`
- [x] Link `compile` docstring on `torch.html` to main index page for compile
- [x] Create a new index page that describes why people should care
  - [x] easy perf, memory reduction, 1 line
  - [x] Short benchmark table
  - [x] How to guide
  - [x] TOC that links to the more technical pages folks have written, make the existing docs we have a Technical overview
- [x] Highlight the new APIs for `torch._inductor.list_options()` and `torch._inductor.list_mode_options()` - clarify these are inductor specific and add more prose around which ones are most interesting

He also highlighted an interesting way to think about who is reading this doc we have

- [x] End users, that just want things to run fast
- [x] Library maintainers wrapping torch.compile which would care for example about understanding when in their code they should compile a model, which backends are supported
- [x] Debuggers who needs are somewhat addressed by the troubleshooting guide and faq but those could be dramatically reworked to say what we expect to break

And in a seperate PR I'll work on the below with @SherlockNoMad
- [ ] Authors of new backends that care about how to plug into dynamo or inductor layer so need to explain some more internals like
  - [ ] IR
  - [ ] Where to plugin, dynamo? inductor? triton?

Pull Request resolved: https://github.com/pytorch/pytorch/pull/96706
Approved by: https://github.com/svekars"
pytorch/pytorch,e01b09270512cf9f9ab250ed7a6559f4c7bbaa8a,"inductor: don't remember pre-loop order if pre loop has loop collapse (#96640)

Given the following case from timm **ese_vovnet19b_dw**:

```
import torch
import torch._dynamo

class Model(torch.nn.Module):
    def __init__(self):
        super(Model, self).__init__()
        self.conv1 = torch.nn.Conv2d(256, 256, kernel_size=1, padding=0)
        self.conv2 = torch.nn.Conv2d(256, 256, kernel_size=1, padding=0)
        self.pool = torch.nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True)

    def forward(self, x):
        x = self.conv1(x)
        x2 = self.conv2(x)
        y = x2 * x
        return self.pool(y)

model = Model().to(memory_format=torch.channels_last).eval()
x = torch.randn(128, 256, 56, 56).to(memory_format=torch.channels_last)
opt_model = torch._dynamo.optimize('inductor')(model)
with torch.no_grad():
    for i in range(2):
        y = opt_model(x
```

before this PR, the max_pooling can't be vectorized:

```
extern ""C"" void kernel(float* in_out_ptr0,
                       const float* in_ptr0,
                       float* out_ptr0)
{
    #pragma omp parallel num_threads(40)
    {
        {
            #pragma omp for
            for(long i0=0; i0<6422528; i0+=1)
            {
                auto tmp0 = at::vec::Vectorized<float>::loadu(in_ptr0 + 16*i0);
                auto tmp1 = at::vec::Vectorized<float>::loadu(in_out_ptr0 + 16*i0);
                auto tmp2 = tmp0 * tmp1;
                tmp2.store(in_out_ptr0 + 16*i0);
            }
            #pragma omp for simd simdlen(8)
            for(long i0=102760448; i0<102760448; i0+=1)
            {
                auto tmp0 = in_ptr0[i0];
                auto tmp1 = in_out_ptr0[i0];
                auto tmp2 = tmp0 * tmp1;
                in_out_ptr0[i0] = tmp2;
            }
        }
        {
            #pragma omp for
            for(long i0=0; i0<128; i0+=1)
            {
                #pragma GCC ivdep
                for(long i1=0; i1<256; i1+=1)
                {
                    #pragma GCC ivdep
                    for(long i2=0; i2<28; i2+=1)
                    {
                        #pragma GCC ivdep
                        for(long i3=0; i3<28; i3+=1)
                        {
                            auto tmp0 = static_cast<long>(2*i2);
                            auto tmp1 = static_cast<long>(0);
                            auto tmp2 = tmp0 >= tmp1;
                            auto tmp3 = static_cast<long>(56);
                            auto tmp4 = tmp0 < tmp3;
                            auto tmp5 = tmp2 & tmp4;
                            auto tmp6 = static_cast<long>(2*i3);
                            auto tmp7 = tmp6 >= tmp1;
                            auto tmp8 = tmp6 < tmp3;
                            auto tmp9 = tmp7 & tmp8;
                            auto tmp10 = tmp5 & tmp9;
                            auto tmp11 = [&]
                            {
                                auto tmp12 = in_out_ptr0[i1 + (512*i3) + (28672*i2) + (802816*i0)];
                                return tmp12;
                            }
                            ;
                            auto tmp13 = tmp10 ? tmp11() : -std::numeric_limits<decltype(tmp11())>::infinity();
                            auto tmp14 = static_cast<long>(1 + (2*i3));
                            auto tmp15 = tmp14 >= tmp1;
                            auto tmp16 = tmp14 < tmp3;
                            auto tmp17 = tmp15 & tmp16;
                            auto tmp18 = tmp5 & tmp17;
                            auto tmp19 = [&]
                            {
                                auto tmp20 = in_out_ptr0[256 + i1 + (512*i3) + (28672*i2) + (802816*i0)];
                                return tmp20;
                            }
                            ;
                            auto tmp21 = tmp18 ? tmp19() : -std::numeric_limits<decltype(tmp19())>::infinity();
                            auto tmp22 = (tmp13 != tmp13) ? tmp13 : std::max(tmp21, tmp13);
                            auto tmp23 = static_cast<long>(2 + (2*i3));
                            auto tmp24 = tmp23 >= tmp1;
                            auto tmp25 = tmp23 < tmp3;
                            auto tmp26 = tmp24 & tmp25;
                            auto tmp27 = tmp5 & tmp26;
                            auto tmp28 = [&]
                            {
                                auto tmp29 = in_out_ptr0[512 + i1 + (512*i3) + (28672*i2) + (802816*i0)];
                                return tmp29;
                            }
                            ;
                            auto tmp30 = tmp27 ? tmp28() : -std::numeric_limits<decltype(tmp28())>::infinity();
                            auto tmp31 = (tmp22 != tmp22) ? tmp22 : std::max(tmp30, tmp22);
                            auto tmp32 = static_cast<long>(1 + (2*i2));
                            auto tmp33 = tmp32 >= tmp1;
                            auto tmp34 = tmp32 < tmp3;
                            auto tmp35 = tmp33 & tmp34;
                            auto tmp36 = tmp35 & tmp9;
                            auto tmp37 = [&]
                            {
                                auto tmp38 = in_out_ptr0[14336 + i1 + (512*i3) + (28672*i2) + (802816*i0)];
                                return tmp38;
                            }
                            ;
                            auto tmp39 = tmp36 ? tmp37() : -std::numeric_limits<decltype(tmp37())>::infinity();
                            auto tmp40 = (tmp31 != tmp31) ? tmp31 : std::max(tmp39, tmp31);
                            auto tmp41 = tmp35 & tmp17;
                            auto tmp42 = [&]
                            {
                                auto tmp43 = in_out_ptr0[14592 + i1 + (512*i3) + (28672*i2) + (802816*i0)];
                                return tmp43;
                            }
                            ;
                            auto tmp44 = tmp41 ? tmp42() : -std::numeric_limits<decltype(tmp42())>::infinity();
                            auto tmp45 = (tmp40 != tmp40) ? tmp40 : std::max(tmp44, tmp40);
                            auto tmp46 = tmp35 & tmp26;
                            auto tmp47 = [&]
                            {
                                auto tmp48 = in_out_ptr0[14848 + i1 + (512*i3) + (28672*i2) + (802816*i0)];
                                return tmp48;
                            }
                            ;
                            auto tmp49 = tmp46 ? tmp47() : -std::numeric_limits<decltype(tmp47())>::infinity();
                            auto tmp50 = (tmp45 != tmp45) ? tmp45 : std::max(tmp49, tmp45);
                            auto tmp51 = static_cast<long>(2 + (2*i2));
                            auto tmp52 = tmp51 >= tmp1;
                            auto tmp53 = tmp51 < tmp3;
                            auto tmp54 = tmp52 & tmp53;
                            auto tmp55 = tmp54 & tmp9;
                            auto tmp56 = [&]
                            {
                                auto tmp57 = in_out_ptr0[28672 + i1 + (512*i3) + (28672*i2) + (802816*i0)];
                                return tmp57;
                            }
                            ;
                            auto tmp58 = tmp55 ? tmp56() : -std::numeric_limits<decltype(tmp56())>::infinity();
                            auto tmp59 = (tmp50 != tmp50) ? tmp50 : std::max(tmp58, tmp50);
                            auto tmp60 = tmp54 & tmp17;
                            auto tmp61 = [&]
                            {
                                auto tmp62 = in_out_ptr0[28928 + i1 + (512*i3) + (28672*i2) + (802816*i0)];
                                return tmp62;
                            }
                            ;
                            auto tmp63 = tmp60 ? tmp61() : -std::numeric_limits<decltype(tmp61())>::infinity();
                            auto tmp64 = (tmp59 != tmp59) ? tmp59 : std::max(tmp63, tmp59);
                            auto tmp65 = tmp54 & tmp26;
                            auto tmp66 = [&]
                            {
                                auto tmp67 = in_out_ptr0[29184 + i1 + (512*i3) + (28672*i2) + (802816*i0)];
                                return tmp67;
                            }
                            ;
                            auto tmp68 = tmp65 ? tmp66() : -std::numeric_limits<decltype(tmp66())>::infinity();
                            auto tmp69 = (tmp64 != tmp64) ? tmp64 : std::max(tmp68, tmp64);
                            out_ptr0[i1 + (256*i3) + (7168*i2) + (200704*i0)] = tmp69;
                        }
                    }
                }
            }
        }
    }
}
''')
```

We always avoid reordering when pre-loop has a loop collapse: https://github.com/pytorch/pytorch/blob/2cbce06feebf5f52ef5539a3b1ae2e003217b6ac/torch/_inductor/ir.py#L2267-L2273.

This PR adds a check that only reuses pre-loop ordering when not having loop collapse.

After this PR, the generated code is
```

extern ""C"" void kernel(float* in_out_ptr0,
                       const float* in_ptr0,
                       float* out_ptr0)
{
    #pragma omp parallel num_threads(40)
    {
        {
            #pragma omp for
            for(long i0=0; i0<6422528; i0+=1)
            {
                auto tmp0 = at::vec::Vectorized<float>::loadu(in_ptr0 + 16*i0);
                auto tmp1 = at::vec::Vectorized<float>::loadu(in_out_ptr0 + 16*i0);
                auto tmp2 = tmp0 * tmp1;
                tmp2.store(in_out_ptr0 + 16*i0);
            }
            #pragma omp for simd simdlen(8)
            for(long i0=102760448; i0<102760448; i0+=1)
            {
                auto tmp0 = in_ptr0[i0];
                auto tmp1 = in_out_ptr0[i0];
                auto tmp2 = tmp0 * tmp1;
                in_out_ptr0[i0] = tmp2;
            }
        }
        {
            #pragma omp for
            for(long i0=0; i0<128; i0+=1)
            {
                #pragma GCC ivdep
                for(long i1=0; i1<28; i1+=1)
                {
                    #pragma GCC ivdep
                    for(long i2=0; i2<28; i2+=1)
                    {
                        for(long i3=0; i3<16; i3+=1)
                        {
                            auto tmp0 = at::vec::Vectorized<int>(static_cast<int>(2*i1));
                            auto tmp1 = at::vec::Vectorized<int>(static_cast<int>(0));
                            auto tmp2 = tmp0 >= tmp1;
                            auto tmp3 = at::vec::Vectorized<int>(static_cast<int>(56));
                            auto tmp4 = tmp0 < tmp3;
                            auto tmp5 = tmp2 & tmp4;
                            auto tmp6 = at::vec::Vectorized<int>(static_cast<int>(2*i2));
                            auto tmp7 = tmp6 >= tmp1;
                            auto tmp8 = tmp6 < tmp3;
                            auto tmp9 = tmp7 & tmp8;
                            auto tmp10 = tmp5 & tmp9;
                            auto tmp11 = [&]
                            {
                                auto tmp12 = at::vec::Vectorized<float>::loadu(in_out_ptr0 + (16*i3) + (512*i2) + (28672*i1) + (802816*i0));
                                return tmp12;
                            }
                            ;
                            auto tmp13 = decltype(tmp11())::blendv(at::vec::Vectorized<float>(-std::numeric_limits<float>::infinity()), tmp11(), to_float_mask(tmp10) != at::vec::Vectorized<float>(0));
                            auto tmp14 = at::vec::Vectorized<int>(static_cast<int>(1 + (2*i2)));
                            auto tmp15 = tmp14 >= tmp1;
                            auto tmp16 = tmp14 < tmp3;
                            auto tmp17 = tmp15 & tmp16;
                            auto tmp18 = tmp5 & tmp17;
                            auto tmp19 = [&]
                            {
                                auto tmp20 = at::vec::Vectorized<float>::loadu(in_out_ptr0 + 256 + (16*i3) + (512*i2) + (28672*i1) + (802816*i0));
                                return tmp20;
                            }
                            ;
                            auto tmp21 = decltype(tmp19())::blendv(at::vec::Vectorized<float>(-std::numeric_limits<float>::infinity()), tmp19(), to_float_mask(tmp18) != at::vec::Vectorized<float>(0));
                            auto tmp22 = at::vec::maximum(tmp21, tmp13);
                            auto tmp23 = at::vec::Vectorized<int>(static_cast<int>(2 + (2*i2)));
                            auto tmp24 = tmp23 >= tmp1;
                            auto tmp25 = tmp23 < tmp3;
                            auto tmp26 = tmp24 & tmp25;
                            auto tmp27 = tmp5 & tmp26;
                            auto tmp28 = [&]
                            {
                                auto tmp29 = at::vec::Vectorized<float>::loadu(in_out_ptr0 + 512 + (16*i3) + (512*i2) + (28672*i1) + (802816*i0));
                                return tmp29;
                            }
                            ;
                            auto tmp30 = decltype(tmp28())::blendv(at::vec::Vectorized<float>(-std::numeric_limits<float>::infinity()), tmp28(), to_float_mask(tmp27) != at::vec::Vectorized<float>(0));
                            auto tmp31 = at::vec::maximum(tmp30, tmp22);
                            auto tmp32 = at::vec::Vectorized<int>(static_cast<int>(1 + (2*i1)));
                            auto tmp33 = tmp32 >= tmp1;
                            auto tmp34 = tmp32 < tmp3;
                            auto tmp35 = tmp33 & tmp34;
                            auto tmp36 = tmp35 & tmp9;
                            auto tmp37 = [&]
                            {
                                auto tmp38 = at::vec::Vectorized<float>::loadu(in_out_ptr0 + 14336 + (16*i3) + (512*i2) + (28672*i1) + (802816*i0));
                                return tmp38;
                            }
                            ;
                            auto tmp39 = decltype(tmp37())::blendv(at::vec::Vectorized<float>(-std::numeric_limits<float>::infinity()), tmp37(), to_float_mask(tmp36) != at::vec::Vectorized<float>(0));
                            auto tmp40 = at::vec::maximum(tmp39, tmp31);
                            auto tmp41 = tmp35 & tmp17;
                            auto tmp42 = [&]
                            {
                                auto tmp43 = at::vec::Vectorized<float>::loadu(in_out_ptr0 + 14592 + (16*i3) + (512*i2) + (28672*i1) + (802816*i0));
                                return tmp43;
                            }
                            ;
                            auto tmp44 = decltype(tmp42())::blendv(at::vec::Vectorized<float>(-std::numeric_limits<float>::infinity()), tmp42(), to_float_mask(tmp41) != at::vec::Vectorized<float>(0));
                            auto tmp45 = at::vec::maximum(tmp44, tmp40);
                            auto tmp46 = tmp35 & tmp26;
                            auto tmp47 = [&]
                            {
                                auto tmp48 = at::vec::Vectorized<float>::loadu(in_out_ptr0 + 14848 + (16*i3) + (512*i2) + (28672*i1) + (802816*i0));
                                return tmp48;
                            }
                            ;
                            auto tmp49 = decltype(tmp47())::blendv(at::vec::Vectorized<float>(-std::numeric_limits<float>::infinity()), tmp47(), to_float_mask(tmp46) != at::vec::Vectorized<float>(0));
                            auto tmp50 = at::vec::maximum(tmp49, tmp45);
                            auto tmp51 = at::vec::Vectorized<int>(static_cast<int>(2 + (2*i1)));
                            auto tmp52 = tmp51 >= tmp1;
                            auto tmp53 = tmp51 < tmp3;
                            auto tmp54 = tmp52 & tmp53;
                            auto tmp55 = tmp54 & tmp9;
                            auto tmp56 = [&]
                            {
                                auto tmp57 = at::vec::Vectorized<float>::loadu(in_out_ptr0 + 28672 + (16*i3) + (512*i2) + (28672*i1) + (802816*i0));
                                return tmp57;
                            }
                            ;
                            auto tmp58 = decltype(tmp56())::blendv(at::vec::Vectorized<float>(-std::numeric_limits<float>::infinity()), tmp56(), to_float_mask(tmp55) != at::vec::Vectorized<float>(0));
                            auto tmp59 = at::vec::maximum(tmp58, tmp50);
                            auto tmp60 = tmp54 & tmp17;
                            auto tmp61 = [&]
                            {
                                auto tmp62 = at::vec::Vectorized<float>::loadu(in_out_ptr0 + 28928 + (16*i3) + (512*i2) + (28672*i1) + (802816*i0));
                                return tmp62;
                            }
                            ;
                            auto tmp63 = decltype(tmp61())::blendv(at::vec::Vectorized<float>(-std::numeric_limits<float>::infinity()), tmp61(), to_float_mask(tmp60) != at::vec::Vectorized<float>(0));
                            auto tmp64 = at::vec::maximum(tmp63, tmp59);
                            auto tmp65 = tmp54 & tmp26;
                            auto tmp66 = [&]
                            {
                                auto tmp67 = at::vec::Vectorized<float>::loadu(in_out_ptr0 + 29184 + (16*i3) + (512*i2) + (28672*i1) + (802816*i0));
                                return tmp67;
                            }
                            ;
                            auto tmp68 = decltype(tmp66())::blendv(at::vec::Vectorized<float>(-std::numeric_limits<float>::infinity()), tmp66(), to_float_mask(tmp65) != at::vec::Vectorized<float>(0));
                            auto tmp69 = at::vec::maximum(tmp68, tmp64);
                            tmp69.store(out_ptr0 + (16*i3) + (256*i2) + (7168*i1) + (200704*i0));
                        }
                        #pragma omp simd simdlen(8)
                        for(long i3=256; i3<256; i3+=1)
                        {
                            auto tmp0 = static_cast<long>(2*i1);
                            auto tmp1 = static_cast<long>(0);
                            auto tmp2 = tmp0 >= tmp1;
                            auto tmp3 = static_cast<long>(56);
                            auto tmp4 = tmp0 < tmp3;
                            auto tmp5 = tmp2 & tmp4;
                            auto tmp6 = static_cast<long>(2*i2);
                            auto tmp7 = tmp6 >= tmp1;
                            auto tmp8 = tmp6 < tmp3;
                            auto tmp9 = tmp7 & tmp8;
                            auto tmp10 = tmp5 & tmp9;
                            auto tmp11 = [&]
                            {
                                auto tmp12 = in_out_ptr0[i3 + (512*i2) + (28672*i1) + (802816*i0)];
                                return tmp12;
                            }
                            ;
                            auto tmp13 = tmp10 ? tmp11() : -std::numeric_limits<decltype(tmp11())>::infinity();
                            auto tmp14 = static_cast<long>(1 + (2*i2));
                            auto tmp15 = tmp14 >= tmp1;
                            auto tmp16 = tmp14 < tmp3;
                            auto tmp17 = tmp15 & tmp16;
                            auto tmp18 = tmp5 & tmp17;
                            auto tmp19 = [&]
                            {
                                auto tmp20 = in_out_ptr0[256 + i3 + (512*i2) + (28672*i1) + (802816*i0)];
                                return tmp20;
                            }
                            ;
                            auto tmp21 = tmp18 ? tmp19() : -std::numeric_limits<decltype(tmp19())>::infinity();
                            auto tmp22 = (tmp13 != tmp13) ? tmp13 : std::max(tmp21, tmp13);
                            auto tmp23 = static_cast<long>(2 + (2*i2));
                            auto tmp24 = tmp23 >= tmp1;
                            auto tmp25 = tmp23 < tmp3;
                            auto tmp26 = tmp24 & tmp25;
                            auto tmp27 = tmp5 & tmp26;
                            auto tmp28 = [&]
                            {
                                auto tmp29 = in_out_ptr0[512 + i3 + (512*i2) + (28672*i1) + (802816*i0)];
                                return tmp29;
                            }
                            ;
                            auto tmp30 = tmp27 ? tmp28() : -std::numeric_limits<decltype(tmp28())>::infinity();
                            auto tmp31 = (tmp22 != tmp22) ? tmp22 : std::max(tmp30, tmp22);
                            auto tmp32 = static_cast<long>(1 + (2*i1));
                            auto tmp33 = tmp32 >= tmp1;
                            auto tmp34 = tmp32 < tmp3;
                            auto tmp35 = tmp33 & tmp34;
                            auto tmp36 = tmp35 & tmp9;
                            auto tmp37 = [&]
                            {
                                auto tmp38 = in_out_ptr0[14336 + i3 + (512*i2) + (28672*i1) + (802816*i0)];
                                return tmp38;
                            }
                            ;
                            auto tmp39 = tmp36 ? tmp37() : -std::numeric_limits<decltype(tmp37())>::infinity();
                            auto tmp40 = (tmp31 != tmp31) ? tmp31 : std::max(tmp39, tmp31);
                            auto tmp41 = tmp35 & tmp17;
                            auto tmp42 = [&]
                            {
                                auto tmp43 = in_out_ptr0[14592 + i3 + (512*i2) + (28672*i1) + (802816*i0)];
                                return tmp43;
                            }
                            ;
                            auto tmp44 = tmp41 ? tmp42() : -std::numeric_limits<decltype(tmp42())>::infinity();
                            auto tmp45 = (tmp40 != tmp40) ? tmp40 : std::max(tmp44, tmp40);
                            auto tmp46 = tmp35 & tmp26;
                            auto tmp47 = [&]
                            {
                                auto tmp48 = in_out_ptr0[14848 + i3 + (512*i2) + (28672*i1) + (802816*i0)];
                                return tmp48;
                            }
                            ;
                            auto tmp49 = tmp46 ? tmp47() : -std::numeric_limits<decltype(tmp47())>::infinity();
                            auto tmp50 = (tmp45 != tmp45) ? tmp45 : std::max(tmp49, tmp45);
                            auto tmp51 = static_cast<long>(2 + (2*i1));
                            auto tmp52 = tmp51 >= tmp1;
                            auto tmp53 = tmp51 < tmp3;
                            auto tmp54 = tmp52 & tmp53;
                            auto tmp55 = tmp54 & tmp9;
                            auto tmp56 = [&]
                            {
                                auto tmp57 = in_out_ptr0[28672 + i3 + (512*i2) + (28672*i1) + (802816*i0)];
                                return tmp57;
                            }
                            ;
                            auto tmp58 = tmp55 ? tmp56() : -std::numeric_limits<decltype(tmp56())>::infinity();
                            auto tmp59 = (tmp50 != tmp50) ? tmp50 : std::max(tmp58, tmp50);
                            auto tmp60 = tmp54 & tmp17;
                            auto tmp61 = [&]
                            {
                                auto tmp62 = in_out_ptr0[28928 + i3 + (512*i2) + (28672*i1) + (802816*i0)];
                                return tmp62;
                            }
                            ;
                            auto tmp63 = tmp60 ? tmp61() : -std::numeric_limits<decltype(tmp61())>::infinity();
                            auto tmp64 = (tmp59 != tmp59) ? tmp59 : std::max(tmp63, tmp59);
                            auto tmp65 = tmp54 & tmp26;
                            auto tmp66 = [&]
                            {
                                auto tmp67 = in_out_ptr0[29184 + i3 + (512*i2) + (28672*i1) + (802816*i0)];
                                return tmp67;
                            }
                            ;
                            auto tmp68 = tmp65 ? tmp66() : -std::numeric_limits<decltype(tmp66())>::infinity();
                            auto tmp69 = (tmp64 != tmp64) ? tmp64 : std::max(tmp68, tmp64);
                            out_ptr0[i3 + (256*i2) + (7168*i1) + (200704*i0)] = tmp69;
                        }
                    }
                }
            }
        }
    }
```

After this PR, we can get a **18%** performance improvement for timm **ese_vovnet19b_dw** on skx-4148(```python -m torch.backends.xeon.run_cpu --node_id 0 benchmarks/dynamo/timm_models.py --performance --float32 -dcpu -n50 --inductor  --channels-last --no-skip --dashboard --only ese_vovnet19b_dw```):

Pull Request resolved: https://github.com/pytorch/pytorch/pull/96640
Approved by: https://github.com/jgong5, https://github.com/EikanWang, https://github.com/jansel"
pytorch/pytorch,5396f85c91536a2717c2da60f133aef21e0584f2,"Propagate dynamo dynamic_shapes config to backwards (#96771)

This fixes

```
  File ""/data/users/ezyang/a/pytorch/torch/_inductor/codegen/triton.py"", line 1642, in codegen_node_schedule
    indexing_dtype_strength_reduction(node._body)
  File ""/data/users/ezyang/a/pytorch/torch/_inductor/optimize_indexing.py"", line 310, in indexing_dtype_strength_reduction
    OptimizeIndexing(loop_body, indices, indexing).run()
  File ""/data/users/ezyang/a/pytorch/torch/_inductor/optimize_indexing.py"", line 96, in __init__
    self.replace_indirect(k, ValueRanges(0, v))
  File ""/data/users/ezyang/a/pytorch/torch/utils/_sympy/value_ranges.py"", line 67, in __init__
    upper = simple_sympify(upper)
  File ""/data/users/ezyang/a/pytorch/torch/utils/_sympy/value_ranges.py"", line 33, in simple_sympify
    assert not e.free_symbols, f""free variables NYI: {e}""
AssertionError: free variables NYI: s0
```

Signed-off-by: Edward Z. Yang <ezyang@meta.com>

Pull Request resolved: https://github.com/pytorch/pytorch/pull/96771
Approved by: https://github.com/eellison"
pytorch/pytorch,dd5e6e85531c1b91b13551af722a866770aa273c,"[BE]: Merge startswith calls - rule PIE810 (#96754)

Merges startswith, endswith calls to into a single call that feeds in a tuple. Not only are these calls more readable, but it will be more efficient as it iterates through each string only once.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/96754
Approved by: https://github.com/ezyang"
pytorch/pytorch,2eed44933b5460623d135c2f453000b1d636c333,"Update perf smoke test threshold in check_hf_bert_perf_csv.py (#96772)

Reduce the threshold a little further due to runner to runner performance variations.  e.g. https://github.com/pytorch/pytorch/actions/runs/4419276220/jobs/7747985757  https://github.com/pytorch/pytorch/actions/runs/4419548525/jobs/7748553775  failed to meet 1.145 but were above 1.140.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/96772
Approved by: https://github.com/seemethere, https://github.com/huydhn, https://github.com/atalman"
pytorch/pytorch,cf12edee02a44009c4f06e36efa97d9a7372ab35,"add amp support for custom backend (#96188)

Fixes #ISSUE_NUMBER
1、add amp support for custom backend
2、optimize the file `backend_registration.py`, and rename it with `custom_backend_registration.py`. And then we would register other funcs for custom backend.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/96188
Approved by: https://github.com/bdhirsh"
pytorch/pytorch,190e284bd393fd2ddd0d71454d724b4a06942a24,"[Inductor] apply vec float mask on logical comparison ops in cpp (#96502)

Fix https://github.com/pytorch/pytorch/issues/96446
The root cause is that the logical comparison op works on the integer vector which is later used in the `where` op that expects a float vector.
1. Make sure float vec mask is applied on logical comparison ops.
2. Fix vec int specialization for `to_float_mask`. Assume int mask as input and returns the float mask with reinterpret cast.
3. Add a no-op specialization for `to_float_mask` function with the float vec as input.
4. Pass value instead of ref to `to_float_mask`. Passing by value should be efficient enough.
5. Remove a conditional check `!=0` in `masked()` since `to_float_mask` is guaranteed to return a float mask.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/96502
Approved by: https://github.com/EikanWang, https://github.com/XiaobingSuper, https://github.com/jansel"
pytorch/pytorch,62c1e33fc90183fb25199ef5e4f80ac867603fa2,"[BE] Remove fast_nvcc tool (#96665)

As of CUDA-11.4+ this functionality can be mimicked by passing
[`--threads`](https://docs.nvidia.com/cuda/cuda-compiler-driver-nvcc/#threads-number-t) option to CUDA compiler

Fixes #ISSUE_NUMBER

Pull Request resolved: https://github.com/pytorch/pytorch/pull/96665
Approved by: https://github.com/atalman, https://github.com/PaliC"
pytorch/pytorch,f5a0b31a956e7d77fa7eee13d206dfe84e125a48,"[FSDP][optim_state_dict] Make FSDP optim_state_dict aware of DDP prefix (#96415)

Summary: When wrapping FSDP within DDP, optimizer state_dict may be broken due to the prefix of DDP. This PR fixes the issue.

Test Plan: CI

Differential Revision: D43893609

Pull Request resolved: https://github.com/pytorch/pytorch/pull/96415
Approved by: https://github.com/zhaojuanmao"
pytorch/pytorch,32f11f58c971f4ae8b1e9071ad39cd9d00150875,"DDP native mixed precision (#92882)

Implements native mixed precision support for DDP in a similar fashion to how it is enabled for FSDP. The implementation works as follows:

1. In DDP init, we save `_mp_param` and `_fp_param` variables to manage mixed precision parameter usage. In particular, _mp_param will represent the parameter in the reduced precision, while _fp_param will represent the param in regular precision. During forward/backward, we swap back and forth as needed.
2. The root module gets a root pre-forward hook that kicks off copies to the reduced precision for all submodules. An event is recorded for each submodule to allow for waiting, as we run these asynchronously.
3. Each module gets a pre-forward hook that waits on its corresponding event. note that modules might be reused during training, in this case the wait is only done for the first module. After this wait, the module's parameters are in reduced precision.
4. In the pre-forward hook, we register a backward hook on the lower precision parameters in order to run reduced precision allreduce + parameter upcast. We can't rely on the Reducer's constructor setting up these hooks because the gradient is accumulated on the low precision param, so we need to register them ourselves.
5. In the backward pass, when the hook runs, we first run allreduce + divide in the reduced precision. Next, we upcast parameters and gradients back to fp32 asynchronously. We also queue a callback at the end of backward to wait on these upcasts so that the upcast is complete before optim.step() runs.
6. Parameters that don't require grad are also cast since they may be used in computation, they are upcast back in the final autograd callback.
7. DDP Ignored parameters are not touched.

Follow-ups:

1. Unify comm hooks and make it work with apply optimizer in backward
2. implement keep_low_precision_grads,
3. allow BN, LN, or custom units to run in reduced precision,
4. support for cast_forward_inputs
5. Unify certain APIs / helpers with FSDP where possible, such as for _cast_forward_inputs
6. Integrate this with replicate() API.
7. The order in which we kick off copies and wait for them is set by the iteration order of module.modules(), but this might not be how the modules are used in the actual training. In the worst case, the last module in module.modules() could be used first which would result in waiting for all copies unnecessarily. For static graphs, we should record the module execution order and copy / wait in this order.
8. Entirely unused modules probably don't need to be cast.

Differential Revision: [D42515803](https://our.internmc.facebook.com/intern/diff/D42515803/)
Pull Request resolved: https://github.com/pytorch/pytorch/pull/92882
Approved by: https://github.com/zhaojuanmao"
pytorch/pytorch,4b372e3958f8b2b6bad40f267e523451988879dc,"[memory profiling] C++ tracing support (#95357)

Adds the ability to quickly generate stack traces for C++,
and combine Python, TorchScript, and C++ frames into a single trace.

This makes it possible for the memory tracer to record allocations inside
C++ code (e.g. convolution temporaries, backward operators).

The unwinder code is ~10x faster than execinfo.h's backward because it
cache fast unwinder routines for instruction pointers that have already been seen.
It is also only 1.2--2x slower than copying the entire stack (the approach perf takes),
while using 2 orders of magnitude less space per stack.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/95357
Approved by: https://github.com/bertmaher"
pytorch/pytorch,41991710b27ddd328ce5f5c9ea7094b01387ad39,"Revert ""[PyTorch] Use c10::FastMap for memoizing in Pickler (#96360)"" (#96547)

This reverts commit 69d3fa2e4d93f3367ceb3af62d78aedd317dca6c.

Reason: breaks internal meta tests. See [D43926671](https://www.internalfb.com/diff/D43926671)

Pull Request resolved: https://github.com/pytorch/pytorch/pull/96547
Approved by: https://github.com/seemethere, https://github.com/malfet"
pytorch/pytorch,49eed50d191b4e274a257ced11d89d58a1fdfd0b,"[Inductor Perf CI] Lower the threshold of performance smoke test speedup. (#96531)

Avoids issues with https://github.com/pytorch/pytorch/issues/96530

Pull Request resolved: https://github.com/pytorch/pytorch/pull/96531
Approved by: https://github.com/seemethere"
pytorch/pytorch,55d4842a48d38425b22b39e3488713a439ec37e9,"[SPMD] Add defunctionalize_optimizer feature (#96323)

Summary: The manually adding dependencies between _foreach_add_, _fused_adam_, and output can cause issues when lowering to Inductor. This API removes those dependencies.

Test Plan: CI

Differential Revision: D43916450

Pull Request resolved: https://github.com/pytorch/pytorch/pull/96323
Approved by: https://github.com/kumpera"
pytorch/pytorch,076792a3e142c4de441c41e9e720e7c84fbe9d31,"[ONNX][Diagnostics] Speed up 'python_call_stack' by 'traceback' (#96348)

`inspect.stack()` retrieves all stacktraces, and is not performant. `inspect.stack(0)`
speeds up the call greatly, but loses line snippet.
Rewrite with `traceback.extract_stack` which is better in both regards.
Speeds up `export` call in `test_gpt2_tiny` from ~30s to ~4s under profiling.

Before
```log
│...├─ 30.794 export_after_normalizing_args_and_kwargs  <@beartype(torch.onnx._internal.fx.exporter.export_after_normalizing_args_and_kwargs) at 0x7f815cba0700>:1
│...│  └─ 30.794 export_after_normalizing_args_and_kwargs  torch/onnx/_internal/fx/exporter.py:580
```

After
```log
│...├─ 4.427 export_after_normalizing_args_and_kwargs  <@beartype(torch.onnx._internal.fx.exporter.export_after_normalizing_args_and_kwargs) at 0x7fd8281b3700>:1
│...│  └─ 4.427 export_after_normalizing_args_and_kwargs  torch/onnx/_internal/fx/exporter.py:580
```

Pull Request resolved: https://github.com/pytorch/pytorch/pull/96348
Approved by: https://github.com/titaiwangms, https://github.com/justinchuby"
pytorch/pytorch,a87f3f612e5bf5605486ccb5a86c06c9683a9e72,"[MPS] Fall back multi-layer LSTM on macOS 12 (#90909)

The native implementation of LSTM has been fixed on macOS 13.

On macOS 12, the multi-layer LSTM still has a numerical correctness issue that cannot be resolved on OS's side.

Thus, we fall back the multi-layer LSTM on macOS 12 to LSTMCell iteration. It might have performance impact but will make LSTM on macOS 12 fully usable.

Fixes: #90421
Issues related: #80306, #83144

Pull Request resolved: https://github.com/pytorch/pytorch/pull/90909
Approved by: https://github.com/albanD, https://github.com/kulinseth"
pytorch/pytorch,69d3fa2e4d93f3367ceb3af62d78aedd317dca6c,"[PyTorch] Use c10::FastMap for memoizing in Pickler (#96360)

These maps don't rely on reference stability, so FastMap should be fine.

Differential Revision: [D43926671](https://our.internmc.facebook.com/intern/diff/D43926671/)
Pull Request resolved: https://github.com/pytorch/pytorch/pull/96360
Approved by: https://github.com/ezyang"
pytorch/pytorch,cc699c56dc14af5b652a49e3044c0fe7c8ca70ee,"reland #96248 [inductor] show performance for each autotune config for a kernel (#96458)

Pull Request resolved: https://github.com/pytorch/pytorch/pull/96458
Approved by: https://github.com/ngimel"
pytorch/pytorch,2f6a371ae901fa266d69fb16585436e04c52d87f,"Revert ""Optimize nn.Module __call__ fast path for dynamo (#95931)"" (#96242)

Reverting due to concerns over silent unsoundness (skipped hooks) if users have directly added hooks dicts without using official torch APIs.

This reverts commit 26045336ca323fd27cff2a7340fe896117d5fb6e.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/96242
Approved by: https://github.com/albanD"
pytorch/pytorch,4662ae5b62f4d5bb5323136722df0c878bb0600b,"Add missing types to inductor IR assert (#96221)

Unclear if there is a more efficient way to define the allowed types for IR (or if we even need this, perhaps we just ditch the assert?)  But Inductor experts can deteremine if these added ops are appropriate and if so they fix the reported issue.

Fixes #96204

Pull Request resolved: https://github.com/pytorch/pytorch/pull/96221
Approved by: https://github.com/ezyang"
pytorch/pytorch,b3a079810e3d3ecf7ece4c3c10fb54f3ff4074ef,"[CI] Add a workflow for quick perf comparison (#96166)

Summary: ciflow/inductor-perf-test-nightly now contains full dashboard
run which takes a very long time. Ed proposed a simplification of the
perf run there, but it is still worth to have a set of fast perf test
which only includes one configuration (--training --amp).
Pull Request resolved: https://github.com/pytorch/pytorch/pull/96166
Approved by: https://github.com/huydhn, https://github.com/weiwangmeta"
pytorch/pytorch,11aab72dc9da488832326a066d2e47520e4ab2b3,"[SDPA] Add an optional scale kwarg (#95259)

# Summary
This PR adds an optional kwarg to torch torch.nn.functional.scaled_dot_product_attention()
The new kwarg is a scaling factor that is applied after the q@k.T step of the computation. Made updates to the efficient kernel to support but flash and math were minimally updated to support as well.

Will reduce the complexity of: #94729 and has been asked for by a couple of users.

# Review Highlights
- As far as I know I did this the correct way and this both BC and FC compliant. However I always seem to break internal workloads so I would love if someone can advice I did this right?
- I named the optional arg 'scale'. This is probably dumb and I should name it 'scale_factor'. I will make this change but this is annoying and it will require someone thinking we should rename.
- 'scale' is interpreted as `Q@K.T * (scale)`

Pull Request resolved: https://github.com/pytorch/pytorch/pull/95259
Approved by: https://github.com/cpuhrsch"
pytorch/pytorch,93ff71ec37e3c946603600a46edef70b42f81213,"[ET] Add RuntimeContext to ET Aten mode (#96084)

Summary:
In ATen mode, we add the RuntimeContext arg, so we have something like
```
TORCH_API inline at::Tensor & gelu_outf(torch::executor::RuntimeContext & context, const at::Tensor & self, c10::string_view approximate, at::Tensor & out) {
    return at::gelu_outf(self, approximate, out);
}
```
and user can use `<namespace like aten>::gelu_outf` and we will automatically dispatch the registered function in aten kernel using `at::gelu_outf` (dispatched by ATen/Functions.h header)

In optimized kernel tests, we can now automatically handle between aten kernel and optimized kernel.

The implication is that the test must depend on the correctness of codegen; an error in codegen can break the kernel tests.

Test Plan: CI

Differential Revision: D43777848

Pull Request resolved: https://github.com/pytorch/pytorch/pull/96084
Approved by: https://github.com/larryliu0820"
pytorch/pytorch,3623cfb79095b1a3b6b41b9a5edea278d0a619ef,"[FSDP] Speed up first iter order check (part 2) (#96220)

For a tensor on GPU, moving it once to CPU and operating on it on CPU is faster than moving it element by element from CPU to GPU. This is a follow-up to also move `world_indices`.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/96220
Approved by: https://github.com/zhaojuanmao"
pytorch/pytorch,32eb3ab7e8209452f47e14b5e88721a6f4212c1a,"[FSDP] Speed up first iter order check (#96146)

For a tensor on GPU, moving it once to CPU and operating on it on CPU is faster than moving it element by element from CPU to GPU. The relevant tensor in this case is `world_num_valid_indices`.

This closes https://github.com/pytorch/pytorch/issues/95728.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/96146
Approved by: https://github.com/zhaojuanmao, https://github.com/rohan-varma"
pytorch/pytorch,9a575e77ca8a0be7a3f3625c4dfdc6321d2a0c2d,"inductor: align baddbmm behavior with eager mode for beta=0 and input has nan value (#96087)

For ```torch.baddbmm(input, mat1,mat2, beta=0)```, if ```beta``` is zero, the multiplication of value ```input*beta``` will be ignored for the eager mode(always gets zero number, see https://pytorch.org/docs/stable/generated/torch.baddbmm.html?highlight=torch+baddbmm#torch.baddbmm), but the inductor is not, the inductor will get a different value if the input has a ```nan``` of ```inf``` value:

```
def fn_test(input, mat1, mat2):
    return torch.baddbmm(input, mat1, mat2, beta=0.0)

opt_fn = torch._dynamo.optimize(""inductor"")(fn_test)
a, b, c = [torch.rand((3,2,2)) for _ in range(3)]

real_out = fn_test(a, b, c)
a[:] = torch.nan
compiled_out = opt_fn(a, b,c)

print(compiled_out)
print(real_out)

```
before this PR, the output will be like this:

```
tensor([[[0.4272, 0.6037],
         [0.4279, 0.4219]],

        [[0.0838, 0.4873],
         [0.1210, 0.5516]],

        [[   nan,    nan],
         [   nan,    nan]]])
tensor([[[0.4272, 0.6037],
         [0.4279, 0.4219]],

        [[0.0838, 0.4873],
         [0.1210, 0.5516]],

        [[0.4985, 0.1072],
         [0.0857, 0.0186]]])

```

Pull Request resolved: https://github.com/pytorch/pytorch/pull/96087
Approved by: https://github.com/jansel, https://github.com/ngimel, https://github.com/jgong5"
pytorch/pytorch,98a4d74a68d58f12590410d7444a8cea175ffdb8,"COO intersection primitives: performance improvement (#96094)

Pull Request resolved: https://github.com/pytorch/pytorch/pull/96094
Approved by: https://github.com/pearu"
pytorch/pytorch,18e8aa95f10b6cc4db651aac95a339929b0f16fe,"Restore _graph_executor_optimize flag after JIT test_profiler (#96135)

Fixes https://github.com/pytorch/pytorch/issues/91483

Using a separate test class here, so that there is no need to run setup and teardown for all tests in TestJit.  The root cause here is that test_profiler could be flaky and fail in the middle without the chance to restore `torch._C._set_graph_executor_optimize` to its original value (https://github.com/pytorch/pytorch/issues/81626). This causes issues for all future tests running after as shown in https://github.com/pytorch/pytorch/issues/91483.

I suspect that is also the same root cause for several other flaky tests in the same file https://github.com/search?q=repo%3Apytorch%2Fpytorch+DISABLED+test_jit.TestScript&type=issues.  After this fix is merged, I would let retry bot does it job and close these issues after 2 weeks.

### Testing
The issue https://github.com/pytorch/pytorch/issues/91483 can now be reproduced by adding `torch._C._set_graph_executor_optimize(False)` locally to see if the test fails:

```
diff --git a/test/test_jit.py b/test/test_jit.py
index 2d1161d7466..17745d39182 100644
--- a/test/test_jit.py
+++ b/test/test_jit.py
@@ -5413,6 +5413,8 @@ a"")
             FileCheck().check(""int ="").check(""ListConstruct"").check(""aten::cat"").run(str(g))

     def test_stack(self):
+        torch._C._set_graph_executor_optimize(False)
+
         with enable_profiling_mode_for_profiling_tests():
             @torch.jit.script
             def func(x):
```

It indeed fails:

```
======================================================================
FAIL [0.006s]: test_stack (test_jit.TestScript)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/var/lib/jenkins/workspace/test/test_jit.py"", line 5437, in test_stack
    self.assertAutodiffNode(func2.graph_for(x, y), True, ['aten::stack'], [])
  File ""/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/testing/_internal/common_jit.py"", line 282, in assertAutodiffNode
    self.assertEqual(should_autodiff_node,
##[endgroup]
  File ""/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/testing/_internal/common_utils.py"", line 2975, in assertEqual
    raise error_metas[0].to_error(
AssertionError: Booleans mismatch: True is not False

Failure in testing nodes' autodifferentiation. One or more nodes were expected to be autodiffed, but were not found in specified fusible/nonfusible DifferentiableGraph groups.
Specifically:
  ['aten::stack'] were not in one of the DifferentiableGraphs when they were expected to be. Did you intend for these nodes to be autodiffed? If not, remove them from the list of nonfusible nodes.

----------------------------------------------------------------------
Ran 2677 tests in 84.596s

FAILED (failures=1, skipped=136, expected failures=13)
```

Pull Request resolved: https://github.com/pytorch/pytorch/pull/96135
Approved by: https://github.com/clee2000"
pytorch/pytorch,19ee61f7fa6d13f16efaf7c68f0e6f452e72b598,"Upload torch dynamo performance stats to S3 before Rockset (#96165)

I have a minor tweak on the uploading workflow to upload to S3 first before Rockset as `upload-test-stats` and  `upload-torch-dynamo-perf-stats` both run when inductor-A100-perf finished.  There is a potential race condition where the test reports are not yet no S3 when `upload-torch-dynamo-perf-stats` runs (it gets the data from S3).  `inductor-A100-perf` is now handled exclusively by `upload-torch-dynamo-perf-stats` to avoid this.  It will upload test reports to S3 first before getting them to Rockset.

The uploading script works fine with the test reports from https://hud.pytorch.org/pr/95685.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/96165
Approved by: https://github.com/desertfire"
pytorch/pytorch,e168dbb90a0c0a6cc6e799386fe410408f6d2cb4,"[inductor] improve cpp vec implementation of square (#96072)

For cpp vectorization of `square`, the current implementation is not efficient. The implementation would also affect the performance of `batch normalization` as it uses `square` when calculating variance. This PR replaces the `power` with `multiplication` to gain more performance.

Micro-benchmark performance for eager v.s. inductor:
op=`aten.native_batch_norm.default`
<html xmlns:v=""urn:schemas-microsoft-com:vml""
xmlns:o=""urn:schemas-microsoft-com:office:office""
xmlns:x=""urn:schemas-microsoft-com:office:excel""
xmlns=""http://www.w3.org/TR/REC-html40"">

<head>

<meta name=ProgId content=Excel.Sheet>
<meta name=Generator content=""Microsoft Excel 15"">
<link id=Main-File rel=Main-File
href=""file:///C:/Users/xuanliao/AppData/Local/Temp/msohtmlclip1/01/clip.htm"">
<link rel=File-List
href=""file:///C:/Users/xuanliao/AppData/Local/Temp/msohtmlclip1/01/clip_filelist.xml"">
</head>

<body link=""#0563C1"" vlink=""#954F72"">

suite | improvement_0.2 | improvement_0.5 | improvement_0.8 | current_speedup_0.2 | new_speedup_0.2 | current_speedup_0.5 | new_speedup_0.5 | current_speedup_0.8 | new_speedup_0.8
-- | -- | -- | -- | -- | -- | -- | -- | -- | --
torchbench | 8.82% | 5.53% | 32.19% | 0.608006834 | 0.661613139 | 0.691743711 | 0.729987622 | 0.76176223 | 1.00694842
timm | 59.30% | 63.01% | 94.77% | 0.650648524 | 1.036498047 | 0.676425152 | 1.102667387 | 0.695693384 | 1.354992423

</body>

</html>

Model training performance for eager v.s. inductor:
<html xmlns:v=""urn:schemas-microsoft-com:vml""
xmlns:o=""urn:schemas-microsoft-com:office:office""
xmlns:x=""urn:schemas-microsoft-com:office:excel""
xmlns=""http://www.w3.org/TR/REC-html40"">

<head>

<meta name=ProgId content=Excel.Sheet>
<meta name=Generator content=""Microsoft Excel 15"">
<link id=Main-File rel=Main-File
href=""file:///C:/Users/xuanliao/AppData/Local/Temp/msohtmlclip1/01/clip.htm"">
<link rel=File-List
href=""file:///C:/Users/xuanliao/AppData/Local/Temp/msohtmlclip1/01/clip_filelist.xml"">
</head>

<body link=""#0563C1"" vlink=""#954F72"">

model | improvement | current_speedup | new_speedup
-- | -- | -- | --
lcnet_050 multi-thread | 5.16% | 1.046 | 1.1
lcnet_050 single-thread | 21.81% | 0.94 | 1.145
mobilenet_v2 multi-thread | 3.88% | 1.135 | 1.179
mobilenet_v2 single-thread | 37.46% | 0.929 | 1.277

</body>

</html>

Pull Request resolved: https://github.com/pytorch/pytorch/pull/96072
Approved by: https://github.com/jgong5, https://github.com/jansel, https://github.com/desertfire"
pytorch/pytorch,962b3f78bd7ce6de2ab357391174f8f5acc473e1,"[inductor] run all kernel benchmarks individually in a compiled module (#95845)

This is a follow up for PR #95506 to run all the triton kernels in a compiled module individually as suggested by Horace.

Here are the steps:
1. Run the model as usual with a benchmark script and with TORCHINDUCTOR_BENCHMARK_KERNEL enabled. e.g.
```
TORCHINDUCTOR_BENCHMARK_KERNEL=1 python benchmarks/dynamo/torchbench.py --backend inductor --amp --performance --dashboard --only resnet18 --disable-cudagraphs --training
```
2. From the output we will see 3 lines like
```
Compiled module path: /tmp/torchinductor_shunting/rs/crsuc6zrt3y6lktz33jjqgpkuahya56xj6sentyiz7iv4pjud43j.py
```
That's because we have one graph module for fwd/bwd/optitimizer respectively. Each graph module will have one such output corresponding to the compiled module.

3. We can run the compiled module directly. Without any extra arguments, we just maintain the previous behavior to run the call function -- which just does what the original graph module does but in a more efficient way. But if we add the '-k' argument, we will run benchmark for each individual kernels in the file.

```
python /tmp/torchinductor_shunting/rs/crsuc6zrt3y6lktz33jjqgpkuahya56xj6sentyiz7iv4pjud43j.py -k
```

Example output:
<img width=""430"" alt=""Screenshot 2023-03-01 at 4 51 06 PM"" src=""https://user-images.githubusercontent.com/52589240/222302996-814a85be-472b-463c-9e85-39d2c9d20e1a.png"">

Note: I use the first 10 characters of the hash to identify each kernel since
1. hash is easier to get in the code :)
2. name like `triton__3` only makes sense within a compiled module, but a hash can make sense even without specifying the compiled module (assuming we have enough bytes for the hash)

If we found a triton kernel with hash like c226iuf2wi having poor performance, we can look it up in the original compiled module file. It works since we comment each compiled triton kernel with the full hash.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/95845
Approved by: https://github.com/Chillee"
pytorch/pytorch,3beafc91d1f9fa3fa8c6c8836d1e82aa1286a732,"USE_FAST_NVCC Windows (#95206)

USE_FAST_NVCC now works on Windows.

Fixes #67100

Pull Request resolved: https://github.com/pytorch/pytorch/pull/95206
Approved by: https://github.com/ezyang"
pytorch/pytorch,dcc159d3b660815ec0ee29ec3fc175b84cdf2967,"inductor: pre-convert a TensorBox's layout to FixedLayout at FX side if one user of it is a CPU external customer kernel (#95873)

Given the following case:

```
import torch
import torch._dynamo

class Model(torch.nn.Module):
    def __init__(self):
        super(Model, self).__init__()
        self.conv1 =  torch.nn.Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))
        self.conv2 =  torch.nn.Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))
        self.silu = torch.nn.SiLU(inplace=False)

    def forward(self, x,):
        x = self.silu(x)
        y1 = self.conv1(x)
        y2 = self.conv2(x)
        return y1, y2

model = Model().eval()
model = model.to(memory_format=torch.channels_last).eval()
opt_model = torch._dynamo.optimize('inductor')(model)

x = torch.randn(128, 64, 112, 112).to(memory_format=torch.channels_last)
with torch.no_grad():
    for i in range(3):
        out = opt_model(x)
```

the silu is used by two external kernels, and there always have redundant memory copy:

```
kernel_cpp_0 = async_compile.cpp('''
#include ""/tmp/torchinductor_xiaobing/dl/cdljpywww2h2ag4o35mwbvm45hhasxnxkhqgbupxnk3y7olula65.h""
extern ""C"" void kernel(const float* __restrict__ in_ptr0,
                       float* __restrict__ out_ptr0,
                       float* __restrict__ out_ptr1)
{
    #pragma omp parallel num_threads(40)
    {
        {
            #pragma omp for
            for(long i0=0; i0<6422528; i0+=1)
            {
                auto tmp0 = at::vec::Vectorized<float>::loadu(in_ptr0 + 16*i0);
                auto tmp1 = decltype(tmp0)(1)/(decltype(tmp0)(1) + tmp0.neg().exp());
                auto tmp2 = tmp0 * tmp1;
                tmp2.store(out_ptr0 + 16*i0);
                tmp2.store(out_ptr1 + 16*i0);
            }
            #pragma omp for simd simdlen(8)
            for(long i0=102760448; i0<102760448; i0+=1)
            {
                auto tmp0 = in_ptr0[i0];
                auto tmp1 = decltype(tmp0)(1) / (decltype(tmp0)(1) + std::exp(-tmp0));
                auto tmp2 = tmp0 * tmp1;
                out_ptr0[i0] = tmp2;
                out_ptr1[i0] = tmp2;
            }
        }
    }
}
''')
```
This PR will pre-convert the `silu`'s layout to FixedLayout at FX side(will be realized to avoid multi-realize at external kernel) if one user of it is a CPU external customer kernel, after this PR, the output code is:

```
kernel_cpp_0 = async_compile.cpp('''
#include ""/tmp/torchinductor_xiaobing/dl/cdljpywww2h2ag4o35mwbvm45hhasxnxkhqgbupxnk3y7olula65.h""
extern ""C"" void kernel(const float* __restrict__ in_ptr0,
                       float* __restrict__ out_ptr0)
{
    #pragma omp parallel num_threads(40)
    {
        {
            #pragma omp for
            for(long i0=0; i0<6422528; i0+=1)
            {
                auto tmp0 = at::vec::Vectorized<float>::loadu(in_ptr0 + 16*i0);
                auto tmp1 = decltype(tmp0)(1)/(decltype(tmp0)(1) + tmp0.neg().exp());
                auto tmp2 = tmp0 * tmp1;
                tmp2.store(out_ptr0 + 16*i0);
            }
            #pragma omp for simd simdlen(8)
            for(long i0=102760448; i0<102760448; i0+=1)
            {
                auto tmp0 = in_ptr0[i0];
                auto tmp1 = decltype(tmp0)(1) / (decltype(tmp0)(1) + std::exp(-tmp0));
                auto tmp2 = tmp0 * tmp1;
                out_ptr0[i0] = tmp2;
            }
        }
    }
}
''')
```

Currently, this PR only considers the CPU external customer kernel, but for other external kernels, there may have the same issue.

For Timm **eca_halonext26ts** , this PR will give about **8%** performance improvement(BS=128, 20 cores on SKX).

Pull Request resolved: https://github.com/pytorch/pytorch/pull/95873
Approved by: https://github.com/jansel"
pytorch/pytorch,26045336ca323fd27cff2a7340fe896117d5fb6e,"Optimize nn.Module __call__ fast path for dynamo (#95931)

This PR optimizes the guards overhead introduced by dynamo tracing module forward hooks.

It can and maybe should be followed by a wider change proposed by @voznesenskym to optimize specialized nnmodules by 'observing' any user mutations and directly invalidating the root guard, obviating the need to install other nnmodule guards.  (But this observer change seems more involved...)

Idea: maintain a flag, and keep it up to date whenever adding or
removing hooks. Use the flag rather than dict checks to enter the call fast path.
  - need to extend RemovableHandle to keep a ref to nnModule so it can update the flag on removal.
  - also need to handle the flag in ScriptModule which still uses the python call impl when called from python.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/95931
Approved by: https://github.com/ezyang, https://github.com/voznesenskym"
pytorch/pytorch,7d765cdc662722ed04db143c0b5780c76cc7867e,"Fix wrong handling of `grad_scale` & `found_inf` in fused optimizers (#95847)

Fixes #95781.
The cause seems to be that the current implementation doesn't correctly pass `found_inf` when `grad_scale` is `None`. Therefore parameters can get mistakenly updated by gradients whose some elements are invalid, i.e. nan or inf.

Related #94060

I forgot about this wrong handling after #94344

Pull Request resolved: https://github.com/pytorch/pytorch/pull/95847
Approved by: https://github.com/janeyx99"
pytorch/pytorch,d7637801d385f3ddc551320b5e9ba810eca9f846,"Revert ""COO intersection primitives: performance improvement (#92976)""

This reverts commit b033594943876d68b9278d4c2ed04fc3c968f001.

Reverted https://github.com/pytorch/pytorch/pull/92976 on behalf of https://github.com/seemethere due to Need to revert this so I can revert https://github.com/pytorch/pytorch/pull/94048 cleanly"
pytorch/pytorch,b033594943876d68b9278d4c2ed04fc3c968f001,"COO intersection primitives: performance improvement (#92976)

This PR improves COO intersection primitives by:
* making it sync-less (dims <= 8, can be changed to any value that fits stack).
* improving performance with much less kernel calls.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/92976
Approved by: https://github.com/cpuhrsch, https://github.com/pearu"
pytorch/pytorch,f7b26bdd2237e3d0750474335a06163b5733644a,"Remove mention of dynamo.optimize() in docs (#95802)

This should be self containable to merge but other stuff that's been bugging me is
* Instructions on debugging IMA issues
* Dynamic shape instructions
* Explaining config options better

Will look at adding a config options doc

Pull Request resolved: https://github.com/pytorch/pytorch/pull/95802
Approved by: https://github.com/svekars"
pytorch/pytorch,de86538f5513f640f2cb19a4bda888c158523fc7,"[ROCM] Restrict pytorch rocm to only use triton 2.0.x (#95793)

To align with upstream, we are requiring triton dependency to be between 2.0.0 and 2.1.  This will allow PyTorch 2.0 on ROCM to stay flexible enough to pick up any performance/stability improvements from Triton, without needing to cut a separate PyTorch version.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/95793
Approved by: https://github.com/huydhn"
pytorch/pytorch,5d29b68bbccdbdf222e855aec02ddb64f56ca490,"[inductor] generate triton kernel benchmark (#95506)

A PR to generate benchmark code for individual triton kernels. We can explore improving autotuning with the saved compiled kernel directly. This potentially can speedup our iteration and separate the concern with the upstream components that generate the compiled module.

Since I'm still ramping up on inductor, I'll reflect what I learned here so people can correct me if I'm wrong.  In inductor, WrapperCodeGen class is used to generate the compiled module for CUDA (or triton). Here is an example compiled module for a toy model like: `def f(x): return sin(x) + cos(x)` https://gist.github.com/shunting314/c6ed9f571919e3b414166f1696dcc61b .  A compiled module contains the following part:
- various triton kernels
- a wrapper (or a method named call . The name is hardcoded) that calls the triton kernels and potentially ATen kernels to efficiently do the same work as the original Fx graph being compiled by inductor
- some utility code that generate random inputs and run the wrapper

The triton kernels in the compiled module are annotated with decorator like pointwise which is used for autotuning.

This PR add a config so enabling it will just trigger the path of the compiled module being printed. It can be controlled from environment variable as well.

The path to each compiled triton kernel is added as comment in the compiled module. E.g.
```
# kernel path: /tmp/torchinductor_shunting/gn/cgn6x3mqoltu7q77gjnu2elwfupinsvcovqwibc6fhsoiy34tvga.py
triton__0 = async_compile.triton('''
import triton
import triton.language as tl
...
"""""")
````

Example command:
```
TORCHINDUCTOR_OUTPUT_COMPILED_MODULE_PATH=1 TORCHINDUCTOR_BENCHMARK_KERNEL=1 python benchmarks/dynamo/huggingface.py --backend inductor --amp --performance --training --dashboard --only AlbertForMaskedLM --disable-cudagraphs
```

Pull Request resolved: https://github.com/pytorch/pytorch/pull/95506
Approved by: https://github.com/Chillee"
pytorch/pytorch,e970dd9dcfbc376e7af608211715b0357ea407b8,"[CI] Compile on M1 natively (#95719)

We have plenty of runners now, let's use them for compilation as well.
To achieve that, remove `xcode-version: ""13.3.1""` property and tweak Metal framework detection logic to work with command line tools(which are installed in `/Library/Developer/CommandLineTools`) and SDK is in `/Library/Developer/CommandLineTools/SDKs/MacOSX.sdk`) rather than full Xcode installation.

TODO: Fix/enable OpenMP accelerated native builds (which are currently broken with `OMP: Error #15: Initializing libomp.dylib, but found libomp.dylib already initialized.`), but this matches existing behavior as cross-builds are compiled  with OpenMP disabled.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/95719
Approved by: https://github.com/huydhn"
pytorch/pytorch,60a1d29585241400178c55d30c00ff28db78e769,"Correct OneCycleLR doc example code to explicitly call optimizer.step() (#95730)

Fixes #89358 as suggested in the issue comment

A screenshot of the example code in the built docs:
<img width=""1168"" alt=""Screenshot 2023-02-28 at 4 46 45 PM"" src=""https://user-images.githubusercontent.com/31816267/221999156-02b28f2a-85b3-4aa8-841d-e4c66a39a33f.png"">

Pull Request resolved: https://github.com/pytorch/pytorch/pull/95730
Approved by: https://github.com/janeyx99"
pytorch/pytorch,d9cd9a13bcee3a9bccc7fa9b6f98a306a1f99a37,"[BE][DDPOptimizer] De-dup `p` and `param` (#95654)

The `param` from `param = target.get_parameter(name)` should be the same as `p` from `target.named_parameters()`.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/95654
Approved by: https://github.com/wconstab"
pytorch/pytorch,dc10ab15b7b6f54e6fceae30272ae487765c917b,"Warn on modification of OptimizedModule.forward (#95673)

Partially addresses #95641

Pull Request resolved: https://github.com/pytorch/pytorch/pull/95673
Approved by: https://github.com/ezyang"
pytorch/pytorch,6bdef7a5ff83d947f9e6092fdf71de5034de626e,"Warn on dynamo OptimizedModule.forward() (#95672)

Partially addresses #95641

Pull Request resolved: https://github.com/pytorch/pytorch/pull/95672
Approved by: https://github.com/ezyang"
pytorch/pytorch,b87229f19d14f8e0c835696fe3a78968832a8a7a,"Reland #94719 - Update ideep to add primitive cache for ARM (#95688)

### Description
This PR is to update ideep to add primitive cache in order to speed up ARM's PyTorch workloads.
Reland https://github.com/pytorch/pytorch/pull/94719, which is unintentional reverted by https://github.com/pytorch/pytorch/pull/94939#issuecomment-1447501258.
Fixes https://github.com/pytorch/pytorch/issues/94264.

### Performance test
Use TorchBench test in ICX with 40 cores
Intel OpenMP & jemalloc were preloaded
![image](https://user-images.githubusercontent.com/61222868/221760391-fb6cbabe-6d88-4155-b216-348e718e68b9.png)

Pull Request resolved: https://github.com/pytorch/pytorch/pull/95688
Approved by: https://github.com/ezyang"
pytorch/pytorch,9b7abc4facea1dff69f81e2819570afcd36b35f5,"Run slow gradcheck tests sequentially (#95494)

Also redo https://github.com/pytorch/pytorch/pull/95246 as there are many more still run OOM
Pull Request resolved: https://github.com/pytorch/pytorch/pull/95494
Approved by: https://github.com/clee2000"
pytorch/pytorch,a12e92d8e44c16c0bc0351fb8c8c348e50260eb2,"Support nn.Module forward hooks in torchdynamo (#92125)

Tweak dynamo behavior in 2 places when calling nn.Modules,
to route the call to __call__  instead of .forward(), since
__call__ is the codepath that eager users hit and will dispatch
to hooks correctly.
 (1) inside NNModuleVariable.call_function, which covers the common case
     of calling a module from code dynamo is already tracing
 (2) at the OptimizedModule layer, which is the entrypoint
     into a top-level nn.Module dynamo is about to compile

This exposes a new bug: NNModuleVariable used to special-case calling
module.forward() (which is a method) as a UserFunctionVariable with an extra
'self' arg.  After tracing into module.__call__, there is no longer a special
case for the eventual call into .forward, and it gets wrapped in a
UserDefinedObjectVariable following standard behavior of ._wrap().  UDOV can't be
called, so this broke some tests.

- Fix: add a new special case in _wrap() that treats methods as a UserDefinedMethod
  instead of UserDefinedObjectVariable.  Now, the forward method can be called.

Also, fix NNModuleVar.call_method routing forward back to __call__

Pull Request resolved: https://github.com/pytorch/pytorch/pull/92125
Approved by: https://github.com/ezyang, https://github.com/jansel, https://github.com/voznesenskym"
pytorch/pytorch,24dd37ef51f7e9f3e91fe6b430f248fc5120b973,"Add BOOL_FALSE guard to optimize empty container case (#95248)

There is a fast way to implement a guard for an empty dict, which is to check its bool() value.

However, we can't use this guard in general, since we can only safely apply it at runtime if the runtime value actually is a dict (or, another type that works with 'bool' in the same way).  A counterexample is when a tensor is passed instead of a dict, and throws on bool() operator.

So we can put a type check in the guard, but that is slow enough it defeats the purpose.

Instead, we note that for the case of NNModuleVariables (which are specialized NNModules not unspecialized ones), we already have a hook in place to invalidate the guards if setattr is called.  I am claiming that setattr is the only way that the type of a property on an NNModule could change.  If I'm right, then it's safe to (a) only use this guard for NNModuleVariables, (b) not do a type check inside the guard.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/95248
Approved by: https://github.com/voznesenskym"
pytorch/pytorch,0eeb04652a64f549b1bcf1f43212d79ff85bfa74,"[vulkan] Pad channels when using texture storage instead of ""tight packing"" (#95251)

Currently, in Vulkan 4D tensors are represented in GPU textures by simply combining the batch and channel dimensions into the depth axis. However, if the number of channels is not a multiple of 4, then data belonging to the same batch can cross texel boundaries.

For instance, consider a tensor with `N=2`, `C=3`. The depth axis of the texture would contain the data

```
|tex1|tex2|
-----------
|AAAB|BB00|
```
Where A represents data from `n=1`and B represents data form `n=2`.

This packing structure (""tight packing"") makes some ops that care about batch boundaries more complex and inefficient to implement. Therefore this diff introduces channel padding when storing tensors as image textures.

The same tensor with `N=2`, `C=3` would now have the depth axis contain

```
|tex1|tex2|
-----------
|AAA0|BBB0|
```

Differential Revision: [D43068669](https://our.internmc.facebook.com/intern/diff/D43068669/)

**NOTE FOR REVIEWERS**: This PR has internal Meta-specific changes or comments, please review them on [Phabricator](https://our.internmc.facebook.com/intern/diff/D43068669/)!
Pull Request resolved: https://github.com/pytorch/pytorch/pull/95251
Approved by: https://github.com/salilsdesai"
pytorch/pytorch,c97275acf6518746e9fd06d5e005685d4e43c126,"Fix OOMing periodic shards (#95246)

Tests have been consistently failing with the error on the following shards with the error `RuntimeError: CUDA error: out of memory`
- `periodic / linux-bionic-cuda11.7-py3-gcc7-slow-gradcheck / test (default, 1, 2, linux.4xlarge.nvidia.gpu)`
- `periodic / linux-bionic-cuda11.7-py3-gcc7-slow-gradcheck / test (default, 2, 2, linux.4xlarge.nvidia.gpu)`

Seeing if serializing those test files makes the periodic jobs succeed again.  This feels a bit off since there are so many different test files that have failed and need to be serialized, indicating a potential perf regression somewhere

Failures on hud: https://hud.pytorch.org/hud/pytorch/pytorch/master/1?per_page=100&name_filter=periodic%20%2F%20linux-bionic-cuda11.7-py3-gcc7-slow-gradcheck%20%2F%20test%20(default%2C%20
Pull Request resolved: https://github.com/pytorch/pytorch/pull/95246
Approved by: https://github.com/Skylion007, https://github.com/huydhn"
pytorch/pytorch,586ac98cde911ac53570e57288bb8ba43467d7dc,"Bugfix nested mem_efficient path in SDPA when E_qk != E_v (#95330)

Pull Request resolved: https://github.com/pytorch/pytorch/pull/95330
Approved by: https://github.com/drisspg, https://github.com/cpuhrsch"
pytorch/pytorch,f6f413c6b6912e33884a7510cc04a3488416a8a4,"Second part of splitting #91254 in two (#92749)

This handles the disabling masks if numel is a multiple of BLOCK.
It currently introduces a performance regression, but the triton
it generates does not seem to have any issues: all the change does
is cause xmask to be removed from load/stores in cases where it safely
can be removed. It seems it must be coming from some issue in triton
optimizer.

FWIW, if you try this change with current triton master (instead of
pinned version) it does _not_ cause a performance regression.
However, upgradign to triton master by itself already causes
significant performance regressions so it's not an option
to just bump up the pin.

I'm going to leave this PR open until we manage to increase
the triton pin past the big refactoring. Once we do that
I will check if it still causes a performance regression.

UPDATE:

The triton pin has been moved and I retried this PR. As expected, there's no longer a performance regression for hf_Bert:

```
tspin python benchmarks/dynamo/torchbench.py  --performance  --backend inductor --float16 --training --batch-size-file $(realpath benchmarks/dynamo/torchbench_models_list.txt) --only hf_Bert -n 5 --diff-branch viable/strict 2> err
batch size: 16
cuda train hf_Bert                             numel_BLOCK                1.175x p=0.00
batch size: 16
cuda train hf_Bert                             viable/strict              1.161x p=0.00
```
Re-opening this, should be okay to merge now I expect.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/92749
Approved by: https://github.com/jansel"
pytorch/pytorch,5fa937886cb731a85f33141c3f11017192f7d9a2,"[DCP][nit] Rename variables + minor documentation fix for optimizer.py (#95264)

Pull Request resolved: https://github.com/pytorch/pytorch/pull/95264
Approved by: https://github.com/rohan-varma"
pytorch/pytorch,30d0112bf34971d8fcd447ce3b2c4b1630d1e7d6,"fix performance issue in torch.sparse.mm reduce mode (#94969)

Fix performance bug for `torch.sparse.mm()` with reduce flag.

Found this bug within internal benchmarking.
Made a mistake when updating previous patch which causes load imbalance between threads:

Test on ogbn-products datasets on Xeon CLX with 24 cores:

#### before
```
sparse.mm: mean: 1156.148 ms
sparse.mm: sum: 1163.754 ms
sparse.mm: (using mkl): 703.227 ms
```

#### after
```
sparse.mm: mean: 662.578 ms
sparse.mm: sum: 662.301 ms
sparse.mm: (using mkl): 700.178 ms
```

The result also indicates that the current spmm kernel is no worse than MKL's sparse_mm .

Also update results on `pyg benchmark` with:
```
python gnn.py --use_sage --epochs=3 --runs=1 --inference
```

* Out of box: `13.32s`
* Without the fix in this PR: `5.87s`
* With the fix in this PR: `3.19s`

Pull Request resolved: https://github.com/pytorch/pytorch/pull/94969
Approved by: https://github.com/jgong5"
pytorch/pytorch,6ae06e49ac92442e583f05e6b88f58670cecebaa,"Inductor: fix incorrect result of inplace unsqueeze (#94797)

This pr aims to fix the incorrect result in the following test case.
```
@torch._dynamo.optimize(""inductor"")
def fn(a):
    unsqueeze_ = torch.ops.aten.unsqueeze_.default(a, 0)
    return unsqueeze_

args = [
      ((1, 1, 1, 12, 11, 3), (396, 396, 396, 33, 3, 1), torch.int64, ""cpu"")
       ]
args = [rand_strided(sh, st, dt, dev) for (sh, st, dt, dev) in args]

with torch.no_grad():
    out = fn(*args)

# expected result: (396, 396, 396, 396, 33, 3, 1) torch.Size([1, 1, 1, 1, 12, 11, 3])
print(args[0].stride(), args[0].shape) # incorrect result: (396, 396, 396, 396, 396, 396, 33, 3, 1) torch.Size([1, 1, 1, 1, 1, 1, 12, 11, 3])
```
**Root cause**

1. [fake_tensor](https://github.com/pytorch/pytorch/blob/master/torch/_dynamo/variables/builder.py#L140) is changed during [tracer.run](https://github.com/pytorch/pytorch/blob/master/torch/_dynamo/convert_frame.py#L311), then it will [pass incorrect inputs to inductor](https://github.com/pytorch/pytorch/blob/master/torch/_dynamo/output_graph.py#L670).
2. example_inputs are changed during [propagate](https://github.com/pytorch/pytorch/blob/master/torch/_inductor/mkldnn.py#L509)

Pull Request resolved: https://github.com/pytorch/pytorch/pull/94797
Approved by: https://github.com/jgong5, https://github.com/jansel"
pytorch/pytorch,ff7772317b9070d04881577d20baf719fb95f92f,"Stub all TensorImpl bools; do not go to Python if not hinted. (#94431)

The basic idea behind this PR is that we want to continue using the guarding implementations of contiguity tests, if all of the elements are backend (aka, have hints). If they don't have hints, we'll have to do something slower (use the non-short circuiting, non guarding implementations of contiguity), but most of the time you aren't dealing with unbacked SymInts.

So this PR has three parts.

1. We expose `has_hint` on `SymNode`. This allows us to query whether or not a SymInt is backed or not from C++. Fairly self explanatory. Will require LTC/XLA updates; but for backends that don't support unbacked SymInts you can just always return true.
2. We update `compute_non_overlapping_and_dense` to test if the inputs are hinted. If they are all hinted, we use the conventional C++ implementation. Otherwise we call into Python. The Python case is not heavily tested right now because I haven't gotten all of the pieces for unbacked SymInts working yet. Coming soon.
3. We add stubs for all of the other contiguity tests. The intention is to apply the same treatment to them as well, but this is not wired up yet for safety reasons.

Signed-off-by: Edward Z. Yang <ezyang@meta.com>
Pull Request resolved: https://github.com/pytorch/pytorch/pull/94431
Approved by: https://github.com/voznesenskym"
pytorch/pytorch,1d7133c542fd5f87cd4efd87da449d4484050aaa,"inductor(cpu): fix C++ compile error when sigmoid's post ops is a reduction op (#94890)

For timm **nfnet_l0** model. CPU path has the following error: `torch._dynamo.exc.BackendCompilerFailed: inductor raised CppCompileError: C++ compile error`.

There has a simple test case:

```
def fn(x):
    x = torch.ops.aten.sigmoid.default(x)
    return torch.ops.aten.mean.dim(x, [-1, -2], True)

x = torch.randn((1, 8, 8, 8))
opt_fn = torch._dynamo.optimize(""inductor"")(fn)
opt_fn(x)

real_out = fn(x)
compiled_out = opt_fn(x)
tol = 0.0001
print(torch.allclose(real_out, compiled_out, atol=tol, rtol=tol))

```

before:

```
extern ""C"" void kernel(float* __restrict__ in_out_ptr0,
                       const float* __restrict__ in_ptr0)
{
    auto out_ptr0 = in_out_ptr0;
    {
        #pragma GCC ivdep
        for(long i0=0; i0<8; i0+=1)
        {
            {
                #pragma omp declare reduction(+:at::vec::Vectorized<float>:omp_out += omp_in) initializer(omp_priv={{0}})
                float tmp2 = 0;
                auto tmp2_vec = at::vec::Vectorized<float>(tmp2);
                for(long i1=0; i1<4; i1+=1)
                {
                    auto tmp0 = at::vec::Vectorized<float>::loadu(in_ptr0 + (16*i1) + (64*i0));
                    auto tmp1 = decltype(tmp0)(1)/(decltype(tmp0)(1) + tmp0.neg().exp());
                    tmp2_vec += tmp1;
                }
                #pragma omp simd simdlen(8)  reduction(+:tmp3)
                for(long i1=64; i1<64; i1+=1)
                {
                    auto tmp0 = in_ptr0[i1 + (64*i0)];
                    auto tmp1 = std::exp(-tmp0);
                    auto tmp2 = 1 / (1 + tmp1);
                    tmp3 += tmp2;
                }
                tmp2 += at::vec::vec_reduce_all<float>([](at::vec::Vectorized<float>& x, at::vec::Vectorized<float>&y) {return x + y;}, tmp2_vec);
                out_ptr0[i0] = tmp3;
            }
        }
    }
    {
        for(long i0=0; i0<0; i0+=1)
        {
            auto tmp0 = at::vec::Vectorized<float>::loadu(out_ptr0 + 16*i0);
            auto tmp1 = at::vec::Vectorized<float>(static_cast<float>(64));
            auto tmp2 = tmp0 / tmp1;
            tmp2.store(in_out_ptr0 + 16*i0);
        }
        #pragma omp simd simdlen(8)
        for(long i0=0; i0<8; i0+=1)
        {
            auto tmp0 = out_ptr0[i0];
            auto tmp1 = static_cast<float>(64);
            auto tmp2 = tmp0 / tmp1;
            in_out_ptr0[i0] = tmp2;
        }
    }
}
```

after:
```
extern ""C"" void kernel(float* __restrict__ in_out_ptr0,
                       const float* __restrict__ in_ptr0)
{
    auto out_ptr0 = in_out_ptr0;
    #pragma omp parallel num_threads(40)
    {
        {
            #pragma omp for
            for(long i0=0; i0<8; i0+=1)
            {
                {
                    #pragma omp declare reduction(+:at::vec::Vectorized<float>:omp_out += omp_in) initializer(omp_priv={{0}})
                    float tmp2 = 0;
                    auto tmp2_vec = at::vec::Vectorized<float>(tmp2);
                    for(long i1=0; i1<4; i1+=1)
                    {
                        auto tmp0 = at::vec::Vectorized<float>::loadu(in_ptr0 + (16*i1) + (64*i0));
                        auto tmp1 = decltype(tmp0)(1)/(decltype(tmp0)(1) + tmp0.neg().exp());
                        tmp2_vec += tmp1;
                    }
                    #pragma omp simd simdlen(8)  reduction(+:tmp2)
                    for(long i1=64; i1<64; i1+=1)
                    {
                        auto tmp0 = in_ptr0[i1 + (64*i0)];
                        auto tmp1 = decltype(tmp0)(1) / (decltype(tmp0)(1) + std::exp(-tmp0));
                        tmp2 += tmp1;
                    }
                    tmp2 += at::vec::vec_reduce_all<float>([](at::vec::Vectorized<float>& x, at::vec::Vectorized<float>&y) {return x + y;}, tmp2_vec);
                    out_ptr0[i0] = tmp2;
                }
            }
        }
        #pragma omp single
        {
            {
                for(long i0=0; i0<0; i0+=1)
                {
                    auto tmp0 = at::vec::Vectorized<float>::loadu(out_ptr0 + 16*i0);
                    auto tmp1 = at::vec::Vectorized<float>(static_cast<float>(64));
                    auto tmp2 = tmp0 / tmp1;
                    tmp2.store(in_out_ptr0 + 16*i0);
                }
                #pragma omp simd simdlen(8)
                for(long i0=0; i0<8; i0+=1)
                {
                    auto tmp0 = out_ptr0[i0];
                    auto tmp1 = static_cast<float>(64);
                    auto tmp2 = tmp0 / tmp1;
                    in_out_ptr0[i0] = tmp2;
                }
            }
        }
    }
}
''')
```

Pull Request resolved: https://github.com/pytorch/pytorch/pull/94890
Approved by: https://github.com/EikanWang, https://github.com/jgong5, https://github.com/lezcano"
pytorch/pytorch,8261c600b70b849a2272af6807a46745a5b7d533,"Update ideep to add primitive cache for ARM (#94719)

### Description
This PR is to update ideep to add primitive cache in order to speed up ARM's PyTorch workloads.
Fixes #94264.

### Performance test
Use TorchBench test in ICX with 40 cores
Intel OpenMP & jemalloc were preloaded
![image](https://user-images.githubusercontent.com/61222868/218937895-c97f5a5f-644b-4113-a3f5-7fe11fad7516.png)

Pull Request resolved: https://github.com/pytorch/pytorch/pull/94719
Approved by: https://github.com/jgong5"
pytorch/pytorch,e0a954f531461fb27aeb82718dbe0b66c278b59e,"call `zero_grad` in foreach/fused optimizers tests (#94724)

the tests calling this method haven't failed because `iter` is a built-in function's name

Signed-off-by: Masaki Kozuki <mkozuki@nvidia.com>

Pull Request resolved: https://github.com/pytorch/pytorch/pull/94724
Approved by: https://github.com/Skylion007"
pytorch/pytorch,d1d5d16df3766bc6a6c9ebe1168ff652bcdcaa5e,"dynamo: handle straight-line graph breaks for autocast context manager with constant args (#94137)

Fixes https://github.com/pytorch/pytorch/issues/93890

We do the following:
1. fix __init__constructor for `AutocastModeVariable` with exisiting `mode` while copying
2. `resume_execution` is made aware of constant args (`target_values`), by storing said args in `ReenterWith`. To propagate between subgraphs (in straightline code), we also store the constant args in the downstream's `code_options[""co_consts""]` if not already.

---

Future work:
1. handle instantiating context manager in non-inlineable functions. Simultaneously fix nested grad mode bug.
2. generalize to general `ContextManager`s
3. generalize to variable arguments passed to context manager, with guards around the variable.

---

Actually, if we look at the repro: https://github.com/pytorch/pytorch/blob/74592a43d0d33a6c809fdcfc20249e1c93e7216e/test/dynamo/test_repros.py#L1249, we can see that the method in this PR doesn't work for graph breaks in function calls, in particular, in function calls that don't get inlined.

Why inlining functions with graph breaks is hard:
- When we handle graph breaks, we create a new code object for the remainder of the code. It's hard to imagine doing this when you are inside a function, then we need a frame stack. And we just want to deal with the current frame as a sequence of straight line codes.

Why propagating context manager information is hard:
- If we do not inline the function, the frame does not contain any information about the parent `block_stack` or `co_consts`. So we cannot store it on local objects like the eval frame. It has to be a global object in the output_graph.

---

Anyway, I'm starting to see clearly that dynamo must indeed be optimized for torch use-case. Supporting more general cases tends to run into endless corner-cases and caveats.

One direction that I see as viable to handle function calls which have graph breaks and `has_tensor_in_frame` is stick with not inlining them, while installing a global `ContextManagerManager`, similar to the `CleanupManager` (which cleans up global variables). We can know which context managers are active at any given point, so that we can install their setup/teardown code on those functions and their fragments.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/94137
Approved by: https://github.com/yanboliang"
pytorch/pytorch,9fb921947867843df46f14bbece9cc1b2e5e985f,"Make DDPOptimizer work with torch._dynamo.explain() (#94749)

GraphModules that were created during DDPOptimizer graph breaking
lacked `compile_subgraph_reason`, which caused an exception when
running .explain().

Now the reason is provided and users can use .explain() to find out
that DDPOptimizer is causing graph breaks.

Fixes #94579

Pull Request resolved: https://github.com/pytorch/pytorch/pull/94749
Approved by: https://github.com/voznesenskym"
pytorch/pytorch,fb55f12cb05ddd09bdb385ce9066fa96c20c4320,"[cpu][inductor] improve cpu vec implementations of cos & sin (#94577)

The current Torchinductor's `cos` & `sin` implementations will call `sleef` functions in `aten::Vec` which show worse performance than Aten's `cos` & `sin` implementations that invoke `MKL` functions. The reason is that the `sleef` algorithms sacrifice performance in order to have a higher precision. This PR changes Torchinductor's `cos` & `sin` implementations from the `sleef` functions with `1.0` ULP error bound to the ones with `3.5` ULP error bound.

**Performance data for eager v.s. inductor:**
<html xmlns:v=""urn:schemas-microsoft-com:vml""
xmlns:o=""urn:schemas-microsoft-com:office:office""
xmlns:x=""urn:schemas-microsoft-com:office:excel""
xmlns=""http://www.w3.org/TR/REC-html40"">

<head>

<meta name=ProgId content=Excel.Sheet>
<meta name=Generator content=""Microsoft Excel 15"">
<link id=Main-File rel=Main-File
href=""file:///C:/Users/xuanliao/AppData/Local/Temp/msohtmlclip1/01/clip.htm"">
<link rel=File-List
href=""file:///C:/Users/xuanliao/AppData/Local/Temp/msohtmlclip1/01/clip_filelist.xml"">

</head>

<body link=blue vlink=purple>

suite=huggingface |   |   |   |   |  
-- | -- | -- | -- | -- | --
op | improved_ratio | speedup_old | RSD(3) | speedup_new | RSD(3)
cos | 62.12% | 0.653826147 | 4.48% | 1.059999006 | 3.38%
sin | 38.12% | 0.745482927 | 0.72% | 1.029642026 | 5.33%

</body>

</html>

**Accuracy data for eager v.s. inductor:**
Each tol has been tested for 1000 times.
<html xmlns:v=""urn:schemas-microsoft-com:vml""
xmlns:o=""urn:schemas-microsoft-com:office:office""
xmlns:x=""urn:schemas-microsoft-com:office:excel""
xmlns=""http://www.w3.org/TR/REC-html40"">

<head>

<meta name=ProgId content=Excel.Sheet>
<meta name=Generator content=""Microsoft Excel 15"">
<link id=Main-File rel=Main-File
href=""file:///C:/Users/xuanliao/AppData/Local/Temp/msohtmlclip1/01/clip.htm"">
<link rel=File-List
href=""file:///C:/Users/xuanliao/AppData/Local/Temp/msohtmlclip1/01/clip_filelist.xml"">

</head>

<body link=blue vlink=purple>

error_bound | tol=1e-7 | tol=1e-8
-- | -- | --
1.0 ULP | PASS | FAIL
3.5 ULP | PASS | FAIL

</body>

</html>

Pull Request resolved: https://github.com/pytorch/pytorch/pull/94577
Approved by: https://github.com/EikanWang, https://github.com/jgong5, https://github.com/Chillee, https://github.com/desertfire, https://github.com/jansel"
pytorch/pytorch,2acac8a83a8c4ee56f385bd892eb6ad7a87321b5,"Logcumsumexp for CUDA (build-time optimized) (#94310)

Hopefully fixes #89205.
This is another version of #90847 where it was reverted because it increases the compile-time significantly.
From my discussion with @ngimel in https://github.com/pytorch/pytorch/pull/93153#issuecomment-1409051528, it seems the option of jiterator would be very tricky if not impossible.
So what I did was to optimize the compile-time in my computer.

To optimize the build time, first I compile the pytorch as a whole, then only change the `LogcumsumexpKernel.cu` file to see how it changes the compile time.
Here are my results for the compilation time of only the `LogcumsumexpKernel.cu` file in my computer:

- Original version (without any complex implementations): 56s (about 1 minute)
- The previous PR (#90847): 13m 57s (about 14 minutes)
- This PR: 3m 35s (about 3.5 minutes)

If the previous PR increases the build time by 30 mins in pytorch's computer, then this PR reduces the increment of build time to about 6 mins. Hopefully this is an acceptable level of build-time increase.

What I did was (sorted by how significant it reduces the build time from the most significant one):

- Substituting `log(x)` to `log1p(x - 1)`. This is applied in the infinite case, so we don't really care about precision.
- Implementing complex exponential manually

tag: @malfet, @albanD
Pull Request resolved: https://github.com/pytorch/pytorch/pull/94310
Approved by: https://github.com/Skylion007, https://github.com/malfet"
pytorch/pytorch,45edf9a2eaedb07a8fce292635e164ab9be0f5ac,"Reland: [Autograd] Use in-place input accumulation fast path for dense Tensors. (#90217)

Identical to https://github.com/pytorch/pytorch/pull/88339 except with a `.has_storage()` check before `.storage()`.

Differential Revision: [D41737935](https://our.internmc.facebook.com/intern/diff/D41737935/)

Pull Request resolved: https://github.com/pytorch/pytorch/pull/90217
Approved by: https://github.com/ngimel"
pytorch/pytorch,c620ece726a197740538c1de3d8e0ff62253ac73,"port sparse_mm.reduce to pytorch and optimize it on CPU (#83727)

### Motivation of this PR

This patch is to migrate `spmm_reduce` from `torch-sparse` (a 3rd party dependency for PyG) to `torch`, which is a response to the initial proposal for fusion of **Gather, Apply Scatter** in Message Passing of GNN inference/training. https://github.com/pytorch/pytorch/issues/71300

**GAS** is the major step for Message Passing, the behavior of **GAS** can be classified into 2 kinds depending on the storage type of `EdgeIndex` which records the connections of nodes:

* COO: the hotspot is `scatter_reduce`
* CSR: the hotspot is `spmm_reduce`

The reduce type can be choose from: ""max"", ""mean"", ""max"",  ""min"".

extend `torch.sparse.mm` with an `reduce` argument, maps to `torch.sparse_mm.reduce` internally.
`sparse_mm_reduce` is registered under the TensorTypeId of `SparseCsrCPU`, and this operator requires an internal interface `_sparse_mm_reduce_impl` which has dual outputs:
* `out` - the actual output
* `arg_out` - records output indices in the non zero elements if the reduce type is ""max"" or ""min"", this is only useful for training. So for inference, it will not be calculated.

### Performance

Benchmark on GCN for obgn-products on Xeon single socket, the workload is improved by `4.3x` with this patch.

Performance benefit for training will be bigger, the original backward impl for `sum|mean` is sequential; the original backward impl for `max|min` is not fused.

#### before:
```
-----------------------------  ------------  ------------  ------------  ------------  ------------  ------------
                         Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg    # of Calls
-----------------------------  ------------  ------------  ------------  ------------  ------------  ------------
       torch_sparse::spmm_sum        97.09%       56.086s        97.09%       56.088s        6.232s             9
                 aten::linear         0.00%      85.000us         1.38%     795.485ms      88.387ms             9
                 aten::matmul         0.00%      57.000us         1.38%     795.260ms      88.362ms             9
                     aten::mm         1.38%     795.201ms         1.38%     795.203ms      88.356ms             9
                   aten::relu         0.00%      50.000us         0.76%     440.434ms      73.406ms             6
              aten::clamp_min         0.76%     440.384ms         0.76%     440.384ms      73.397ms             6
                   aten::add_         0.57%     327.801ms         0.57%     327.801ms      36.422ms             9
            aten::log_softmax         0.00%      23.000us         0.10%      55.503ms      18.501ms             3
```

#### after
```
-----------------------------  ------------  ------------  ------------  ------------  ------------  ------------
                         Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg    # of Calls
-----------------------------  ------------  ------------  ------------  ------------  ------------  ------------
               aten::spmm_sum        87.35%       11.826s        87.36%       11.827s        1.314s             9
                 aten::linear         0.00%      92.000us         5.87%     794.451ms      88.272ms             9
                 aten::matmul         0.00%      62.000us         5.87%     794.208ms      88.245ms             9
                     aten::mm         5.87%     794.143ms         5.87%     794.146ms      88.238ms             9
                   aten::relu         0.00%      53.000us         3.35%     452.977ms      75.496ms             6
              aten::clamp_min         3.35%     452.924ms         3.35%     452.924ms      75.487ms             6
                   aten::add_         2.58%     348.663ms         2.58%     348.663ms      38.740ms             9
                 aten::argmax         0.42%      57.473ms         0.42%      57.475ms      14.369ms             4
            aten::log_softmax         0.00%      22.000us         0.39%      52.605ms      17.535ms             3
```

Pull Request resolved: https://github.com/pytorch/pytorch/pull/83727
Approved by: https://github.com/jgong5, https://github.com/cpuhrsch, https://github.com/rusty1s, https://github.com/pearu"
pytorch/pytorch,a21bddcc903e75fdf6337bacf83a5fb7dc029b6c,"WelfordOps: Remove combine_t and use acc_scalar_t instead (#94522)

`combine_t` is the type used to represent the number of elements seen so far as
a floating point value (acc.nf). It is always used in calculations with other
values of type `acc_scalar_t` so there is no performance gained by making this a
separate template argument. Furthermore, when calculating the variance on CUDA
it is always set to `float` which means values are unnecessarily truncated
before being immediately promoted to `double`.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/94522
Approved by: https://github.com/ngimel"
pytorch/pytorch,02b8a7f4733268ed07d6d5529d4c33e110cbd183,"inductor: don't do transpose vectoriztion if input ld depends on most inner var (#94493)

Fixed https://github.com/pytorch/pytorch/issues/94269.

For the following case:

```
**import torch
import torchvision
#import intel_extension_for_pytorch

import torch._dynamo
from torch._inductor import config

class Model(torch.nn.Module):
    def __init__(self):
        super(Model, self).__init__()

    def forward(self, x):
        constant_pad_nd = x
        # File: /home/xiaobing/miniconda3/envs/pytorch_te_binary/lib/python3.8/site-packages/timm/models/layers/halo_attn.py:195, code: kv = kv.unfold(2, self.win_size, self.block_size).unfold(3, self.win_size, self.block_size)
        as_strided: f32[1, 384, 2, 20, 12] = torch.ops.aten.as_strided.default(constant_pad_nd, [1, 384, 2, 20, 12], [153600, 1, 61440, 384, 7680]);  constant_pad_nd = None
        as_strided_1: f32[1, 384, 2, 2, 12, 12] = torch.ops.aten.as_strided.default(as_strided, [1, 384, 2, 2, 12, 12], [153600, 1, 61440, 3072, 7680, 384]);  as_strided = None

        # File: /home/xiaobing/miniconda3/envs/pytorch_te_binary/lib/python3.8/site-packages/timm/models/layers/halo_attn.py:197, code: kv = kv.reshape(
        clone_1: f32[1, 384, 2, 2, 12, 12] = torch.ops.aten.clone.default(as_strided_1, memory_format = torch.contiguous_format);  as_strided_1 = None
        _unsafe_view_1: f32[8, 48, 4, 144] = torch.ops.aten._unsafe_view.default(clone_1, [8, 48, 4, 144]);  clone_1 = None
        permute_2: f32[8, 4, 144, 48] = torch.ops.aten.permute.default(_unsafe_view_1, [0, 2, 3, 1]);  _unsafe_view_1 = None
        # File: /home/xiaobing/miniconda3/envs/pytorch_te_binary/lib/python3.8/site-packages/timm/models/layers/halo_attn.py:202, code: k, v = torch.split(kv, [self.dim_head_qk, self.dim_head_v], dim=-1)
        split_with_sizes = torch.ops.aten.split_with_sizes.default(permute_2, [16, 32], -1);  permute_2 = None
        getitem: f32[8, 4, 144, 16] = split_with_sizes[0]
        getitem_1: f32[8, 4, 144, 32] = split_with_sizes[1];  split_with_sizes = None
        permute_3: f32[8, 4, 16, 144] = torch.ops.aten.permute.default(getitem, [0, 1, 3, 2]);  getitem = None
        expand_1: f32[8, 4, 16, 144] = torch.ops.aten.expand.default(permute_3, [8, 4, 16, 144]);  permute_3 = None
        clone_3: f32[8, 4, 16, 144] = torch.ops.aten.clone.default(expand_1, memory_format = torch.contiguous_format);  expand_1 = None
        return clone_3

model = Model().eval()
opt_model = torch._dynamo.optimize('inductor')(model)
x = torch.randn(1, 384, 20, 20).to(memory_format=torch.channels_last)

ref = model(x)

with torch.no_grad():
    for i in range(3):
        out = opt_model(x)

print(torch.equal(ref, out))
```

The generated code before this PR is:

```
from ctypes import c_void_p, c_long
import torch
import random
from torch import empty_strided, as_strided, device
from torch._inductor.codecache import AsyncCompile
from torch._inductor.select_algorithm import extern_kernels

aten = torch.ops.aten
assert_size_stride = torch._C._dynamo.guards.assert_size_stride
async_compile = AsyncCompile()

kernel_cpp_0 = async_compile.cpp('''
#include ""/tmp/torchinductor_xiaobing/ni/cniims6nap7c5wars7cmtbjr3mw6b5cxyoyxmsu7ro2l5fkrwatl.h""
extern ""C"" void kernel(const float* __restrict__ in_ptr0,
                       float* __restrict__ out_ptr0)
{
    {
        #pragma GCC ivdep
        for(long i0=0; i0<8; i0+=1)
        {
            #pragma GCC ivdep
            for(long i1=0; i1<4; i1+=1)
            {
                #pragma GCC ivdep
                for(long i2=0; i2<1; i2+=1)
                {
                    #pragma GCC ivdep
                    for(long i3=0; i3<9; i3+=1)
                    {
                        float tmp0[16*16] __attribute__ ((aligned (16)));
                        at::vec::transpose_mxn<float,16,16>(in_ptr0 + (16*i2) + (48*i0) + (384*((16*i3) % 12)) + (3072*(i1 % 2)) + (7680*(((4*i3) / 3))) + (61440*(i1 / 2)), ((-7680)*(i3 / 12)) + ((-384)*(i3 % 12)) + (384*((1 + i3) % 12)) + (7680*(((1 + i3) / 12))), tmp0, 16);
                        for (long i2_inner = 0; i2_inner < 16; i2_inner++)
                        {
                            auto tmp1 = at::vec::Vectorized<float>::loadu(tmp0 + 16*i2_inner);
                            tmp1.store(out_ptr0 + (16*i3) + (144*i2_inner) + (2304*i1) + (2304*i2) + (9216*i0));
                        }
                    }
                    #pragma GCC ivdep
                    for(long i3=144; i3<144; i3+=1)
                    {
                        for (long i2_inner = 0; i2_inner < 16; i2_inner++)
                        {
                            auto tmp0 = in_ptr0[i2_inner + (16*i2) + (48*i0) + (384*(i3 % 12)) + (3072*(i1 % 2)) + (7680*(i3 / 12)) + (61440*(i1 / 2))];
                            out_ptr0[i3 + (144*i2_inner) + (2304*i1) + (2304*i2) + (9216*i0)] = tmp0;
                        }
                    }
                }
                #pragma GCC ivdep
                for(long i2=16; i2<16; i2+=1)
                {
                    #pragma GCC ivdep
                    for(long i3=0; i3<144; i3+=1)
                    {
                        auto tmp0 = in_ptr0[i2 + (48*i0) + (384*(i3 % 12)) + (3072*(i1 % 2)) + (7680*(i3 / 12)) + (61440*(i1 / 2))];
                        out_ptr0[i3 + (144*i2) + (2304*i1) + (9216*i0)] = tmp0;
                    }
                }
            }
        }
    }
}
''')

async_compile.wait(globals())
del async_compile

def call(args):
    arg0_1, = args
    args.clear()
    buf0 = empty_strided((8, 4, 16, 144), (9216, 2304, 144, 1), device='cpu', dtype=torch.float32)
    kernel_cpp_0(c_void_p(arg0_1.data_ptr()), c_void_p(buf0.data_ptr()))
    del arg0_1
    return (buf0, )
```

After:

```
from ctypes import c_void_p, c_long
import torch
import random
from torch import empty_strided, as_strided, device
from torch._inductor.codecache import AsyncCompile
from torch._inductor.select_algorithm import extern_kernels

aten = torch.ops.aten
assert_size_stride = torch._C._dynamo.guards.assert_size_stride
async_compile = AsyncCompile()

kernel_cpp_0 = async_compile.cpp('''
#include ""/tmp/torchinductor_xiaobing/dm/cdmaihqxwe73zkb3he2zizktpq5uujetg2db26c3r4lgsmlx3b4c.h""
extern ""C"" void kernel(const float* __restrict__ in_ptr0,
                       float* __restrict__ out_ptr0)
{
    {
        #pragma GCC ivdep
        for(long i0=0; i0<8; i0+=1)
        {
            #pragma GCC ivdep
            for(long i1=0; i1<4; i1+=1)
            {
                #pragma GCC ivdep
                for(long i2=0; i2<16; i2+=1)
                {
                    #pragma GCC ivdep
                    for(long i3=0; i3<144; i3+=1)
                    {
                        auto tmp0 = in_ptr0[i2 + (48*i0) + (384*(i3 % 12)) + (3072*(i1 % 2)) + (7680*(i3 / 12)) + (61440*(i1 / 2))];
                        out_ptr0[i3 + (144*i2) + (2304*i1) + (9216*i0)] = tmp0;
                    }
                }
            }
        }
    }
}
''')

async_compile.wait(globals())
del async_compile

def call(args):
    arg0_1, = args
    args.clear()
    buf0 = empty_strided((8, 4, 16, 144), (9216, 2304, 144, 1), device='cpu', dtype=torch.float32)
    kernel_cpp_0(c_void_p(arg0_1.data_ptr()), c_void_p(buf0.data_ptr()))
    del arg0_1
    return (buf0, )

if __name__ == ""__main__"":
    from torch._dynamo.testing import rand_strided
    from torch._inductor.utils import print_performance
    arg0_1 = rand_strided((1, 384, 20, 20), (153600, 1, 7680, 384), device='cpu', dtype=torch.float32)
    print_performance(lambda: call([arg0_1]))

```

Pull Request resolved: https://github.com/pytorch/pytorch/pull/94493
Approved by: https://github.com/jgong5, https://github.com/jansel, https://github.com/EikanWang"
pytorch/pytorch,544c04f2dfb0e0a1eeba128e0f012ec31aab549c,"Add uint8 support for interpolate for CPU images (#90771)

Joint work with @vfdev-5

This PR introduces native uint8 support for `interpolate()`, for `bilinear` ~and `bicubic`~ modes for CPU images (`mode=nearest[_exact]` was already supported ).

On a typical torchvision training job on ImageNet, the speedup are ~4X when AVX2 is supported, comparing the uint8 native (this PR) vs torchvision's current `Resize()`:

```
AA = antialias
float = uint8->float->interpolate()->round()->clamp()->uint8 (what Resize() currently does)

input_size         output_size channels_last AA    mode       num_threads  speed-up float vs uint8 (this PR)
(1, 3, 270, 268) -> (224, 224)     True    True    bilinear   num_threads=1   4X    2.6ms vs 0.7ms
(1, 3, 270, 268) -> (224, 224)     True    False   bilinear   num_threads=1   2.1X  1.3ms vs 0.6ms
(1, 3, 270, 268) -> (224, 224)     False   True    bilinear   num_threads=1   3X    2.1ms vs 0.7ms
(1, 3, 270, 268) -> (224, 224)     False   False   bilinear   num_threads=1   4X    2.4ms vs 0.6ms

(Note: we removed bicubic support for now)
(1, 3, 270, 268) -> (224, 224)     True    True    bicubic    num_threads=1   4X    2.9ms vs 0.7ms
(1, 3, 270, 268) -> (224, 224)     True    False   bicubic    num_threads=1   5X    3.1ms vs 0.7ms
(1, 3, 270, 268) -> (224, 224)     False   True    bicubic    num_threads=1   3X    2.4ms vs 0.7ms
(1, 3, 270, 268) -> (224, 224)     False   False   bicubic    num_threads=1   4X    2.8ms vs 0.7ms

```

There is still room for further speed-ups (see TODOs in the code).

#### More benchmark details

with AVX2 support - speedups typically range from 1.5X to 10X. A few edge-cases are slower, worth investigating why.

<details>

```
AA = antialias
float = uint8->float->interpolate()->round()->clamp()->uint8 (what Resize() currently does)

input_size         output_size channels_last AA    mode       num_threads  speed-up float vs uint8 (this PR)
(1, 3, 64, 64) -> (224, 224)       True    True    bilinear   num_threads=1   5X    1.1ms vs 0.2ms
(1, 3, 64, 64) -> (224, 224)       True    False   bilinear   num_threads=1   5X    1.2ms vs 0.2ms
(1, 3, 64, 64) -> (224, 224)       False   True    bilinear   num_threads=1   2.8X  0.6ms vs 0.2ms
(1, 3, 64, 64) -> (224, 224)       False   False   bilinear   num_threads=1   7X    1.6ms vs 0.2ms
(1, 3, 64, 64) -> (224, 224)       True    True    bicubic    num_threads=1   5X    1.2ms vs 0.2ms
(1, 3, 64, 64) -> (224, 224)       True    False   bicubic    num_threads=1   12X   2.9ms vs 0.2ms
(1, 3, 64, 64) -> (224, 224)       False   True    bicubic    num_threads=1   3X    0.8ms vs 0.2ms
(1, 3, 64, 64) -> (224, 224)       False   False   bicubic    num_threads=1   7X    1.8ms vs 0.2ms

(1, 3, 64, 64) -> (224, 224)       True    True    bilinear   num_threads=2   2.6X  0.6ms vs 0.2ms
(1, 3, 64, 64) -> (224, 224)       True    False   bilinear   num_threads=2   2.8X  0.6ms vs 0.2ms
(1, 3, 64, 64) -> (224, 224)       False   True    bilinear   num_threads=2   1.7X  0.4ms vs 0.2ms
(1, 3, 64, 64) -> (224, 224)       False   False   bilinear   num_threads=2   1.4X  0.3ms vs 0.2ms
(1, 3, 64, 64) -> (224, 224)       True    True    bicubic    num_threads=2   2.7X  0.7ms vs 0.2ms
(1, 3, 64, 64) -> (224, 224)       True    False   bicubic    num_threads=2   7X    1.6ms vs 0.2ms
(1, 3, 64, 64) -> (224, 224)       False   True    bicubic    num_threads=2   1.8X  0.4ms vs 0.2ms
(1, 3, 64, 64) -> (224, 224)       False   False   bicubic    num_threads=2   4X    1.0ms vs 0.2ms

(1, 3, 224, 224) -> (270, 268)     True    True    bilinear   num_threads=1   4X    2.5ms vs 0.6ms
(1, 3, 224, 224) -> (270, 268)     True    False   bilinear   num_threads=1   3.0X  1.8ms vs 0.6ms
(1, 3, 224, 224) -> (270, 268)     False   True    bilinear   num_threads=1   3X    1.8ms vs 0.6ms
(1, 3, 224, 224) -> (270, 268)     False   False   bilinear   num_threads=1   4X    2.3ms vs 0.6ms
(1, 3, 224, 224) -> (270, 268)     True    True    bicubic    num_threads=1   4X    2.7ms vs 0.6ms
(1, 3, 224, 224) -> (270, 268)     True    False   bicubic    num_threads=1   7X    4.3ms vs 0.6ms
(1, 3, 224, 224) -> (270, 268)     False   True    bicubic    num_threads=1   3X    2.1ms vs 0.6ms
(1, 3, 224, 224) -> (270, 268)     False   False   bicubic    num_threads=1   4X    2.6ms vs 0.6ms

(1, 3, 224, 224) -> (270, 268)     True    True    bilinear   num_threads=2   2.7X  1.6ms vs 0.6ms
(1, 3, 224, 224) -> (270, 268)     True    False   bilinear   num_threads=2   2.6X  1.5ms vs 0.6ms
(1, 3, 224, 224) -> (270, 268)     False   True    bilinear   num_threads=2   2.1X  1.2ms vs 0.6ms
(1, 3, 224, 224) -> (270, 268)     False   False   bilinear   num_threads=2   1.6X  0.9ms vs 0.6ms
(1, 3, 224, 224) -> (270, 268)     True    True    bicubic    num_threads=2   2.8X  1.7ms vs 0.6ms
(1, 3, 224, 224) -> (270, 268)     True    False   bicubic    num_threads=2   5X    2.8ms vs 0.6ms
(1, 3, 224, 224) -> (270, 268)     False   True    bicubic    num_threads=2   2.3X  1.4ms vs 0.6ms
(1, 3, 224, 224) -> (270, 268)     False   False   bicubic    num_threads=2   3X    1.9ms vs 0.6ms

(1, 3, 256, 256) -> (1024, 1024)   True    True    bilinear   num_threads=1   4X    26.6ms vs 6.7ms
(1, 3, 256, 256) -> (1024, 1024)   True    False   bilinear   num_threads=1   4X    23.9ms vs 6.8ms
(1, 3, 256, 256) -> (1024, 1024)   False   True    bilinear   num_threads=1   2.5X  16.8ms vs 6.8ms
(1, 3, 256, 256) -> (1024, 1024)   False   False   bilinear   num_threads=1   5X    33.1ms vs 6.8ms
(1, 3, 256, 256) -> (1024, 1024)   True    True    bicubic    num_threads=1   4X    25.9ms vs 7.3ms
(1, 3, 256, 256) -> (1024, 1024)   True    False   bicubic    num_threads=1   8X    59.6ms vs 7.3ms
(1, 3, 256, 256) -> (1024, 1024)   False   True    bicubic    num_threads=1   1.9X  14.3ms vs 7.4ms
(1, 3, 256, 256) -> (1024, 1024)   False   False   bicubic    num_threads=1   5X    35.4ms vs 7.3ms

(1, 3, 256, 256) -> (1024, 1024)   True    True    bilinear   num_threads=2   2.0X  13.6ms vs 6.8ms
(1, 3, 256, 256) -> (1024, 1024)   True    False   bilinear   num_threads=2   2.2X  14.8ms vs 6.7ms
(1, 3, 256, 256) -> (1024, 1024)   False   True    bilinear   num_threads=2   1.3X  8.8ms vs 6.9ms
(1, 3, 256, 256) -> (1024, 1024)   False   False   bilinear   num_threads=2   1.2X  8.4ms vs 6.8ms
(1, 3, 256, 256) -> (1024, 1024)   True    True    bicubic    num_threads=2   1.8X  12.8ms vs 7.3ms
(1, 3, 256, 256) -> (1024, 1024)   True    False   bicubic    num_threads=2   4X    32.1ms vs 7.2ms
(1, 3, 256, 256) -> (1024, 1024)   False   True    bicubic    num_threads=2   1.4X  10.1ms vs 7.3ms
(1, 3, 256, 256) -> (1024, 1024)   False   False   bicubic    num_threads=2   2.9X  20.9ms vs 7.3ms

(1, 3, 224, 224) -> (64, 64)       True    True    bilinear   num_threads=1   1.4X  0.5ms vs 0.3ms
(1, 3, 224, 224) -> (64, 64)       True    False   bilinear   num_threads=1   0.7X  0.2ms vs 0.3ms
(1, 3, 224, 224) -> (64, 64)       False   True    bilinear   num_threads=1   1.3X  0.4ms vs 0.3ms
(1, 3, 224, 224) -> (64, 64)       False   False   bilinear   num_threads=1   1.4X  0.4ms vs 0.3ms
(1, 3, 224, 224) -> (64, 64)       True    True    bicubic    num_threads=1   2.1X  0.7ms vs 0.3ms
(1, 3, 224, 224) -> (64, 64)       True    False   bicubic    num_threads=1   1.3X  0.4ms vs 0.3ms
(1, 3, 224, 224) -> (64, 64)       False   True    bicubic    num_threads=1   1.9X  0.6ms vs 0.3ms
(1, 3, 224, 224) -> (64, 64)       False   False   bicubic    num_threads=1   1.0X  0.3ms vs 0.3ms

(1, 3, 224, 224) -> (64, 64)       True    True    bilinear   num_threads=2   1.0X  0.3ms vs 0.3ms
(1, 3, 224, 224) -> (64, 64)       True    False   bilinear   num_threads=2   0.6X  0.2ms vs 0.3ms
(1, 3, 224, 224) -> (64, 64)       False   True    bilinear   num_threads=2   0.8X  0.3ms vs 0.3ms
(1, 3, 224, 224) -> (64, 64)       False   False   bilinear   num_threads=2   1.4X  0.4ms vs 0.3ms
(1, 3, 224, 224) -> (64, 64)       True    True    bicubic    num_threads=2   1.4X  0.5ms vs 0.3ms
(1, 3, 224, 224) -> (64, 64)       True    False   bicubic    num_threads=2   1.2X  0.4ms vs 0.3ms
(1, 3, 224, 224) -> (64, 64)       False   True    bicubic    num_threads=2   1.2X  0.4ms vs 0.4ms
(1, 3, 224, 224) -> (64, 64)       False   False   bicubic    num_threads=2   0.9X  0.3ms vs 0.3ms

(1, 3, 270, 268) -> (224, 224)     True    True    bilinear   num_threads=1   4X    2.6ms vs 0.7ms
(1, 3, 270, 268) -> (224, 224)     True    False   bilinear   num_threads=1   2.1X  1.3ms vs 0.6ms
(1, 3, 270, 268) -> (224, 224)     False   True    bilinear   num_threads=1   3X    2.1ms vs 0.7ms
(1, 3, 270, 268) -> (224, 224)     False   False   bilinear   num_threads=1   4X    2.4ms vs 0.6ms
(1, 3, 270, 268) -> (224, 224)     True    True    bicubic    num_threads=1   4X    2.9ms vs 0.7ms
(1, 3, 270, 268) -> (224, 224)     True    False   bicubic    num_threads=1   5X    3.1ms vs 0.7ms
(1, 3, 270, 268) -> (224, 224)     False   True    bicubic    num_threads=1   3X    2.4ms vs 0.7ms
(1, 3, 270, 268) -> (224, 224)     False   False   bicubic    num_threads=1   4X    2.8ms vs 0.7ms

(1, 3, 270, 268) -> (224, 224)     True    True    bilinear   num_threads=2   1.5X  1.0ms vs 0.7ms
(1, 3, 270, 268) -> (224, 224)     True    False   bilinear   num_threads=2   1.2X  0.8ms vs 0.6ms
(1, 3, 270, 268) -> (224, 224)     False   True    bilinear   num_threads=2   2.3X  1.5ms vs 0.7ms
(1, 3, 270, 268) -> (224, 224)     False   False   bilinear   num_threads=2   1.9X  1.2ms vs 0.6ms
(1, 3, 270, 268) -> (224, 224)     True    True    bicubic    num_threads=2   1.6X  1.2ms vs 0.7ms
(1, 3, 270, 268) -> (224, 224)     True    False   bicubic    num_threads=2   4X    2.4ms vs 0.7ms
(1, 3, 270, 268) -> (224, 224)     False   True    bicubic    num_threads=2   2.4X  1.6ms vs 0.7ms
(1, 3, 270, 268) -> (224, 224)     False   False   bicubic    num_threads=2   2.8X  1.8ms vs 0.6ms

(1, 3, 1024, 1024) -> (256, 256)   True    True    bilinear   num_threads=1   2.1X  12.8ms vs 6.1ms
(1, 3, 1024, 1024) -> (256, 256)   True    False   bilinear   num_threads=1   0.6X  3.8ms vs 5.9ms
(1, 3, 1024, 1024) -> (256, 256)   False   True    bilinear   num_threads=1   1.2X  7.1ms vs 6.1ms
(1, 3, 1024, 1024) -> (256, 256)   False   False   bilinear   num_threads=1   1.9X  11.0ms vs 5.9ms
(1, 3, 1024, 1024) -> (256, 256)   True    True    bicubic    num_threads=1   2.0X  12.6ms vs 6.4ms
(1, 3, 1024, 1024) -> (256, 256)   True    False   bicubic    num_threads=1   1.0X  6.1ms vs 6.0ms
(1, 3, 1024, 1024) -> (256, 256)   False   True    bicubic    num_threads=1   1.8X  11.3ms vs 6.4ms
(1, 3, 1024, 1024) -> (256, 256)   False   False   bicubic    num_threads=1   0.8X  4.6ms vs 6.0ms

(1, 3, 1024, 1024) -> (256, 256)   True    True    bilinear   num_threads=2   1.6X  9.3ms vs 6.0ms
(1, 3, 1024, 1024) -> (256, 256)   True    False   bilinear   num_threads=2   0.3X  2.0ms vs 5.8ms
(1, 3, 1024, 1024) -> (256, 256)   False   True    bilinear   num_threads=2   1.2X  7.2ms vs 6.0ms
(1, 3, 1024, 1024) -> (256, 256)   False   False   bilinear   num_threads=2   0.3X  1.6ms vs 5.8ms
(1, 3, 1024, 1024) -> (256, 256)   True    True    bicubic    num_threads=2   1.1X  7.1ms vs 6.5ms
(1, 3, 1024, 1024) -> (256, 256)   True    False   bicubic    num_threads=2   0.6X  3.3ms vs 5.9ms
(1, 3, 1024, 1024) -> (256, 256)   False   True    bicubic    num_threads=2   0.9X  5.9ms vs 6.3ms
(1, 3, 1024, 1024) -> (256, 256)   False   False   bicubic    num_threads=2   0.4X  2.4ms vs 5.9ms
```

</details>

without AVX2 support - no significant speed-up, but there are various possible improvements (see TODOs)

<details>

```
AA = antialias
float = uint8->float->interpolate()->round()->clamp()->uint8 (what Resize() currently does)

input_size         output_size channels_last AA    mode       num_threads  speed-up float vs uint8 (this PR)
(1, 3, 64, 64) -> (224, 224)       True    True    bilinear   num_threads=1   0.9X  1.5ms vs 1.6ms
(1, 3, 64, 64) -> (224, 224)       True    False   bilinear   num_threads=1   0.9X  1.5ms vs 1.6ms
(1, 3, 64, 64) -> (224, 224)       False   True    bilinear   num_threads=1   0.8X  0.9ms vs 1.1ms
(1, 3, 64, 64) -> (224, 224)       False   False   bilinear   num_threads=1   1.5X  1.7ms vs 1.1ms
(1, 3, 64, 64) -> (224, 224)       True    True    bicubic    num_threads=1   0.9X  1.6ms vs 1.8ms
(1, 3, 64, 64) -> (224, 224)       True    False   bicubic    num_threads=1   2.1X  3.9ms vs 1.9ms
(1, 3, 64, 64) -> (224, 224)       False   True    bicubic    num_threads=1   0.8X  1.1ms vs 1.4ms
(1, 3, 64, 64) -> (224, 224)       False   False   bicubic    num_threads=1   1.7X  2.4ms vs 1.5ms

(1, 3, 64, 64) -> (224, 224)       True    True    bilinear   num_threads=2   0.9X  0.8ms vs 0.8ms
(1, 3, 64, 64) -> (224, 224)       True    False   bilinear   num_threads=2   0.9X  0.8ms vs 0.8ms
(1, 3, 64, 64) -> (224, 224)       False   True    bilinear   num_threads=2   0.9X  0.5ms vs 0.6ms
(1, 3, 64, 64) -> (224, 224)       False   False   bilinear   num_threads=2   0.7X  0.5ms vs 0.7ms
(1, 3, 64, 64) -> (224, 224)       True    True    bicubic    num_threads=2   0.9X  0.9ms vs 1.0ms
(1, 3, 64, 64) -> (224, 224)       True    False   bicubic    num_threads=2   2.1X  2.0ms vs 1.0ms
(1, 3, 64, 64) -> (224, 224)       False   True    bicubic    num_threads=2   0.8X  0.6ms vs 0.8ms
(1, 3, 64, 64) -> (224, 224)       False   False   bicubic    num_threads=2   1.7X  1.3ms vs 0.8ms

(1, 3, 224, 224) -> (270, 268)     True    True    bilinear   num_threads=1   1.0X  3.0ms vs 3.0ms
(1, 3, 224, 224) -> (270, 268)     True    False   bilinear   num_threads=1   1.0X  2.8ms vs 2.9ms
(1, 3, 224, 224) -> (270, 268)     False   True    bilinear   num_threads=1   1.0X  2.3ms vs 2.2ms
(1, 3, 224, 224) -> (270, 268)     False   False   bilinear   num_threads=1   1.4X  3.3ms vs 2.3ms
(1, 3, 224, 224) -> (270, 268)     True    True    bicubic    num_threads=1   1.0X  3.5ms vs 3.5ms
(1, 3, 224, 224) -> (270, 268)     True    False   bicubic    num_threads=1   1.7X  6.1ms vs 3.5ms
(1, 3, 224, 224) -> (270, 268)     False   True    bicubic    num_threads=1   0.9X  2.6ms vs 2.9ms
(1, 3, 224, 224) -> (270, 268)     False   False   bicubic    num_threads=1   1.4X  4.2ms vs 2.9ms

(1, 3, 224, 224) -> (270, 268)     True    True    bilinear   num_threads=2   1.0X  1.7ms vs 1.7ms
(1, 3, 224, 224) -> (270, 268)     True    False   bilinear   num_threads=2   0.9X  1.6ms vs 1.8ms
(1, 3, 224, 224) -> (270, 268)     False   True    bilinear   num_threads=2   0.9X  1.3ms vs 1.4ms
(1, 3, 224, 224) -> (270, 268)     False   False   bilinear   num_threads=2   0.7X  1.1ms vs 1.6ms
(1, 3, 224, 224) -> (270, 268)     True    True    bicubic    num_threads=2   1.0X  2.0ms vs 2.0ms
(1, 3, 224, 224) -> (270, 268)     True    False   bicubic    num_threads=2   1.7X  3.2ms vs 1.9ms
(1, 3, 224, 224) -> (270, 268)     False   True    bicubic    num_threads=2   0.8X  1.5ms vs 1.9ms
(1, 3, 224, 224) -> (270, 268)     False   False   bicubic    num_threads=2   1.2X  2.3ms vs 1.9ms

(1, 3, 256, 256) -> (1024, 1024)   True    True    bilinear   num_threads=1   1.1X  34.7ms vs 32.4ms
(1, 3, 256, 256) -> (1024, 1024)   True    False   bilinear   num_threads=1   1.0X  31.2ms vs 32.4ms
(1, 3, 256, 256) -> (1024, 1024)   False   True    bilinear   num_threads=1   1.0X  23.5ms vs 22.7ms
(1, 3, 256, 256) -> (1024, 1024)   False   False   bilinear   num_threads=1   1.9X  42.5ms vs 22.7ms
(1, 3, 256, 256) -> (1024, 1024)   True    True    bicubic    num_threads=1   0.9X  33.9ms vs 37.4ms
(1, 3, 256, 256) -> (1024, 1024)   True    False   bicubic    num_threads=1   2.2X  84.0ms vs 37.5ms
(1, 3, 256, 256) -> (1024, 1024)   False   True    bicubic    num_threads=1   1.0X  28.4ms vs 28.8ms
(1, 3, 256, 256) -> (1024, 1024)   False   False   bicubic    num_threads=1   2.0X  56.7ms vs 28.8ms

(1, 3, 256, 256) -> (1024, 1024)   True    True    bilinear   num_threads=2   1.1X  17.5ms vs 16.4ms
(1, 3, 256, 256) -> (1024, 1024)   True    False   bilinear   num_threads=2   1.1X  17.7ms vs 16.4ms
(1, 3, 256, 256) -> (1024, 1024)   False   True    bilinear   num_threads=2   0.8X  8.8ms vs 11.4ms
(1, 3, 256, 256) -> (1024, 1024)   False   False   bilinear   num_threads=2   1.0X  11.1ms vs 11.4ms
(1, 3, 256, 256) -> (1024, 1024)   True    True    bicubic    num_threads=2   1.1X  19.9ms vs 18.8ms
(1, 3, 256, 256) -> (1024, 1024)   True    False   bicubic    num_threads=2   2.3X  42.5ms vs 18.7ms
(1, 3, 256, 256) -> (1024, 1024)   False   True    bicubic    num_threads=2   1.0X  14.1ms vs 14.5ms
(1, 3, 256, 256) -> (1024, 1024)   False   False   bicubic    num_threads=2   2.0X  28.4ms vs 14.5ms

(1, 3, 224, 224) -> (64, 64)       True    True    bilinear   num_threads=1   1.0X  0.6ms vs 0.6ms
(1, 3, 224, 224) -> (64, 64)       True    False   bilinear   num_threads=1   0.7X  0.3ms vs 0.4ms
(1, 3, 224, 224) -> (64, 64)       False   True    bilinear   num_threads=1   0.9X  0.5ms vs 0.6ms
(1, 3, 224, 224) -> (64, 64)       False   False   bilinear   num_threads=1   1.7X  0.6ms vs 0.4ms
(1, 3, 224, 224) -> (64, 64)       True    True    bicubic    num_threads=1   1.0X  0.8ms vs 0.8ms
(1, 3, 224, 224) -> (64, 64)       True    False   bicubic    num_threads=1   1.1X  0.5ms vs 0.5ms
(1, 3, 224, 224) -> (64, 64)       False   True    bicubic    num_threads=1   0.9X  0.7ms vs 0.8ms
(1, 3, 224, 224) -> (64, 64)       False   False   bicubic    num_threads=1   0.9X  0.4ms vs 0.4ms

(1, 3, 224, 224) -> (64, 64)       True    True    bilinear   num_threads=2   1.0X  0.4ms vs 0.4ms
(1, 3, 224, 224) -> (64, 64)       True    False   bilinear   num_threads=2   0.8X  0.2ms vs 0.3ms
(1, 3, 224, 224) -> (64, 64)       False   True    bilinear   num_threads=2   0.9X  0.3ms vs 0.3ms
(1, 3, 224, 224) -> (64, 64)       False   False   bilinear   num_threads=2   1.3X  0.3ms vs 0.2ms
(1, 3, 224, 224) -> (64, 64)       True    True    bicubic    num_threads=2   1.0X  0.5ms vs 0.5ms
(1, 3, 224, 224) -> (64, 64)       True    False   bicubic    num_threads=2   1.3X  0.4ms vs 0.3ms
(1, 3, 224, 224) -> (64, 64)       False   True    bicubic    num_threads=2   0.9X  0.5ms vs 0.5ms
(1, 3, 224, 224) -> (64, 64)       False   False   bicubic    num_threads=2   1.2X  0.3ms vs 0.3ms

(1, 3, 270, 268) -> (224, 224)     True    True    bilinear   num_threads=1   0.8X  2.1ms vs 2.5ms
(1, 3, 270, 268) -> (224, 224)     True    False   bilinear   num_threads=1   0.7X  1.6ms vs 2.4ms
(1, 3, 270, 268) -> (224, 224)     False   True    bilinear   num_threads=1   1.2X  2.4ms vs 2.1ms
(1, 3, 270, 268) -> (224, 224)     False   False   bilinear   num_threads=1   1.3X  2.6ms vs 2.0ms
(1, 3, 270, 268) -> (224, 224)     True    True    bicubic    num_threads=1   1.1X  3.4ms vs 3.0ms
(1, 3, 270, 268) -> (224, 224)     True    False   bicubic    num_threads=1   1.7X  4.8ms vs 2.8ms
(1, 3, 270, 268) -> (224, 224)     False   True    bicubic    num_threads=1   1.1X  2.9ms vs 2.7ms
(1, 3, 270, 268) -> (224, 224)     False   False   bicubic    num_threads=1   1.4X  3.5ms vs 2.4ms

(1, 3, 270, 268) -> (224, 224)     True    True    bilinear   num_threads=2   0.9X  1.2ms vs 1.3ms
(1, 3, 270, 268) -> (224, 224)     True    False   bilinear   num_threads=2   1.3X  1.6ms vs 1.2ms
(1, 3, 270, 268) -> (224, 224)     False   True    bilinear   num_threads=2   0.8X  0.9ms vs 1.1ms
(1, 3, 270, 268) -> (224, 224)     False   False   bilinear   num_threads=2   1.3X  1.3ms vs 1.0ms
(1, 3, 270, 268) -> (224, 224)     True    True    bicubic    num_threads=2   1.4X  2.2ms vs 1.6ms
(1, 3, 270, 268) -> (224, 224)     True    False   bicubic    num_threads=2   1.9X  2.8ms vs 1.5ms
(1, 3, 270, 268) -> (224, 224)     False   True    bicubic    num_threads=2   0.8X  1.1ms vs 1.4ms
(1, 3, 270, 268) -> (224, 224)     False   False   bicubic    num_threads=2   1.7X  2.1ms vs 1.3ms

(1, 3, 1024, 1024) -> (256, 256)   True    True    bilinear   num_threads=1   1.0X  10.0ms vs 9.9ms
(1, 3, 1024, 1024) -> (256, 256)   True    False   bilinear   num_threads=1   0.7X  4.6ms vs 6.2ms
(1, 3, 1024, 1024) -> (256, 256)   False   True    bilinear   num_threads=1   0.9X  9.1ms vs 9.8ms
(1, 3, 1024, 1024) -> (256, 256)   False   False   bilinear   num_threads=1   1.7X  9.4ms vs 5.7ms
(1, 3, 1024, 1024) -> (256, 256)   True    True    bicubic    num_threads=1   1.0X  15.2ms vs 14.8ms
(1, 3, 1024, 1024) -> (256, 256)   True    False   bicubic    num_threads=1   1.0X  7.6ms vs 7.5ms
(1, 3, 1024, 1024) -> (256, 256)   False   True    bicubic    num_threads=1   0.9X  13.3ms vs 14.4ms
(1, 3, 1024, 1024) -> (256, 256)   False   False   bicubic    num_threads=1   0.8X  5.9ms vs 7.0ms

(1, 3, 1024, 1024) -> (256, 256)   True    True    bilinear   num_threads=2   1.2X  6.0ms vs 5.2ms
(1, 3, 1024, 1024) -> (256, 256)   True    False   bilinear   num_threads=2   0.7X  2.3ms vs 3.2ms
(1, 3, 1024, 1024) -> (256, 256)   False   True    bilinear   num_threads=2   1.0X  4.8ms vs 5.0ms
(1, 3, 1024, 1024) -> (256, 256)   False   False   bilinear   num_threads=2   0.7X  1.9ms vs 2.9ms
(1, 3, 1024, 1024) -> (256, 256)   True    True    bicubic    num_threads=2   1.6X  12.3ms vs 7.5ms
(1, 3, 1024, 1024) -> (256, 256)   True    False   bicubic    num_threads=2   1.0X  3.9ms vs 3.9ms
(1, 3, 1024, 1024) -> (256, 256)   False   True    bicubic    num_threads=2   1.0X  7.0ms vs 7.3ms
(1, 3, 1024, 1024) -> (256, 256)   False   False   bicubic    num_threads=2   0.9X  3.0ms vs 3.5ms

```

</details>

Benchmark code
<details>

```py
import operator_benchmark as op_bench
import torch

""""""Microbenchmarks for interpolate operator.""""""

class InterpolateBenchmark(op_bench.TorchBenchmarkBase):
    def init(self, input_size, output_size, channels_last=False, mode='linear', antialias=False, dtype=torch.float):

        input_image = torch.randint(0, 256, size=input_size, dtype=torch.uint8, device='cpu')

        if channels_last:
            input_image = input_image.contiguous(memory_format=torch.channels_last)

        self.inputs = {
            ""input_image"": input_image,
            ""output_size"": output_size,
            ""mode"": mode,
            ""antialias"": antialias,
            ""dtype"":dtype,
        }

        self.set_module_name(""interpolate"")

    def forward(self, input_image, output_size, mode, antialias, dtype):
        if dtype == torch.float:
            input_image = input_image.float()

        out = torch.nn.functional.interpolate(input_image, size=output_size, mode=mode, align_corners=False, antialias=antialias)
        if dtype == torch.float:
            out = out.round().clamp(min=0, max=256).to(torch.uint8)

def make_config():
    sizes = (
        ((224, 224), (64, 64)),
        ((270, 268), (224, 224)),
        ((256, 256), (1024, 1024)),
    )

    attrs = []
    for (HW1, HW2) in sizes:
        attrs.append([(1, 3, *HW1), HW2])  # 3 channels
        # attrs.append([(1, 1, *HW1), HW2])  # 1 channel

        attrs.append([(1, 3, *HW2), HW1])  # 3 channels
        # attrs.append([(1, 1, *HW2), HW1])  # 1 channel

    config = op_bench.config_list(
        attr_names=[""input_size"", ""output_size""],
        attrs=attrs,
        cross_product_configs={
            'channels_last': [True, False],
            'mode': [""bilinear"", ""bicubic""],
            'antialias': [True, False],
            # 'dtype': [torch.float, torch.uint8]
            # 'dtype': [torch.uint8]
            'dtype': [torch.float]
        },
        tags=[""short""],
    )

    return config

config = make_config()
op_bench.generate_pt_test(config, InterpolateBenchmark)

if __name__ == ""__main__"":
    op_bench.benchmark_runner.main()

```

```py
import re
import argparse

parser = argparse.ArgumentParser()
parser.add_argument(""f1"", nargs=""?"", default=""main"")
parser.add_argument(""f2"", nargs=""?"", default=""new"")
args = parser.parse_args()

with open(args.f1) as f:
    main = f.readlines()
with open(args.f2) as f:
    new = f.readlines()

out = []

for main_line, new_line in zip(main, new):
    # num_threads=1  # TODO: remove
    if main_line.startswith(""num_threads=""):
        num_threads = int(main_line.split(""="")[-1])
    if main_line.startswith(""# Input""):
        deets = f""{main_line.strip()}, {num_threads=}""
    if main_line.startswith(""Forward""):
        main_time = float(main_line.split()[-1])
        new_time = float(new_line.split()[-1])
        ratio = main_time / new_time
        fmt = "".1f"" if ratio < 3 else "".0f""
        improv = f""{ratio:{fmt}}X""
        time_fmt = "",.3f"" if new_time < 100 else "",.1f""
        deets = deets.strip().replace(""# Input: "", """")
        deets = deets.replace("": "", ""="")
        deets = deets.replace(""input_size="", """")
        deets = deets.replace("", output_size="", "" -> "")
        deets = deets.replace(""dtype=torch."", """")
        deets = deets.replace(""mode="", """")
        deets = deets.replace(""antialias="", """")
        deets = deets.replace(""channels_last="", """")
        # deets = deets.replace(""channels_last=True, "", """")
        split = deets.split("","")

        # size = ','.join(split[:-3])
        # mode, dtype, threads = split[-3:]
        # deets = f""{size:<30} {mode:<15} {dtype:<10} {threads:<15}""

        size = ','.join(split[:-5])
        channels_last, mode, antialias, dtype, threads= split[-5:]
        deets = f""{size:<33} {channels_last:<7} {antialias:<7} {mode:<10} {threads:<15}""

        l = f""{deets}  {improv:<5} {main_time / 1000:{time_fmt}}ms vs {new_time / 1000:{time_fmt}}ms""
        out.append(l)

def key(s):
    # s = ''.join(s.split()[1:]) # remove ""N.nX"" part
    num_threads = (int(re.findall(r""num_threads=(\d+)"", s)[0]),)

    input_shape, output_shape = re.findall(""\(.*?\)"", s)
    input_shape = input_shape[1:-1]  # remove parenthesis
    input_HW = tuple(int(x) for x in input_shape.split("","")[-2:])
    input_C = (-int(input_shape.split("","")[1]),)

    output_HW = tuple(int(x) for x in output_shape[1:-1].split("",""))
    is_downsample = (output_HW[0] < input_HW[0],)
    if ""linear"" in s:
        mode = ""linear""
    elif ""nearest-exact"" in s:
        mode = ""nearest-exact""
    else:
        # assert ""nearest"" in s
        mode = ""nearest""
    mode = (mode,)
    return is_downsample + input_HW + output_HW + num_threads + input_C + mode

for i, l in enumerate(sorted(out, key=key)):
    if i % 8 == 0:
        print()
    # if i % 10 == 0 and i % 40 != 0:
    #     print()
    # if i % 40 == 0:
    #     print(""-"" * 100)
    print(l)

```

</details>

Pull Request resolved: https://github.com/pytorch/pytorch/pull/90771
Approved by: https://github.com/peterbell10, https://github.com/ngimel"
pytorch/pytorch,92620aface4588da69ec65c3ac20099825ded340,"[DCP]Update optimizer.py docstring (#94379)

Update load_sharded_optimizer_state_dict() docstring.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/94379
Approved by: https://github.com/fduwjj"
pytorch/pytorch,dc70b00d0b8c47a7952aa91fc1c7c6c6220532f6,"Track and record hint on SymNode and use when possible (#94201)

Historically, we work out `size_hint` by working it out on the fly by doing a substitution on the sympy expression with the `var_to_val` mapping. With this change, we also maintain the hint directly on SymNode (in `expr._hint`) and use it in lieu of Sympy substitution when it is available (mostly guards on SymInt, etc; in particular, in idiomatic Inductor code, we typically manipulate Sympy expressions directly and so do not have a way to conveniently maintain hints.)

While it's possible this will give us modest performance improvements, this is not the point of this PR; the goal is to make it easier to carefully handle unbacked SymInts, where hints are expected not to be available. You can now easily test if a SymInt is backed or not by checking `symint.node.hint is None`.

Signed-off-by: Edward Z. Yang <ezyang@meta.com>

Pull Request resolved: https://github.com/pytorch/pytorch/pull/94201
Approved by: https://github.com/voznesenskym"
pytorch/pytorch,b27ac6dc56ddf44646b4b513f4cab9cfbb0e1d16,"[ONNX] Add full checker mode in torch.onnx.export (#83186)

Fix #82589
Why:
1. **full_check** works in `onnx::checker::check_model` function as it turns on **strict_mode** in `onnx::shape_inference::InferShapes()` which I think that was the intention of this part of code.
2. **strict_mode** catches failed shape type inference (invalid ONNX model from onnx perspective) and ONNXRUNTIME can't run these invalid models, as ONNXRUNTIME actually rely on ONNX shape type inference to optimize ONNX graph. Why we don't set it True for default? >>> some of existing users use other platform, such as caffe2 to run ONNX model which doesn't need valid ONNX model to run.
3. This PR doesn't change the original behavior of `check_onnx_proto`, but add a warning message for those models which can't pass strict shape type inference, saying the models would fail on onnxruntime.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/83186
Approved by: https://github.com/justinchuby, https://github.com/thiagocrepaldi, https://github.com/jcwchen, https://github.com/BowenBao"
pytorch/pytorch,768e54754309a43a4e4221d00894512e612485f5,"Fix SIGFPE in slow_conv3d_forward_out_cpu (#94325)

Set number of groups to 0 if weights second dimension is zero.

`slow_conv_shape_check` will raise an exception if groups are zero anyway.

Fixes SIGFPE reported in https://github.com/pytorch/pytorch/issues/94125

Pull Request resolved: https://github.com/pytorch/pytorch/pull/94325
Approved by: https://github.com/albanD"
pytorch/pytorch,5f25c0831c789a7e5097bf75bb4fb0de621f568e,"Cleanup hung Windows processes (#94357)

Follow https://stackoverflow.com/questions/40585754/powershell-wont-terminate-hung-process to see if the hung python process can be killed completely

```
C:\Jenkins\Miniconda3\python.exe -bb test_ops.py -v --use-pytest -vv -rfEX -x --reruns=2 --shard-id=0 --num-shards=2 ""-k=not linalg_cholesky"" --import-slow-tests --import-disabled-tests
```

The command `Get-Process -Name $process -ErrorAction Stop | Stop-Process -Force` doesn't stop this process as expect

### Testing

1. Spinning up a local python process on Windows runner `C:\Jenkins\Miniconda3\python.exe debug.py`
2. See that the process is runnning

```
Get-WmiObject -Class Win32_Process -Filter ""Name LIKE 'python%' AND CommandLine LIKE '%debug%'""

__GENUS                    : 2
__CLASS                    : Win32_Process
__SUPERCLASS               : CIM_Process
__DYNASTY                  : CIM_ManagedSystemElement
__RELPATH                  : Win32_Process.Handle=""8812""
__PROPERTY_COUNT           : 45
__DERIVATION               : {CIM_Process, CIM_LogicalElement, CIM_ManagedSystemElement}
__SERVER                   : EC2AMAZ-S19AQ2Q
__NAMESPACE                : root\cimv2
__PATH                     : \\EC2AMAZ-S19AQ2Q\root\cimv2:Win32_Process.Handle=""8812""
Caption                    : python.exe
CommandLine                : ""C:\Jenkins\Miniconda3\python.exe"" debug.py
CreationClassName          : Win32_Process
CreationDate               : 20230208002358.569943+000
CSCreationClassName        : Win32_ComputerSystem
CSName                     : EC2AMAZ-S19AQ2Q
Description                : python.exe
ExecutablePath             : C:\Jenkins\Miniconda3\python.exe
ExecutionState             :
Handle                     : 8812
HandleCount                : 82
InstallDate                :
KernelModeTime             : 312500
MaximumWorkingSetSize      : 1380
MinimumWorkingSetSize      : 200
Name                       : python.exe
OSCreationClassName        : Win32_OperatingSystem
OSName                     : Microsoft Windows Server 2019 Datacenter|C:\Windows|\Device\Harddisk0\Partition1
OtherOperationCount        : 1135
OtherTransferCount         : 150908
PageFaults                 : 2442
PageFileUsage              : 5020
ParentProcessId            : 5396
PeakPageFileUsage          : 5120
PeakVirtualSize            : 4368465920
PeakWorkingSetSize         : 9424
Priority                   : 8
PrivatePageCount           : 5140480
ProcessId                  : 8812
QuotaNonPagedPoolUsage     : 8
QuotaPagedPoolUsage        : 63
QuotaPeakNonPagedPoolUsage : 8
QuotaPeakPagedPoolUsage    : 63
ReadOperationCount         : 88
ReadTransferCount          : 519894
SessionId                  : 0
Status                     :
TerminationDate            :
ThreadCount                : 1
UserModeTime               : 156250
VirtualSize                : 4362371072
WindowsVersion             : 10.0.17763
WorkingSetSize             : 9592832
WriteOperationCount        : 0
WriteTransferCount         : 0
PSComputerName             : EC2AMAZ-S19AQ2Q
ProcessName                : python.exe
Handles                    : 82
VM                         : 4362371072
WS                         : 9592832
Path                       : C:\Jenkins\Miniconda3\python.exe
```

3. Kill it
```
(Get-WmiObject -Class Win32_Process -Filter ""Name LIKE 'python%' AND CommandLine LIKE '%debug%'"").terminate()

__GENUS          : 2
__CLASS          : __PARAMETERS
__SUPERCLASS     :
__DYNASTY        : __PARAMETERS
__RELPATH        :
__PROPERTY_COUNT : 1
__DERIVATION     : {}
__SERVER         :
__NAMESPACE      :
__PATH           :
ReturnValue      : 0
PSComputerName   :
```

4. Confirm that the process is killed
Pull Request resolved: https://github.com/pytorch/pytorch/pull/94357
Approved by: https://github.com/clee2000, https://github.com/malfet"
pytorch/pytorch,83275d8cdf7721285c4e1b921c28295dc215ba7c,"add torch.autograd._set_view_replay_enabled, use in aot autograd (#92588)

tldr; this should fix some minor perf regressions that were caused by adding more as_strided() calls in aot autograd.

This PR adds a new context manager, `torch.autograd._set_view_replay_enabled()`.

Context: AOT Autograd has special handling for ""outputs that alias graph intermediates"". E.g. given this function:

```
def f(x):
    y = torch.mul(x, 2)
    out = y.view(-1)
    return out
```

AOT Autograd will do the following:

```
def fn_to_compile(x):
    y = torch.mul(x, 2)
    out = y.view(-1)
    # return the graph intermediate
    return y, out

compiled_fn = compile(fn_to_compile)

def wrapper(x):
    y, out = compiled_fn(x)
    # regenerate the alias of the graph intermediate
    return out._view_func(y)
```

What's annoying is that `out._view_func()` will result in a `.as_strided` call, because `out` is an ordinary runtime tensor. This (likely?) caused a perf regression, because when running the backward, out `as_strided_backward()` is slower than our `view_backward()`.

In this PR, I added some TLS for instructing autograd to do view replay instead of as_strided, even when given a normal tensor. I'm definitely interested in thoughts from autograd folks (cc @albanD @soulitzer). A few points that I want to bring up:

(1) One reason that this API seems generally useful to me is because of the case where you `torch.compile()` a function, and you pass in two inputs that alias each other, and mutate one of the inputs. Autograd is forced to add a bunch of as_strided() calls into the graph when this happens, but this would give users an escape hatch for better compiled perf in this situation

(2) To be fair, AOT Autograd probably won't need this TLS in the long term. There's a better (more complicated) solution, where AOT Autograd manually precomputes the view chain off of graph intermediates during tracing, and re-applies them at runtime. This is kind of complicated though and feels lower priority to implement immediately.

(3) Given all of that I made the API private, but lmk what you all think.

This is a followup of https://github.com/pytorch/pytorch/pull/92255.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/92588
Approved by: https://github.com/ezyang, https://github.com/albanD"
pytorch/pytorch,3ce1ebb6fb54666288f0016e58a1af9e024af7bb,"Apply some safe comprehension optimizations (#94323)

Optimize unnecessary collection cast calls, unnecessary calls to list, tuple, and dict, and simplify calls to the sorted builtin. This should strictly improve speed and improve readability.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/94323
Approved by: https://github.com/albanD"
pytorch/pytorch,ab4fe01e72c8de5e38d107647d16d78296699708,"[FSDP][optim_state_dict] Returns the initial states of the empty parameters for KeyedOptimizer/NamedOptimizer (#94130)

KeyedOptimizer and NamedOptimizer expect the states exist in the state_dict when `load_state_dict` is called even if the corresponding parameters are empty (size == 0). This PR adds the support to make KeyedOptimizer work with `use_orig_params=True`.

Differential Revision: [D43019458](https://our.internmc.facebook.com/intern/diff/D43019458/)
Pull Request resolved: https://github.com/pytorch/pytorch/pull/94130
Approved by: https://github.com/rohan-varma"
pytorch/pytorch,748bac8757f79895f0ac0ec3925868f52c4bf5d1,"[BE]: Apply pyupgrade yield from and unit test alias upgrades (#94309)

Applies some more harmless pyupgrades. This one gets rid of deprecated aliases in unit_tests and more upgrades yield for loops into yield from generators which are more performance and propagates more information / exceptions from original generator. This is the modern recommended way of forwarding generators.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/94309
Approved by: https://github.com/albanD"
pytorch/pytorch,d690a596dcbccf943f90c3e5353394cadc29dfd1,"Fast path binary ops in fake tensor (#94047)

Fast path execution of a few binary ops in fake tensor, to speed up trace time. When testing `python benchmarks/dynamo/timm_models.py --accuracy --timing --backend aot_eager --dynamic-shapes --float32 --only hrnet_w18`, I get the following trace speedup.

Before:

```
cuda eval  hrnet_w18                           PASS
TIMING: entire_frame_compile:53.97591 backend_compile:33.60832
STATS: call_* op count: 1369 | FakeTensor.__torch_dispatch__:4995 | FakeTensorMode.__torch_dispatch__:89985 | ProxyTorchDispatchMode.__torch_dispatch__:3010
```

After:

```
cuda eval  hrnet_w18                           PASS
TIMING: entire_frame_compile:40.18931 backend_compile:25.28828
STATS: call_* op count: 1369 | FakeTensor.__torch_dispatch__:4995 | FakeTensorMode.__torch_dispatch__:69478 | attempt fast:4399 | fast is_contiguous:4399 | ProxyTorchDispatchMode.__torch_dispatch__:3010
```

My experiment notebook can be found at https://docs.google.com/document/d/1_dTIQUwjIVnEWmiFAavJQYVF8uzXqD9Dk6b9gGQLF_U/edit#

This is not the ""most"" optimized version of the code; compared with Horace/Voz roofline experiment:

```
diff --git a/torch/_subclasses/fake_tensor.py b/torch/_subclasses/fake_tensor.py
index e3bf545f3b8..395942c6ffe 100644
--- a/torch/_subclasses/fake_tensor.py
+++ b/torch/_subclasses/fake_tensor.py
@@ -774,6 +774,10 @@ class FakeTensorMode(TorchDispatchMode):
     def __torch_dispatch__(self, func, types, args=(), kwargs=None):
         kwargs = kwargs if kwargs else {}

+        with no_dispatch():
+            if func in {aten.mul.Tensor, aten.add.Tensor, aten.sub.Tensor, aten.relu.default}:
+                return FakeTensor(self, torch.empty(args[0].shape, device='meta'), device='cuda')
+
         if func == torch.ops.prim.device.default:
             assert len(args) == 1 and isinstance(args[0], FakeTensor)
             if args[0].fake_mode.in_kernel_invocation:
```

I am still leaving about 5s of trace time improvement on the table (3s of which is attributable to not yet handling relu.)

The implementation here is based off of https://github.com/pytorch/pytorch/pull/93118/ but I modeled the short circuit logic off of TensorIterator's implementation, for ease of code review and correctness verification. However, there are some important divergences:

* Traditional fast setup in TensorIterator only short circuits if the shapes of all input elements are equal. On hrnet_w18, only 5% of fastpath'ed binary operators actually satisfy this. So instead, I compute the broadcasted shape, but then I only allow the fast path if (1) at least one input tensor has a shape that is exactly the output size, and (2) all the tensors are contiguous (or if all the tensors are channels last).
* I had to manually adjust the logic to handle wrapped numbers (which ordinarily are handled by wrapping into tensors). I think I got this right.

Some evidence that this heuristic is correct is here in: https://gist.github.com/ezyang/b22fa7b72b7349137211d8dc7041f758 I exhaustively test all dim=3 tensors with sizes [1, 2] and show that we get the same significant strides between PrimTorch and the new algorithm. In fact, there ARE differences between this algorithm and PrimTorch, but in fact this algorithm agrees with TensorIterator where PrimTorch is wrong (sample case: size=(1, 1, 2), stride=(1, 1, 1), stride=(1, 1, 1))

Signed-off-by: Edward Z. Yang <ezyang@meta.com>
Pull Request resolved: https://github.com/pytorch/pytorch/pull/94047
Approved by: https://github.com/eellison"
pytorch/pytorch,e0950fccfad0b467f4956a7c7d56de54d7f90f4e,"[SDPA] Add expanded autograd testing for fused kernels and disable head_dim128 sm86 mem-efficient (#94009)

# Summary
- Adds a large parameter sweep for testing the various configs a user can call sdpa with and compares the deviation of the fused kernels vs the eager math fallback to test for correctness.
- Sm86 + head_dim==128 is throwing an IMA  for memory efficient attention. We add a filter for use_mem_efficient_attention().  This has since been fixed in the upstream Xformers version but will likely not make it for branch cut.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/94009
Approved by: https://github.com/cpuhrsch"
pytorch/pytorch,c4544bc169bd4010de2e78ae69c556697608c2c6,"Fix thread-allocation in `_vec_log_softmax_lastdim` (#85398)

## Problem history

There seems to always have been a bug in `_vec_log_softmax_lastdim `.
In particular, there were two issues with it -

#### Bug 1
 Before AVX512 support was added, `CHUNK_SIZE` had been heuristically chosen in `_vec_log_softmax_lastdim`:
 `CHUNK_SIZE = (128 / sizeof(scalar_t)) * Vec::size();`

It was  `256` for float32, bfloat16, and float16.
When AVX512 support was added, `CHUNK_SIZE` became `512`.

The rationale behind determining `CHUNK_SIZE` has not been described, and seems flawed, since the number of OpenMP threads used currently depends upon it.

#### Bug 2
`grain_size` had been defined as `internal::GRAIN_SIZE / (16 * dim_size * CHUNK_SIZE)`
So, `grain_size` was usually 0, as it was `8 / (dim_size)`, so, it's always replaced by `CHUNK_SIZE`, viz. 256.
Since `256` was always the `grain_size` for `at::parallel_for`, few threads were used in certain cases.

#### Problem caused by bugs
With `outer_size` of say, 700, only 3 threads would have been used with AVX2, irrespective of the value of `dim_size`!
When AVX512 support was added, since `CHUNK_SIZE` became `512`, only 2 threads were used if `outer_dim` was 700.
In the Transformers training example, `log_softmax` was computed on the last dim of a tensor of shape `(700, 23258)`.
AVX512 thus appeared to be quite slower, cloaking the actual issue that even AVX2 performance for the kernel was quite poor due to inefficient work distribution amongst OpenMP threads.

## Solution
Distribute work more efficiently, which would result in higher performance for both AVX2 & AVX512 than now,
and fixes the regression observed with AVX512 (AVX512 kernel would now be faster than its AVX2 counterpart).

## Benchmarks

##### Machine-config:
Intel(R) Xeon(R) Platinum 8371HC CPU (Cooper Lake)
One socket of 26 physical cores was used.
Intel OpenMP & tcmalloc were preloaded.

Example of a command to run benchmark:
`ATEN_CPU_CAPABILITY=avx512 KMP_AFFINITY=granularity=fine,verbose,compact,1,0 KMP_BLOCKTIME=1 KMP_SETTINGS=1 MKL_NUM_THREADS=26 OMP_NUM_THREADS=26 numactl --membind=0 --cpunodebind=0 python3.8 -m pt.softmax_test --test_name LogSoftmax_N1024_seq_len23258_dim1_cpu`

Benchmark | Old implementation time (us) | New implementation time (us) | Speedup ratio (old/new)
-- | -- | -- | --
LogSoftmax_N1024_seq_len23258_dim1_cpu AVX2 | 11069.281 | 2651.186 | 4.17x
LogSoftmax_N1024_seq_len23258_dim1_cpu  AVX512 | 18292.928 | 2586.550| 7.07x
LogSoftmax_N700_seq_len23258_dim1_cpu  AVX2 | 9611.902 | 1762.833 | 5.452x
LogSoftmax_N700_seq_len23258_dim1_cpu  AVX512 | 12168.371  | 1717.824 | 7.08x

Pull Request resolved: https://github.com/pytorch/pytorch/pull/85398
Approved by: https://github.com/jgong5, https://github.com/mingfeima, https://github.com/peterbell10, https://github.com/lezcano"
pytorch/pytorch,900e09c8721fb38672b9d1da8f3a136956132e0f,"[Dynamo] Support torch.Tensor.fn as TorchVariable, not UserDefinedObjectVariable, preventing graph break (#93243)

As found in #92709, thanks to @ngimel and @jansel, currently `torch.Tensor.fn` points to `UserDefinedObjectVariable` rather than `TorchVariable`. The root cause is due to https://github.com/pytorch/pytorch/pull/92709#pullrequestreview-1273357406. To prevent this, build `TorchVariable`  of `torch.Tensor.fn` pointing to `torch.ops.aten.fn`.

This issue propagates to `torch.Tensor.fn` causing graph break with `nopython=True`.
```python
import torch
import torch._dynamo as dynamo

#op = torch.ops.aten.abs_ # no graph break
op = torch.Tensor.abs_ # graph break
args = torch.empty(10)

def foo(args):
    return op(args)

opt_foo = dynamo.optimize(""inductor"", nopython=True)(foo)
y_ = opt_foo(args)

```

Pull Request resolved: https://github.com/pytorch/pytorch/pull/93243
Approved by: https://github.com/jansel"
pytorch/pytorch,6ba041fcae1d6cfabbf9751ed71cc0135a548a17,"Look up `group[""capturable""]`, not `defaults[""capturable""]` in Adam(W) (#94149)

We could set different values in each `param_group` when calling dunder init of `torch.optim` optimizers as in e.g.  https://github.com/pytorch/pytorch/issues/89987.

So check whether or not `capturable` is `True` among all the `param_group`s.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/94149
Approved by: https://github.com/albanD"
pytorch/pytorch,9b2e7d3b4f0ce5937a71df170adfcf0b2c2ae98c,"[Inductor] Performance smoke test - hf bert performance increased  (#94088)

therefore bumping up from 1.185 to 1.200 to better detect regression
logurl date                          model                             speedup
https://ossci-raw-job-status.s3.amazonaws.com/log/11101705328	2023-02-03T23:05:19.5738026Z hf_Bert                            1.2122
https://ossci-raw-job-status.s3.amazonaws.com/log/11101331469	2023-02-03T22:54:18.0252738Z hf_Bert                            1.2129
https://ossci-raw-job-status.s3.amazonaws.com/log/11101288841	2023-02-03T22:52:17.6331332Z hf_Bert                            1.2189
https://ossci-raw-job-status.s3.amazonaws.com/log/11101190372	2023-02-03T22:50:28.6010460Z hf_Bert                            1.2117
https://ossci-raw-job-status.s3.amazonaws.com/log/11101101525	2023-02-03T22:27:18.5573576Z hf_Bert                            1.2088
https://ossci-raw-job-status.s3.amazonaws.com/log/11101034545	2023-02-03T22:24:33.8710157Z hf_Bert                            1.2229
https://ossci-raw-job-status.s3.amazonaws.com/log/11101004878	2023-02-03T22:22:38.0506379Z hf_Bert                            1.2074
https://ossci-raw-job-status.s3.amazonaws.com/log/11100834787	2023-02-03T22:12:34.9376779Z hf_Bert                            1.2142
https://ossci-raw-job-status.s3.amazonaws.com/log/11100413479	2023-02-03T21:47:55.7536822Z hf_Bert                            1.2112
https://ossci-raw-job-status.s3.amazonaws.com/log/11100372087	2023-02-03T21:46:19.6411599Z hf_Bert                            1.2175
https://ossci-raw-job-status.s3.amazonaws.com/log/11100291417	2023-02-03T21:41:01.3427726Z hf_Bert                            1.2068
https://ossci-raw-job-status.s3.amazonaws.com/log/11100137256	2023-02-03T21:32:14.4491714Z hf_Bert                            1.2089
https://ossci-raw-job-status.s3.amazonaws.com/log/11098980986	2023-02-03T20:30:13.4082966Z hf_Bert                            1.2109
https://ossci-raw-job-status.s3.amazonaws.com/log/11098634747	2023-02-03T20:12:57.4921305Z hf_Bert                            1.2169
https://ossci-raw-job-status.s3.amazonaws.com/log/11096295932	2023-02-03T18:58:55.1214750Z hf_Bert                            1.2196
https://ossci-raw-job-status.s3.amazonaws.com/log/11095904757	2023-02-03T18:49:48.4541355Z hf_Bert                            1.22
https://ossci-raw-job-status.s3.amazonaws.com/log/11095292402	2023-02-03T18:10:54.6924201Z hf_Bert                            1.2122
https://ossci-raw-job-status.s3.amazonaws.com/log/11095026691	2023-02-03T18:11:26.7384107Z hf_Bert                            1.2228
https://ossci-raw-job-status.s3.amazonaws.com/log/11094943489	2023-02-03T17:53:00.0989341Z hf_Bert                            1.2165
https://ossci-raw-job-status.s3.amazonaws.com/log/11093227145	2023-02-03T16:04:18.7935799Z hf_Bert                            1.2208
https://ossci-raw-job-status.s3.amazonaws.com/log/11092910912	2023-02-03T15:51:28.1977577Z hf_Bert                            1.2188
https://ossci-raw-job-status.s3.amazonaws.com/log/11091775528	2023-02-03T15:27:21.7984395Z hf_Bert                            1.2231
https://ossci-raw-job-status.s3.amazonaws.com/log/11091768252	2023-02-03T15:12:33.0339859Z hf_Bert                            1.2167
https://ossci-raw-job-status.s3.amazonaws.com/log/11091051563	2023-02-03T14:44:42.7011287Z hf_Bert                            1.2214
https://ossci-raw-job-status.s3.amazonaws.com/log/11088539227	2023-02-03T12:41:29.9098435Z hf_Bert                            1.2192
https://ossci-raw-job-status.s3.amazonaws.com/log/11088428613	2023-02-03T12:35:38.4674850Z hf_Bert                            1.2108
https://ossci-raw-job-status.s3.amazonaws.com/log/11088405279	2023-02-03T12:34:54.0870617Z hf_Bert                            1.2197
https://ossci-raw-job-status.s3.amazonaws.com/log/11087037337	2023-02-03T12:06:58.2426787Z hf_Bert                            1.2174
https://ossci-raw-job-status.s3.amazonaws.com/log/11085381881	2023-02-03T10:19:20.8764019Z hf_Bert                            1.2189
https://ossci-raw-job-status.s3.amazonaws.com/log/11085190037	2023-02-03T10:14:41.5234245Z hf_Bert                            1.2046
https://ossci-raw-job-status.s3.amazonaws.com/log/11085016390	2023-02-03T09:50:59.7484273Z hf_Bert                            1.2155
https://ossci-raw-job-status.s3.amazonaws.com/log/11084948754	2023-02-03T09:47:15.7358069Z hf_Bert                            1.2083
https://ossci-raw-job-status.s3.amazonaws.com/log/11084675155	2023-02-03T09:42:35.6628268Z hf_Bert                            1.2126
https://ossci-raw-job-status.s3.amazonaws.com/log/11081270865	2023-02-03T06:05:22.1828269Z hf_Bert                            1.2083
https://ossci-raw-job-status.s3.amazonaws.com/log/11081252914	2023-02-03T05:43:59.0680872Z hf_Bert                            1.2097
https://ossci-raw-job-status.s3.amazonaws.com/log/11081252670	2023-02-03T05:44:17.0945428Z hf_Bert                            1.2143
https://ossci-raw-job-status.s3.amazonaws.com/log/11081244430	2023-02-03T05:43:43.6811750Z hf_Bert                            1.2204
https://ossci-raw-job-status.s3.amazonaws.com/log/11081191493	2023-02-03T05:38:43.7833293Z hf_Bert                            1.2079
https://ossci-raw-job-status.s3.amazonaws.com/log/11081191168	2023-02-03T05:38:21.1397044Z hf_Bert                            1.2067
https://ossci-raw-job-status.s3.amazonaws.com/log/11081189846	2023-02-03T05:38:53.5914557Z hf_Bert                            1.2073
https://ossci-raw-job-status.s3.amazonaws.com/log/11080883297	2023-02-03T05:13:25.0077772Z hf_Bert                            1.2105
https://ossci-raw-job-status.s3.amazonaws.com/log/11080456108	2023-02-03T04:34:34.0934838Z hf_Bert                            1.204
https://ossci-raw-job-status.s3.amazonaws.com/log/11079957300	2023-02-03T03:53:18.9091026Z hf_Bert                            1.207
https://ossci-raw-job-status.s3.amazonaws.com/log/11078579407	2023-02-03T02:03:11.2254812Z hf_Bert                            1.2049
https://ossci-raw-job-status.s3.amazonaws.com/log/11078204621	2023-02-03T01:58:39.0887941Z hf_Bert                            1.2214
https://ossci-raw-job-status.s3.amazonaws.com/log/11078126527	2023-02-03T01:38:20.2183225Z hf_Bert                            1.2061
https://ossci-raw-job-status.s3.amazonaws.com/log/11077409013	2023-02-03T00:48:51.8981496Z hf_Bert                            1.2086
https://ossci-raw-job-status.s3.amazonaws.com/log/11077176061	2023-02-03T00:27:27.2594172Z hf_Bert                            1.2077
https://ossci-raw-job-status.s3.amazonaws.com/log/11077075809	2023-02-03T00:21:54.4916449Z hf_Bert                            1.2103
https://ossci-raw-job-status.s3.amazonaws.com/log/11076629886	2023-02-02T23:50:38.3512367Z hf_Bert                            1.2191
https://ossci-raw-job-status.s3.amazonaws.com/log/11076577074	2023-02-02T23:46:06.5987589Z hf_Bert                            1.2061
https://ossci-raw-job-status.s3.amazonaws.com/log/11076403972	2023-02-02T23:35:49.7931367Z hf_Bert                            1.2088
https://ossci-raw-job-status.s3.amazonaws.com/log/11076234469	2023-02-02T23:25:55.7300688Z hf_Bert                            1.2099
https://ossci-raw-job-status.s3.amazonaws.com/log/11075752070	2023-02-02T22:57:25.4280216Z hf_Bert                            1.2048
https://ossci-raw-job-status.s3.amazonaws.com/log/11074434992	2023-02-02T22:10:58.4127805Z hf_Bert                            1.2084
https://ossci-raw-job-status.s3.amazonaws.com/log/11074370082	2023-02-02T22:10:06.8153498Z hf_Bert                            1.2075
https://ossci-raw-job-status.s3.amazonaws.com/log/11073914614	2023-02-02T21:25:53.3262334Z hf_Bert                            1.2058
https://ossci-raw-job-status.s3.amazonaws.com/log/11073616418	2023-02-02T21:12:03.0024412Z hf_Bert                            1.2053
https://ossci-raw-job-status.s3.amazonaws.com/log/11072632121	2023-02-02T20:25:37.5689220Z hf_Bert                            1.2082
https://ossci-raw-job-status.s3.amazonaws.com/log/11072091471	2023-02-02T20:00:08.5175281Z hf_Bert                            1.2079
https://ossci-raw-job-status.s3.amazonaws.com/log/11069395867	2023-02-02T18:29:04.6481423Z hf_Bert                            1.2071
https://ossci-raw-job-status.s3.amazonaws.com/log/11069169921	2023-02-02T18:18:36.5701242Z hf_Bert                            1.2036
https://ossci-raw-job-status.s3.amazonaws.com/log/11069070631	2023-02-02T18:15:32.2345859Z hf_Bert                            1.2055
https://ossci-raw-job-status.s3.amazonaws.com/log/11067153829	2023-02-02T16:38:27.4201129Z hf_Bert                            1.2133
https://ossci-raw-job-status.s3.amazonaws.com/log/11066885021	2023-02-02T16:28:44.4489971Z hf_Bert                            1.2043

The above are the result of running a rockset query which returns links to the log and wget the logs and grep ""Z hf_Bert""

Pull Request resolved: https://github.com/pytorch/pytorch/pull/94088
Approved by: https://github.com/desertfire"
pytorch/pytorch,26cba842ada1f65875aa8d73bc4bfdacb105fdbf,"Optimize ConvTransposed2D with mkldnn float32 and bfloat16 on CPU (#92530)

this PR optimized `ConvTranspose2d` with oneDNN and add channels last support for it. Also the fallback path `slow_conv_transpose2d` also have channels last support. So the memory format propagation behavior would stay the same with or without oneDNN.

Replacement of https://github.com/pytorch/pytorch/pull/77060, https://github.com/pytorch/pytorch/pull/70897 and https://github.com/pytorch/pytorch/pull/74023 which enables oneDNN for `ConvTranspose2d` and `ConvTranspose3d`

The following results collects on Skylake Xeon 8180, dual sockets, 28 cores per socket.
### single core channels last

configs | forward before/ms | forward after/ms | ratio | backward   before/ms | backward after/ms | ratio
-- | -- | -- | -- | -- | -- | --
input size: (32, 32, 100, 100), weight size: (32, 32, 3, 3) | 181.36 | 91.16 | 1.99 | 531.38 | 124.08 | 4.28
input size:   (32, 16, 200, 200), weight size: (16, 16, 3, 3) | 324.35 | 153.50 | 2.11 | 973.16 | 185.97 | 5.23
input size:   (32, 128, 100, 100), weight size: (128, 128, 3, 3) | 1086.82 | 671.52 | 1.62 | 3008.94 | 1453.33 | 2.07

### single core channels first

configs | forward before/ms | forward after/ms | ratio | backward   before/ms | backward after/ms | ratio
-- | -- | -- | -- | -- | -- | --
input size: (32, 32, 100, 100), weight size: (32, 32, 3, 3) | 138.10 | 5.94 | 23.23 | 37.97 | 11.25 | 3.38
input size:   (32, 16, 200, 200), weight size: (16, 16, 3, 3) | 236.43 | 8.75 | 27.03 | 87.77 | 18.58 | 4.72
input size:   (32, 128, 100, 100), weight size: (128, 128, 3, 3) | 484.39 | 37.69 | 12.85 | 185.40 | 90.57 | 2.05

### single socket channels last

configs | forward before/ms | forward after/ms | ratio | backward   before/ms | backward after/ms | ratio
-- | -- | -- | -- | -- | -- | --
input size: (32, 32, 100, 100), weight size: (32, 32, 3, 3) | 138.10 | 5.94 | 23.23 | 37.97 | 11.25 | 3.38
input size:   (32, 16, 200, 200), weight size: (16, 16, 3, 3) | 236.43 | 8.75 | 27.03 | 87.77 | 18.58 | 4.72
input size:   (32, 128, 100, 100), weight size: (128, 128, 3, 3) | 484.39 | 37.69 | 12.85 | 185.40 | 90.57 | 2.0

### single socket channels first

configs | forward before/ms | forward after/ms | ratio | backward   before/ms | backward after/ms | ratio
-- | -- | -- | -- | -- | -- | --
input size: (32, 32, 100,   100), weight size: (32, 32, 3, 3) | 132.56 | 7.19 | 18.43 | 31.43 | 11.20 | 2.81
input size:   (32, 16, 200, 200), weight size: (16, 16, 3, 3) | 227.94 | 13.33 | 17.11 | 63.00 | 23.41 | 2.69
input size:   (32, 128, 100, 100), weight size: (128, 128, 3, 3) | 473.68 | 52.79 | 8.97 | 150.40 | 87.33 | 1.72

Pull Request resolved: https://github.com/pytorch/pytorch/pull/92530
Approved by: https://github.com/jgong5, https://github.com/ezyang"
pytorch/pytorch,3693039bb70828828b90b82191c16aa01547482f,"perf: fix missing noexcepts on minpybind in functorch (#94135)

Noticed this performance bug in functorch. We got a pretty big perf in pybind11 improvement by explicitly marking at noexcept, see https://quuxplusone.github.io/blog/2022/08/26/vector-pessimization/

Pull Request resolved: https://github.com/pytorch/pytorch/pull/94135
Approved by: https://github.com/ezyang"
pytorch/pytorch,4207d3c330c2b723caf0e1c4681ffd80f0b1deb7,"`FusedAdam(W)` should take `OptState` into account before unscaling grads (#94060)

the optimizers have to consult `OptState` before unscaling gradients because we could call `GradScaler.unscale_` explicitly to for e.g. `clip_grad_norm_` as mentioned in https://github.com/pytorch/pytorch/blob/e52786f3d177a7ca5d490a516cf52e236ef072cb/torch/cuda/amp/grad_scaler.py#L235-L266 and https://pytorch.org/docs/stable/notes/amp_examples.html#working-with-unscaled-gradients

Related #90752

Pull Request resolved: https://github.com/pytorch/pytorch/pull/94060
Approved by: https://github.com/albanD"
pytorch/pytorch,1a32db15e73c5127db37ddc6b4294d4f1aa8e8c1,"Some performance fixes (#94034)

Applies some performance fixes

Pull Request resolved: https://github.com/pytorch/pytorch/pull/94034
Approved by: https://github.com/Skylion007"
pytorch/pytorch,e98a94239922f45ca006a97540cef8d5d8c31096,"[PTD] Land 'to_std' utility parser fix #93209 (#94023)

Land https://github.com/pytorch/pytorch/pull/93209 faster.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/94023
Approved by: https://github.com/wz337"
pytorch/pytorch,63115b70f03da883a4d2864245eadd2bd5879c1e,"Fixed issue with --diff-branch arg in dynamo benchmarks (#93989)

As @peterbell10 pointed out, it was giving incorrect results for `compression_ratio`
and `compression_latency` when you used `--diff-branch`.

This fixes this by running a separate subprocess for each branch to make sure you are not being affected by run for other branch.

Also added a couple of more significant figures
to numbers in summary table.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/93989
Approved by: https://github.com/jansel"
pytorch/pytorch,dd7d47c4ac4b2dbf80e2b528badda887e7738718,"abstract vectorized reduction utils on CPU (#92284)

This PR abstracts some reduction utils on CPU, which can be shared by multiple reduction operators, such as `scatter_reduce`, `segment_reduce`, `spmm_reduce`.

No functional change or performance change.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/92284
Approved by: https://github.com/ezyang"
pytorch/pytorch,3d020b690338c0500200201630c50505308e01ec,"inductor: separate bias from PackeLinear for better performance (#93348)

For PakedLinear with has bias, we always copy bias to output before doing the computation:
https://github.com/pytorch/pytorch/blob/d7a3f2128fb4457dd60fd5d23e77d2c66a8b0f02/aten/src/ATen/native/mkldnn/Linear.cpp#L389-L397.

This PR separates bias from it which can make the bias add fused with the post-op.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/93348
Approved by: https://github.com/jgong5, https://github.com/desertfire, https://github.com/jansel"
pytorch/pytorch,4b0f1cc1ee1e0e7327fadeea5f2a2f8197e9ad3a,"[FSDP][optim_state_dict][10/N] Make optim_state_dict and optim_state_dict_to_load public (#92118)

Make optim_state_dict and optim_state_dict_to_load public APIs and consolidate them with state_dict by using the same state_dict_type to decide how to perform the optimizer state_dict save and load.

Differential Revision: [D42488022](https://our.internmc.facebook.com/intern/diff/D42488022/)
Pull Request resolved: https://github.com/pytorch/pytorch/pull/92118
Approved by: https://github.com/rohan-varma"
pytorch/pytorch,db873964747631f6ac4a09b0bfc990b91c32a263,"inductor: align the decomposition output stride with none-decomposition path for torch.lerp (#93336)

As title, we need to align the decomposition output stride with the none-decomposition path for torch.lerp. And also enable it's lowering path for inductor.

After this PR for the following case:

```

def fn(i0, i1):
    # i0: (10, 3, 10)
    # i1: (3, 10, 10)
    x1 = i0.transpose(-2, -3)
    #y = torch.lerp(x1, x1, 70000)
    z = torch.lerp(i1, x1, 70000)
    return z

x0 = torch.rand(10, 3, 10)
x1 = torch.rand(3, 10, 10)
ret_eager = fn(x0, x1)
print('==== Eager mode OK! ====')
compiled = torch.compile(fn, fullgraph=True)
ret_compiled = compiled(x0, x1)
print('==== compile mode OK! ====')
ret_compiled = compiled(x0, x1)
print(torch.equal(ret_eager, ret_compiled))
print(ret_eager.stride()==ret_compiled.stride())
```

the inductor output code will be like(CPU):

```

from ctypes import c_void_p, c_long
import torch
import random
from torch import empty_strided, as_strided, device
from torch._inductor.codecache import AsyncCompile
from torch._inductor.select_algorithm import extern_kernels

aten = torch.ops.aten
assert_size_stride = torch._C._dynamo.guards.assert_size_stride
async_compile = AsyncCompile()

kernel_cpp_0 = async_compile.cpp('''
#include ""/tmp/torchinductor_xiaobing/77/c7773nj5pwikpmm2pwa62rcudlf7p3if7eyqb5k4sjsvewwje4le.h""
extern ""C"" void kernel(const float* __restrict__ in_ptr0,
                       const float* __restrict__ in_ptr1,
                       float* __restrict__ out_ptr0)
{
    {
        #pragma GCC ivdep
        for(long i0=0; i0<3; i0+=1)
        {
            #pragma GCC ivdep
            for(long i1=0; i1<10; i1+=1)
            {
                for(long i2=0; i2<0; i2+=1)
                {
                    auto tmp7 = at::vec::Vectorized<float>::loadu(in_ptr0 + (10*i0) + (16*i2) + (30*i1));
                    auto tmp8 = at::vec::Vectorized<float>::loadu(in_ptr1 + (10*i1) + (16*i2) + (100*i0));
                    auto tmp0 = at::vec::Vectorized<float>(static_cast<float>(70000.0));
                    auto tmp1 = tmp0.abs();
                    auto tmp2 = at::vec::Vectorized<float>(static_cast<float>(0.5));
                    auto tmp3 = tmp1 >= tmp2;
                    auto tmp4 = at::vec::Vectorized<float>(static_cast<float>(1));
                    auto tmp5 = tmp0 - tmp4;
                    auto tmp6 = decltype(tmp5)::blendv(tmp0, tmp5, tmp3);
                    auto tmp9 = tmp7 - tmp8;
                    auto tmp10 = tmp6 * tmp9;
                    auto tmp11 = decltype(tmp7)::blendv(tmp8, tmp7, tmp3);
                    auto tmp12 = tmp10 + tmp11;
                    tmp12.store(out_ptr0 + (10*i1) + (16*i2) + (100*i0));
                }
                #pragma omp simd simdlen(8)
                for(long i2=0; i2<10; i2+=1)
                {
                    auto tmp7 = in_ptr0[i2 + (10*i0) + (30*i1)];
                    auto tmp8 = in_ptr1[i2 + (10*i1) + (100*i0)];
                    auto tmp0 = static_cast<float>(70000.0);
                    auto tmp1 = std::abs(tmp0);
                    auto tmp2 = static_cast<float>(0.5);
                    auto tmp3 = tmp1 >= tmp2;
                    auto tmp4 = static_cast<float>(1);
                    auto tmp5 = tmp0 - tmp4;
                    auto tmp6 = tmp3 ? tmp5 : tmp0;
                    auto tmp9 = tmp7 - tmp8;
                    auto tmp10 = tmp6 * tmp9;
                    auto tmp11 = tmp3 ? tmp7 : tmp8;
                    auto tmp12 = tmp10 + tmp11;
                    out_ptr0[i2 + (10*i1) + (100*i0)] = tmp12;
                }
            }
        }
    }
}
''')

async_compile.wait(globals())
del async_compile

def call(args):
    arg0_1, arg1_1 = args
    args.clear()
    buf1 = empty_strided((3, 10, 10), (100, 10, 1), device='cpu', dtype=torch.float32)
    kernel_cpp_0(c_void_p(arg0_1.data_ptr()), c_void_p(arg1_1.data_ptr()), c_void_p(buf1.data_ptr()))
    del arg0_1
    del arg1_1
    return (buf1, )

if __name__ == ""__main__"":
    from torch._dynamo.testing import rand_strided
    from torch._inductor.utils import print_performance
    arg0_1 = rand_strided((10, 3, 10), (30, 10, 1), device='cpu', dtype=torch.float32)
    arg1_1 = rand_strided((3, 10, 10), (100, 10, 1), device='cpu', dtype=torch.float32)
    print_performance(lambda: call([arg0_1, arg1_1]))

```

Pull Request resolved: https://github.com/pytorch/pytorch/pull/93336
Approved by: https://github.com/jansel"
pytorch/pytorch,e7ace1ff930db30deb907c753dc9e41b26f2586f,"[PT-D][NamedOptimizer][6/N] Upstream init_state from keyed to NamedOptimizer (#93887)

Pull Request resolved: https://github.com/pytorch/pytorch/pull/93887
Approved by: https://github.com/rohan-varma"
pytorch/pytorch,653dc73df0b6222938e6fe5aeefcb5fb0707e924,"[SDPA] Wire up FlashAttention's backward (#92917)

# Summary
This PR creates _flash_attention_backward and _scaled_dot_product_flash_attention_backward native functions and registers them to the respective derivatives.yaml.

The goal is to replicate the torch.autograd.Function defined in the FlashAttention repo [here](https://github.com/HazyResearch/flash-attention/blob/33e0860c9c5667fded5af674882e731909096a7f/flash_attn/flash_attn_interface.py#L126) natively in PyTorch.  One thing that we don't have access to is ctx.save_for_backward in native PyTorch so in order to save these variables I extended the returned objects from the forward functions.

### MetaFunctions
I also updated the FlashAttention meta functions to mirror the real outputs now. As well I added a meta registration for backwards. I have an XLMR training script and while eager training now works with FlashAttention compiling this module fails with the inductor error down below.

### Questions?
Performance issues vs mem efficient when using torch.nn.mha_forward

TorchCompile -> See purposed solution below.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/92917
Approved by: https://github.com/cpuhrsch"
pytorch/pytorch,54eedf6fa6a5104dbdf92f868b0236c43c90dd21,"Fix test_jit_cuda_archflags on Windows (#93332)

Fixes https://github.com/pytorch/pytorch/issues/61655

The test is flaky and fails whenever `test_jit_cuda_archflags` is run.  The latter `test_jit_cuda_archflags` was slow test in the old Windows runner.  It's currently running again on trunk due to the problem with populating slow-test JSON file ~Interestingly, its performance is getting better in the new Windows G5 runner and it becomes a borderline slow test, where it run sometimes~.  Whenever it runs, the next test `test_jit_cuda_extension` will fail.

* Build and load different CUDA arch modules from `test_jit_cuda_archflags` in separate processes to avoid importing them into the current one.  The test only checks the build artifacts.  Importing them cause `test_jit_cuda_extension` to fail as describe in https://github.com/pytorch/pytorch/issues/61655
* Clean up the temp build dir on Windows.  Windows CUDA runner is non-ephemeral, so it's better to clean thing up properly to avoid any funny business the next time the runner is used
Pull Request resolved: https://github.com/pytorch/pytorch/pull/93332
Approved by: https://github.com/davidberard98"
pytorch/pytorch,1dcd2609b5857775bb7e614fe49949807e12fa9a,"Add retries for get_workflow_job_id and try catch in upload_test_stats (#93401)

upload_test_stats keeps failing b/c it can't handle when the id is workflow-<workflow_id> so add a try catch for this.

Add retries to get_workflow_job_id to try and reduce the number of times the id can't be found

Failure to upload test stats and inability to get the job id cause our sharding infra and slow test infra (probably also flaky test detection) to be less effective.  This does not completely resolve the issue since we do rely on the job id

Failure to get the workflow job id happens tragically often, hopefully retries will help
Pull Request resolved: https://github.com/pytorch/pytorch/pull/93401
Approved by: https://github.com/huydhn"
pytorch/pytorch,298075e1837678f81fcffbb951e5bfcec909df22,"use aten parallel on lu factor (#93037)

https://github.com/pytorch/pytorch/issues/91536. One issue mentioned torch.inv is pretty slow for large batches with small matrices on cuda.

I checked the CPU implementations and found we have an optimize opportunity.
For torch.inv, the CPU pass chooses to solve it by `lu_factor` + `lu_solve`.
The `lu_factor` loop on `batch_size` dimension and the parallel happened inside lapack
 - For small matrix, the computational complexity is too tiny to parallel inside lapack.
 - Even for large matrix, the parallelization efficiency is not good in lapack ( it performs worse than using at::parallel outside)
 - Only for small batch size + small matrix size, the omp overhead will take too large overhead.

Based on the above observations, using at::parallel outside on lu_factor will have a pretty large benefit.

Here is the code/data collected on 32 core ICX system.
```python
import torch
import time

def bench(bs, r):
    x = torch.randn(int(bs), r, r)
    start = time.time()
    for i in range(100):
        y1 = torch.linalg.lu_factor(x)
    end = time.time()
    print(r, bs)
    print(end - start)
    print((end - start)/(r**3))

for r in (4, 16, 64):
    for bs in (1e2, 1e4, 1e6):
        bench(bs, r)
```

| bs/rank | 100/4 |  10000/4 |  1000000/4 | 100/16 |  10000/16|  1000000/16| 100/64|  10000/64|  1000000/64|
| ---- | --- | --- | --- | --- | --- | --- | --- | --- | --- |
| parallel inside lapack | 0.0028 |1.077 | 11.99|0.0163 | 1.5260|153.17 |0.2021|20.93 | 1877|
| parallel outside lapack | 0.0087 | 0.0247 | 1.566| 0.0044|0.1678 |17.63|0.038|2.311 | 208.6|
|speed up ratio| 0.32x | 43.6x  | 7.65x|3.70x |9.09x |8.69x |5.32x |9.06x |9x |

Pull Request resolved: https://github.com/pytorch/pytorch/pull/93037
Approved by: https://github.com/lezcano"
pytorch/pytorch,b484d17c24bc2ab75c3c6833e59913a3c6862988,"_sparse_coo_tensor_with_dims_and_tensors backward: simplify and optimize (#91704)

Pull Request resolved: https://github.com/pytorch/pytorch/pull/91704
Approved by: https://github.com/albanD, https://github.com/cpuhrsch"
pytorch/pytorch,9a56997fe1e7bfe46ff148691a357a73c3ecaf84,"[dtensor][5/N] add cached propagator for TP (#90734)

This PR adds a cached propagator for TP use, it caches the sharding
prop decision for the same input sharding on an operator. This could
improve eager mode performance.

Differential Revision: [D42876249](https://our.internmc.facebook.com/intern/diff/D42876249)
Pull Request resolved: https://github.com/pytorch/pytorch/pull/90734
Approved by: https://github.com/XilunWu, https://github.com/fduwjj"
pytorch/pytorch,e77f28a03d6c77742d6ac0b29f6462a5c47b8faa,"[Quant] Add fused ConvAddReLU2d module for onednn backend (#91154)

**Summary**
Post op fusion can reduce data movement overhead and improve inference performance. This PR adds fused ConvAddReLU2d module for onednn backend, which will be used for int8 inference with onednn backend. Cannot call this module with other quantization backends otherwise an error is thrown.

**Test plan**
```
python -m pytest test_quantization.py -k test_conv2d_add_relu
```

Pull Request resolved: https://github.com/pytorch/pytorch/pull/91154
Approved by: https://github.com/jgong5, https://github.com/jerryzh168"
pytorch/pytorch,53c3555a6a228e29f0a2a4f9cda7fa83238ebef8,"[Quant] Add fused ConvAdd2d module for onednn backend (#91152)

**Summary**
Post op fusion can reduce data movement overhead and improve inference performance. This PR adds fused `ConvAdd2d` module for onednn backend, which will be used for int8 inference with onednn backend. Cannot call this module with other quantization backends otherwise an error is thrown.

**Test plan**
```
python -m pytest test_quantization.py -k test_conv2d_add
```

Pull Request resolved: https://github.com/pytorch/pytorch/pull/91152
Approved by: https://github.com/jgong5, https://github.com/jerryzh168"
pytorch/pytorch,7bcc446ede5a5910e4591559178223a7d191d687,"[Vulkan][Optimize for Mobile] Avoid dereferencing element [0] if the vector is empty (#92918)

Summary:
Avoid dereferencing element [0] if the vector is empty.
___

In ```transferInputOutputBackends```, one of the rewrite passes for Vulkan ```optimize_for_mobile```, an out of bounds access happens when trying to insert a backend transfer for an input if that input's ```uses()``` is empty. This diff corrects that issue.

Test Plan:
Run tests
___

Phabricator + CI Tests

Reviewed By: SS-JIA

Differential Revision: D41296037

Pull Request resolved: https://github.com/pytorch/pytorch/pull/92918
Approved by: https://github.com/SS-JIA, https://github.com/kirklandsign"
pytorch/pytorch,888771dc5d2055ccb95a56a8ca756dae8c94c62a,"[FSDP][optim_state_dict] Fix `_is_named_optimizer` when the state is empty (#93303)

Optimizer state is not eager initializaion -- only NamedOptimizer and KeyedOptimizer are. This PR makes it `_is_named_optimizer` work with regular optimizers.

Differential Revision: [D42858589](https://our.internmc.facebook.com/intern/diff/D42858589/)
Pull Request resolved: https://github.com/pytorch/pytorch/pull/93303
Approved by: https://github.com/fduwjj"
pytorch/pytorch,a71d9a928fafe7e00e3f2d6466ea50980de5bcc1,"[Quant] Add fused conv2d_add_relu op for onednn backend (#90364)

**Summary**
Post op fusion can reduce data movement overhead and improve inference performance. This PR adds fused conv2d_add_relu op for onednn backend, which will be used for int8 inference with onednn backend. Cannot call this op with other quantization backends otherwise an error is thrown.

**Test Plan**
```
python -m pytest test_quantization.py::TestQuantizedConv
```

Pull Request resolved: https://github.com/pytorch/pytorch/pull/90364
Approved by: https://github.com/jgong5, https://github.com/jerryzh168"
pytorch/pytorch,24b501903c4d03ca885cb2e9f5b6e2d730e8a648,"Minor sympy usage fix in fbcode (#93171)

Summary: To supports older versions of sympy.

Test Plan:
```
buck2 run @//mode/opt @//mode/inplace -c python.package_style=inplace -c fbcode.enable_gpu_sections=true //caffe2/benchmarks/dynamo:torchbench -- -dcuda --performance --inductor --only hf_T5
```

Differential Revision: D42812188

Pull Request resolved: https://github.com/pytorch/pytorch/pull/93171
Approved by: https://github.com/eellison"
pytorch/pytorch,4fc19e1a71b7a9a6a4687cfc42f53ca0d43266ce,"[optim][adam] use fastest impl whenever possible, add util (#93184)

This allows it so that ONLY when the users don't set anything for foreach or fused do we switch the default and cascades adam so that we default to fused, then foreach, then single-tensor.

To clarify:
* if the user puts True in foreach _only_, it will run the foreach implementation.
* if the user puts True in fused _only_, it will run the fused implementation.
* if the user puts True in foreach AND for fused, it will run the fused implementation.

And:
* if the user puts False in foreach _only_, it will run the single tensor implementation.
* if the user puts False in fused _only_, it will still run the single tensor implementation.
* if the user puts False in foreach AND for fused, it will run the single tensor implementation.

I also didn't trust myself that much with the helper function, so I ran some local asserts on _default_to_fused_or_foreach. The only point left to really test is the type(p) -- torch.Tensor but I think the distributed tests will catch that in CI.
```
cuda_only_fp_list = [
    torch.rand((1, 2), device=""cuda"", dtype=torch.float32),
    torch.rand((1, 2), device=""cuda"", dtype=torch.float64),
    torch.rand((1, 2), device=""cuda"", dtype=torch.float16),
    torch.rand((1, 2), device=""cuda"", dtype=torch.bfloat16),
]

cuda_only_int_list = [
    torch.randint(1024, (1, 2), device=""cuda"", dtype=torch.int64),
]

cpu_list = [
    torch.rand((1, 2), device=""cpu"", dtype=torch.float32),
    torch.rand((1, 2), device=""cpu"", dtype=torch.float64),
    torch.rand((1, 2), device=""cpu"", dtype=torch.float16),
]

none_list = [None]

# differentiable should always make it return false for both
assert _default_to_fused_or_foreach([cuda_only_fp_list], True, True) == (False, False)
assert _default_to_fused_or_foreach([cuda_only_fp_list], True, False) == (False, False)

# cpu lists should always make it return false for both
assert _default_to_fused_or_foreach([cuda_only_fp_list, cpu_list], False, True) == (False, False)
assert _default_to_fused_or_foreach([cpu_list], False, True) == (False, False)
assert _default_to_fused_or_foreach([cuda_only_fp_list, cpu_list], False, False) == (False, False)
assert _default_to_fused_or_foreach([cpu_list], False, False) == (False, False)

# has fused triggers correctly
assert _default_to_fused_or_foreach([cuda_only_fp_list], False, True) == (True, False)
assert _default_to_fused_or_foreach([cuda_only_fp_list], False, False) == (False, True)

# ints always goes to foreach
assert _default_to_fused_or_foreach([cuda_only_fp_list, cuda_only_int_list], False, True) == (False, True)
assert _default_to_fused_or_foreach([cuda_only_fp_list, cuda_only_int_list], False, False) == (False, True)

# Nones don't error
assert _default_to_fused_or_foreach([cuda_only_fp_list, none_list], False, True) == (True, False)
assert _default_to_fused_or_foreach([cuda_only_fp_list, cuda_only_int_list, none_list], False, True) == (False, True)
assert _default_to_fused_or_foreach([none_list], False, True) == (True, False)
assert _default_to_fused_or_foreach([none_list], False, False) == (False, True)
```

Pull Request resolved: https://github.com/pytorch/pytorch/pull/93184
Approved by: https://github.com/albanD"
pytorch/pytorch,3e4d0e8d82a77958db902f8897195331f17791da,"[Reland][FSDP] Do not clean FQNs for `use_orig_params=True` (#92662)

The last PR (https://github.com/pytorch/pytorch/pull/91767/) had a land race relating to `_NamedOptimizer` + FSDP and got reverted. This is a re-land.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/92662
Approved by: https://github.com/rohan-varma"
pytorch/pytorch,0247ed27cc9c4550a2415c85aee94a6863c4d89b,"Apply Clang-Tidy readability-container-size-empty (#93236)

Not only is this change usually shorter and more readable, it also can yield better performance. size() is not always a constant time operation (such as on LinkedLists), but empty() always is.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/93236
Approved by: https://github.com/malfet"
pytorch/pytorch,0dceaf07cd1236859953b6f85a61dc4411d10f87,"Add two decomps for optimizer fusion (#93193)

Fixes #ISSUE_NUMBER

Pull Request resolved: https://github.com/pytorch/pytorch/pull/93193
Approved by: https://github.com/ngimel, https://github.com/jansel"
pytorch/pytorch,9a2becf60a49c327f3d29f3bfffc3ee302145bf1,"inductor: fix inplace op's wrong lowering issue when preop is NopKernel (#92247)

For TIMM ghostnet_100, there has such case, concat+inplace_add:

```
import torch
from torch._inductor import config
config.debug = True
torch._dynamo.config.verbose=True

class MockModule(torch.nn.Module):
    def __init__(self):
        super().__init__()

    def forward(self, x, y, z):
        out = torch.cat([x, y], dim=1)
        out+=z
        return out

mod = MockModule().eval()
inputs = (
                torch.randn([1, 64, 16, 16]),
                torch.randn([1, 64, 16, 16]),
                torch.randn([1, 128, 16, 16]),
            )
ref = mod(*inputs)

with torch.no_grad():
    opt_model = torch._dynamo.optimize('inductor')(mod)
    out = opt_model(*inputs)
    out = opt_model(*inputs)
    out = opt_model(*inputs)
print(torch.equal(ref, out))
```

the inductor always get a wrong result, I find that inductor get a wrong code:

```

from ctypes import c_void_p, c_long
import torch
import random
from torch import empty_strided, as_strided, device
from torch._inductor.codecache import AsyncCompile
from torch._inductor.select_algorithm import extern_kernels

aten = torch.ops.aten
assert_size_stride = torch._C._dynamo.guards.assert_size_stride
async_compile = AsyncCompile()

kernel_cpp_0 = async_compile.cpp('''
#include ""/tmp/torchinductor_xiaobing/77/c7773nj5pwikpmm2pwa62rcudlf7p3if7eyqb5k4sjsvewwje4le.h""
extern ""C"" void kernel(const float* __restrict__ in_ptr0,
                       const float* __restrict__ in_ptr1,
                       const float* __restrict__ in_ptr2,
                       const float* __restrict__ in_ptr3,
                       float* __restrict__ out_ptr0,
                       float* __restrict__ out_ptr1,
                       float* __restrict__ out_ptr2)
{
    {
        for(long i0=0; i0<1024; i0+=1)
        {
            auto tmp0 = at::vec::Vectorized<float>::loadu(in_ptr0 + 16*i0);
            tmp0.store(out_ptr0 + 16*i0);
        }
        #pragma omp simd simdlen(8)
        for(long i0=16384; i0<16384; i0+=1)
        {
            auto tmp0 = in_ptr0[i0];
            out_ptr0[i0] = tmp0;
        }
    }
    {
        for(long i0=0; i0<1024; i0+=1)
        {
            auto tmp0 = at::vec::Vectorized<float>::loadu(in_ptr1 + 16*i0);
            tmp0.store(out_ptr1 + 16*i0);
        }
        #pragma omp simd simdlen(8)
        for(long i0=16384; i0<16384; i0+=1)
        {
            auto tmp0 = in_ptr1[i0];
            out_ptr1[i0] = tmp0;
        }
    }
    {
        for(long i0=0; i0<2048; i0+=1)
        {
            auto tmp0 = at::vec::Vectorized<float>::loadu(in_ptr2 + 16*i0);
            auto tmp1 = at::vec::Vectorized<float>::loadu(in_ptr3 + 16*i0);
            auto tmp2 = tmp0 + tmp1;
            tmp2.store(out_ptr2 + 16*i0);
        }
        #pragma omp simd simdlen(8)
        for(long i0=32768; i0<32768; i0+=1)
        {
            auto tmp0 = in_ptr2[i0];
            auto tmp1 = in_ptr3[i0];
            auto tmp2 = tmp0 + tmp1;
            out_ptr2[i0] = tmp2;
        }
    }
}
''')

async_compile.wait(globals())
del async_compile

def call(args):
    arg0_1, arg1_1, arg2_1 = args
    args.clear()
    buf3 = empty_strided((1, 128, 16, 16), (32768, 256, 16, 1), device='cpu', dtype=torch.float32)
    buf0 = as_strided(buf3, (1, 64, 16, 16), (32768, 256, 16, 1))  # alias
    buf1 = as_strided(buf3, (1, 64, 16, 16), (32768, 256, 16, 1), 16384)  # alias
    buf2 = empty_strided((1, 128, 16, 16), (32768, 256, 16, 1), device='cpu', dtype=torch.float32)
    kernel_cpp_0(c_void_p(arg0_1.data_ptr()), c_void_p(arg1_1.data_ptr()), c_void_p(buf2.data_ptr()), c_void_p(arg2_1.data_ptr()), c_void_p(buf0.data_ptr()), c_void_p(buf1.data_ptr()), c_void_p(buf3.data_ptr()))
    del arg0_1
    del arg1_1
    del arg2_1
    return (buf3, )

if __name__ == ""__main__"":
    from torch._dynamo.testing import rand_strided
    from torch._inductor.utils import print_performance
    arg0_1 = rand_strided((1, 64, 16, 16), (16384, 256, 16, 1), device='cpu', dtype=torch.float32)
    arg1_1 = rand_strided((1, 64, 16, 16), (16384, 256, 16, 1), device='cpu', dtype=torch.float32)
    arg2_1 = rand_strided((1, 128, 16, 16), (32768, 256, 16, 1), device='cpu', dtype=torch.float32)
    print_performance(lambda: call([arg0_1, arg1_1, arg2_1]))

```
you can see that the add operation always adds a random value, see the ir code:

1. **ir_pre_fusion.txt**
```
buf0: SchedulerNode(ComputedBuffer)
buf0.writes = [MemoryDep(name='buf0', index=c0, size=(16384,))]
buf0.unmet_dependencies = []
buf0.met_dependencies = [MemoryDep(name='arg0_1', index=c0, size=(16384,))]
buf0.group.device = cpu
buf0.group.iteration = ((16384,), ())
buf0.sizes = ([16384], [])
buf0.aliases = ['buf3']
class buf0_loop_body:
    var_ranges = {z0: 16384}
    index0 = z0
    def body(self, ops):
        get_index = self.get_index('index0')
        load = ops.load('arg0_1', get_index)
        get_index_1 = self.get_index('index0')
        store = ops.store('buf0', get_index_1, load, None)
        return store

buf1: SchedulerNode(ComputedBuffer)
buf1.writes = [MemoryDep(name='buf1', index=c0, size=(16384,))]
buf1.unmet_dependencies = []
buf1.met_dependencies = [MemoryDep(name='arg1_1', index=c0, size=(16384,))]
buf1.group.device = cpu
buf1.group.iteration = ((16384,), ())
buf1.sizes = ([16384], [])
buf1.aliases = ['buf3']
class buf1_loop_body:
    var_ranges = {z0: 16384}
    index0 = z0
    def body(self, ops):
        get_index = self.get_index('index0')
        load = ops.load('arg1_1', get_index)
        get_index_1 = self.get_index('index0')
        store = ops.store('buf1', get_index_1, load, None)
        return store

buf2: NopKernelSchedulerNode(ConcatKernel)
buf2.writes = [StarDep(name='buf2')]
buf2.unmet_dependencies = [StarDep(name='buf0'), StarDep(name='buf1')]
buf2.met_dependencies = []

buf3: SchedulerNode(ComputedBuffer)
buf3.writes = [MemoryDep(name='buf3', index=c0, size=(32768,))]
buf3.unmet_dependencies = [MemoryDep(name='buf2', index=c0, size=(32768,))]
buf3.met_dependencies = [MemoryDep(name='arg2_1', index=c0, size=(32768,))]
buf3.group.device = cpu
buf3.group.iteration = ((32768,), ())
buf3.sizes = ([32768], [])
class buf3_loop_body:
    var_ranges = {z0: 32768}
    index0 = z0
    def body(self, ops):
        get_index = self.get_index('index0')
        load = ops.load('buf2', get_index)
        get_index_1 = self.get_index('index0')
        load_1 = ops.load('arg2_1', get_index_1)
        add = ops.add(load, load_1)
        get_index_2 = self.get_index('index0')
        store = ops.store('buf3', get_index_2, add, None)
        return store

```
2. **ir_post_fusion.txt**
```
buf0: SchedulerNode(ComputedBuffer)
buf0.writes = [MemoryDep(name='buf0', index=c0, size=(16384,))]
buf0.unmet_dependencies = []
buf0.met_dependencies = [MemoryDep(name='arg0_1', index=c0, size=(16384,))]
buf0.group.device = cpu
buf0.group.iteration = ((16384,), ())
buf0.sizes = ([16384], [])
buf0.aliases = ['buf3']
class buf0_loop_body:
    var_ranges = {z0: 16384}
    index0 = z0
    def body(self, ops):
        get_index = self.get_index('index0')
        load = ops.load('arg0_1', get_index)
        get_index_1 = self.get_index('index0')
        store = ops.store('buf0', get_index_1, load, None)
        return store

buf1: SchedulerNode(ComputedBuffer)
buf1.writes = [MemoryDep(name='buf1', index=c0, size=(16384,))]
buf1.unmet_dependencies = []
buf1.met_dependencies = [MemoryDep(name='arg1_1', index=c0, size=(16384,))]
buf1.group.device = cpu
buf1.group.iteration = ((16384,), ())
buf1.sizes = ([16384], [])
buf1.aliases = ['buf3']
class buf1_loop_body:
    var_ranges = {z0: 16384}
    index0 = z0
    def body(self, ops):
        get_index = self.get_index('index0')
        load = ops.load('arg1_1', get_index)
        get_index_1 = self.get_index('index0')
        store = ops.store('buf1', get_index_1, load, None)
        return store

buf2: NopKernelSchedulerNode(ConcatKernel)
buf2.writes = [StarDep(name='buf2')]
buf2.unmet_dependencies = [StarDep(name='buf0'), StarDep(name='buf1')]
buf2.met_dependencies = []

buf3: SchedulerNode(ComputedBuffer)
buf3.writes = [MemoryDep(name='buf3', index=c0, size=(32768,))]
buf3.unmet_dependencies = [MemoryDep(name='buf2', index=c0, size=(32768,))]
buf3.met_dependencies = [MemoryDep(name='arg2_1', index=c0, size=(32768,))]
buf3.group.device = cpu
buf3.group.iteration = ((32768,), ())
buf3.sizes = ([32768], [])
class buf3_loop_body:
    var_ranges = {z0: 32768}
    index0 = z0
    def body(self, ops):
        get_index = self.get_index('index0')
        load = ops.load('buf2', get_index)
        get_index_1 = self.get_index('index0')
        load_1 = ops.load('arg2_1', get_index_1)
        add = ops.add(load, load_1)
        get_index_2 = self.get_index('index0')
        store = ops.store('buf3', get_index_2, add, None)
        return store
```

From the ir code, you can see the buf3 always adds an empty buf2 which has never been written. The root cause is that there has a potential issue when doing the mutation for inplace add when its' input is a NopKernel.

After this PR, the ir will be like(**ir_pre_fusion.txt**):

```
buf0: SchedulerNode(ComputedBuffer)
buf0.writes = [MemoryDep(name='buf0', index=c0, size=(16384,))]
buf0.unmet_dependencies = []
buf0.met_dependencies = [MemoryDep(name='arg0_1', index=c0, size=(16384,))]
buf0.group.device = cpu
buf0.group.iteration = ((16384,), ())
buf0.sizes = ([16384], [])
buf0.aliases = ['buf2']
class buf0_loop_body:
    var_ranges = {z0: 16384}
    index0 = z0
    def body(self, ops):
        get_index = self.get_index('index0')
        load = ops.load('arg0_1', get_index)
        get_index_1 = self.get_index('index0')
        store = ops.store('buf0', get_index_1, load, None)
        return store

buf1: SchedulerNode(ComputedBuffer)
buf1.writes = [MemoryDep(name='buf1', index=c0, size=(16384,))]
buf1.unmet_dependencies = []
buf1.met_dependencies = [MemoryDep(name='arg1_1', index=c0, size=(16384,))]
buf1.group.device = cpu
buf1.group.iteration = ((16384,), ())
buf1.sizes = ([16384], [])
buf1.aliases = ['buf2']
class buf1_loop_body:
    var_ranges = {z0: 16384}
    index0 = z0
    def body(self, ops):
        get_index = self.get_index('index0')
        load = ops.load('arg1_1', get_index)
        get_index_1 = self.get_index('index0')
        store = ops.store('buf1', get_index_1, load, None)
        return store

buf2: NopKernelSchedulerNode(ConcatKernel)
buf2.writes = [StarDep(name='buf2')]
buf2.unmet_dependencies = [StarDep(name='buf0'), StarDep(name='buf1')]
buf2.met_dependencies = []

buf3: SchedulerNode(ComputedBuffer)
buf3.writes = [MemoryDep(name='buf3', index=c0, size=(32768,))]
buf3.unmet_dependencies = [MemoryDep(name='buf2', index=c0, size=(32768,)), StarDep(name='buf2')]
buf3.met_dependencies = [MemoryDep(name='arg2_1', index=c0, size=(32768,))]
buf3.group.device = cpu
buf3.group.iteration = ((32768,), ())
buf3.sizes = ([32768], [])
buf3.mutations = ['buf2']
class buf3_loop_body:
    var_ranges = {z0: 32768}
    index0 = z0
    def body(self, ops):
        get_index = self.get_index('index0')
        load = ops.load('buf2', get_index)
        get_index_1 = self.get_index('index0')
        load_1 = ops.load('arg2_1', get_index_1)
        add = ops.add(load, load_1)
        get_index_2 = self.get_index('index0')
        store = ops.store('buf3', get_index_2, add, None)
        return store

```

Pull Request resolved: https://github.com/pytorch/pytorch/pull/92247
Approved by: https://github.com/ngimel, https://github.com/desertfire, https://github.com/jansel"
pytorch/pytorch,900f8886e2df633fcc24e1f6d8a1dddfb35bcbe1,"inductor: make as_strided support  non-contiguous input and always fix it's input layout using eager stride (#92063)

GIven the following small case:

```
import torch
import torch._dynamo

class Model(torch.nn.Module):
    def __init__(self):
        super(Model, self).__init__()

    def forward(self, x):
        return torch.as_strided(x + 1, (8, 384, 2, 20, 12), (153600, 1, 61440, 384, 7680))+ 2

x = torch.randn(8, 384, 20, 20).to(memory_format=torch.channels_last)
model= Model().eval()
model = model.to(memory_format=torch.channels_last)
ref = model(x)

with torch.no_grad():
    opt_model = torch._dynamo.optimize('inductor')(model)

with torch.no_grad():
    for i in range(2):
        y1 = opt_model(x)

print(torch.equal(ref, y1))

```

inductor always gets a wrong result:

```
from ctypes import c_void_p, c_long
import torch
import random
from torch import empty_strided, as_strided, device
from torch._inductor.codecache import AsyncCompile
from torch._inductor.select_algorithm import extern_kernels

aten = torch.ops.aten
assert_size_stride = torch._C._dynamo.guards.assert_size_stride
async_compile = AsyncCompile()

kernel_cpp_0 = async_compile.cpp('''
#include ""/tmp/torchinductor_xiaobing/77/c7773nj5pwikpmm2pwa62rcudlf7p3if7eyqb5k4sjsvewwje4le.h""
extern ""C"" void kernel(const float* __restrict__ in_ptr0,
                       float* __restrict__ out_ptr0,
                       float* __restrict__ out_ptr1)
{
    #pragma omp parallel num_threads(40)
    {
        {
            #pragma omp for
            for(long i0=0; i0<8; i0+=1)
            {
                #pragma GCC ivdep
                for(long i1=0; i1<384; i1+=1)
                {
                    #pragma GCC ivdep
                    for(long i2=0; i2<400; i2+=1)
                    {
                        auto tmp0 = in_ptr0[i1 + (384*i2) + (153600*i0)];
                        auto tmp1 = static_cast<float>(1);
                        auto tmp2 = tmp0 + tmp1;
                        out_ptr0[i2 + (400*i1) + (153600*i0)] = tmp2;
                    }
                }
            }
        }
        {
            #pragma omp for  collapse(2)
            for(long i0=0; i0<8; i0+=1)
            {
                for(long i1=0; i1<2; i1+=1)
                {
                    for(long i2=0; i2<5760; i2+=1)
                    {
                        auto tmp0 = at::vec::Vectorized<float>::loadu(out_ptr0 + (16*i2) + (61440*i1) + (153600*i0));
                        auto tmp1 = at::vec::Vectorized<float>(static_cast<float>(2));
                        auto tmp2 = tmp0 + tmp1;
                        tmp2.store(out_ptr1 + (16*i2) + (92160*i1) + (184320*i0));
                    }
                    #pragma omp simd simdlen(8)
                    for(long i2=92160; i2<92160; i2+=1)
                    {
                        auto tmp0 = out_ptr0[i2 + (61440*i1) + (153600*i0)];
                        auto tmp1 = static_cast<float>(2);
                        auto tmp2 = tmp0 + tmp1;
                        out_ptr1[i2 + (92160*i1) + (184320*i0)] = tmp2;
                    }
                }
            }
        }
    }
}
''')

async_compile.wait(globals())
del async_compile

def call(args):
    arg0_1, = args
    args.clear()
    buf0 = empty_strided((8, 384, 20, 20), (153600, 400, 20, 1), device='cpu', dtype=torch.float32)
    buf1 = empty_strided((8, 384, 2, 20, 12), (184320, 1, 92160, 384, 7680), device='cpu', dtype=torch.float32)
    kernel_cpp_0(c_void_p(arg0_1.data_ptr()), c_void_p(buf0.data_ptr()), c_void_p(buf1.data_ptr()))
    del arg0_1
    return (buf1, )

if __name__ == ""__main__"":
    from torch._dynamo.testing import rand_strided
    from torch._inductor.utils import print_performance
    arg0_1 = rand_strided((8, 384, 20, 20), (153600, 1, 7680, 384), device='cpu', dtype=torch.float32)
    print_performance(lambda: call([arg0_1]))

```

the reason is that there always convert the input to a contiguous layout at **as_strided** lowering step, which is not aligned with the eager model input stride.

```
class <lambda>(torch.nn.Module):
    def forward(self, arg0_1: f32[8, 384, 20, 20]):
        # File: model_test.py:52, code: return torch.as_strided(x + 1, (8, 384, 2, 20, 12), (153600, 1, 61440, 384, 7680))+ 2
        add: f32[8, 384, 20, 20] = torch.ops.aten.add.Tensor(arg0_1, 1);  arg0_1 = None
        as_strided: f32[8, 384, 2, 20, 12] = torch.ops.aten.as_strided.default(add, [8, 384, 2, 20, 12], [153600, 1, 61440, 384, 7680]);  add = None
        add_1: f32[8, 384, 2, 20, 12] = torch.ops.aten.add.Tensor(as_strided, 2);  as_strided = None
        return (add_1,)

```

This PR will always fix **as_strided** stride with eager model's stride, and also make **as_strided** support channels_last input:

```
from ctypes import c_void_p, c_long
import torch
import random
from torch import empty_strided, as_strided, device
from torch._inductor.codecache import AsyncCompile
from torch._inductor.select_algorithm import extern_kernels

aten = torch.ops.aten
assert_size_stride = torch._C._dynamo.guards.assert_size_stride
async_compile = AsyncCompile()

kernel_cpp_0 = async_compile.cpp('''
#include ""/tmp/torchinductor_xiaobing/77/c7773nj5pwikpmm2pwa62rcudlf7p3if7eyqb5k4sjsvewwje4le.h""
extern ""C"" void kernel(const float* __restrict__ in_ptr0,
                       float* __restrict__ out_ptr0,
                       float* __restrict__ out_ptr1)
{
    #pragma omp parallel num_threads(40)
    {
        {
            #pragma omp for
            for(long i0=0; i0<76800; i0+=1)
            {
                auto tmp0 = at::vec::Vectorized<float>::loadu(in_ptr0 + 16*i0);
                auto tmp1 = at::vec::Vectorized<float>(static_cast<float>(1));
                auto tmp2 = tmp0 + tmp1;
                tmp2.store(out_ptr0 + 16*i0);
            }
            #pragma omp for simd simdlen(8)
            for(long i0=1228800; i0<1228800; i0+=1)
            {
                auto tmp0 = in_ptr0[i0];
                auto tmp1 = static_cast<float>(1);
                auto tmp2 = tmp0 + tmp1;
                out_ptr0[i0] = tmp2;
            }
        }
        {
            #pragma omp for  collapse(2)
            for(long i0=0; i0<8; i0+=1)
            {
                for(long i1=0; i1<2; i1+=1)
                {
                    for(long i2=0; i2<5760; i2+=1)
                    {
                        auto tmp0 = at::vec::Vectorized<float>::loadu(out_ptr0 + (16*i2) + (61440*i1) + (153600*i0));
                        auto tmp1 = at::vec::Vectorized<float>(static_cast<float>(2));
                        auto tmp2 = tmp0 + tmp1;
                        tmp2.store(out_ptr1 + (16*i2) + (92160*i1) + (184320*i0));
                    }
                    #pragma omp simd simdlen(8)
                    for(long i2=92160; i2<92160; i2+=1)
                    {
                        auto tmp0 = out_ptr0[i2 + (61440*i1) + (153600*i0)];
                        auto tmp1 = static_cast<float>(2);
                        auto tmp2 = tmp0 + tmp1;
                        out_ptr1[i2 + (92160*i1) + (184320*i0)] = tmp2;
                    }
                }
            }
        }
    }
}
''')

async_compile.wait(globals())
del async_compile

def call(args):
    arg0_1, = args
    args.clear()
    buf0 = empty_strided((8, 384, 20, 20), (153600, 1, 7680, 384), device='cpu', dtype=torch.float32)
    buf1 = empty_strided((8, 384, 2, 20, 12), (184320, 1, 92160, 384, 7680), device='cpu', dtype=torch.float32)
    kernel_cpp_0(c_void_p(arg0_1.data_ptr()), c_void_p(buf0.data_ptr()), c_void_p(buf1.data_ptr()))
    del arg0_1
    return (buf1, )

if __name__ == ""__main__"":
    from torch._dynamo.testing import rand_strided
    from torch._inductor.utils import print_performance
    arg0_1 = rand_strided((8, 384, 20, 20), (153600, 1, 7680, 384), device='cpu', dtype=torch.float32)
    print_performance(lambda: call([arg0_1]))
```

Pull Request resolved: https://github.com/pytorch/pytorch/pull/92063
Approved by: https://github.com/jansel"
pytorch/pytorch,648202ceb92c0240a61ca0f1f6c5699541f5f651,"Improve DDPOptimizer by avoiding small preamble graph (#93162)

This optimizes an edge case where some compute-only ops (e.g. add)
could end up in an orphan graph at the input side due to the bucket
for the next graph being full already.  The fix is to fuse this
graph (which is ""empty"" in parameter count) together with the adjoining
""full"" bucket.

Note: i encountered this when trying to repro some suspected duplicate
argument errors, but this is unrelated and I have not yet repro'd
a duplicate arg issue.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/93162
Approved by: https://github.com/davidberard98"
pytorch/pytorch,a62fc09a1f8655c3d9444f92f8c987aba20cddbb,"[Quant] Add fused conv2d_add op for onednn backend (#90262)

**Summary**
Post op fusion can reduce data movement overhead and improve inference performance. This PR adds fused `conv2d_add` op for onednn backend, which will be used for int8 inference with onednn backend. Cannot call this op with other quantization backends otherwise an error is thrown.

**Test Plan**
```
python -m pytest test_quantization.py::TestQuantizedConv
```

Pull Request resolved: https://github.com/pytorch/pytorch/pull/90262
Approved by: https://github.com/jgong5, https://github.com/jerryzh168"
pytorch/pytorch,7fade4f771c222c72c0be05ec3686f3fef9c4919,"fixing flag to skip nvfuser_tests build (#93080)

Slowly pushing cmake cleanup to upstream.

avoids building nvfuser_tests when BUILD_TEST is disabled.
nvfuser_tests uses googletest from pytorch, which is only dragged when BUILD_TEST is enabled.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/93080
Approved by: https://github.com/davidberard98, https://github.com/huydhn, https://github.com/malfet"
pytorch/pytorch,f6f46ba3bb3ae2a5458e5f5d2c330c970edd2061,"[Reland] aot autograd explicitly errors on double backward  (#92893)

This reverts commit fb980581a7b41a5ea570fcb03829463b806b3bbc.

Testing: `python benchmarks/dynamo/timm_models.py  --float32 --training --only=mobilevit_s --performance --inductor --disable-cudagraphs`

```
main:               memory: eager: 12.30 GB, dynamo: 12.28 GB, ratio: 1.00
+ #90896 reverted:  memory: eager: 12.30 GB, dynamo: 8.81 GB, ratio: 1.40
+ this PR:          memory: eager: 12.30 GB, dynamo: 8.81 GB, ratio: 1.40
```

For comparison, if we apply old version of this PR instead:
```
main:
+ #90896 reverted:         memory: eager: 12.30 GB, dynamo: 8.81 GB, ratio: 1.40
+ old version of this PR   memory: eager: 12.30 GB, dynamo: 10.36 GB, ratio: 1.19
```

Pull Request resolved: https://github.com/pytorch/pytorch/pull/92893
Approved by: https://github.com/bdhirsh"
pytorch/pytorch,7012d985fa21b2b25e04b853906009dba1787eaa,"Revert ""Improve `bsr @ strided` performance in `baddmm` for `bfloat16/half` with Triton kernels. (#88078)""

This reverts commit 46f16b93636615a81242b0d5cded84c5a57fd2e2.

Reverted https://github.com/pytorch/pytorch/pull/88078 on behalf of https://github.com/ZainRizvi due to Causing a test to fail consistently: test_decomp.py::HasDecompTest::test_has_decomposition"
pytorch/pytorch,46f16b93636615a81242b0d5cded84c5a57fd2e2,"Improve `bsr @ strided` performance in `baddmm` for `bfloat16/half` with Triton kernels. (#88078)

As per title.

Additionally we also introduce support for:
- Rectangular block sizes which are powers of 2 and at least 16 (triton's `dot` limitation).
- Batch support with broadcasting for either of the arguments.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/88078
Approved by: https://github.com/cpuhrsch"
pytorch/pytorch,5441f2c067ec82a2f1a5a49a73036c19359cd2c1,"Fix DDPOptimizer fake_mode execution (#92986)

Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):

* __->__ #92986

When running compiled submods for the purpose of producing outputs to pass
to the compilation step for the next submod, we use fake parameters and
assume fake inputs, but we forgot to activate our fake_mode during execution.

This caused certain edge cases where tensors other than activations or parameters
got created during execution, such as scalar->tensor expansion in the case
of executing torch.where(tensor, scalar, scalar).

Also add a test and clarify behavior of DDPOptimizer via comments.

Fixes #92941

Pull Request resolved: https://github.com/pytorch/pytorch/pull/92986
Approved by: https://github.com/bdhirsh"
pytorch/pytorch,67689c823f81b930034f97d90a3093d48a0f098f,"refactor: move dynamo/TorchXLA bridge to pytorch/xla repo (#92601)

This is a follow up from the previous PR: https://github.com/pytorch/pytorch/pull/88449 , to move the dynamo/TorchXLA bridge from pytorch repo to xla repo.

Overall the dynamo/TorchXLA integration has the following four layers of code
- pybind layer: This is the bottom layer containing various pybind APIs as the foundation. This part resident in xla repo
- bridge layer: build upon the pybind layer to implement the trace once functionality. This layer and it's corresponding unit test are in pytorch repro previously. This PR (and the corresponding xla pr https://github.com/pytorch/xla/pull/4476 ) moves them to the xla repo.
- dynamo backend registration: this a thin layer registers 4 dynamo backends (training/inference/trace_once/trace_everytime). It remains in pytorch repo.
- benchmark script: the torchbench.py script in dynamo is adapted so it can be used in dynamo/TorchXLA integration. This one remains in pytorch repo.

We think the new code organization is cleaner.

I'll wait for the xla PR in first before trying to merge this one.

Tests
1. run the unit tests moved to the xla repo
2. Test for inference:  `GPU_NUM_DEVICES=1 python benchmarks/dynamo/torchbench.py --randomize-input --performance --trace-on-xla --backend=torchxla_trace_once --only resnet18`
3. Test for training: `GPU_NUM_DEVICES=1 python benchmarks/dynamo/torchbench.py --randomize-input --performance --trace-on-xla --training --backend=aot_torchxla_trace_once --only resnet18 --collect-outputs`

Pull Request resolved: https://github.com/pytorch/pytorch/pull/92601
Approved by: https://github.com/wconstab"
pytorch/pytorch,99ced6482aab680aeacbee4e7029bea0f4dffc19,"Disable vml's abs and log1p (#92113)

I noticed that `torch.log1p` is ridiculously slow compared to `torch.log`
on CPU, and looking at the assembly it seems vsLog1p doesn't use any
vector instructions. I saw the same for abs, though AFAICT this is
dead code anyway as `abs` is implemented with `cpu_kernel_vec`.

Locally I see a 14x speedup in `torch.log1p`.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/92113
Approved by: https://github.com/jgong5"
pytorch/pytorch,d4c8e37b8563a6a8448b6c0d6c0868fe0eb11366,"Improve performance for unary kernels using vml (#91963)

This gives some speedups for kernels implemented with `at::vml`:
- Make vml ops serial and use `TensorIterator.for_each` for better parallism
with discontiguous tensors
- Reduce buffer size for discontiguous data to 8 KiB to increase chance of
fitting in L1d cache, but is still wide enough to utilize AVX-512.
- Avoid a copy if only one of input and output is discontiguous

There is no change for contiguous tensors, but I see significant speedup for
the following benchmarks:
```
import torch
a = torch.randn(2*10**6, device=""cpu"")
%timeit a.view(100, 20000)[:,::2].sqrt()
%timeit a.view(200, 10000)[::2].sqrt()
```
For discontiguous last dimension I see a 27x speedup and for discontiguous
batch dimension I see an 8x speedup.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/91963
Approved by: https://github.com/jgong5"
pytorch/pytorch,a3715efd8b8386a1c35dcf7341514b71e88f227a,"Remove windows check for cmake to build Fused kernels (#91909)

# Summary
Add support for fused attention kernels (FlashAttention and memory-efficient attention) on Windows. Previously we could not do this because the fixes required c++17 to do this but we have since update the PyTorch standard.

This PR:
- Changes invocations of unsigned long to the fixed width integer type
- Adds in the #define FP16_SWITCH(COND, ...) which has been added to the flash_attention main branch
- Changes the some macros used within mem-efficient attention code in order to work around the VA_ARG discrepancy between clang/gcc and msvc. An alternative would be setting the global flag Zc:preprocessor
- Selectively applies /Zc:lambda to only the mem-efficient sources since applying this globally caused quantization files to not compile

Pull Request resolved: https://github.com/pytorch/pytorch/pull/91909
Approved by: https://github.com/cpuhrsch"
pytorch/pytorch,16f7db52874d91c36a1f015cf9ff86215e7f146f,"Don't fail-fast for docs, only push on schedule and some tags (#92853)

Fixes #ISSUE_NUMBER

Pull Request resolved: https://github.com/pytorch/pytorch/pull/92853
Approved by: https://github.com/malfet, https://github.com/huydhn, https://github.com/ZainRizvi"
pytorch/pytorch,e137dcc2c86f4f4193b624bbf1183923918a2f40,"Splitting #91254 into two PRs (#92748)

This one handles the xnumel=1 part, and introduces no performance
regression.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/92748
Approved by: https://github.com/lezcano, https://github.com/jansel"
pytorch/pytorch,df14650f0b14b80db132b0c1797dc595fbee1054,"[SDPA] Update SDPA API and make function Public (#92189)

# Summary
In preparation for pt 2.0 launch this PR updates SDPA's API and makes the function a nn.funcitonal public function.

## Changes
### API
Previously the the function signature was:
`scaled_dot_product_attention(query, key, value, attn_mask=None, need_attn_weights=False, dropout_p=0.0, is_causal=False) -> (Tensor, Tensor)`
Updated signature:
`scaled_dot_product_attention(query, key, value, attn_mask=None, dropout_p=0.0, is_causal=False) -> Tensor`

This PR removes the need_attn_weights optional boolean variable and updates the return type to a singular tensor.

#### Reasoning:
The main goal of this function is to provide an easy interface for users to call into fused attention kernels e.g.  (FlashAttention). The fused kernels do not currently support arbitrary attn_mask or dropout but there is a PR to mem-efficient attention to enable these. We want to have the API surface ready for when the backing kernels get updated.

The fused kernels save on memory usage by not materializing the weights and it is unlikely that a fast fused implementation will enable this feature so we are removing.

Discussed with folks at FAIR/Xformers and +1 this API change.

#### Make function Public
In preparation for the pt 2.0 launch we make the function public to start to generate user feedback

Pull Request resolved: https://github.com/pytorch/pytorch/pull/92189
Approved by: https://github.com/cpuhrsch"
pytorch/pytorch,e994e7839770a6544305709c47d760d91c5b13f4,"Added vectorized horizontal flip path for channels last for NcHW (#91806)

## Description

- Added AVX2-only vectorization for horizontal flip op applied on channels last NCHW input, where **2 <= C * sizeof(dtype) <= 16**. PR is a bit faster than Pillow and largely faster (x2 - x5) than Nightly.
- ~Still keeping `cpu_vflip_memcpy` code ([it's PR](https://github.com/pytorch/pytorch/pull/89414) was reverted and is under investigations)~

## Benchmarks

```
[---------------------------------------------------------------------- Horizontal flip ----------------------------------------------------------------------]
                                                                  |  torch (2.0.0a0+gitf6d73f3) PR  |    Pillow (9.4.0)   |  torch (2.0.0a0+git4386f31) nightly
1 threads: ----------------------------------------------------------------------------------------------------------------------------------------------------
      channels=2, size=256, dtype=torch.uint8, mf=channels_last   |         31.859 (+-0.498)        |                     |          190.599 (+-7.579)
      channels=2, size=520, dtype=torch.uint8, mf=channels_last   |         60.648 (+-0.074)        |                     |          706.895 (+-11.219)
      channels=2, size=712, dtype=torch.uint8, mf=channels_last   |         95.994 (+-2.510)        |                     |         1340.685 (+-169.279)

      channels=3, size=256, dtype=torch.uint8, mf=channels_last   |         45.490 (+-0.108)        |   47.359 (+-0.942)  |          179.520 (+-2.916)
      channels=3, size=520, dtype=torch.uint8, mf=channels_last   |        146.802 (+-2.175)        |  174.201 (+-4.124)  |          707.765 (+-2.691)
      channels=3, size=712, dtype=torch.uint8, mf=channels_last   |        215.148 (+-0.925)        |  313.606 (+-3.972)  |         1346.678 (+-89.854)

      channels=3, size=256, dtype=torch.int8, mf=channels_last    |         43.618 (+-0.160)        |                     |          191.613 (+-16.252)
      channels=3, size=520, dtype=torch.int8, mf=channels_last    |        147.487 (+-0.691)        |                     |          755.020 (+-25.045)
      channels=3, size=712, dtype=torch.int8, mf=channels_last    |        216.687 (+-0.906)        |                     |         1314.854 (+-31.137)

      channels=4, size=256, dtype=torch.uint8, mf=channels_last   |         32.169 (+-0.092)        |                     |          195.415 (+-3.647)
      channels=4, size=520, dtype=torch.uint8, mf=channels_last   |         89.465 (+-0.154)        |                     |          776.459 (+-14.845)
      channels=4, size=712, dtype=torch.uint8, mf=channels_last   |        152.773 (+-0.610)        |                     |         1456.304 (+-45.280)

      channels=8, size=256, dtype=torch.uint8, mf=channels_last   |         43.444 (+-0.158)        |                     |          163.669 (+-4.580)
      channels=8, size=520, dtype=torch.uint8, mf=channels_last   |        151.285 (+-0.602)        |                     |          642.396 (+-13.500)
      channels=8, size=712, dtype=torch.uint8, mf=channels_last   |        278.471 (+-0.912)        |                     |         1205.472 (+-47.609)

      channels=16, size=256, dtype=torch.uint8, mf=channels_last  |         75.176 (+-0.188)        |                     |          181.278 (+-3.388)
      channels=16, size=520, dtype=torch.uint8, mf=channels_last  |        291.105 (+-1.163)        |                     |          716.906 (+-30.842)
      channels=16, size=712, dtype=torch.uint8, mf=channels_last  |        893.267 (+-10.899)       |                     |         1434.931 (+-40.399)

      channels=2, size=256, dtype=torch.int16, mf=channels_last   |         31.437 (+-0.143)        |                     |          195.299 (+-2.916)
      channels=2, size=520, dtype=torch.int16, mf=channels_last   |         89.834 (+-0.175)        |                     |          774.940 (+-8.638)
      channels=2, size=712, dtype=torch.int16, mf=channels_last   |        154.806 (+-0.550)        |                     |         1443.435 (+-37.799)

      channels=3, size=256, dtype=torch.int16, mf=channels_last   |         70.909 (+-0.146)        |                     |          195.347 (+-1.986)
      channels=3, size=520, dtype=torch.int16, mf=channels_last   |        212.998 (+-1.181)        |                     |          776.282 (+-15.598)
      channels=3, size=712, dtype=torch.int16, mf=channels_last   |        382.991 (+-0.968)        |                     |          1441.674 (+-9.873)

      channels=4, size=256, dtype=torch.int16, mf=channels_last   |         43.574 (+-0.157)        |                     |          163.176 (+-1.941)
      channels=4, size=520, dtype=torch.int16, mf=channels_last   |        151.289 (+-0.557)        |                     |          641.169 (+-9.457)
      channels=4, size=712, dtype=torch.int16, mf=channels_last   |        275.275 (+-0.874)        |                     |         1186.589 (+-12.063)

      channels=8, size=256, dtype=torch.int16, mf=channels_last   |         74.455 (+-0.292)        |                     |          181.191 (+-1.721)
      channels=8, size=520, dtype=torch.int16, mf=channels_last   |        289.591 (+-1.134)        |                     |          715.755 (+-2.368)
      channels=8, size=712, dtype=torch.int16, mf=channels_last   |        923.831 (+-68.807)       |                     |         1437.078 (+-14.649)

      channels=2, size=256, dtype=torch.int32, mf=channels_last   |         44.217 (+-0.203)        |                     |          163.011 (+-1.497)
      channels=2, size=520, dtype=torch.int32, mf=channels_last   |        150.920 (+-0.950)        |                     |          640.761 (+-1.882)
      channels=2, size=712, dtype=torch.int32, mf=channels_last   |        281.648 (+-1.163)        |                     |         1188.464 (+-10.374)

      channels=3, size=256, dtype=torch.int32, mf=channels_last   |        103.708 (+-0.517)        |                     |          165.001 (+-1.315)
      channels=3, size=520, dtype=torch.int32, mf=channels_last   |        409.785 (+-8.004)        |                     |          647.939 (+-11.431)
      channels=3, size=712, dtype=torch.int32, mf=channels_last   |        790.819 (+-16.471)       |                     |          1219.206 (+-9.503)

      channels=4, size=256, dtype=torch.int32, mf=channels_last   |         72.975 (+-0.155)        |                     |          181.298 (+-1.059)
      channels=4, size=520, dtype=torch.int32, mf=channels_last   |        291.584 (+-0.905)        |                     |          716.033 (+-4.824)
      channels=4, size=712, dtype=torch.int32, mf=channels_last   |        938.790 (+-15.930)       |                     |         1434.134 (+-15.060)

Times are in microseconds (us).
```

[Source](https://gist.github.com/vfdev-5/8e8c989d35835d7ab20567bff36632be#file-20230123-143303-pr_vs_nightly-md)

## Context:

Follow-up work to PRs : https://github.com/pytorch/pytorch/pull/88989, https://github.com/pytorch/pytorch/pull/89414 and https://github.com/pytorch/pytorch/pull/90013

Pull Request resolved: https://github.com/pytorch/pytorch/pull/91806
Approved by: https://github.com/peterbell10, https://github.com/lezcano"
pytorch/pytorch,a43b55e135c76cd12cec9bade6e1310ccc2769dd,"A few usability improvements for the dynamo benchmarks. (#92713)

--diff_main renamed to --diff-branch BRANCH and now works again
Summary table splits results per branch.
csv output now has column with branch name when run in this mode

Added --progress flag so you can track how many models are going to be
run.

Example output:
```
$ python benchmarks/dynamo/torchbench.py  --quiet --performance --backend inductor --float16 --batch-size-file $(realpath benchmarks/dynamo/torchbench_models_list.txt)   --filter 'alexnet|vgg16' --progress  --diff viable/strict
Running model 1/2
batch size: 1024
cuda eval  alexnet                             dynamo_bench_diff_branch   1.251x p=0.00
cuda eval  alexnet                             viable/strict              1.251x p=0.00
Running model 2/2
batch size: 128
cuda eval  vgg16                               dynamo_bench_diff_branch   1.344x p=0.00
cuda eval  vgg16                               viable/strict              1.342x p=0.00

Summary for tag=dynamo_bench_diff_branch:
speedup             gmean=1.30x mean=1.30x
abs_latency         gmean=24.09x mean=25.26x
compilation_latency mean=2.0 seconds
compression_ratio   mean=0.9x

Summary for tag=viable/strict:
speedup             gmean=1.30x mean=1.30x
abs_latency         gmean=24.11x mean=25.29x
compilation_latency mean=0.5 seconds
compression_ratio   mean=1.0x
```

Pull Request resolved: https://github.com/pytorch/pytorch/pull/92713
Approved by: https://github.com/jansel"
pytorch/pytorch,d90d92e733f1b79014ccbd67c43dab8938591e80,"Don't fail-fast Docker builds (#92816)

Signed-off-by: Edward Z. Yang <ezyang@meta.com>
Pull Request resolved: https://github.com/pytorch/pytorch/pull/92816
Approved by: https://github.com/Skylion007, https://github.com/malfet"
pytorch/pytorch,387d769156692047882634518aa772898e202c9d,"[BE]: Replace string compares with more efficient cpp comparisons (#92765)

Replace cpp string comparisons with more efficient equality operators. These string comparisons are not just more readable, but they also allow for short-circuiting for faster string equality checks.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/92765
Approved by: https://github.com/ezyang"
pytorch/pytorch,020c0d589529bbb96714f582d551b90f37c4ea35,"Add debugability comments to DDPOptimizer (#89802)

Pull Request resolved: https://github.com/pytorch/pytorch/pull/89802
Approved by: https://github.com/davidberard98"
pytorch/pytorch,c0fe41f98399381de4c26330849bf145ee6103c4,"Use SymBool for is_contiguous computation (#92229)

This changes TensorImpl to store SymBool instead of bool. However, it doesn't actually compute these quantities symbolically (outside of some top level disjunctions.) The purpose of this PR is to make it easier to diagnose performance problems in the next PR, as after this change we can switch to guardless implementations without modifying TensorImpl.h

Signed-off-by: Edward Z. Yang <ezyang@meta.com>
Pull Request resolved: https://github.com/pytorch/pytorch/pull/92229
Approved by: https://github.com/Skylion007, https://github.com/albanD"
pytorch/pytorch,85851b1e8fc12fc824edd24aa1f1e0f9075b0cd7,"remove useless clang-tidy suppression  (#92287)

remove NOLINTNEXTLINE(cppcoreguidelines-pro-type-member-init)
remove NOLINTNEXTLINE(performance-move-const-arg)
remove NOLINTNEXTLINE(performance-no-automatic-move)
Pull Request resolved: https://github.com/pytorch/pytorch/pull/92287
Approved by: https://github.com/albanD"
pytorch/pytorch,85a1f0223a0d6c2953602bd6210b84d13c4d6180,"Add a warning about performance cost of set_default_device (#92703)

Signed-off-by: Edward Z. Yang <ezyang@meta.com>
Pull Request resolved: https://github.com/pytorch/pytorch/pull/92703
Approved by: https://github.com/albanD"
pytorch/pytorch,5c6f5439b7e6a5e63a68b36b4cf093a00d46e8be,"Implement SymBool (#92149)

We have known for a while that we should in principle support SymBool as a separate concept from SymInt and SymFloat ( in particular, every distinct numeric type should get its own API). However, recent work with unbacked SymInts in, e.g., https://github.com/pytorch/pytorch/pull/90985 have made this a priority to implement. The essential problem is that our logic for computing the contiguity of tensors performs branches on the passed in input sizes, and this causes us to require guards when constructing tensors from unbacked SymInts. Morally, this should not be a big deal because, we only really care about the regular (non-channels-last) contiguity of the tensor, which should be guaranteed since most people aren't calling `empty_strided` on the tensor, however, because we store a bool (not a SymBool, prior to this PR it doesn't exist) on TensorImpl, we are forced to *immediately* compute these values, even if the value ends up not being used at all. In particular, even when a user allocates a contiguous tensor, we still must compute channels-last contiguity (as some contiguous tensors are also channels-last contiguous, but others are not.)

This PR implements SymBool, and makes TensorImpl use SymBool to store the contiguity information in ExtraMeta. There are a number of knock on effects, which I now discuss below.

* I introduce a new C++ type SymBool, analogous to SymInt and SymFloat. This type supports logical and, logical or and logical negation. I support the bitwise operations on this class (but not the conventional logic operators) to make it clear that logical operations on SymBool are NOT short-circuiting. I also, for now, do NOT support implicit conversion of SymBool to bool (creating a guard in this case). This does matter too much in practice, as in this PR I did not modify the equality operations (e.g., `==` on SymInt) to return SymBool, so all preexisting implicit guards did not need to be changed. I also introduced symbolic comparison functions `sym_eq`, etc. on SymInt to make it possible to create SymBool. The current implementation of comparison functions makes it unfortunately easy to accidentally introduce guards when you do not mean to (as both `s0 == s1` and `s0.sym_eq(s1)` are valid spellings of equality operation); in the short term, I intend to prevent excess guarding in this situation by unit testing; in the long term making the equality operators return SymBool is probably the correct fix.
* ~~I modify TensorImpl to store SymBool for the `is_contiguous` fields and friends on `ExtraMeta`. In practice, this essentially meant reverting most of the changes from https://github.com/pytorch/pytorch/pull/85936 . In particular, the fields on ExtraMeta are no longer strongly typed; at the time I was particularly concerned about the giant lambda I was using as the setter getting a desynchronized argument order, but now that I have individual setters for each field the only ""big list"" of boolean arguments is in the constructor of ExtraMeta, which seems like an acceptable risk. The semantics of TensorImpl are now that we guard only when you actually attempt to access the contiguity of the tensor via, e.g., `is_contiguous`. By in large, the contiguity calculation in the implementations now needs to be duplicated (as the boolean version can short circuit, but the SymBool version cannot); you should carefully review the duplicate new implementations. I typically use the `identity` template to disambiguate which version of the function I need, and rely on overloading to allow for implementation sharing. The changes to the `compute_` functions are particularly interesting; for most of the functions, I preserved their original non-symbolic implementation, and then introduce a new symbolic implementation that is branch-less (making use of our new SymBool operations). However, `compute_non_overlapping_and_dense` is special, see next bullet.~~ This appears to cause performance problems, so I am leaving this to an update PR.
* (Update: the Python side pieces for this are still in this PR, but they are not wired up until later PRs.) While the contiguity calculations are relatively easy to write in a branch-free way, `compute_non_overlapping_and_dense` is not: it involves a sort on the strides. While in principle we can still make it go through by using a data oblivious sorting network, this seems like too much complication for a field that is likely never used (because typically, it will be obvious that a tensor is non overlapping and dense, because the tensor is contiguous.) So we take a different approach: instead of trying to trace through the logic computation of non-overlapping and dense, we instead introduce a new opaque operator IsNonOverlappingAndDenseIndicator which represents all of the compute that would have been done here. This function returns an integer 0 if `is_non_overlapping_and_dense` would have returned `False`, and an integer 1 otherwise, for technical reasons (Sympy does not easily allow defining custom functions that return booleans). The function itself only knows how to evaluate itself if all of its arguments are integers; otherwise it is left unevaluated. This means we can always guard on it (as `size_hint` will always be able to evaluate through it), but otherwise its insides are left a black box. We typically do NOT expect this custom function to show up in actual boolean expressions, because we will typically shortcut it due to the tensor being contiguous. It's possible we should apply this treatment to all of the other `compute_` operations, more investigation necessary. As a technical note, because this operator takes a pair of a list of SymInts, we need to support converting `ArrayRef<SymNode>` to Python, and I also unpack the pair of lists into a single list because I don't know if Sympy operations can actually validly take lists of Sympy expressions as inputs. See for example `_make_node_sizes_strides`
* On the Python side, we also introduce a SymBool class, and update SymNode to track bool as a valid pytype. There is some subtlety here: bool is a subclass of int, so one has to be careful about `isinstance` checks (in fact, in most cases I replaced `isinstance(x, int)` with `type(x) is int` for expressly this reason.) Additionally, unlike, C++, I do NOT define bitwise inverse on SymBool, because it does not do the correct thing when run on booleans, e.g., `~True` is `-2`. (For that matter, they don't do the right thing in C++ either, but at least in principle the compiler can warn you about it with `-Wbool-operation`, and so the rule is simple in C++; only use logical operations if the types are statically known to be SymBool). Alas, logical negation is not overrideable, so we have to introduce `sym_not` which must be used in place of `not` whenever a SymBool can turn up. To avoid confusion with `__not__` which may imply that `operators.__not__` might be acceptable to use (it isn't), our magic method is called `__sym_not__`. The other bitwise operators `&` and `|` do the right thing with booleans and are acceptable to use.
* There is some annoyance working with booleans in Sympy. Unlike int and float, booleans live in their own algebra and they support less operations than regular numbers. In particular, `sympy.expand` does not work on them. To get around this, I introduce `safe_expand` which only calls expand on operations which are known to be expandable.

TODO: this PR appears to greatly regress performance of symbolic reasoning. In particular, `python test/functorch/test_aotdispatch.py -k max_pool2d` performs really poorly with these changes. Need to investigate.

Signed-off-by: Edward Z. Yang <ezyang@meta.com>
Pull Request resolved: https://github.com/pytorch/pytorch/pull/92149
Approved by: https://github.com/albanD, https://github.com/Skylion007"
pytorch/pytorch,34e8eb229db76f7f5eb8f18c062dbd1ee47f8b12,"Dispatch the auxiliary frobenius_norm and nuclear_norm to better implementations and deprecate them (#81763)

These functions will be legacy functions. We deprecate them, but we also
take this chance to dispatch to a more efficient and consistent implementation.
Doing so should help writing a conversion rule for these to be able to
remove them once and for all

Differential Revision: [D42354776](https://our.internmc.facebook.com/intern/diff/D42354776)
Pull Request resolved: https://github.com/pytorch/pytorch/pull/81763
Approved by: https://github.com/ngimel"
pytorch/pytorch,5644059489f17a7197bdd0488903f4c16f576df7,"[inductor] Lower torch.exp2 and use it for torch.pow(2, x) (#92632)

Before
```python
    tmp0 = 2.0
    tmp2 = tl.libdevice.pow(tmp0, tmp1)
```

After
```python
    tmp1 = tl.libdevice.exp2(tmp0)
```

I've benchmarked on CPU and CUDA with the following examples
```
@torch._dynamo.optimize()
def exp2(x):
    return torch.pow(2, x)

@torch._dynamo.optimize()
def logaddexp2(a, b):
    m = torch.maximum(a, b)
    return m + torch.log2(1 + torch.pow(2, -torch.abs(a-b)))
```

triton is able to specialize `pow(2, x)` such that this makes
no difference, but on CPU I see a surprisingly large speedup.

| device | Function  | Master (us) | This PR (us) | Speedup |
|--------|-----------|-------------|--------------|---------|
| CUDA   | exp2      | 64          | 63           | 1.0     |
|        | logaddexp | 109         | 107          | 1.0     |
| CPU    | exp2      | 220         | 40           | 5.5     |
|        | logaddexp | 282         | 140          | 2.0     |

Pull Request resolved: https://github.com/pytorch/pytorch/pull/92632
Approved by: https://github.com/lezcano, https://github.com/ngimel"
pytorch/pytorch,e4d83d54a6214d8fa1a9063f0da65932b45b7207,"Foreach gradient clipping (#91846)

Faster gradient clipping using the foreach functions

```
[------------------------ (tensors, scalar) -------------------------]
                                   |  without foreach  |  with foreach |    apex
1 threads: ----------------------------------------------------------------------
      10 tensors of size 4         |         120.5     |       61.1    |     50.3
      100 tensors of size 4        |         946.2     |      239.5    |    136.3
      1000 tensors of size 4       |        9808.5     |     2151.1    |   1006.9
      10000 tensors of size 4      |       96871.2     |    22637.4    |  10119.1
      10 tensors of size 16        |         121.0     |       64.1    |     52.5
      100 tensors of size 16       |         993.4     |      252.6    |    136.7
      1000 tensors of size 16      |        9427.7     |     2151.2    |   1049.5
      10000 tensors of size 16     |       97437.1     |    22203.1    |  10340.0
      10 tensors of size 256       |         118.9     |       62.3    |     51.5
      100 tensors of size 256      |         955.2     |      243.1    |    134.2
      1000 tensors of size 256     |        9374.9     |     2140.7    |   1009.6
      10000 tensors of size 256    |       95302.5     |    21849.4    |  10215.5
      10 tensors of size 65536     |         118.5     |       62.4    |     51.1
      100 tensors of size 65536    |        1740.7     |      243.3    |    225.3
      1000 tensors of size 65536   |       17364.1     |     2228.7    |   2004.5
      10000 tensors of size 65536  |      177510.1     |    25410.4    |  20678.2
```
Pull Request resolved: https://github.com/pytorch/pytorch/pull/91846
Approved by: https://github.com/janeyx99"
pytorch/pytorch,2af2952c660e21cb17863deeb3226e744caac383,"logaddexp2: Use log1p and exp2 (#92116)

This replaces `log2(1 + x)` with `log1p(x) * (1 / log(2))` which improves
precision when `x` is small by avoiding the truncation from calculating
`(1 + x) - 1`. Noting that `x` is always `<= 1` in this formula.

This also replaces `pow(2, x)` with `exp2(x)` which improves performance,
particularly on CPU where the constant value cannot be inlined into Sleef.
With numel=1e7 for example, I see a 1.35x speedup on CPU.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/92116
Approved by: https://github.com/lezcano"
pytorch/pytorch,5ac22782d13d4ac68024a6562f66d0ce8552c93a,"Optimized vertical flip using memcpy (#89414)

## Description

- Use memcpy for vertical flip
- Added bool type support for horizontal flip
  - channels last input with horizontal flip goes also into cpu_vflip_memcpy and has a speed-up

Previous PRs:
- https://github.com/pytorch/pytorch/pull/90013
- https://github.com/pytorch/pytorch/pull/88989

## Results

### Horizontal flip

- AVX2 (channels last input only)
```
[------------------------------------------------------------------------- Horizontal flip -------------------------------------------------------------------------]
                                                                      |  torch (1.14.0a0+giteb3e189) PR  |    Pillow (9.3.0)   |  torch (1.14.0a0+gitb0bd5c4) nightly
1 threads: ----------------------------------------------------------------------------------------------------------------------------------------------------------
      channels=3, size=256, dtype=torch.int64, mf=channels_last       |        204.813 (+-1.018)         |                     |           308.070 (+-1.573)
      channels=3, size=520, dtype=torch.int64, mf=channels_last       |        844.523 (+-2.302)         |                     |           1226.801 (+-5.069)
      channels=3, size=712, dtype=torch.int64, mf=channels_last       |        2246.512 (+-8.935)        |                     |          2689.692 (+-22.654)

      channels=1, size=256, dtype=torch.int32, mf=channels_last       |         21.024 (+-0.083)         |   44.196 (+-0.131)  |            22.564 (+-0.066)
      channels=1, size=520, dtype=torch.int32, mf=channels_last       |         71.806 (+-0.150)         |  166.653 (+-0.789)  |            72.660 (+-0.160)
      channels=1, size=712, dtype=torch.int32, mf=channels_last       |        129.354 (+-0.385)         |  306.998 (+-0.819)  |           130.094 (+-0.274)

      channels=3, size=256, dtype=torch.uint8, mf=channels_last       |        177.250 (+-0.485)         |   44.232 (+-0.465)  |           289.201 (+-2.837)
      channels=3, size=520, dtype=torch.uint8, mf=channels_last       |        699.055 (+-1.940)         |  166.540 (+-0.903)  |           1172.747 (+-3.645)
      channels=3, size=712, dtype=torch.uint8, mf=channels_last       |        1302.968 (+-5.390)        |  307.210 (+-0.852)  |          2149.396 (+-23.570)

      channels=1, size=256, dtype=torch.int16, mf=channels_last       |         11.943 (+-0.079)         |                     |            12.451 (+-0.033)
      channels=1, size=520, dtype=torch.int16, mf=channels_last       |         39.830 (+-0.093)         |                     |            40.583 (+-0.070)
      channels=1, size=712, dtype=torch.int16, mf=channels_last       |         69.001 (+-0.078)         |                     |            69.590 (+-0.162)

      channels=3, size=256, dtype=torch.int8, mf=channels_last        |        177.378 (+-0.507)         |                     |           283.461 (+-2.957)
      channels=3, size=520, dtype=torch.int8, mf=channels_last        |        698.915 (+-1.840)         |                     |          1061.208 (+-10.449)
      channels=3, size=712, dtype=torch.int8, mf=channels_last        |        1299.365 (+-3.919)        |                     |          1957.424 (+-13.149)

      channels=3, size=256, dtype=torch.int8, mf=channels_first       |         17.955 (+-0.077)         |                     |            89.456 (+-0.285)
      channels=3, size=520, dtype=torch.int8, mf=channels_first       |         56.901 (+-0.081)         |                     |           339.802 (+-0.879)
      channels=3, size=712, dtype=torch.int8, mf=channels_first       |        103.629 (+-0.256)         |                     |           627.845 (+-1.185)

      channels=1, size=256, dtype=torch.float32, mf=channels_last     |         21.179 (+-0.077)         |   44.146 (+-0.260)  |            22.957 (+-0.138)
      channels=1, size=520, dtype=torch.float32, mf=channels_last     |         71.685 (+-0.155)         |  166.666 (+-0.730)  |            72.606 (+-0.124)
      channels=1, size=712, dtype=torch.float32, mf=channels_last     |        129.168 (+-0.288)         |  307.094 (+-1.571)  |           130.156 (+-0.453)

      channels=1, size=256, dtype=torch.float16, mf=channels_last     |         33.049 (+-0.089)         |                     |            33.056 (+-0.477)
      channels=1, size=520, dtype=torch.float16, mf=channels_last     |        116.635 (+-0.299)         |                     |           113.433 (+-0.891)
      channels=1, size=712, dtype=torch.float16, mf=channels_last     |        212.134 (+-0.413)         |                     |           204.394 (+-0.822)

      channels=3, size=256, dtype=torch.float64, mf=channels_last     |        207.214 (+-0.586)         |                     |           302.370 (+-0.670)
      channels=3, size=520, dtype=torch.float64, mf=channels_last     |        846.553 (+-2.301)         |                     |           1223.851 (+-5.280)
      channels=3, size=712, dtype=torch.float64, mf=channels_last     |        2251.687 (+-6.513)        |                     |          2711.557 (+-14.011)

      channels=1, size=256, dtype=torch.bfloat16, mf=channels_last    |         33.237 (+-0.072)         |                     |            33.101 (+-0.070)
      channels=1, size=520, dtype=torch.bfloat16, mf=channels_last    |        113.605 (+-0.337)         |                     |           117.067 (+-0.547)
      channels=1, size=712, dtype=torch.bfloat16, mf=channels_last    |        204.632 (+-0.487)         |                     |           212.590 (+-0.848)

      channels=1, size=256, dtype=torch.bool, mf=channels_last        |         7.950 (+-0.030)          |                     |            37.757 (+-0.080)
      channels=1, size=520, dtype=torch.bool, mf=channels_last        |         23.799 (+-0.080)         |                     |           136.571 (+-0.441)
      channels=1, size=712, dtype=torch.bool, mf=channels_last        |         37.970 (+-0.075)         |                     |           246.894 (+-0.926)

      channels=1, size=256, dtype=torch.bool, mf=channels_first       |         8.009 (+-0.077)          |                     |            37.800 (+-0.100)
      channels=1, size=520, dtype=torch.bool, mf=channels_first       |         23.861 (+-0.099)         |                     |           136.553 (+-0.519)
      channels=1, size=712, dtype=torch.bool, mf=channels_first       |         38.211 (+-0.104)         |                     |           246.939 (+-0.692)

Times are in microseconds (us).
```
[Source](https://gist.github.com/vfdev-5/c2ca615b522aeb1c4636dc8d948fec74#file-20221209-100405-pr_vs_nightly-md)

- AVX512 (channels last input only)
```
[---------------------------------------------------------------------------- Horizontal flip ----------------------------------------------------------------------------]
                                                                      |  torch (1.14.0a0+giteb3e189) PR  |    Pillow (9.3.0)    |  torch (1.14.0.dev20221208+cu116) nightly
1 threads: ----------------------------------------------------------------------------------------------------------------------------------------------------------------
      channels=3, size=256, dtype=torch.int64, mf=channels_last       |        194.708 (+-9.566)         |                      |             372.067 (+-12.430)
      channels=3, size=520, dtype=torch.int64, mf=channels_last       |        765.151 (+-10.098)        |                      |            1524.231 (+-111.283)
      channels=3, size=712, dtype=torch.int64, mf=channels_last       |       1587.229 (+-88.117)        |                      |            2950.081 (+-92.322)

      channels=1, size=256, dtype=torch.int32, mf=channels_last       |         13.328 (+-0.375)         |   49.693 (+-1.193)   |              10.323 (+-0.333)
      channels=1, size=520, dtype=torch.int32, mf=channels_last       |         90.580 (+-0.812)         |  191.936 (+-4.369)   |              92.269 (+-0.980)
      channels=1, size=712, dtype=torch.int32, mf=channels_last       |        163.821 (+-3.174)         |  352.053 (+-10.909)  |             165.661 (+-4.436)

      channels=3, size=256, dtype=torch.uint8, mf=channels_last       |        206.862 (+-4.417)         |   49.336 (+-1.492)   |             287.373 (+-7.266)
      channels=3, size=520, dtype=torch.uint8, mf=channels_last       |        829.736 (+-15.857)        |  191.489 (+-5.645)   |            1166.126 (+-45.667)
      channels=3, size=712, dtype=torch.uint8, mf=channels_last       |       1540.953 (+-28.269)        |  352.171 (+-8.784)   |            2171.570 (+-82.740)

      channels=1, size=256, dtype=torch.int16, mf=channels_last       |         7.856 (+-0.131)          |                      |              7.943 (+-0.148)
      channels=1, size=520, dtype=torch.int16, mf=channels_last       |         34.750 (+-1.195)         |                      |              36.309 (+-0.716)
      channels=1, size=712, dtype=torch.int16, mf=channels_last       |         85.858 (+-0.729)         |                      |              87.306 (+-0.981)

      channels=3, size=256, dtype=torch.int8, mf=channels_last        |        206.896 (+-5.716)         |                      |             262.551 (+-6.598)
      channels=3, size=520, dtype=torch.int8, mf=channels_last        |        828.212 (+-13.441)        |                      |            1077.916 (+-28.810)
      channels=3, size=712, dtype=torch.int8, mf=channels_last        |       1542.748 (+-31.379)        |                      |            2003.661 (+-71.614)

      channels=3, size=256, dtype=torch.int8, mf=channels_first       |         11.038 (+-0.271)         |                      |             126.867 (+-5.590)
      channels=3, size=520, dtype=torch.int8, mf=channels_first       |         90.190 (+-1.185)         |                      |             501.446 (+-13.498)
      channels=3, size=712, dtype=torch.int8, mf=channels_first       |        165.797 (+-3.016)         |                      |             921.131 (+-20.500)

      channels=1, size=256, dtype=torch.float32, mf=channels_last     |         13.516 (+-0.578)         |   49.678 (+-1.966)   |              10.360 (+-0.256)
      channels=1, size=520, dtype=torch.float32, mf=channels_last     |         91.195 (+-0.830)         |  191.778 (+-4.742)   |              91.117 (+-0.855)
      channels=1, size=712, dtype=torch.float32, mf=channels_last     |        168.551 (+-3.352)         |  351.585 (+-8.230)   |             164.199 (+-3.725)

      channels=1, size=256, dtype=torch.float16, mf=channels_last     |         35.832 (+-0.840)         |                      |              35.087 (+-0.972)
      channels=1, size=520, dtype=torch.float16, mf=channels_last     |        133.624 (+-5.293)         |                      |             131.423 (+-6.002)
      channels=1, size=712, dtype=torch.float16, mf=channels_last     |        240.702 (+-5.213)         |                      |             236.876 (+-7.867)

      channels=3, size=256, dtype=torch.float64, mf=channels_last     |        192.351 (+-6.740)         |                      |             313.999 (+-12.141)
      channels=3, size=520, dtype=torch.float64, mf=channels_last     |        766.553 (+-16.669)        |                      |            1270.797 (+-49.828)
      channels=3, size=712, dtype=torch.float64, mf=channels_last     |       1501.700 (+-69.499)        |                      |            2427.303 (+-126.694)

      channels=1, size=256, dtype=torch.bfloat16, mf=channels_last    |         35.386 (+-0.801)         |                      |              34.539 (+-0.844)
      channels=1, size=520, dtype=torch.bfloat16, mf=channels_last    |        132.369 (+-4.107)         |                      |             130.926 (+-3.597)
      channels=1, size=712, dtype=torch.bfloat16, mf=channels_last    |        237.722 (+-6.680)         |                      |             237.072 (+-5.027)

      channels=1, size=256, dtype=torch.bool, mf=channels_last        |         6.796 (+-0.132)          |                      |              44.727 (+-0.905)
      channels=1, size=520, dtype=torch.bool, mf=channels_last        |         24.827 (+-0.669)         |                      |             166.758 (+-5.141)
      channels=1, size=712, dtype=torch.bool, mf=channels_last        |         42.392 (+-0.980)         |                      |             310.830 (+-6.130)

      channels=1, size=256, dtype=torch.bool, mf=channels_first       |         8.114 (+-0.141)          |                      |              44.776 (+-0.707)
      channels=1, size=520, dtype=torch.bool, mf=channels_first       |         24.787 (+-0.787)         |                      |             167.766 (+-5.004)
      channels=1, size=712, dtype=torch.bool, mf=channels_first       |         42.545 (+-0.636)         |                      |             313.715 (+-7.603)

Times are in microseconds (us).
```
[Source](https://gist.github.com/vfdev-5/c2ca615b522aeb1c4636dc8d948fec74#file-20221209-105633-pr_vs_nightly-avx512-md)

### Vertical flip

- AVX2 (all tested cases showing speed-up or same perfs)
```
[-------------------------------------------------------------------------- Vertical flip --------------------------------------------------------------------------]
                                                                      |  torch (1.14.0a0+giteb3e189) PR  |    Pillow (9.3.0)   |  torch (1.14.0a0+gitb0bd5c4) nightly
1 threads: ----------------------------------------------------------------------------------------------------------------------------------------------------------
      channels=3, size=256, dtype=torch.int64, mf=channels_last       |         93.125 (+-3.022)         |                     |           101.064 (+-0.436)
      channels=3, size=520, dtype=torch.int64, mf=channels_last       |        412.942 (+-57.066)        |                     |           461.463 (+-2.098)
      channels=3, size=712, dtype=torch.int64, mf=channels_last       |        1533.265 (+-4.071)        |                     |          1829.713 (+-14.311)

      channels=3, size=256, dtype=torch.int64, mf=channels_first      |        101.134 (+-0.924)         |                     |           102.858 (+-0.319)
      channels=3, size=520, dtype=torch.int64, mf=channels_first      |        421.679 (+-1.101)         |                     |           477.413 (+-1.809)
      channels=3, size=712, dtype=torch.int64, mf=channels_first      |        1550.418 (+-3.647)        |                     |           1877.143 (+-6.622)

      channels=1, size=256, dtype=torch.int32, mf=channels_last       |         20.961 (+-0.063)         |   19.515 (+-0.302)  |            21.980 (+-0.070)
      channels=1, size=520, dtype=torch.int32, mf=channels_last       |         71.199 (+-0.173)         |   70.199 (+-0.332)  |            95.262 (+-0.109)
      channels=1, size=712, dtype=torch.int32, mf=channels_last       |        128.532 (+-0.318)         |  127.325 (+-0.328)  |           167.190 (+-0.370)

      channels=1, size=256, dtype=torch.int32, mf=channels_first      |         21.206 (+-0.059)         |   19.471 (+-0.128)  |            21.469 (+-0.064)
      channels=1, size=520, dtype=torch.int32, mf=channels_first      |         71.284 (+-0.163)         |   70.124 (+-0.388)  |            94.988 (+-0.239)
      channels=1, size=712, dtype=torch.int32, mf=channels_first      |        129.017 (+-0.286)         |  128.088 (+-0.461)  |           167.115 (+-1.075)

      channels=3, size=256, dtype=torch.uint8, mf=channels_last       |         16.909 (+-0.057)         |   19.570 (+-0.353)  |            17.981 (+-0.072)
      channels=3, size=520, dtype=torch.uint8, mf=channels_last       |         55.163 (+-0.138)         |   70.218 (+-0.275)  |           107.938 (+-0.620)
      channels=3, size=712, dtype=torch.uint8, mf=channels_last       |         98.518 (+-0.121)         |  127.737 (+-0.486)  |           170.965 (+-0.436)

      channels=3, size=256, dtype=torch.uint8, mf=channels_first      |         18.150 (+-0.084)         |   19.758 (+-0.221)  |            18.122 (+-0.088)
      channels=3, size=520, dtype=torch.uint8, mf=channels_first      |         56.693 (+-0.200)         |   70.278 (+-0.386)  |            89.018 (+-0.206)
      channels=3, size=712, dtype=torch.uint8, mf=channels_first      |        100.409 (+-0.235)         |  127.772 (+-0.457)  |           168.072 (+-0.436)

      channels=1, size=256, dtype=torch.int16, mf=channels_last       |         12.817 (+-0.041)         |                     |            12.818 (+-0.049)
      channels=1, size=520, dtype=torch.int16, mf=channels_last       |         38.359 (+-0.081)         |                     |            63.378 (+-0.165)
      channels=1, size=712, dtype=torch.int16, mf=channels_last       |         68.246 (+-0.090)         |                     |           116.637 (+-0.583)

      channels=1, size=256, dtype=torch.int16, mf=channels_first      |         12.899 (+-0.054)         |                     |            12.649 (+-0.060)
      channels=1, size=520, dtype=torch.int16, mf=channels_first      |         38.404 (+-0.069)         |                     |            63.448 (+-0.108)
      channels=1, size=712, dtype=torch.int16, mf=channels_first      |         68.378 (+-0.104)         |                     |           116.415 (+-0.332)

      channels=3, size=256, dtype=torch.int8, mf=channels_last        |         17.071 (+-0.044)         |                     |            17.792 (+-0.050)
      channels=3, size=520, dtype=torch.int8, mf=channels_last        |         55.163 (+-0.100)         |                     |           108.539 (+-0.466)
      channels=3, size=712, dtype=torch.int8, mf=channels_last        |         98.537 (+-0.091)         |                     |           171.675 (+-0.553)

      channels=3, size=256, dtype=torch.int8, mf=channels_first       |         17.837 (+-0.071)         |                     |            18.355 (+-0.067)
      channels=3, size=520, dtype=torch.int8, mf=channels_first       |         56.051 (+-0.087)         |                     |            88.261 (+-0.129)
      channels=3, size=712, dtype=torch.int8, mf=channels_first       |        100.603 (+-0.245)         |                     |           169.067 (+-0.430)

      channels=1, size=256, dtype=torch.float32, mf=channels_last     |         21.204 (+-0.063)         |   19.607 (+-0.140)  |            22.202 (+-0.094)
      channels=1, size=520, dtype=torch.float32, mf=channels_last     |         71.356 (+-0.211)         |   69.844 (+-0.343)  |            94.614 (+-0.167)
      channels=1, size=712, dtype=torch.float32, mf=channels_last     |        129.087 (+-0.290)         |  127.065 (+-0.319)  |           166.513 (+-0.444)

      channels=1, size=256, dtype=torch.float32, mf=channels_first    |         21.196 (+-0.065)         |   19.156 (+-0.132)  |            21.516 (+-0.073)
      channels=1, size=520, dtype=torch.float32, mf=channels_first    |         71.422 (+-0.180)         |   70.296 (+-0.136)  |            94.913 (+-0.095)
      channels=1, size=712, dtype=torch.float32, mf=channels_first    |        129.045 (+-0.312)         |  128.023 (+-0.585)  |           166.089 (+-0.409)

      channels=1, size=256, dtype=torch.float16, mf=channels_last     |         12.770 (+-0.045)         |                     |            34.853 (+-0.089)
      channels=1, size=520, dtype=torch.float16, mf=channels_last     |         38.363 (+-0.064)         |                     |           131.969 (+-0.577)
      channels=1, size=712, dtype=torch.float16, mf=channels_last     |         67.954 (+-0.107)         |                     |           239.507 (+-0.835)

      channels=1, size=256, dtype=torch.float16, mf=channels_first    |         12.855 (+-0.067)         |                     |            35.124 (+-0.109)
      channels=1, size=520, dtype=torch.float16, mf=channels_first    |         38.725 (+-0.079)         |                     |           131.708 (+-0.586)
      channels=1, size=712, dtype=torch.float16, mf=channels_first    |         68.931 (+-0.086)         |                     |           239.022 (+-0.914)

      channels=3, size=256, dtype=torch.float64, mf=channels_last     |         90.277 (+-0.083)         |                     |           101.512 (+-0.285)
      channels=3, size=520, dtype=torch.float64, mf=channels_last     |        421.277 (+-1.030)         |                     |           471.913 (+-3.654)
      channels=3, size=712, dtype=torch.float64, mf=channels_last     |        1534.394 (+-7.572)        |                     |          1833.262 (+-12.185)

      channels=3, size=256, dtype=torch.float64, mf=channels_first    |        100.809 (+-0.328)         |                     |           103.166 (+-0.335)
      channels=3, size=520, dtype=torch.float64, mf=channels_first    |        425.535 (+-0.926)         |                     |           482.606 (+-1.450)
      channels=3, size=712, dtype=torch.float64, mf=channels_first    |        1550.832 (+-3.547)        |                     |           1859.098 (+-6.517)

      channels=1, size=256, dtype=torch.bfloat16, mf=channels_last    |         12.954 (+-0.051)         |                     |            12.744 (+-0.046)
      channels=1, size=520, dtype=torch.bfloat16, mf=channels_last    |         41.180 (+-0.064)         |                     |            63.362 (+-0.139)
      channels=1, size=712, dtype=torch.bfloat16, mf=channels_last    |         68.136 (+-0.142)         |                     |           117.009 (+-0.292)

      channels=1, size=256, dtype=torch.bfloat16, mf=channels_first   |         13.049 (+-0.052)         |                     |            12.792 (+-0.076)
      channels=1, size=520, dtype=torch.bfloat16, mf=channels_first   |         38.488 (+-0.092)         |                     |            63.451 (+-0.096)
      channels=1, size=712, dtype=torch.bfloat16, mf=channels_first   |         68.103 (+-0.091)         |                     |           116.693 (+-0.290)

      channels=1, size=256, dtype=torch.bool, mf=channels_last        |         7.572 (+-0.029)          |                     |            8.017 (+-0.071)
      channels=1, size=520, dtype=torch.bool, mf=channels_last        |         22.121 (+-0.061)         |                     |            23.614 (+-0.074)
      channels=1, size=712, dtype=torch.bool, mf=channels_last        |         36.896 (+-0.094)         |                     |            39.460 (+-0.084)

      channels=1, size=256, dtype=torch.bool, mf=channels_first       |         7.671 (+-0.028)          |                     |            8.034 (+-0.058)
      channels=1, size=520, dtype=torch.bool, mf=channels_first       |         21.989 (+-0.053)         |                     |            23.645 (+-0.063)
      channels=1, size=712, dtype=torch.bool, mf=channels_first       |         37.252 (+-0.072)         |                     |            39.477 (+-0.100)

      channels=1, size=256, dtype=torch.complex64, mf=channels_last   |         37.129 (+-0.052)         |                     |            37.801 (+-0.101)
      channels=1, size=520, dtype=torch.complex64, mf=channels_last   |        122.646 (+-0.230)         |                     |           139.074 (+-0.467)
      channels=1, size=712, dtype=torch.complex64, mf=channels_last   |        228.946 (+-0.736)         |                     |           257.589 (+-0.545)

      channels=1, size=256, dtype=torch.complex64, mf=channels_first  |         37.088 (+-0.070)         |                     |            37.894 (+-0.078)
      channels=1, size=520, dtype=torch.complex64, mf=channels_first  |        122.695 (+-0.268)         |                     |           138.933 (+-0.336)
      channels=1, size=712, dtype=torch.complex64, mf=channels_first  |        234.655 (+-0.454)         |                     |           255.787 (+-0.530)

Times are in microseconds (us).
```
[Source](https://gist.github.com/vfdev-5/c2ca615b522aeb1c4636dc8d948fec74#file-20221209-100440-pr_vs_nightly-md)

- AVX512 (all tested cases showing speed-up or same perfs)

```
[---------------------------------------------------------------------------- Vertical flip -----------------------------------------------------------------------------]
                                                                      |  torch (1.14.0a0+giteb3e189) PR  |    Pillow (9.3.0)   |  torch (1.14.0.dev20221208+cu116) nightly
1 threads: ---------------------------------------------------------------------------------------------------------------------------------------------------------------
      channels=3, size=256, dtype=torch.int64, mf=channels_last       |        122.544 (+-1.962)         |                     |             129.161 (+-1.809)
      channels=3, size=520, dtype=torch.int64, mf=channels_last       |        508.274 (+-4.790)         |                     |             533.872 (+-7.457)
      channels=3, size=712, dtype=torch.int64, mf=channels_last       |        951.176 (+-29.534)        |                     |            1073.603 (+-44.676)

      channels=3, size=256, dtype=torch.int64, mf=channels_first      |        127.872 (+-2.700)         |                     |             127.326 (+-2.666)
      channels=3, size=520, dtype=torch.int64, mf=channels_first      |        518.019 (+-4.157)         |                     |             538.094 (+-6.600)
      channels=3, size=712, dtype=torch.int64, mf=channels_first      |       1002.176 (+-42.545)        |                     |            1033.989 (+-42.137)

      channels=1, size=256, dtype=torch.int32, mf=channels_last       |         10.025 (+-0.135)         |   10.054 (+-0.369)  |              10.155 (+-0.285)
      channels=1, size=520, dtype=torch.int32, mf=channels_last       |         89.867 (+-0.994)         |   88.712 (+-0.622)  |             103.029 (+-2.254)
      channels=1, size=712, dtype=torch.int32, mf=channels_last       |        161.787 (+-2.080)         |  161.370 (+-1.801)  |             182.608 (+-7.031)

      channels=1, size=256, dtype=torch.int32, mf=channels_first      |         10.005 (+-0.277)         |   9.965 (+-0.338)   |              10.604 (+-0.334)
      channels=1, size=520, dtype=torch.int32, mf=channels_first      |         89.116 (+-0.996)         |   88.840 (+-0.608)  |             102.103 (+-2.111)
      channels=1, size=712, dtype=torch.int32, mf=channels_first      |        164.328 (+-3.284)         |  161.538 (+-2.739)  |             181.702 (+-3.770)

      channels=3, size=256, dtype=torch.uint8, mf=channels_last       |         8.853 (+-0.148)          |   10.292 (+-0.494)  |              8.961 (+-0.190)
      channels=3, size=520, dtype=torch.uint8, mf=channels_last       |         68.368 (+-1.158)         |   90.068 (+-1.780)  |              81.155 (+-0.945)
      channels=3, size=712, dtype=torch.uint8, mf=channels_last       |        125.458 (+-2.511)         |  163.150 (+-2.532)  |             147.039 (+-4.264)

      channels=3, size=256, dtype=torch.uint8, mf=channels_first      |         10.409 (+-0.435)         |   10.406 (+-0.351)  |              10.263 (+-0.252)
      channels=3, size=520, dtype=torch.uint8, mf=channels_first      |         69.077 (+-1.062)         |   90.057 (+-0.992)  |              79.910 (+-0.884)
      channels=3, size=712, dtype=torch.uint8, mf=channels_first      |        127.286 (+-2.789)         |  162.862 (+-2.953)  |             142.821 (+-2.119)

      channels=1, size=256, dtype=torch.int16, mf=channels_last       |         7.513 (+-0.143)          |                     |              7.364 (+-0.154)
      channels=1, size=520, dtype=torch.int16, mf=channels_last       |         33.140 (+-0.779)         |                     |              42.141 (+-0.820)
      channels=1, size=712, dtype=torch.int16, mf=channels_last       |         86.235 (+-1.187)         |                     |             104.205 (+-2.205)

      channels=1, size=256, dtype=torch.int16, mf=channels_first      |         7.410 (+-0.162)          |                     |              7.075 (+-0.126)
      channels=1, size=520, dtype=torch.int16, mf=channels_first      |         33.656 (+-0.914)         |                     |              40.991 (+-0.893)
      channels=1, size=712, dtype=torch.int16, mf=channels_first      |         86.087 (+-1.191)         |                     |             105.419 (+-1.801)

      channels=3, size=256, dtype=torch.int8, mf=channels_last        |         8.802 (+-0.196)          |                     |              8.627 (+-0.202)
      channels=3, size=520, dtype=torch.int8, mf=channels_last        |         66.348 (+-0.775)         |                     |              80.631 (+-1.832)
      channels=3, size=712, dtype=torch.int8, mf=channels_last        |        126.275 (+-2.318)         |                     |             144.597 (+-4.242)

      channels=3, size=256, dtype=torch.int8, mf=channels_first       |         10.255 (+-0.383)         |                     |              10.101 (+-0.335)
      channels=3, size=520, dtype=torch.int8, mf=channels_first       |         68.124 (+-0.849)         |                     |              79.286 (+-0.748)
      channels=3, size=712, dtype=torch.int8, mf=channels_first       |        127.118 (+-2.225)         |                     |             142.029 (+-2.507)

      channels=1, size=256, dtype=torch.float32, mf=channels_last     |         9.850 (+-0.453)          |   9.299 (+-0.253)   |              10.030 (+-0.234)
      channels=1, size=520, dtype=torch.float32, mf=channels_last     |         91.506 (+-1.319)         |   90.265 (+-0.824)  |             107.570 (+-2.093)
      channels=1, size=712, dtype=torch.float32, mf=channels_last     |        167.820 (+-3.883)         |  162.871 (+-2.397)  |             180.046 (+-8.952)

      channels=1, size=256, dtype=torch.float32, mf=channels_first    |         10.118 (+-0.359)         |   10.433 (+-0.479)  |              10.204 (+-0.344)
      channels=1, size=520, dtype=torch.float32, mf=channels_first    |         90.862 (+-1.486)         |   90.138 (+-0.969)  |             107.011 (+-1.801)
      channels=1, size=712, dtype=torch.float32, mf=channels_first    |        163.931 (+-3.653)         |  163.155 (+-2.673)  |             186.707 (+-2.248)

      channels=1, size=256, dtype=torch.float16, mf=channels_last     |         7.304 (+-0.134)          |                     |              24.141 (+-0.444)
      channels=1, size=520, dtype=torch.float16, mf=channels_last     |         35.186 (+-0.656)         |                     |             101.523 (+-1.465)
      channels=1, size=712, dtype=torch.float16, mf=channels_last     |         85.707 (+-0.841)         |                     |             192.640 (+-4.942)

      channels=1, size=256, dtype=torch.float16, mf=channels_first    |         7.286 (+-0.142)          |                     |              24.155 (+-0.555)
      channels=1, size=520, dtype=torch.float16, mf=channels_first    |         33.819 (+-1.009)         |                     |             101.620 (+-3.034)
      channels=1, size=712, dtype=torch.float16, mf=channels_first    |         84.811 (+-0.993)         |                     |             192.286 (+-4.707)

      channels=3, size=256, dtype=torch.float64, mf=channels_last     |        126.273 (+-2.519)         |                     |             128.831 (+-1.975)
      channels=3, size=520, dtype=torch.float64, mf=channels_last     |        551.861 (+-4.159)         |                     |             517.343 (+-4.501)
      channels=3, size=712, dtype=torch.float64, mf=channels_last     |       1102.465 (+-66.427)        |                     |            1224.532 (+-55.656)

      channels=3, size=256, dtype=torch.float64, mf=channels_first    |        129.965 (+-2.083)         |                     |             130.709 (+-2.261)
      channels=3, size=520, dtype=torch.float64, mf=channels_first    |        526.332 (+-5.354)         |                     |             515.399 (+-4.320)
      channels=3, size=712, dtype=torch.float64, mf=channels_first    |       1169.215 (+-78.889)        |                     |            1102.536 (+-51.178)

      channels=1, size=256, dtype=torch.bfloat16, mf=channels_last    |         7.478 (+-0.147)          |                     |              7.154 (+-0.162)
      channels=1, size=520, dtype=torch.bfloat16, mf=channels_last    |         33.836 (+-1.022)         |                     |              38.854 (+-0.648)
      channels=1, size=712, dtype=torch.bfloat16, mf=channels_last    |         85.483 (+-0.582)         |                     |              99.190 (+-2.202)

      channels=1, size=256, dtype=torch.bfloat16, mf=channels_first   |         7.416 (+-0.125)          |                     |              7.169 (+-0.121)
      channels=1, size=520, dtype=torch.bfloat16, mf=channels_first   |         34.958 (+-0.717)         |                     |              40.136 (+-0.784)
      channels=1, size=712, dtype=torch.bfloat16, mf=channels_first   |         85.505 (+-1.207)         |                     |              99.793 (+-2.065)

      channels=1, size=256, dtype=torch.bool, mf=channels_last        |         5.856 (+-0.178)          |                     |              5.824 (+-0.118)
      channels=1, size=520, dtype=torch.bool, mf=channels_last        |         12.030 (+-0.330)         |                     |              14.478 (+-0.554)
      channels=1, size=712, dtype=torch.bool, mf=channels_last        |         30.116 (+-0.639)         |                     |              31.163 (+-0.873)

      channels=1, size=256, dtype=torch.bool, mf=channels_first       |         5.804 (+-0.113)          |                     |              5.825 (+-0.102)
      channels=1, size=520, dtype=torch.bool, mf=channels_first       |         12.043 (+-0.363)         |                     |              14.240 (+-0.341)
      channels=1, size=712, dtype=torch.bool, mf=channels_first       |         30.001 (+-1.001)         |                     |              33.199 (+-0.430)

      channels=1, size=256, dtype=torch.complex64, mf=channels_last   |         29.941 (+-0.861)         |                     |              28.229 (+-0.904)
      channels=1, size=520, dtype=torch.complex64, mf=channels_last   |        173.244 (+-2.577)         |                     |             173.173 (+-2.260)
      channels=1, size=712, dtype=torch.complex64, mf=channels_last   |        323.548 (+-3.338)         |                     |             318.318 (+-2.764)

      channels=1, size=256, dtype=torch.complex64, mf=channels_first  |         29.001 (+-1.029)         |                     |              28.565 (+-2.074)
      channels=1, size=520, dtype=torch.complex64, mf=channels_first  |        173.078 (+-1.993)         |                     |             170.664 (+-1.722)
      channels=1, size=712, dtype=torch.complex64, mf=channels_first  |        324.782 (+-3.759)         |                     |             315.745 (+-2.600)

Times are in microseconds (us).

```

[Source](https://gist.github.com/vfdev-5/c2ca615b522aeb1c4636dc8d948fec74#file-20221209-105707-pr_vs_nightly-avx512-md)

Pull Request resolved: https://github.com/pytorch/pytorch/pull/89414
Approved by: https://github.com/peterbell10, https://github.com/lezcano, https://github.com/albanD"
pytorch/pytorch,f6acd95ae51996b821a938bdce1ad05f1be83326,"Fix performance smoke test script bug (#92660)

Fixes the file not found issue in https://github.com/pytorch/pytorch/actions/runs/3963775704

Pull Request resolved: https://github.com/pytorch/pytorch/pull/92660
Approved by: https://github.com/desertfire, https://github.com/huydhn"
pytorch/pytorch,119d5e425c77772e4fa4574ae4b5af54b170be54,"[Inductor] decompose expm1 for CPP vec (#92289)

For micro-bench op `aten.elu.default` in TIMM, the performance is not good even though with vectorization. `Elu` uses `expm1` as a sub-op. It turns out that inductor invokes sleef `expm1` function while aten decomposes it with `exp - 1`. The former one performs worse than the latter one. This PR decomposes `expm1` for cpp vectorization to make performance come back.

Performance data for eager v.s. inductor:
<html xmlns:v=""urn:schemas-microsoft-com:vml""
xmlns:o=""urn:schemas-microsoft-com:office:office""
xmlns:x=""urn:schemas-microsoft-com:office:excel""
xmlns=""http://www.w3.org/TR/REC-html40"">

<head>

<meta name=ProgId content=Excel.Sheet>
<meta name=Generator content=""Microsoft Excel 15"">
<link id=Main-File rel=Main-File
href=""file:///C:/Users/xuanliao/AppData/Local/Temp/msohtmlclip1/01/clip.htm"">
<link rel=File-List
href=""file:///C:/Users/xuanliao/AppData/Local/Temp/msohtmlclip1/01/clip_filelist.xml"">
</head>

<body link=blue vlink=purple>

<html xmlns:v=""urn:schemas-microsoft-com:vml""
xmlns:o=""urn:schemas-microsoft-com:office:office""
xmlns:x=""urn:schemas-microsoft-com:office:excel""
xmlns=""http://www.w3.org/TR/REC-html40"">

<head>

<meta name=ProgId content=Excel.Sheet>
<meta name=Generator content=""Microsoft Excel 15"">
<link id=Main-File rel=Main-File
href=""file:///C:/Users/xuanliao/AppData/Local/Temp/msohtmlclip1/01/clip.htm"">
<link rel=File-List
href=""file:///C:/Users/xuanliao/AppData/Local/Temp/msohtmlclip1/01/clip_filelist.xml"">
</head>

<body link=blue vlink=purple>

suite | improved_ratio_speedup | speedup_old | RSD(3) | speedup_new | RSD(3)
-- | -- | -- | -- | -- | --
timm | 114.38% | 0.803447768 | 8.39% | 1.722458 | 27.74%

</body>

</html>

Pull Request resolved: https://github.com/pytorch/pytorch/pull/92289
Approved by: https://github.com/jgong5, https://github.com/jansel"
pytorch/pytorch,59071ab1e71891d480ab77af0d619bc5e01094c2,"[Executorch][Quantization][BE] Refactor Choose Qparams (#92592)

Summary: Should hopefully be a little faster. Definitely cleaner to not create an observer inside the op

Test Plan: ci

Differential Revision: D42154677

Pull Request resolved: https://github.com/pytorch/pytorch/pull/92592
Approved by: https://github.com/jerryzh168"
pytorch/pytorch,cf5495ac3a24a1f14fba454cbd7f23a918f01558,"Add perf check for inductor smoke test (#92358)

Background: performance smoke test job has been setup for inductor https://github.com/pytorch/pytorch/actions/workflows/inductor-perf-smoke-test.yml
I have used this job to identify that https://github.com/pytorch/pytorch/pull/91254 regressed performance from 1.194x to 1.156x. However, this was by manual checking.

To automatically flag similar regressions, we will add a reference value (which needs to be actively maintained) so that any speedups falling below the reference would be treated regression.

In the back, two A100 instances from GCP would be running the perf check jobs for every push to upstream. So far these two instances give up to 1.204x and 1.197x, so we interpret any output below 1.185x to be suspicious.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/92358
Approved by: https://github.com/ngimel, https://github.com/desertfire"
pytorch/pytorch,60bf851931f58e528be927d96115272248f531fd,"Revert ""Improve `bsr @ strided` performance in `baddmm` for `bfloat16/half` with Triton kernels. (#88078)""

This reverts commit 8383b5c488399f2ae295c7c0f993bdd353dfd75c.

Reverted https://github.com/pytorch/pytorch/pull/88078 on behalf of https://github.com/malfet due to This seems to have broke sm_86 testing, see https://hud.pytorch.org/hud/pytorch/pytorch/master/1?per_page=50&name_filter=sm86%20%2F%20test%20(default%2C%203"
pytorch/pytorch,8383b5c488399f2ae295c7c0f993bdd353dfd75c,"Improve `bsr @ strided` performance in `baddmm` for `bfloat16/half` with Triton kernels. (#88078)

As per title.

Additionally we also introduce support for:
- Rectangular block sizes which are powers of 2 and at least 16 (triton's `dot` limitation).
- Batch support with broadcasting for either of the arguments.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/88078
Approved by: https://github.com/cpuhrsch"
pytorch/pytorch,bdbd3ed312e0fc81e75302239ea78b3445fe95e7,"When nopython=True, Dynamo can't allow graph breaks. (#90970)

I count the number of sub-graphs (for tiny-GPT2 in huggingface) by
```
    class GraphCaptureCompiler:
        def __init__(self):
            self.captured_graphs = []
        def compile(self, gm, example_inputs):
            self.captured_graphs.append(gm)
            return gm
    compiler = GraphCaptureCompiler()
    torch._dynamo.optimize(compiler, nopython=True)(Wrapper(fn))(*args)
```

Although `len(compiler.captured_graphs)` is 2, no error was thrown during the compilation. This observation conflicts with `nopython=True`. After some digging, I found a check is missed before making graph break. This PR adds it.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/90970
Approved by: https://github.com/ezyang, https://github.com/jansel, https://github.com/thiagocrepaldi"
pytorch/pytorch,c5cb46ecdbc0c21697d99e240116bc3411674744,"[optim][asgd] group tensors in foreach to maximize perf (#92364)

faster foreach
Pull Request resolved: https://github.com/pytorch/pytorch/pull/92364
Approved by: https://github.com/albanD"
pytorch/pytorch,e2433e420c3fbda12733a17e086595fa8202fd76,"[optim][adamax] group tensors in foreach to maximize perf (#92363)

make foreach faster
Pull Request resolved: https://github.com/pytorch/pytorch/pull/92363
Approved by: https://github.com/albanD"
pytorch/pytorch,92d412d684924528f30e744ddccb155ac3c600a2,"[FSDP][optim_state_dict][11/N] Let FSDP support NamedOptimizer/KeyedOptimizer when use_orig_params is False (#92184)

Current design of FSDP only support NamedOptimizer/KeyedOptimizer when use_orig_params is True this PR adds the support even if use_orig_params if False. This PR also adds the support for user-defined optimizer states -- states that are not associated with any particular parameters.

Differential Revision: [D42497416](https://our.internmc.facebook.com/intern/diff/D42497416/)
Pull Request resolved: https://github.com/pytorch/pytorch/pull/92184
Approved by: https://github.com/colin2328, https://github.com/rohan-varma"
pytorch/pytorch,6420fecdc4ebf613eac91587b36dc28f0a0640ba,"Introduce sym_min and sym_max (#92107)

It turns out our old max/min implementation didn't do anything, because `__max__` and `__min__` are not actually magic methods in Python. So I give 'em the `sym_` treatment, similar to the other non-overrideable builtins.

NB: I would like to use `sym_max` when computing contiguous strides but this appears to make `python test/functorch/test_aotdispatch.py -v -k test_aot_autograd_symbolic_exhaustive_nn_functional_max_pool2d_cpu_float32` run extremely slowly. Needs investigating.

Signed-off-by: Edward Z. Yang <ezyang@meta.com>

Pull Request resolved: https://github.com/pytorch/pytorch/pull/92107
Approved by: https://github.com/albanD, https://github.com/voznesenskym, https://github.com/Skylion007"
pytorch/pytorch,21d2bd782bf18faba0ac3dfcaf6d58f929367687,"stack_module_state should return unrelated parameters (#92278)

`torch.func.stack_module_state` is our replacement for
`functorch.combine_state_for_ensemble`. The most common usage for
combine_state_for_ensemble is to
- create stacked parameters and buffers
- use vmap to run the forward pass
- use regular PyTorch autograd to run the backward pass (e.g.,
Tensor.backwrd)
- optimize directly over the stacked parameters (this is more performant
than optimizing over the unstacked parameters).

Right now, stack_module_state returns stacked parameters that cannot be
optimized directly (only leaf tensors can have a .grad field); this PR
fixes that by turning the stacked parameters back into leaf tensors.

Test Plan:
- new tests
Pull Request resolved: https://github.com/pytorch/pytorch/pull/92278
Approved by: https://github.com/soulitzer"
pytorch/pytorch,3aa6cec18cf662913e0625d539945ee5ee6406f0,"[dynamo] exclude reset_rng_state when measure timing (#92237)

Fixes inductor performance regression on CPU: https://github.com/pytorch/torchdynamo/issues/2027, https://github.com/pytorch/torchdynamo/issues/2028 and https://github.com/pytorch/torchdynamo/issues/2029.
The details are explained here: https://github.com/pytorch/torchdynamo/issues/2028#issuecomment-1381496678.

### Performance

- Model: lennard_jones
- Machine: IceLake (32 cores per socket)
- Configuration: single instance, 32 cores per instance
- jemalloc and iomp enabled

```bash
python benchmarks/dynamo/torchbench.py  --inductor-settings --inductor --performance --float32 -dcpu -n5000  --no-skip --dashboard --only=lennard_jones --quiet
```

<html xmlns:v=""urn:schemas-microsoft-com:vml""
xmlns:o=""urn:schemas-microsoft-com:office:office""
xmlns:x=""urn:schemas-microsoft-com:office:excel""
xmlns=""http://www.w3.org/TR/REC-html40"">

<head>

<meta name=ProgId content=Excel.Sheet>
<meta name=Generator content=""Microsoft Excel 15"">
<link id=Main-File rel=Main-File
href=""file:///C:/Users/chunyuan/AppData/Local/Temp/msohtmlclip1/01/clip.htm"">
<link rel=File-List
href=""file:///C:/Users/chunyuan/AppData/Local/Temp/msohtmlclip1/01/clip_filelist.xml"">

</head>

<body link=""#0563C1"" vlink=""#954F72"">

Time before   regression | Time after regression | Time with this PR
-- | -- | --
0.00020483799744397402 | 0.0002818034990923479 | 0.00020241099991835654

</body>

</html>

Pull Request resolved: https://github.com/pytorch/pytorch/pull/92237
Approved by: https://github.com/jgong5, https://github.com/jansel"
pytorch/pytorch,368c737603660d4513ce56e6929a55358e556e34,"[PT-D][5/N] Enable add_param_group for named optimizer (#91928)

Pull Request resolved: https://github.com/pytorch/pytorch/pull/91928
Approved by: https://github.com/rohan-varma"
pytorch/pytorch,5a2ae8805c4ffb74498d2d23c27f3f048ffd2030,"[Quant] onednn backend switch to ideep new api without affacting performance (#91056)

> Reopen of https://github.com/pytorch/pytorch/pull/90354

**Summary**
Onednn quantization backend switch to new API in `third_party/ideep`.
- `struct forward_params` for conv/deconv are changed. Modify primitive cache accordingly.
- Use new versions of `prepare` and `compute` API. Fp32 and int8 paths separated. The old ones will be deprecated.
- Now `ideep::tensor::reorder_if_differ_in` supports block-to-block reorder. Use it instead of defining a util function `onednn_utils::try_reorder`.
- For new API of transposed convolution, we can use a flag to keep weight desc align with oneDNN thus needless to transpose it explicitly in PyTorch.
- Use `is_channels_last` flag to specify layout of src/dst when querying expected weight desc.

It won't impact correctness. Performance should be unaffected or slightly better.
FBGEMM and QNNPACK backends are not affected.

Performance results are given below.
1. End-to-end performance of static quantized models (from torchvision)
(throughput: fps, higher is better)
![image](https://user-images.githubusercontent.com/12522207/206105879-45c59996-9804-4531-aa1f-dc962e6db5ab.png)

2. Op benchmark of dynamic quantized linear
(Latency: ms, lower is better)
![image](https://user-images.githubusercontent.com/12522207/206124949-77352991-0fda-4285-a484-e20a5797262b.png)

Test method & env:
- Intel(R) Xeon(R) Platinum 8358 CPU @ 2.60GHz
- Run multi-instances on a single node. Use one core for each instance.
- Use Jemalloc and Intel OpenMP

**Test plan**
python test/test_quantization.py

Pull Request resolved: https://github.com/pytorch/pytorch/pull/91056
Approved by: https://github.com/jgong5"
pytorch/pytorch,3868eeb75f72dc9cfbdf0e388f43db176064e3bd,"fix biasadd OMP perf issue for the packed MKL SGEMM (#92300)

Currently the biasadd of MKL SGEMM was executed using OpenMP macro, this will lead to a performance issue if the SGEMM size is very small (e.g., M = 1, K = 80, N = 256) when we are using many threads.
The reason is that in such case `num_task < num_thread`, and the task cost is too small (e.g., ~1-2 cycles for memcpy), the thread synchronization cost would be very large. Thus it is better to use `at::parallel_for` to run on the main thread directly.
Packed MKL SGEMM (1x80x256) | OpenMP biasadd | `at::parallel_for` biasadd
-- | -- | --
Latency | 2000 us | 21 us

Pull Request resolved: https://github.com/pytorch/pytorch/pull/92300
Approved by: https://github.com/XiaobingSuper, https://github.com/jgong5"
pytorch/pytorch,94a7c011590b9c2b91e512abbc8d90455c87fef4,"Enable oneDNN implementation in LSTM op (#91158)

### Description
This PR is to enable oneDNN implementation in LSTM op to improve the performance of it. Both FP32 and BF16 are supported.

### Performance improvement
In CPX 28C, with setting iomp and jemalloc.
We choose 8 LSTM input options (including input_size, hidden_size, num_layers, bidirectional, bias, batch_first, dropout, batch_size, seq_len), and the final option is a real input from train-clean-100 in LibriSpeech dataset. The performance improvements are shown in the following figures. We can see that LSTM with oneDNN implementation can perform better than the original.

In single socket:
![image](https://user-images.githubusercontent.com/61222868/211182994-833debec-518a-4b35-8504-6b0fadb17930.png)

![image](https://user-images.githubusercontent.com/61222868/211183012-31e1253f-2c60-4c92-a656-c239a971b453.png)

In single core:
![image](https://user-images.githubusercontent.com/61222868/211183017-186e5d47-cb9a-4c1e-914f-fa718e769f1c.png)

![image](https://user-images.githubusercontent.com/61222868/211183022-53266857-5a9e-4a95-b300-33fa34811d08.png)

Pull Request resolved: https://github.com/pytorch/pytorch/pull/91158
Approved by: https://github.com/jgong5, https://github.com/malfet"
pytorch/pytorch,a41f00ed70849943b99e60982ca8f4bc90762049,"[optim][sgd] group tensors in foreach to maximize perf (#92338)

Make foreach faster for SGD
Pull Request resolved: https://github.com/pytorch/pytorch/pull/92338
Approved by: https://github.com/albanD"
pytorch/pytorch,89f1ad08b43cbbe7d7d0629d899b9e088c30478f,"Revert ""Improve `bsr @ strided` performance in `baddmm` for `bfloat16/half` with Triton kernels. (#88078)""

This reverts commit 7f256fff77c49729131aa6d092e60e891d0c4948.

Reverted https://github.com/pytorch/pytorch/pull/88078 on behalf of https://github.com/huydhn due to This breaks lint https://hud.pytorch.org/pytorch/pytorch/commit/7f256fff77c49729131aa6d092e60e891d0c4948"
pytorch/pytorch,7f256fff77c49729131aa6d092e60e891d0c4948,"Improve `bsr @ strided` performance in `baddmm` for `bfloat16/half` with Triton kernels. (#88078)

As per title.

Additionally we also introduce support for:
- Rectangular block sizes which are powers of 2 and at least 16 (triton's `dot` limitation).
- Batch support with broadcasting for either of the arguments.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/88078
Approved by: https://github.com/cpuhrsch"
pytorch/pytorch,88942a31991905ec1849f5b7fdc6266e7843d08d,"Revert ""[FSDP] Do not clean FQNs even for `use_orig_params=True` (#91767)""

This reverts commit d6f3265e1add26abedb504910be93b393b9fb33c.

Reverted https://github.com/pytorch/pytorch/pull/91767 on behalf of https://github.com/malfet due to Looks like it broke `test_compatible_with_named_optimizer` distribued tests, see https://hud.pytorch.org/pytorch/pytorch/commit/d6f3265e1add26abedb504910be93b393b9fb33c"
pytorch/pytorch,1439cb0314bbac387254282af0cf1e8c95217a3d,"[FSDP][optim_state_dict][9/N] Rewrite the all-gather flow of optimizer state to support older GPUs (#91343)

Pull Request resolved: https://github.com/pytorch/pytorch/pull/91343
Approved by: https://github.com/rohan-varma"
pytorch/pytorch,ded2b47bdebc898d5c3dc430be64a250802a9c5d,"Fix AOTAutograd 2.0 perf regression involving as_strided (#92255)

I feel there may be a deeper fix where we avoid as_strided entirely, but in the regressed model the sizes/strides all lined up exactly, so this seems to work to fix the immediate regression.

Repro command: `python benchmarks/dynamo/torchbench.py  --performance  --backend inductor --float16 --training --batch-size-file $(realpath benchmarks/dynamo/torchbench_models_list.txt) --only hf_Bert  `

Before: 1.138x p=0.00
After: 1.162x p=0.00

Natalia pinpointed it to this line by comparing GPU traces and finding that the regressed PyTorch had two extra fill kernels and a memcpy:

Without regression:
![image](https://user-images.githubusercontent.com/13564/212726521-450e183d-7b36-4538-ad14-617e09c689a8.png)

With regression:
![image](https://user-images.githubusercontent.com/13564/212726469-4f3ff4b5-3f68-48cf-94d2-ddebb9216176.png)

...which CPU profiler blamed on `AsStridedBackward`:

![image](https://user-images.githubusercontent.com/13564/212726953-16333bfc-8460-4445-90ad-7fe73c4173c2.png)

...which were then pinpointed to  https://github.com/pytorch/pytorch/pull/92076/files#diff-df954bbf954d2dcb81f687876053267ffa4ddb36ed86b7d2bd76319ff2b94416R486-R489

Signed-off-by: Edward Z. Yang <ezyang@meta.com>
Pull Request resolved: https://github.com/pytorch/pytorch/pull/92255
Approved by: https://github.com/ngimel, https://github.com/bdhirsh"
pytorch/pytorch,9b716a068274e8c501505ff9ce9c414a445f6f4c,"Clean up more clang-tidy supression (#92203)

1. remove unused NOLINTNEXTLINE(performance-move-const-arg)
2. add more std::move

Pull Request resolved: https://github.com/pytorch/pytorch/pull/92203
Approved by: https://github.com/Skylion007"
pytorch/pytorch,85edb5817925913cdf5a291e61b6066ca329281d,"Fix oneDNN double checkout issue and Upgrade oneDNN to v2.7.3 (#92239)

### Descriotion

This PR is to fix oneDNN double checkout issue that mentioned in https://github.com/pytorch/pytorch/pull/87061#issuecomment-1284384276, and upgrade oneDNN to v2.7.3 to fix #92138.

### Performance test

Use TorchBench test in ICX with 40 cores
Intel OpenMP & jemalloc were preloaded
![image](https://user-images.githubusercontent.com/61222868/212634378-b91c20b5-0e85-474f-861c-c1d2f6962de1.png)

Pull Request resolved: https://github.com/pytorch/pytorch/pull/92239
Approved by: https://github.com/jgong5, https://github.com/malfet"
pytorch/pytorch,da43584bef354412991002c95207fd0963525e82,"[Reland] Clean Up MobileOptimizerType Rewrite Flags Public API and Documentation (#92081)

Summary:
X-link: https://github.com/facebookresearch/d2go/pull/459

Reland of D41690203 (https://github.com/pytorch/pytorch/commit/370df963e062d8eb409d4426dd59b3f0cac8c3d1)

Remove MobileOptimizerType and all rewrite flags from torch.X and torch._C.X to clean up torch.X and torch._C.X namespaces

The affected rewrite flags are
- CONV_BN_FUSION
- FUSE_ADD_RELU
- HOIST_CONV_PACKED_PARAMS
- INSERT_FOLD_PREPACK_OPS
- REMOVE_DROPOUT
- VULKAN_AUTOMATIC_GPU_TRANSFER

Bc-Breaking Change:

Before this change, the rewrite flags were accessible through all of
1. torch.utils.mobile_optimizer.MobileOptimizerType.X
2. torch._C.MobileOptimizerType.X
3. torch.X
4. torch.MobileOptimizerType.X
5. torch._C.X

But after this change, only torch.utils.mobile_optimizer.MobileOptimizerType.X  (option 1 above) and the newly added torch._C._MobileOptimizerType.X remain

Corresponding updates to PyTorch Tutorial Docs are in https://github.com/pytorch/tutorials/pull/2163

Test Plan:
```buck test caffe2/test:test_mobile_optimizer```
```
Summary
  Pass: 6
  Skip: 1
    ↻ caffe2/test:test_mobile_optimizer - test_mobilenet_optimize_for_mobile (test_mobile_optimizer.TestOptimizer)
  ListingSuccess: 1
Finished test run: https://www.internalfb.com/intern/testinfra/testrun/4222124793514412
```
___
```buck test caffe2/torch/fb/mobile/tests:model_exporter_tests```
Tests pass
___

With temporary testing changes in D41690204:

```buck run caffe2:test_rewrite_flags_api```
Before:
```
torch.utils.mobile_optimizer.MobileOptimizerType.VULKAN_AUTOMATIC_GPU_TRANSFER
        Expected: ✅ | Result: ✅
torch._C._MobileOptimizerType.VULKAN_AUTOMATIC_GPU_TRANSFER
        Expected: ✅ | Result: ❌ (module 'torch._C' has no attribute '_MobileOptimizerType')
torch._C.MobileOptimizerType.VULKAN_AUTOMATIC_GPU_TRANSFER
        Expected: ❌ | Result: ✅
torch.VULKAN_AUTOMATIC_GPU_TRANSFER
        Expected: ❌ | Result: ✅
torch.MobileOptimizerType.VULKAN_AUTOMATIC_GPU_TRANSFER
        Expected: ❌ | Result: ✅
torch._C.VULKAN_AUTOMATIC_GPU_TRANSFER
        Expected: ❌ | Result: ✅
```
After:
```
torch.utils.mobile_optimizer.MobileOptimizerType.VULKAN_AUTOMATIC_GPU_TRANSFER
        Expected: ✅ | Result: ✅
torch._C._MobileOptimizerType.VULKAN_AUTOMATIC_GPU_TRANSFER
        Expected: ✅ | Result: ✅
torch._C.MobileOptimizerType.VULKAN_AUTOMATIC_GPU_TRANSFER
        Expected: ❌ | Result: ❌ (module 'torch._C' has no attribute 'MobileOptimizerType')
torch.VULKAN_AUTOMATIC_GPU_TRANSFER
        Expected: ❌ | Result: ❌ (module 'torch' has no attribute 'VULKAN_AUTOMATIC_GPU_TRANSFER')
torch.MobileOptimizerType.VULKAN_AUTOMATIC_GPU_TRANSFER
        Expected: ❌ | Result: ❌ (module 'torch' has no attribute 'MobileOptimizerType')
torch._C.VULKAN_AUTOMATIC_GPU_TRANSFER
        Expected: ❌ | Result: ❌ (module 'torch._C' has no attribute 'VULKAN_AUTOMATIC_GPU_TRANSFER')
```

```buck test caffe2/test:public_bindings -- test_no_new_bindings```
```
Summary
  Pass: 1
  ListingSuccess: 1
Finished test run: https://www.internalfb.com/intern/testinfra/testrun/7881299473114294
```

Reviewed By: SS-JIA

Differential Revision: D42442395

Pull Request resolved: https://github.com/pytorch/pytorch/pull/92081
Approved by: https://github.com/albanD"
pytorch/pytorch,4af5939d7ac70cba7f130ab74705080cbda68d7b,"[optim] Improve adadelta foreach, group tensors to maximize fast path (#92048)

Old behavior would have adadelta foreach sending tensors to the slow path if they were not all the same dtype nor on the same device.

This PR adds grouping for adadelta optimizer so that it would run foreach in batches, allowing more users to benefit from foreach perf.

Of course, we should ensure that the new implementation works, so there are new tests to ensure this behavior is not broken.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/92048
Approved by: https://github.com/albanD"
pytorch/pytorch,3779a75fc9ff92f3f3bc6409264764a5cd801201,"Apply noexcept to relevant move methods to improve performance (#92156)

This clang-tidy check is disabled globally due to false positives on containers, but there are a few places here where adding clang-tidy would actually improve performance (by allowing STL containers to use the move operator / assignment)

Pull Request resolved: https://github.com/pytorch/pytorch/pull/92156
Approved by: https://github.com/ngimel"
pytorch/pytorch,f4b804eeaa6352596dda4169ec1c96214746f0c7,"Call profiler step via optimizer post hook (#90101)

This PR adds the `_profile_using_dynolog` function to `torch/__init__.py`. The `_profile_using_dynolog` method allows registering the optimizer step post hook. This is required to collect iteration based traces using dynolog.

Other related changes for tests to pass:
1. Updated `optimizer.pyi`
1. Updated `overrides.py`
1. The test `test_kineto_profiler_multiple_steppers` in `test_profiler.py` has been broken down into two cases:
     - `test_kineto_profiler_multiple_steppers_with_override_True` : this test uses the override argument
     - `test_kineto_profiler_multiple_steppers_with_override_False` : this test uses the environment variable
Pull Request resolved: https://github.com/pytorch/pytorch/pull/90101
Approved by: https://github.com/albanD"
pytorch/pytorch,61cdae0ce58bcbe048b143356fd9ded821225657,"Switch Windows CI jobs to G5 runners (#91727)

### Changelist

* Change Windows TORCH_CUDA_ARCH_LIST from `7.0` to `8.6` to compatible with NVIDIA A10G TPU
* Correctly disable some tests that requires flash attention, which is not available on Windows at the moment. This has been fixed by https://github.com/pytorch/pytorch/pull/91979
* G5 runner has `AMD EPYC 7R32` CPU, not an Intel one
  * This seems to change the behavior of `GetDefaultMobileCPUAllocator` in `cpu_profiling_allocator_test`.  This might need to be investigated further (TODO: TRACKING ISSUE).  In the meantime, the test has been updated accordingly to use `GetDefaultCPUAllocator` correctly instead of `GetDefaultMobileCPUAllocator` for mobile build
  * Also one periodic test `test_cpu_gpu_parity_nn_Conv3d_cuda_float32` fails with Tensor not close error when comparing grad tensors between CPU and GPU. This is fixed by turning off TF32 for the test.

###  Performance gain

* (CURRENT) p3.2xlarge - https://hud.pytorch.org/tts shows each Windows CUDA shards (1-5 + functorch) takes about 2 hours to finish (duration)
* (NEW RUNNER) g5.4xlarge - The very rough estimation of the duration is 1h30m for each shard, meaning a half an hour gain (**25%**)

### Pricing

On demand hourly rate:

* (CURRENT) p3.2xlarge: $3.428. Total = Total hours spent on Windows CUDA tests * 3.428
* (NEW RUNNER) g5.4xlarge: $2.36. Total = Total hours spent on Windows CUDA tests * Duration gain (0.75) * 2.36

So the current runner is not only more expensive but is also slower.  Switching to G5 runners for Windows should cut down the cost by (3.428 - 0.75 * 2.36) / 3.428 = **~45%**

### Rolling out

https://github.com/pytorch/test-infra/pull/1376 needs to be reviewed and approved to ensure the capacity of the runner before PR can be merged.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/91727
Approved by: https://github.com/ZainRizvi, https://github.com/malfet, https://github.com/seemethere"
pytorch/pytorch,3ab58fd5edba2a304ddd23d2fd1d284d51bfc78b,"optimize sampled_addmm performance on CPU (SparseCSR) (#90978)

### Target and Background
This PR is improving the performance of `sampled_addmm` on CPU device. This is part of effort for improving PyG performance on CPU for GNN training/inference.

The current implementation is a reference design which converts `SparseCSR` tensor back to dense tensor and then do the addmm and convert back to `SparseCSR` again: this is going to be very slow and won't be able to run most of the datasets under https://github.com/snap-stanford/ogb (convert to dense would trigger `OOM`).

### Benchmarks

Right now we don't have any hands-on benchmark or workload to test this since this operator is not used in PyG yet. I fetched the dataset from `ogb-products` where:

* number of nodes: 2.4 * 10^6
* number of edges: 1.26 * 10^8
* number of features: 128

So if we store the **adjacency matrix** is dense, it is going to be 2.4 * 2.4 * 4 * 10^12 bytes, this will be OOB on current code. I abstract the first 1k rows to compare, **1100x** speedup:

CPU: Intel(R) Xeon(R) Gold 6248 CPU @ 2.50GHz, dual socket, 20 cores per socket.
```
### before: run 1000 rows from the whole dataset
sampled_addmm: running dataset ogb-products first 1000 rows: each iter takes 1212.000 ms!

### after: run 1000 rows from the whole dataset
sampled_addmm: running dataset ogb-products first 1000 rows: each iter takes 1.102 ms!

### after: run the whole dataset
sampled_addmm: running dataset ogb-products (the whole dataset) 2449029 rows: each iter takes 873.306 ms!
```

Pull Request resolved: https://github.com/pytorch/pytorch/pull/90978
Approved by: https://github.com/pearu, https://github.com/cpuhrsch"
pytorch/pytorch,a76bc410df7d20eac8b2a85ea7902c3257173b9a,"Fix `_foreach_norm` on some tensor sizes (#91844)

This PR fixes 2 bugs with CUDA `_foreach_norm`:

1. Wrong norm when tensors are larger than kChunkSize = 65536
```
>>> torch._foreach_norm([torch.ones(60000, device=""cuda"") for _ in range(1)])
(tensor(244.9490, device='cuda:0', grad_fn=<NotImplemented>),)
>>> torch._foreach_norm([torch.ones(70000, device=""cuda"") for _ in range(1)])
(tensor(256., device='cuda:0', grad_fn=<NotImplemented>),)

>>> torch.ones(60000, device=""cuda"").norm()
tensor(244.9490, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>)
>>> torch.ones(70000, device=""cuda"").norm()
tensor(264.5751, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>)
```

2. Error when a tensor numel is smaller than the number of tensors

```
>> torch._foreach_norm([torch.ones(9, device=""cuda"") for _ in range(10)])
Traceback (most recent call last):
 File ""<stdin>"", line 1, in <module>
IndexError: select(): index 9 out of range for tensor of size [9] at dimension 0
```

This bug could have been caught by tests if `PYTORCH_TEST_WITH_SLOW` was 1, because it would have tested tensors of size 300*300=90000. It's not enabled by default, does someone know if it's ever enabled?

Pull Request resolved: https://github.com/pytorch/pytorch/pull/91844
Approved by: https://github.com/ngimel"
pytorch/pytorch,18677d52493c92ca0124da7de5dde6ef0d333db0,"sparse_mask: faster, with support for uncoalesced mask (#91964)

This PR updates `sparse_mask` to be:
* about 30% faster on CUDA.
* able to support uncoalesced masks.
* much shorted code-wise.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/91964
Approved by: https://github.com/cpuhrsch, https://github.com/pearu"
pytorch/pytorch,dc6916b341904cd156335b13d06a576462c452a6,"optimize gather performance for gnn usage on CPU (#87586)

On classic pyg user case for message passing, `gather` has `index` tensor in a broadcasted shape, e.g. with shape `5000, 128` and stride `[1, 0]`. That indicated gather is done on each row of the self tensor. The current implementation will try to parallel on the inner dimension which is bad performance for CPU and unable to be vectorized.

This PR addressed this use case and optimize in a similar manner to index_select, parallel on outer dimension of `index` and do vectorized copy on inner dimension.

Performance benchmarking on Xeon Icelake single socket on `GCN`: the `gather` reduced from `150.787ms` to `10.926ms`, after this optimization, `gather` will no longer be the major bottleneck for training of GNN models when `EdgeIndex` is in COO format.

for more details, please refer to https://github.com/pyg-team/pytorch_geometric/issues/4891#issuecomment-1288423705

Pull Request resolved: https://github.com/pytorch/pytorch/pull/87586
Approved by: https://github.com/rusty1s, https://github.com/malfet"
pytorch/pytorch,3613ff06b1e870068df8b348f7af6a8c592e0edf,"[MKLDNN] Rename pooling_avg to pooling_avg_exclude_padding (#90247)

**Summary**
Rename `pooling_avg` to `pooling_avg_exclude_padding` to align with onednn v3.0. It does not affect correctness or performance. Same as https://github.com/pytorch/pytorch/pull/87851 . Looks like https://github.com/pytorch/pytorch/pull/87851 did not cover all occurrences.

**Test plan**
python test/test_mkldnn.py
python caffe2/python/ideep/pool_op_test.py

Pull Request resolved: https://github.com/pytorch/pytorch/pull/90247
Approved by: https://github.com/jgong5, https://github.com/malfet"
pytorch/pytorch,4e21fc2075e09dd735746696f95dce093b634c16,"In inductor triton generated code, avoid masking when numel=1 (#91254)

This is implementing an idea from @lezcano : if we have a generated triton kernel with `xnumel=1`, then `xmask` is just `0<1` and can be dropped from all `load`/`store`/`where`.

The `xnumel=1` case actually comes up relatively often when code for reductions is being generated. @lezcano reported some  performance gains in micro-benchmarks (see comment below) and it is a very simple change.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/91254
Approved by: https://github.com/jansel, https://github.com/ngimel"
pytorch/pytorch,c5836153f5332ca83d5cacde38f2829a4d54793e,"Revert ""optimize sampled_addmm performance on CPU (SparseCSR) (#90978)""

This reverts commit 645fb217c06348a4f1ccdf68a93bd711f7158c62.

Reverted https://github.com/pytorch/pytorch/pull/90978 on behalf of https://github.com/seemethere due to This broke internal builds for android due to the new file added being missing in build_variables.bzl"
pytorch/pytorch,6287bb78dc53025c9399e979b2a71394033bf9d9,"[static-runtime] clamp fast_sigmoid result into (0,1) range (#91993)

fast_sigmoid uses fast_tanh under the hood which is not precise;
the op outputs are treated as probability-like numbers;
in a reeeally small percentage of cases the outputs fell out of acceptable range for probabilities

Test Plan: ci

Differential Revision: D42445821

Pull Request resolved: https://github.com/pytorch/pytorch/pull/91993
Approved by: https://github.com/davidberard98"
pytorch/pytorch,3790b5050520dd8d619f73bc1df9521c870a1f53,"inductor: fix .to(memort_format) issue which doesn't generate right stride (#91948)

Motivation: for **.to(memory_format),** the inductor doesn't generate the right stride, see the following example:
```
class Model(torch.nn.Module):
    def __init__(self):
        super(Model, self).__init__()

    def forward(self, x):
        x = x.to(memory_format=torch.contiguous_format)
        return x
```

the generated code doesn't do the memory format change and gets a wrong stride **(802816, 1, 14336, 256)**, it is not a contiguous stride.

```
from ctypes import c_void_p, c_long
import torch
import random
from torch import empty_strided, as_strided, device
from torch._inductor.codecache import AsyncCompile

aten = torch.ops.aten
assert_size_stride = torch._C._dynamo.guards.assert_size_stride
async_compile = AsyncCompile()

async_compile.wait(globals())
del async_compile

def call(args):
    arg0_1, = args
    args.clear()
    return (arg0_1, )

if __name__ == ""__main__"":
    from torch._dynamo.testing import rand_strided
    from torch._inductor.utils import print_performance
    arg0_1 = rand_strided((128, 256, 56, 56), (802816, 1, 14336, 256), device='cpu', dtype=torch.float32)
    print_performance(lambda: call([arg0_1]))
```

After this PR, the will have a memory format change:

```
from ctypes import c_void_p, c_long
import torch
import random
from torch import empty_strided, as_strided, device
from torch._inductor.codecache import AsyncCompile

aten = torch.ops.aten
assert_size_stride = torch._C._dynamo.guards.assert_size_stride
async_compile = AsyncCompile()

kernel_cpp_0 = async_compile.cpp('''
#include ""/tmp/torchinductor_xiaobing/77/c7773nj5pwikpmm2pwa62rcudlf7p3if7eyqb5k4sjsvewwje4le.h""
extern ""C"" void kernel(const float* __restrict__ in_ptr0,
                       float* __restrict__ out_ptr0)
{
    #pragma omp parallel num_threads(40)
    {
        {
            #pragma omp for
            for(long i0=0; i0<128; i0+=1)
            {
                #pragma GCC ivdep
                for(long i1=0; i1<256; i1+=1)
                {
                    #pragma GCC ivdep
                    for(long i2=0; i2<3136; i2+=1)
                    {
                        auto tmp0 = in_ptr0[i1 + (256*i2) + (802816*i0)];
                        out_ptr0[i2 + (3136*i1) + (802816*i0)] = tmp0;
                    }
                }
            }
        }
    }
}
''')

async_compile.wait(globals())
del async_compile

def call(args):
    arg0_1, = args
    args.clear()
    buf1 = empty_strided((128, 256, 56, 56), (802816, 3136, 56, 1), device='cpu', dtype=torch.float32)
    kernel_cpp_0(c_void_p(arg0_1.data_ptr()), c_void_p(buf1.data_ptr()))
    del arg0_1
    return (buf1, )

if __name__ == ""__main__"":
    from torch._dynamo.testing import rand_strided
    from torch._inductor.utils import print_performance
    arg0_1 = rand_strided((128, 256, 56, 56), (802816, 1, 14336, 256), device='cpu', dtype=torch.float32)
    print_performance(lambda: call([arg0_1]))
```

Pull Request resolved: https://github.com/pytorch/pytorch/pull/91948
Approved by: https://github.com/ngimel"
pytorch/pytorch,92855a215b6e4a4ce0eefa590fb49783276b87fb,"[SDPA] Guard mem efficient attention in deterministic mode (#91979)

# Summary
Memory efficient attention is a non deterministic algorithm.

This PR ensures that the sdp_choice will allow for mem-efficient  to be used as the backend to SDPA if we are in warn only mode.  Otherwise  if we have enabled determinism and and set warn_only to False sdp_choice will not return memory efficient attention as the backend.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/91979
Approved by: https://github.com/cpuhrsch"
pytorch/pytorch,8f5f15a64b7f2efabfc9137ddc1ffb3390375626,"optimize scatter_add performance for gnn usage on CPU (#82703)

### Motivation of this PR

This PR is targeting at improving performance of `scatter_add` for GNN usage scenarios on PyG. Currently only CPU optimizations is covered.

`Message Passing` is the major step in GNN learning which means exchanging/aggregating info between nodes. And from the perf point of view, if the `EdgeIndex` is stored as [2, num_edges], `scatter_reduce` would be a major perf hotspot on current pytorch implementations.

To be more specific, in the process of message passing, `scatter_add` is used in a very similar way as `index_select`, except that the `self` tensor is written into while `index_select` is only reading. Therefore, the `index` tensor passed to `scatter_add` is an expanded tensor on dim0, which means all the rest of dims would end up with the same value.

### Algorithm

Current impl on scatter would do parallel on the inner dims for such case which would cause bad perf: non-contiguous memory access pattern and non-vectorized.

This PR did sorting on the `index` to solve the write conflicts if we directly parallel on dim0. The algorithm is equivalent to:
* convert memory format from `COO` to `CSR`
* do spmm reduce

### Perf improvement

The benchmark comes from https://github.com/pyg-team/pytorch_geometric/tree/master/examples, `python reddit.py` which runs model SAGE on dataset reddit.

CPU type: Intel(R) Xeon(R) Gold 6248 CPU @ 2.50GHz

` aten::scatter_add_` has been reduced from **37.797s** to **5.989s**:

* breakdown before
```
-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------
                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg    # of Calls
-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------
                                     aten::scatter_add_        49.00%       37.797s        49.00%       37.797s      41.445ms           912
                                     aten::index_select        19.74%       15.223s        19.74%       15.227s       6.678ms          2280
                                           aten::linear         0.01%       5.706ms        15.04%       11.602s      12.721ms           912
                                            aten::addmm         6.62%        5.108s         7.92%        6.112s      13.403ms           456
                                           aten::matmul         0.00%       2.339ms         7.10%        5.475s      12.006ms           456
```

* breakdown after
```
-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------
                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg    # of Calls
-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------
                                     aten::index_select        32.41%       14.677s        32.42%       14.681s       6.439ms          2280
                                           aten::linear         0.01%       6.665ms        26.43%       11.968s      13.123ms           912
                                            aten::addmm        11.76%        5.328s        13.76%        6.232s      13.667ms           456
                                     aten::scatter_add_        13.22%        5.989s        13.22%        5.989s       6.566ms           912
                                           aten::matmul         0.01%       2.303ms        12.63%        5.720s      12.543ms           456
```

Pull Request resolved: https://github.com/pytorch/pytorch/pull/82703
Approved by: https://github.com/jgong5, https://github.com/ezyang"
pytorch/pytorch,4dcb10e027a6a82c4693e06d8afabab9e3caef99,"Add missing clang-tidy fixes for modernize-use-equals-(default|delete) (#91857)

More clang-tidy for default or deleting more ctors and dtors. This is slightly more efficient and more readable

Pull Request resolved: https://github.com/pytorch/pytorch/pull/91857
Approved by: https://github.com/ezyang"
pytorch/pytorch,645fb217c06348a4f1ccdf68a93bd711f7158c62,"optimize sampled_addmm performance on CPU (SparseCSR) (#90978)

### Target and Background
This PR is improving the performance of `sampled_addmm` on CPU device. This is part of effort for improving PyG performance on CPU for GNN training/inference.

The current implementation is a reference design which converts `SparseCSR` tensor back to dense tensor and then do the addmm and convert back to `SparseCSR` again: this is going to be very slow and won't be able to run most of the datasets under https://github.com/snap-stanford/ogb (convert to dense would trigger `OOM`).

### Benchmarks

Right now we don't have any hands-on benchmark or workload to test this since this operator is not used in PyG yet. I fetched the dataset from `ogb-products` where:

* number of nodes: 2.4 * 10^6
* number of edges: 1.26 * 10^8
* number of features: 128

So if we store the **adjacency matrix** is dense, it is going to be 2.4 * 2.4 * 4 * 10^12 bytes, this will be OOB on current code. I abstract the first 1k rows to compare, **1100x** speedup:

CPU: Intel(R) Xeon(R) Gold 6248 CPU @ 2.50GHz, dual socket, 20 cores per socket.
```
### before: run 1000 rows from the whole dataset
sampled_addmm: running dataset ogb-products first 1000 rows: each iter takes 1212.000 ms!

### after: run 1000 rows from the whole dataset
sampled_addmm: running dataset ogb-products first 1000 rows: each iter takes 1.102 ms!

### after: run the whole dataset
sampled_addmm: running dataset ogb-products (the whole dataset) 2449029 rows: each iter takes 873.306 ms!
```

Pull Request resolved: https://github.com/pytorch/pytorch/pull/90978
Approved by: https://github.com/pearu, https://github.com/cpuhrsch"
pytorch/pytorch,3aeb7127b4430dad7e48a54b5fe878f3752ddbe3,"Revert ""Clean Up MobileOptimizerType Rewrite Flags Public API and Documentation (#91600)""

This reverts commit 370df963e062d8eb409d4426dd59b3f0cac8c3d1.

Reverted https://github.com/pytorch/pytorch/pull/91600 on behalf of https://github.com/facebook-github-bot due to Diff reverted internally"
pytorch/pytorch,370df963e062d8eb409d4426dd59b3f0cac8c3d1,"Clean Up MobileOptimizerType Rewrite Flags Public API and Documentation (#91600)

Summary:
X-link: https://github.com/facebookresearch/d2go/pull/452

Remove MobileOptimizerType and all rewrite flags from torch.X and torch._C.X to clean up torch.X and torch._C.X namespaces

The affected rewrite flags are
- CONV_BN_FUSION
- FUSE_ADD_RELU
- HOIST_CONV_PACKED_PARAMS
- INSERT_FOLD_PREPACK_OPS
- REMOVE_DROPOUT
- VULKAN_AUTOMATIC_GPU_TRANSFER

Bc-Breaking Change:

Before this change, the rewrite flags were accessible through all of
1. torch.utils.mobile_optimizer.MobileOptimizerType.X
2. torch._C.MobileOptimizerType.X
3. torch.X
4. torch.MobileOptimizerType.X
5. torch._C.X

But after this change, only torch.utils.mobile_optimizer.MobileOptimizerType.X  (option 1 above) and the newly added torch._C._MobileOptimizerType.X remain

Corresponding updates to PyTorch Tutorial Docs are in https://github.com/pytorch/tutorials/pull/2163

Test Plan:
```buck test caffe2/test:test_mobile_optimizer```
```
Summary
  Pass: 6
  Skip: 1
    ↻ caffe2/test:test_mobile_optimizer - test_mobilenet_optimize_for_mobile (test_mobile_optimizer.TestOptimizer)
  ListingSuccess: 1
Finished test run: https://www.internalfb.com/intern/testinfra/testrun/4222124793514412
```
___

With temporary testing changes in D41690204:

```buck run caffe2:test_rewrite_flags_api```
Before:
```
torch.utils.mobile_optimizer.MobileOptimizerType.VULKAN_AUTOMATIC_GPU_TRANSFER
        Expected: ✅ | Result: ✅
torch._C._MobileOptimizerType.VULKAN_AUTOMATIC_GPU_TRANSFER
        Expected: ✅ | Result: ❌ (module 'torch._C' has no attribute '_MobileOptimizerType')
torch._C.MobileOptimizerType.VULKAN_AUTOMATIC_GPU_TRANSFER
        Expected: ❌ | Result: ✅
torch.VULKAN_AUTOMATIC_GPU_TRANSFER
        Expected: ❌ | Result: ✅
torch.MobileOptimizerType.VULKAN_AUTOMATIC_GPU_TRANSFER
        Expected: ❌ | Result: ✅
torch._C.VULKAN_AUTOMATIC_GPU_TRANSFER
        Expected: ❌ | Result: ✅
```
After:
```
torch.utils.mobile_optimizer.MobileOptimizerType.VULKAN_AUTOMATIC_GPU_TRANSFER
        Expected: ✅ | Result: ✅
torch._C._MobileOptimizerType.VULKAN_AUTOMATIC_GPU_TRANSFER
        Expected: ✅ | Result: ✅
torch._C.MobileOptimizerType.VULKAN_AUTOMATIC_GPU_TRANSFER
        Expected: ❌ | Result: ❌ (module 'torch._C' has no attribute 'MobileOptimizerType')
torch.VULKAN_AUTOMATIC_GPU_TRANSFER
        Expected: ❌ | Result: ❌ (module 'torch' has no attribute 'VULKAN_AUTOMATIC_GPU_TRANSFER')
torch.MobileOptimizerType.VULKAN_AUTOMATIC_GPU_TRANSFER
        Expected: ❌ | Result: ❌ (module 'torch' has no attribute 'MobileOptimizerType')
torch._C.VULKAN_AUTOMATIC_GPU_TRANSFER
        Expected: ❌ | Result: ❌ (module 'torch._C' has no attribute 'VULKAN_AUTOMATIC_GPU_TRANSFER')
```

```buck test caffe2/test:public_bindings -- test_no_new_bindings```
```
Summary
  Pass: 1
  ListingSuccess: 1
Finished test run: https://www.internalfb.com/intern/testinfra/testrun/7881299473114294
```

Differential Revision: D41690203

Pull Request resolved: https://github.com/pytorch/pytorch/pull/91600
Approved by: https://github.com/albanD, https://github.com/malfet"
pytorch/pytorch,7f2b5ea1e1495a1706ccf88269a0e920354240e3,"Revert ""Avoid device casting for all singleton tensors in optimizer states (#91454)""

This reverts commit 1e725c97470d8cf74e85984ca997e77c76e91a18.

Reverted https://github.com/pytorch/pytorch/pull/91454 on behalf of https://github.com/janeyx99 due to Likely caused regression where checkpoint resume fails during training"
pytorch/pytorch,138a0188e0f17ef07f3238312247776cbd30b0ed,"Add support for logaddexp(float16) in CUDA and implement its reference (#91869)

The reference is implemented so that it generates efficient and
numerically stable triton code.

Fixes https://github.com/pytorch/pytorch/issues/91683

Pull Request resolved: https://github.com/pytorch/pytorch/pull/91869
Approved by: https://github.com/ngimel"
pytorch/pytorch,32356aaee6a77e0ae424435a7e9da3d99e7a4ca5,"[4/N] Add test for partial training for NamedOptimizer (#91344)

Pull Request resolved: https://github.com/pytorch/pytorch/pull/91344
Approved by: https://github.com/rohan-varma"
pytorch/pytorch,5fabd96f3c60c340c1695dc4cf6d96f3273b6894,"[PT-D][3/N] Add FSDP hook with Named Optimizer (#91321)

Pull Request resolved: https://github.com/pytorch/pytorch/pull/91321
Approved by: https://github.com/fegin"
pytorch/pytorch,de99bc39e80b21e82e8d5e0cb9ffdd6b8706f6a8,"[MPS] Remap the view ops to exisiting graph APIs. (#89436)

This helps in performance by avoiding the generic gather/scatter graph.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/89436
Approved by: https://github.com/razarmehr"
pytorch/pytorch,c99a2a43add90dd92b24811f6cb2b8baf0c5887b,"[inductor] decompose tanh in CPP backend (#91687)

## Description
The decomposition of `tanh` has been removed in https://github.com/pytorch/pytorch/pull/90889.
```python
@register_decomposition([aten.tanh])
def tanh(x):
    return 2.0 / (1.0 + torch.exp(-2.0 * x)) - 1.0
```
We've observed performance regression on CPU for `lennard_jones` in the TorchBench suite.
This PR decomposes `tanh` in CPP backend to fix the regression.

### Performance

- Model: lennard_jones
- Machine: IceLake (32 cores per socket)
- Configuration: single instance, 32 cores per instance
- jemalloc and iomp enabled

```bash
python benchmarks/dynamo/torchbench.py  --inductor-settings --inductor --performance --float32 -dcpu -n500  --no-skip --dashboard --only=lennard_jones --quiet
```

<html xmlns:v=""urn:schemas-microsoft-com:vml""
xmlns:o=""urn:schemas-microsoft-com:office:office""
xmlns:x=""urn:schemas-microsoft-com:office:excel""
xmlns=""http://www.w3.org/TR/REC-html40"">

<head>

<meta name=ProgId content=Excel.Sheet>
<meta name=Generator content=""Microsoft Excel 15"">
<link id=Main-File rel=Main-File
href=""file:///C:/Users/chunyuan/AppData/Local/Temp/msohtmlclip1/01/clip.htm"">
<link rel=File-List
href=""file:///C:/Users/chunyuan/AppData/Local/Temp/msohtmlclip1/01/clip_filelist.xml"">

</head>

<body link=""#0563C1"" vlink=""#954F72"">

Time before   regression | Time after regression | Time with this PR
-- | -- | --
0.000262036 | 0.0003618 | 0.000267888

</body>

</html>

Pull Request resolved: https://github.com/pytorch/pytorch/pull/91687
Approved by: https://github.com/jgong5, https://github.com/desertfire"
pytorch/pytorch,25ff10caa7fc061b8a87482f714e2674be3f3072,"inductor:enable conv+unary fusion for torch unary function (#91609)

This PR is about to enable unary fusion which the unary is the torch function, this PR will improve timm models performance a lot.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/91609
Approved by: https://github.com/jgong5, https://github.com/desertfire"
pytorch/pytorch,2175c9414edaf64cb463f793e4aadf8eb3214616,"[cpu] implement erf based on oneDNN algorithm for aten::Vec (#91613)

Aten's `erf` implementation will invoke `MKL` function which shows better performance than current Torchinductor's `erf` implementation who calls `sleef` function in `aten::Vec`. The performance benefits from the algorithm. `sleef` uses the Taylor expansion more precise than `MKL`, resulting in longer time-consuming. As the implementations of `erf` in `oneDNN` and `MKL` are similar, we implement the algorithm of `erf` in `aten::Vec` based on `oneDNN` algorithm.

Performance data for eager v.s. inductor:
`gelu` also benefits from this modification for it uses `erf`.

<html xmlns:v=""urn:schemas-microsoft-com:vml""
xmlns:o=""urn:schemas-microsoft-com:office:office""
xmlns:x=""urn:schemas-microsoft-com:office:excel""
xmlns=""http://www.w3.org/TR/REC-html40"">

<head>

<meta name=ProgId content=Excel.Sheet>
<meta name=Generator content=""Microsoft Excel 15"">
<link id=Main-File rel=Main-File
href=""file:///C:/Users/xuanliao/AppData/Local/Temp/msohtmlclip1/01/clip.htm"">
<link rel=File-List
href=""file:///C:/Users/xuanliao/AppData/Local/Temp/msohtmlclip1/01/clip_filelist.xml"">
</head>

<body link=blue vlink=purple>

suite | op_name | improved_ratio_speedup0.2 | improved_ratio_speedup0.5 | improved_ratio_speedup0.8 | speedup_old_0.2 | RSD(3) | speedup_old_0.5 | RSD(3) | speedup_old_0.8 | RSD(3) | speedup_new_0.2 | RSD(3) | speedup_new_0.5 | RSD(3) | speedup_new_0.8 | RSD(3)
-- | -- | -- | -- | -- | -- | -- | -- | -- | -- | -- | -- | -- | -- | -- | -- | --
torchbench | aten.erf.default | 138.54% | 138.54% | 138.54% | 0.402057897 | 13.54% | 0.402057897 | 13.54% | 0.402057897 | 13.54% | 0.959050302 | 4.21% | 0.959050302 | 4.21% | 0.959050302 | 4.21%
torchbench | aten.gelu.default | 196.94% | 16.28% | 3.28% | 0.303611506 | 0.88% | 0.865411422 | 0.23% | 0.984732108 | 0.15% | 0.901534389 | 1.04% | 1.006314977 | 0.10% | 1.017019831 | 0.37%
huggingface | aten.gelu.default | 178.90% | 153.93% | 22.70% | 0.324031619 | 8.16% | 0.40085369 | 1.67% | 0.839170801 | 1.30% | 0.90371451 | 2.25% | 1.017872459 | 0.47% | 1.029638829 | 0.49%
timm | aten.gelu.default | 12.76% | 3.01% | 1.98% | 0.892005539 | 0.22% | 0.979783341 | 0.16% | 0.998917466 | 0.08% | 1.005821648 | 0.11% | 1.009227094 | 0.07% | 1.018701655 | 0.30%
torchbench | aten.gelu_backward.default | 124.25% | 53.19% | 5.96% | 0.437150835 | 6.11% | 0.664341696 | 0.24% | 0.983091818 | 2.49% | 0.980304388 | 1.86% | 1.017688734 | 0.33% | 1.041684409 | 0.74%
huggingface | aten.gelu_backward.default | 126.26% | 32.55% | 11.61% | 0.446699743 | 0.34% | 0.781550075 | 0.73% | 0.989682073 | 0.28% | 1.010687581 | 1.31% | 1.035929929 | 1.11% | 1.104549968 | 2.68%
timm | aten.gelu_backward.default | 5.65% | 1.79% | 2.58% | 0.955116562 | 0.40% | 0.99782989 | 0.18% | 1.002408412 | 0.13% | 1.00905163 | 0.07% | 1.015649447 | 0.26% | 1.028238613 | 0.24%

</body>

</html>

Pull Request resolved: https://github.com/pytorch/pytorch/pull/91613
Approved by: https://github.com/jgong5, https://github.com/mingfeima, https://github.com/EikanWang, https://github.com/desertfire"
pytorch/pytorch,745dc3a13c75dfb23c984604b139f5706b0f0a70,"[inductor] optimize lowering for empty-related operators (#91350)

For micro-benchmark, `new_empty_strided` and `new_empty` have poor performance with inductor compared to eager. The main reason is that inductor initializes new tensor with 0 during lowering, which generates a useless cpp kernel. Actually, it is not needed for operator semantics, but costs additional time. The same problem is also found in lowerings of `empty_strided` and `empty`. This PR tends to remove useless cpp kernel of tensor initialization by generating a NopKernelSchedulerNode instead of a SchedulerNode. The lowering functions of following operators are optimized:

- `torch.empty`
- `aten.empty`
- `aten.new_empty`
- `aten.empty_strided`
- `aten.new_empty_strided`

We take output code of `new_empty_strided` as example.

_Before change_
```
kernel_cpp_0 = async_compile.cpp('''
#include ""/tmp/torchinductor_root/77/c7773nj5pwikpmm2pwa62rcudlf7p3if7eyqb5k4sjsvewwje4le.h""
extern ""C"" void kernel(float* __restrict__ out_ptr0)
{
    #pragma omp parallel num_threads(28)
    {
        #pragma omp for
        for(long i0=0; i0<57600; i0+=1)
        {
            auto tmp0 = at::vec::Vectorized<float>(static_cast<float>(0));
            tmp0.store(out_ptr0 + 16*i0);
        }
        #pragma omp for simd simdlen(8)
        for(long i0=921600; i0<921600; i0+=1)
        {
            auto tmp0 = static_cast<float>(0);
            out_ptr0[i0] = tmp0;
        }
    }
}
''')

async_compile.wait(globals())
del async_compile

def call(args):
    arg0_1, = args
    args.clear()
    buf0 = empty_strided((60, 60, 256), (15360, 256, 1), device='cpu', dtype=torch.float32)
    kernel_cpp_0(c_void_p(buf0.data_ptr()))
    return (buf0, )

if __name__ == ""__main__"":
    from torch._dynamo.testing import rand_strided
    from torch._inductor.utils import print_performance
    arg0_1 = rand_strided((60, 60, 256), (60, 1, 3600), device='cpu', dtype=torch.float32)
    print_performance(lambda: call([arg0_1]))
```
_After change_
```
async_compile.wait(globals())
del async_compile

def call(args):
    arg0_1, = args
    args.clear()
    buf0 = empty_strided((60, 60, 256), (15360, 256, 1), device='cpu', dtype=torch.float32)
    return (buf0, )

if __name__ == ""__main__"":
    from torch._dynamo.testing import rand_strided
    from torch._inductor.utils import print_performance
    arg0_1 = rand_strided((60, 60, 256), (60, 1, 3600), device='cpu', dtype=torch.float32)
    print_performance(lambda: call([arg0_1]))
```

Performance data for eager v.s. inductor:
<html xmlns:v=""urn:schemas-microsoft-com:vml""
xmlns:o=""urn:schemas-microsoft-com:office:office""
xmlns:x=""urn:schemas-microsoft-com:office:excel""
xmlns=""http://www.w3.org/TR/REC-html40"">

<head>

<meta name=ProgId content=Excel.Sheet>
<meta name=Generator content=""Microsoft Excel 15"">
<link id=Main-File rel=Main-File
href=""file:///C:/Users/xuanliao/AppData/Local/Temp/msohtmlclip1/01/clip.htm"">
<link rel=File-List
href=""file:///C:/Users/xuanliao/AppData/Local/Temp/msohtmlclip1/01/clip_filelist.xml"">

</head>

<body link=""#0563C1"" vlink=""#954F72"">

suite | op_name | improved_ratio_speedup0.2 | improved_ratio_speedup0.5 | improved_ratio_speedup0.8 | speedup_old_0.2 | RSD(3) | speedup_old_0.5 | RSD(3) | speedup_old_0.8 | RSD(3) | speedup_new_0.2 | RSD(3) | speedup_new_0.5 | RSD(3) | speedup_new_0.8 | RSD(3)
-- | -- | -- | -- | -- | -- | -- | -- | -- | -- | -- | -- | -- | -- | -- | -- | --
torchbench | aten.new_empty_strided.default | 235.94% | 100.94% | 50.23% | 0.325947 | 2.96% | 0.550267 | 2.03% | 0.747997 | 2.93% | 1.094985 | 0.81% | 1.105722 | 0.55% | 1.12372 | 0.68%
huggingface | aten.new_empty_strided.default | 120.58% | 81.16% | 87.41% | 0.503116 | 28.27% | 0.668831 | 5.85% | 0.705637 | 2.76% | 1.109785 | 1.70% | 1.211641 | 0.74% | 1.322434 | 0.82%
timm | aten.new_empty_strided.default | 129.24% | 72.75% | 47.91% | 0.490658 | 15.87% | 0.76711 | 13.11% | 0.904033 | 4.44% | 1.124806 | 1.19% | 1.325182 | 0.65% | 1.337114 | 1.01%
torchbench | aten.new_empty.default | 69.41% | 1.60% | 0.90% | 0.732117 | 5.24% | 1.228356 | 1.18% | 1.241341 | 0.81% | 1.24031 | 1.96% | 1.248061 | 1.70% | 1.252525 | 1.84%
huggingface | aten.new_empty.default | 150.01% | 79.29% | 39.91% | 0.49547 | 12.67% | 0.692498 | 22.11% | 0.889526 | 27.37% | 1.238706 | 1.58% | 1.241606 | 1.49% | 1.244506 | 1.41%
timm | aten.new_empty.default | 11.61% | 11.13% | 11.07% | 1.115127 | 0.65% | 1.124302 | 0.80% | 1.132986 | 1.38% | 1.244582 | 1.12% | 1.249459 | 1.31% | 1.258416 | 1.14%

</body>

</html>

Pull Request resolved: https://github.com/pytorch/pytorch/pull/91350
Approved by: https://github.com/EikanWang, https://github.com/anijain2305, https://github.com/jgong5, https://github.com/desertfire"
pytorch/pytorch,a5f32f89788befb083aa664d7911bafcbf98e3cb,"training support for dynamo+torchxla integration (#88449)

We've already shown some promising perf result by integrating dynamo with torchxla for inference. To provide consistent UX for training and for inference, in this PR we try to enable training for dynamo/torchxla.

Training is trickier than inference and we may not expect much perf gains since
1. in training case, torchxla only generate a single combined graph for fwd/bwd/optimizer while in `torchxla_trace_once` bridge we added in dynamo, due to how AOT_Autograd works, we will generate 3 graphs: one for forward, one for backward and one for the optimizer. XLA favors larger graph to do more optimizations.
2. in training case, tracing overhead can be overlapped with computation. Tracing overhead is not as a big deal for training as for inference. After all training cares more about throughput while inference cares more about latency.
3. in training case, people can increase batch size to 'mitigate' the tracing overhead. Increase batch size does not change tracing overhead, thus it shows like the tracing overhead 'per example' reduces.

But we still want to add training support to dynamo/torchxla to make the work complete.

We added '--iterations-per-run' argument to control how may iterations we do per measure/device sync. This is to understand the impact of item 2 above.

Results:

With '--iterations-per-run' equals to 1, here are the perf numbers:
```
+-------------------------+--------------------+-------------------------+
| Model                   |   XLA (trace once) |   XLA (trace everytime) |
+=========================+====================+=========================+
| resnet18                |             0.91   |                0.959    |
+-------------------------+--------------------+-------------------------+
| resnet50                |             0.917  |                0.932    |
+-------------------------+--------------------+-------------------------+
| resnext50_32x4d         |             0.912  |                0.905    |
+-------------------------+--------------------+-------------------------+
| alexnet                 |             1.038  |                0.974    |
+-------------------------+--------------------+-------------------------+
| mobilenet_v2            |             0.881  |                0.835    |
+-------------------------+--------------------+-------------------------+
| mnasnet1_0              |             0.903  |                0.931    |
+-------------------------+--------------------+-------------------------+
| vgg16                   |             0.914  |                0.967    |
+-------------------------+--------------------+-------------------------+
| BERT_pytorch            |             1.359  |                0.84     |
+-------------------------+--------------------+-------------------------+
| timm_vision_transformer |             1.288  |                0.893    |
+-------------------------+--------------------+-------------------------+
| geomean                 |             1.0006 |                0.913794 |
+-------------------------+--------------------+-------------------------+
```

Overall it looks like graph break indeed cause perf loss. But for BERT_pytorch and timm_vision_transformer we still see perf gain. We need do more experiments with larger '--iterations-per-run'

NOTE:
In torchbench.py I added the following code to do a few workaround:
```
from myscripts import workaround # TODO will remove this line before landing
```

Here are the content of workaround.py:
```
import torch
from torch import nn
import os

# override max_pool2d with avg_pool2d
if os.environ.get(""REPLACE_MAXPOOL"", ""0"") == ""1"":
    torch.nn.MaxPool2d = torch.nn.AvgPool2d

```

It work around a few issues we found
1. MaxPool2d does not work for training in dynamo/torchxla: https://github.com/pytorch/torchdynamo/issues/1837 . WIP fix from Brian in https://github.com/pytorch/pytorch/pull/90226 , https://github.com/pytorch/xla/pull/4276/files (WIP)
2. recent change ( this PR https://github.com/pytorch/pytorch/pull/88697 ) in op decomposition cause batch_norm ops to fallback in torchxla. Fix from jack in https://github.com/pytorch/xla/pull/4282#event-7969608134 . (confirmed the fix after adding Deduper to handle duplicated return from fx graph generated by AOTAutograd)
3. we have issue to handle dropout because of random seed out of sync issue. Here is the fix: https://github.com/pytorch/xla/pull/4293 (confirmed the fix)

Example command:
```
REPLACE_MAXPOOL=1 USE_FAKE_TENSOR=0 GPU_NUM_DEVICES=1 python benchmarks/dynamo/torchbench.py --randomize-input --performance --trace-on-xla --training --backend=aot_torchxla_trace_once --only vgg16
```

Pull Request resolved: https://github.com/pytorch/pytorch/pull/88449
Approved by: https://github.com/wconstab, https://github.com/qihqi, https://github.com/malfet"
pytorch/pytorch,9ca37d65273e67394900fe8f64f32a30f85fcad3,"[MPS] Improve the performance of torch.linear() (#91114)

* Clean up redundant headers and namespaces from Linear.mm
* This should improve the Bert sample in #77799  by ~3x

Pull Request resolved: https://github.com/pytorch/pytorch/pull/91114
Approved by: https://github.com/DenisVieriu97, https://github.com/malfet, https://github.com/kulinseth"
pytorch/pytorch,3d1772857e58ff15f4f6d4159f936e1f6b0d3a9c,"Apply clang-tidy perf improvements to aten and torch/jit/passes/onnx (#91726)

Applies some minor performance fixups to pytorch regarding an implicit promotion and unnecessary copies (when const ref would have worked just as well).

Pull Request resolved: https://github.com/pytorch/pytorch/pull/91726
Approved by: https://github.com/ezyang"
pytorch/pytorch,f630294f595210fc765c7652f15b16fa82d52ded,"Optimize GELU BFloat16 Impl in CPU path (#79378)

### Description
For slow path (with non-contiguous inputs) with `none` or `tanh` approximate, current bfloat16 impl is not performance friendly in ATen. This PR uses float32 as an immediate type, in order to reduce the heavy cost of converting bf16 to fp32.

### Test
IceLake 2S 32C (Intel(R) Xeon(R) Platinum 8358 CPU @ 2.60GHz)

**single socket (32 cores):**
approximate is `none`:
|input shapes  | forward ( base) (ms) | backward (base) (ms) | forward (optimized) (ms) | backward (optimized) (ms)
|--|------| --| --| --|
|[16, 32, 32] | 0.361 | 1.055 | 0.348 | 0.672
|[32, 32, 64] | 0.084 | 2.003 | 0.076 | 1.426
|[32, 64, 128] | 0.237 | 2.007 | 0.22 | 1.454
|[64, 128, 128] | 2.23 | 6.348 | 1.943 | 4.103

approximate is `tanh`:
|input shapes  | forward ( base) (ms) | backward (base) (ms) | forward (optimized) (ms) | backward (optimized) (ms)
|--|------| --| --| --|
[16, 32, 32] | 0.203 | 1.209 | 0.138 | 0.474
[32, 32, 64] | 0.063 | 2.497 | 0.043 | 0.985
[32, 64, 128] | 0.201 | 2.707 | 0.141 | 1.205
[64, 128, 128] | 1.549 | 8.749 | 1.065 | 3.635

**single core:**
approximate is `none`:
|input shapes  | forward ( base) (ms) | backward (base) (ms) | forward (optimized) (ms) | backward (optimized) (ms)
|--|------| --| --| --|
[16, 32, 32] | 0.359 | 1.055 | 0.267 | 0.592
[32, 32, 64] | 1.11 | 3.483 | 1.063 | 2.373
[32, 64, 128] | 4.478 | 13.866 | 4.27 | 9.426
[64, 128, 128] | 17.675 | 55.231 | 16.805 | 37.509

approximate is `tanh`:
|input shapes  | forward ( base) (ms) | backward (base) (ms) | forward (optimized) (ms) | backward (optimized) (ms)
|--|------| --| --| --|
[16, 32, 32] | 0.202 | 1.212 | 0.138 | 0.473
[32, 32, 64] | 0.776 | 4.843 | 0.531 | 1.872
[32, 64, 128] | 3.203 | 19.267 | 2.16 | 7.243
[64, 128, 128] | 12.33 | 76.834 | 8.286 | 29.553

Pull Request resolved: https://github.com/pytorch/pytorch/pull/79378
Approved by: https://github.com/mingfeima"
pytorch/pytorch,56db21aec17693ad7ee3a2d5db53cb611df2d565,"[Checkpoint][Test] Add test for optimizer state_dict and resharding to 2d checkpoint test (#91092)

This PR updates the 2d checkpoint model state test to include:
1. optimizer state dict test
2. simple resharding test  (pg change)
3. rename test
Pull Request resolved: https://github.com/pytorch/pytorch/pull/91092
Approved by: https://github.com/fduwjj"
pytorch/pytorch,1e725c97470d8cf74e85984ca997e77c76e91a18,"Avoid device casting for all singleton tensors in optimizer states (#91454)

Fixes #75224
Pull Request resolved: https://github.com/pytorch/pytorch/pull/91454
Approved by: https://github.com/janeyx99"
pytorch/pytorch,3120054c151d37fdf43963dbb60ab420908f48cf,"Vectorize norm(double, p=2) on cpu (#91502)

This gives a speed up of 100x on my machine:

```
[------------------ Master -------------------]
                                |  (200000, 3)
32 threads: ----------------------------------
      torch linalg_norm         |     10000
      torch linalg_vector_norm  |     10000
      torch custom              |       397
      numpy norm                |      3123
      numpy custom_np           |      3119

Times are in microseconds (us).

[------------------- PR -------------------]
                                |  (200000, 3)
32 threads: ----------------------------------
      torch linalg_norm         |       107
      torch linalg_vector_norm  |       100
      torch custom              |       400
      numpy norm                |      3170
      numpy custom_np           |      3162

Times are in microseconds (us).
```

Fixes https://github.com/pytorch/pytorch/issues/91373

Pull Request resolved: https://github.com/pytorch/pytorch/pull/91502
Approved by: https://github.com/mingfeima, https://github.com/ngimel"
pytorch/pytorch,aa0ca994cadfabf455a4df75abdd436184c0081d,"[Inductor] add missing ops for cpp vectorization overrides (#90750)

For micro-benchmark, aten.elu.default and aten.elu_backward.default have poor performance with inductor compared to eager. The main reason is lack of the vectorization. With adding missing ops for cpp vectorization overrides, the vectorization could be successfully applied.

Performance data for eager v.s. inductor:
<html xmlns:v=""urn:schemas-microsoft-com:vml""
xmlns:o=""urn:schemas-microsoft-com:office:office""
xmlns:x=""urn:schemas-microsoft-com:office:excel""
xmlns=""http://www.w3.org/TR/REC-html40"">

<head>

<meta name=ProgId content=Excel.Sheet>
<meta name=Generator content=""Microsoft Excel 15"">
<link id=Main-File rel=Main-File
href=""file:///C:/Users/xuanliao/AppData/Local/Temp/msohtmlclip1/01/clip.htm"">
<link rel=File-List
href=""file:///C:/Users/xuanliao/AppData/Local/Temp/msohtmlclip1/01/clip_filelist.xml"">
<!--table
	{mso-displayed-decimal-separator:""\."";
	mso-displayed-thousand-separator:""\,"";}
@page
	{margin:.75in .7in .75in .7in;
	mso-header-margin:.3in;
	mso-footer-margin:.3in;}
tr
	{mso-height-source:auto;}
col
	{mso-width-source:auto;}
br
	{mso-data-placement:same-cell;}
td
	{padding-top:1px;
	padding-right:1px;
	padding-left:1px;
	mso-ignore:padding;
	color:black;
	font-size:11.0pt;
	font-weight:400;
	font-style:normal;
	text-decoration:none;
	font-family:Calibri, sans-serif;
	mso-font-charset:0;
	mso-number-format:General;
	text-align:general;
	vertical-align:bottom;
	border:none;
	mso-background-source:auto;
	mso-pattern:auto;
	mso-protection:locked visible;
	white-space:nowrap;
	mso-rotate:0;}
.xl63
	{mso-number-format:Percent;}
.xl64
	{color:gray;}
-->
</head>

<body link=""#0563C1"" vlink=""#954F72"">

op | speedup_old | RSD (3) | speedup_new | RSD (3) | increased_performance
-- | -- | -- | -- | -- | --
aten.elu.default | 0.205947276 | 1.73% | 0.995302802 | 4.76% | 383.28%
aten.elu_backward.default | 0.336280639 | 0.58% | 1.69473642 | 1.96% | 403.96%

</body>

</html>

The new supported ops for cpp vectorization overrides:
- eq
- ne
- lt
- gt
- le
- ge

Pull Request resolved: https://github.com/pytorch/pytorch/pull/90750
Approved by: https://github.com/jgong5, https://github.com/EikanWang, https://github.com/jansel, https://github.com/desertfire"
pytorch/pytorch,6f9a4ae5c952e969f87f43f8843d7d38085ea2d2,"Revert ""Populate the eviction_policy field for load/store properly (#91316)""

This reverts commit 3f4e87beaf67ec44d609605777d9da9e65cfbdd9.

Reverted https://github.com/pytorch/pytorch/pull/91316 on behalf of https://github.com/ngimel due to regresses performance"
pytorch/pytorch,3a60debe9dca7aa413467ba94328325e738e2d96,"implement ordering (#91362)

# Summary

In some cases, dependent on input, flash-attention is not the fastest fused kernel and memory-efficient attention is better. This implements a simple heuristic function for deciding the ordering of kernel functions.  This was based off of the xformer function found here: https://github.com/fairinternal/xformers/blob/15bff4986c3a4376176a4e6fa3dc0f2a120fa0bb/xformers/ops/fmha/dispatch.py#L13
Pull Request resolved: https://github.com/pytorch/pytorch/pull/91362
Approved by: https://github.com/cpuhrsch"
pytorch/pytorch,122245985a544d9d74d7b5037493541f5e525498,"Dispatch the auxiliary frobenius_norm and nuclear_norm to better implementations and deprecate them (#81763)

These functions will be legacy functions. We deprecate them, but we also
take this chance to dispatch to a more efficient and consistent implementation.
Doing so should help writing a conversion rule for these to be able to
remove them once and for all
Pull Request resolved: https://github.com/pytorch/pytorch/pull/81763
Approved by: https://github.com/ngimel"
pytorch/pytorch,77c2a8a11f7b5164c255b5b49dbc66a3f6533e9d,"Clang-Tidy: Improve ctors by removing unnecessary copies and initializations (#91538)

Apply clang-tidy fixups to prefer member initializer and modernize-pass-by-value. This is a mostly a noop, but it should make a few ctors slighlty more readable and more efficient. Also drops in some missing moves that prevents a lot of unnecessary copying.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/91538
Approved by: https://github.com/ezyang"
pytorch/pytorch,73436af43fd84891b6765f4d1e5eeb0073a10a11,"[cuDNN][cuDNN V8 API] Improve hot path heuristics performance in V8 (#90811)

Small optimization for the hot path when thrashing the cache with dynamic shapes; in most cases we don't need the fallback generator so we can omit it unless needed later.

CC @ptrblck @ngimel
Pull Request resolved: https://github.com/pytorch/pytorch/pull/90811
Approved by: https://github.com/ngimel"
pytorch/pytorch,6f034dc0b09a96c50421cf92ddb8709c59d95edf,"(non-batch) BSR/BSC to COO performance improvement. (#91389)

This PR improves the aforementioned conversions by reducing memory footprint and the number of kernels run, and also by removing the sync imposed by `at::where(condition)`.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/91389
Approved by: https://github.com/pearu, https://github.com/kit1980"
pytorch/pytorch,a34a9c3471864a3535612f8af667684672534493,"Perf: Apply more clang-tidy fixups to torch headers (#91445)

Applies so more fixes to headers that may have been missed before for performance optimization.cc @jgong5 @mingfeima @XiaobingSuper @sanchitintel @ashokei @jingxu10 @EikanWang @ezyang since this more in the series of the clang-tidy fixup

This is PR fixes 3 main issues:
1. Use emplacement more in headers
1. Avoid unnecessary copies and use const ref when possible
1. Default any special functions when possible to make them potentially trivial and more readable.
1. There is also one change in this PR that tries to prevent unnecessary math promotion, the rest of these changes are in another PR
Pull Request resolved: https://github.com/pytorch/pytorch/pull/91445
Approved by: https://github.com/ezyang"
pytorch/pytorch,553b592824f32a008731f344a741dd327ff03fc4,"Clang-Tidy: use modern for each loops and transparent functors (#91449)

This applies some more clang-tidy fixups. Particularly, this applies the modernize loops and modernize-use-transparent-functors checks. Transparent functors are less error prone since you don't have to worry about accidentally specifying the wrong type and are newly available as of C++17.

Modern foreach loops tend be more readable and can be more efficient to iterate over since the loop condition is removed.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/91449
Approved by: https://github.com/ezyang"
pytorch/pytorch,0417da228813efba6b73ae2b459a08822eaf2fa1,"Set a timeout value when testing multiprocess DataLoader (#91476)

Setting a timeout value when testing multiprocess DataLoader to prevent ASAN jobs timing out after 4 hours.

We are seeing multiple timeout issue running ASAN tests on HUD https://hud.pytorch.org/hud/pytorch/pytorch/master/1?per_page=50&name_filter=asan for examples

* Without mem leak check enabled https://github.com/pytorch/pytorch/actions/runs/3794216079/jobs/6455118197
* With mem leak check https://github.com/pytorch/pytorch/actions/runs/3792743994/jobs/6449356306

Looking a bit closer into the test, the hanging happens when multiprocess DataLoader is used in `test_utils`.  Here is the snapshot of those processes when I log into the hang runner:

```
UID        PID  PPID  C STIME TTY          TIME CMD
jenkins      1     0  0 Dec28 pts/0    00:00:00 bash
jenkins      8     0  0 Dec28 pts/1    00:00:00 sh -c pip install dist/torch-2.0.0a0+git97db9fd-cp37-cp37m-linux_x86_64.whl[opt-einsum] && .jenkins/pytorch/test.sh
jenkins     20     8  0 Dec28 pts/1    00:00:00 /bin/bash .jenkins/pytorch/test.sh
jenkins    764    20  0 Dec28 pts/1    00:00:07 python test/run_test.py --exclude-jit-executor --exclude-distributed-tests --shard 5 5 --verbose
jenkins    788   764  0 Dec28 pts/1    00:00:00 /opt/conda/bin/python -c from multiprocessing.semaphore_tracker import main;main(6)
jenkins   3743   764  0 Dec28 pts/1    00:00:05 /opt/conda/bin/python -c from multiprocessing.spawn import spawn_main; spawn_main(tracker_fd=7, pipe_handle=11) --multiprocessing-fork
jenkins   3766  3743  0 Dec28 pts/1    00:00:06 /opt/conda/bin/python -bb test_utils.py -v --import-slow-tests --import-disabled-tests
jenkins   3878  3766  0 Dec28 pts/1    00:00:06 /opt/conda/bin/python -bb test_utils.py -v --import-slow-tests --import-disabled-tests
jenkins   3879  3766  0 Dec28 pts/1    00:00:00 /opt/conda/bin/python -bb test_utils.py -v --import-slow-tests --import-disabled-tests
jenkins   3880  3766  0 Dec28 pts/1    00:00:00 /opt/conda/bin/python -bb test_utils.py -v --import-slow-tests --import-disabled-tests
jenkins   3881  3766  0 Dec28 pts/1    00:00:00 /opt/conda/bin/python -bb test_utils.py -v --import-slow-tests --import-disabled-tests
jenkins   3893     0  0 01:45 pts/2    00:00:00 /bin/bash
jenkins   3904  3893  0 01:46 pts/2    00:00:00 ps -ef
```

The specific hanging test was `test_random_seed` which spawned 4 subprocesses to load data.  After I killed one of them, the test could continue and printed the following stacktrace:

```
    test_random_seed (__main__.TestDataLoaderUtils) ... [W ParallelNative.cpp:230] Warning: Cannot set number of intraop threads after parallel work has started or after set_num_threads call when using native parallel backend (function set_num_threads)
  [W ParallelNative.cpp:230] Warning: Cannot set number of intraop threads after parallel work has started or after set_num_threads call when using native parallel backend (function set_num_threads)
  [W ParallelNative.cpp:230] Warning: Cannot set number of intraop threads after parallel work has started or after set_num_threads call when using native parallel backend (function set_num_threads)
  [W ParallelNative.cpp:230] Warning: Cannot set number of intraop threads after parallel work has started or after set_num_threads call when using native parallel backend (function set_num_threads)
  [W ParallelNative.cpp:230] Warning: Cannot set number of intraop threads after parallel work has started or after set_num_threads call when using native parallel backend (function set_num_threads)
  [W ParallelNative.cpp:230] Warning: Cannot set number of intraop threads after parallel work has started or after set_num_threads call when using native parallel backend (function set_num_threads)
  [W ParallelNative.cpp:230] Warning: Cannot set number of intraop threads after parallel work has started or after set_num_threads call when using native parallel backend (function set_num_threads)
  [W ParallelNative.cpp:230] Warning: Cannot set number of intraop threads after parallel work has started or after set_num_threads call when using native parallel backend (function set_num_threads)
  ERROR (9345.840s)
    test_random_seed (__main__.TestDataLoaderUtils) ...     test_random_seed errored - num_retries_left: 3
  Traceback (most recent call last):
    File ""/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py"", line 1134, in _try_get_data
      data = self._data_queue.get(timeout=timeout)
    File ""/opt/conda/lib/python3.7/multiprocessing/queues.py"", line 104, in get
      if not self._poll(timeout):
    File ""/opt/conda/lib/python3.7/multiprocessing/connection.py"", line 257, in poll
      return self._poll(timeout)
    File ""/opt/conda/lib/python3.7/multiprocessing/connection.py"", line 414, in _poll
      r = wait([self], timeout)
    File ""/opt/conda/lib/python3.7/multiprocessing/connection.py"", line 921, in wait
      ready = selector.select(timeout)
    File ""/opt/conda/lib/python3.7/selectors.py"", line 415, in select
      fd_event_list = self._selector.poll(timeout)
    File ""/opt/conda/lib/python3.7/site-packages/torch/utils/data/_utils/signal_handling.py"", line 66, in handler
      _error_if_any_worker_fails()
  RuntimeError: DataLoader worker (pid 3878) is killed by signal: Terminated.
  The above exception was the direct cause of the following exception:
  Traceback (most recent call last):
    File ""test_utils.py"", line 469, in test_random_seed
      x2 = run()
    File ""test_utils.py"", line 464, in run
      return next(iter(dataloader))
    File ""/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py"", line 635, in __next__
      data = self._next_data()
    File ""/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py"", line 1330, in _next_data
      idx, data = self._get_data()
    File ""/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py"", line 1296, in _get_data
      success, data = self._try_get_data()
    File ""/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py"", line 1147, in _try_get_data
      raise RuntimeError('DataLoader worker (pid(s) {}) exited unexpectedly'.format(pids_str)) from e
  RuntimeError: DataLoader worker (pid(s) 3878) exited unexpectedly
  [W ParallelNative.cpp:230] Warning: Cannot set number of intraop threads after parallel work has started or after set_num_threads call when using native parallel backend (function set_num_threads)
  [W ParallelNative.cpp:230] Warning: Cannot set number of intraop threads after parallel work has started or after set_num_threads call when using native parallel backend (function set_num_threads)
  [W ParallelNative.cpp:230] Warning: Cannot set number of intraop threads after parallel work has started or after set_num_threads call when using native parallel backend (function set_num_threads)
  [W ParallelNative.cpp:230] Warning: Cannot set number of intraop threads after parallel work has started or after set_num_threads call when using native parallel backend (function set_num_threads)
  [W ParallelNative.cpp:230] Warning: Cannot set number of intraop threads after parallel work has started or after set_num_threads call when using native parallel backend (function set_num_threads)
  [W ParallelNative.cpp:230] Warning: Cannot set number of intraop threads after parallel work has started or after set_num_threads call when using native parallel backend (function set_num_threads)
  [W ParallelNative.cpp:230] Warning: Cannot set number of intraop threads after parallel work has started or after set_num_threads call when using native parallel backend (function set_num_threads)
  [W ParallelNative.cpp:230] Warning: Cannot set number of intraop threads after parallel work has started or after set_num_threads call when using native parallel backend (function set_num_threads)
  ok (0.137s)
```

This doesn't fix the issue which I'll need to follow up to see why they hang.  However, this should allow the test to terminate gracefully and report errors.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/91476
Approved by: https://github.com/kit1980"
pytorch/pytorch,274d3b24c3f4b6dc8964ee6642e909ce4b992112,"use scatter_add for index_add when dim is the most inner dim (#88729)

### Motivation
When dim is -1 and the slice of source or result is noncontiguous, original `index_add` is slow as it uses add for the sliced tensor, which is serial on index and parallel on sliced tensor to avoid write conflict. Doing parallel on the sliced tensor is not optimal as the size of sliced tensor may be not big enough to parallel and also causes multiple parallelizations.

`scatter_add ` is used to speedup for this case as `scatter_add ` parallels on the outer dimension of input and is serial on the inner dimension to avoid write conflict. `scatter_add ` only need one parallel and the size of outer dimensions is bigger to do parallel.

### Testing

- Single core:

Before:

shape | fp32 / s | bf16 / s
-- | -- | --
[10, 128, 20, 20] | 2.82E-03 | 2.11E-03
[10, 128, 50, 50] | 0.023604 | 0.023794

After:

shape | fp32 / s | bf16 / s
-- | -- | --
[10, 128, 20, 20] | 9.30E-04 | 1.66E-03
[10, 128, 50, 50] | 0.005995 | 0.010003

- Single socket (28 cores):

Before:

shape | fp32 / s | bf16 / s
-- | -- | --
[10, 128, 20, 20] | 2.96E-03 | 2.52E-03
[10, 128, 50, 50] | 0.012208 | 0.012568

After:

shape | fp32 / s | bf16 / s
-- | -- | --
[10, 128, 20, 20] | 7.44E-05 | 1.33E-04
[10, 128, 50, 50] | 0.000333 | 0.000469

Pull Request resolved: https://github.com/pytorch/pytorch/pull/88729
Approved by: https://github.com/mingfeima, https://github.com/jgong5, https://github.com/malfet"
pytorch/pytorch,67c53d50e5b34a536cce2ad98b9ca5f65ff8a34d,"Revert ""Fix conda install on distributions with strict POSIX sh (#91371)""

This reverts commit 57dcd93c4103c6db043f341a0242596a42188081.

Reverted https://github.com/pytorch/pytorch/pull/91371 on behalf of https://github.com/kit1980 due to trunk / cuda11.6-py3.10-gcc7-sm86 / test (slow, 1, 2, linux.g5.4xlarge.nvidia.gpu) started to fail after this PR with mypy error"
pytorch/pytorch,bb24185ff42bc4d774711c1e3a5752280058091d,"Fix _check_no_differentiable_outputs for forward ad (#91391)

This `is_forward_ad` isn't propagated, which leads to this line creating a
slow-gradcheck failure on master:
```
    if not is_forward_ad and any(o.is_complex() for o in outputs):
        raise ValueError(""Expected output to be non-complex. get_numerical_jacobian no ""
                         ""longer supports functions that return complex outputs."")
```

Pull Request resolved: https://github.com/pytorch/pytorch/pull/91391
Approved by: https://github.com/albanD"
pytorch/pytorch,a061f139dccb5f56c9d14e25ef54ff821b4dd3c8,"[optim] Adam defaults to fused when CUDA + differentiable=False (#90865)

Step 1 in faster default optimizers.

Preliminary benchmarks show gaps in improvement on CUDA for BERT_pytorch and resnet18:
![image](https://user-images.githubusercontent.com/31798555/207707118-14221802-77ce-4ee0-96e3-04638c07924c.png)

Pull Request resolved: https://github.com/pytorch/pytorch/pull/90865
Approved by: https://github.com/albanD"
pytorch/pytorch,8b3d31cfc5b9287696b15b31a3a25123532b6583,"Add A ValueRange Analysis Pass to convert int64 indexing to int32 (#91028)

Builds up sympy expressions computing the lower and upper bound of ranges, and then finds `op.to_dtype(x, torch.int64)` nodes whose dominated values can all be computed in a lower precision. I haven't gotten all the way to work with dynamic shapes but it should be a fairly small change. There's still additional work to get torchinductor to work with large tensors (see https://github.com/pytorch/torchdynamo/issues/1819) because we would need to add explicit dtype annotations to int64 which we're not doing right now.

Fix for https://github.com/pytorch/torchdynamo/issues/1293.

Performance Before OpBench aten.upsample_bilinear2d.vec float32:
(25th %, 50th %, 75th %)
Before
[0.7521964636710751, 0.8645357996607477, 2.8746003906598494]
After:
[0.9511363478204263, 1.0295566597806718, 3.2662165264101755]
Pull Request resolved: https://github.com/pytorch/pytorch/pull/91028
Approved by: https://github.com/jansel"
pytorch/pytorch,6cea4f3d57927b30c3fc0a2f7103684fde0c75ea,"[FSDP][optim_state_dict][7/N] Make FSDP support NamedOptimizer (#91160)

**What does this PR do?**
This PR refactors FSDP optimizer state_dict APIs to accept `NamedOptimizer` as the input optimizer. The key difference is that the state_dict returned by `NamedOptimizer` is already keyed as FQN. This PR majorly changes the internal mapping to allows the optimizer state_dict to be keyed as FQN.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/91160
Approved by: https://github.com/fduwjj, https://github.com/rohan-varma"
pytorch/pytorch,8f16524598e7c282459cbff6b737d5b39c5b9f34,"Run test_spectral_ops serially to fix CUDA illegal memory access (#91264)

Fixes https://github.com/pytorch/pytorch/issues/88916

* Running this test sequentially is not flaky after 1000 reruns `pytest --verbose test_spectral_ops.py -k test_fft_round_trip_cuda_float32 --flake-finder --flake-runs=1000`
* On the other hand, the curious thing is that when I run this same command on an active runner with some testing processs running in the background, the reruns could fail with CUDA illegal memory access error (hard to reproduce though) https://paste.sh/6sZdRn95#pve73riXC5XehCLqxlCbnjea.  This points to the fact that running the `test_spectral_ops` test in parallel with others might be the surface-level cause of flakiness

So this PR adds the test to the serial list instead.  This shouldn't cause any issue w.r.t TTS because the test takes only half a minute at most to finish.

```
+---------------------+-------------------------------------------------+-------------+---------------------+
| file                | base_name                                       | test_config | time                |
+---------------------+-------------------------------------------------+-------------+---------------------+
| ""test_spectral_ops"" | ""cuda11.6-py3.10-gcc7-sm86""                     | ""default""   | 5.991666666666661   |
| ""test_spectral_ops"" | ""cuda11.6-py3.10-gcc7-sm86""                     | ""slow""      | 0.18433333333333346 |
| ""test_spectral_ops"" | ""linux-bionic-cuda11.6-py3-gcc7-slow-gradcheck"" | ""default""   | 9.866000000000003   |
| ""test_spectral_ops"" | ""linux-bionic-cuda11.6-py3.10-gcc7""             | ""default""   | 10.591333333333337  |
| ""test_spectral_ops"" | ""linux-bionic-cuda11.6-py3.7-gcc7-debug""        | ""default""   | 11.395000000000003  |
| ""test_spectral_ops"" | ""linux-bionic-cuda11.7-py3.10-gcc7""             | ""default""   | 9.424               |
| ""test_spectral_ops"" | ""linux-bionic-cuda11.7-py3.7-gcc7-debug""        | ""default""   | 8.889000000000003   |
| ""test_spectral_ops"" | ""linux-bionic-py3.7-clang9""                     | ""crossref""  | 6.280333333333329   |
| ""test_spectral_ops"" | ""linux-bionic-py3.7-clang9""                     | ""default""   | 12.182999999999998  |
| ""test_spectral_ops"" | ""linux-bionic-py3.7-clang9""                     | ""dynamo""    | 11.124999999999984  |
| ""test_spectral_ops"" | ""linux-bionic-py3.7-clang9-slow""                | ""slow""      | 0.1916666666666668  |
| ""test_spectral_ops"" | ""linux-focal-py3.7-clang7-asan""                 | ""default""   | 20.899666666666658  |
| ""test_spectral_ops"" | ""linux-focal-py3.7-gcc7""                        | ""default""   | 5.097999999999996   |
| ""test_spectral_ops"" | ""linux-focal-rocm5.3-py3.8-slow""                | ""slow""      | 0.23700000000000018 |
| ""test_spectral_ops"" | ""macos-12-py3-arm64""                            | ""default""   | 2.8396666666666626  |
| ""test_spectral_ops"" | ""macos-12-py3-x86-64""                           | ""default""   | 8.838999999999997   |
| ""test_spectral_ops"" | ""parallelnative-linux-focal-py3.7-gcc7""         | ""default""   | 5.016999999999998   |
| ""test_spectral_ops"" | ""win-vs2019-cpu-py3""                            | ""default""   | 8.351666666666665   |
| ""test_spectral_ops"" | ""win-vs2019-cuda11.6-py3""                       | ""default""   | 27.121666666666687  |
| ""test_spectral_ops"" | ""win-vs2019-cuda11.7-py3""                       | ""default""   | 24.567000000000025  |
+---------------------+-------------------------------------------------+-------------+---------------------+
```
Pull Request resolved: https://github.com/pytorch/pytorch/pull/91264
Approved by: https://github.com/clee2000"
pytorch/pytorch,b50f379cece085a3f69b735ccb61e46b2aa7d8fd,"Remove inductor performance from ciflow/nightly as infra is not ready to handle these jobs… (#91271)

… yet.

https://github.com/pytorch/pytorch/actions/workflows/inductor-perf-test-nightly.yml currently shows there are several commits waiting for A100 runners but the infra is not able to automatically respond to these dynamic requests. Therefore disabling ciflow/nightly tag and only use scheduled and workflow_dispatch.

Also remove postnightly filter as the [postnightly pull request ](https://github.com/pytorch/pytorch/pull/27167) is no longer running ci tests.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/91271
Approved by: https://github.com/kit1980, https://github.com/izaitsevfb, https://github.com/malfet"
pytorch/pytorch,68e9da68cbeb1288b904022d237c32e88e0372fd,"use scatter_add for index_add when dim is the most inner dim (#88729)

### Motivation
When dim is -1 and the slice of source or result is noncontiguous, original `index_add` is slow as it uses add for the sliced tensor, which is serial on index and parallel on sliced tensor to avoid write conflict. Doing parallel on the sliced tensor is not optimal as the size of sliced tensor may be not big enough to parallel and also causes multiple parallelizations.

`scatter_add ` is used to speedup for this case as `scatter_add ` parallels on the outer dimension of input and is serial on the inner dimension to avoid write conflict. `scatter_add ` only need one parallel and the size of outer dimensions is bigger to do parallel.

### Testing

- Single core:

Before:

shape | fp32 / s | bf16 / s
-- | -- | --
[10, 128, 20, 20] | 2.82E-03 | 2.11E-03
[10, 128, 50, 50] | 0.023604 | 0.023794

After:

shape | fp32 / s | bf16 / s
-- | -- | --
[10, 128, 20, 20] | 9.30E-04 | 1.66E-03
[10, 128, 50, 50] | 0.005995 | 0.010003

- Single socket (28 cores):

Before:

shape | fp32 / s | bf16 / s
-- | -- | --
[10, 128, 20, 20] | 2.96E-03 | 2.52E-03
[10, 128, 50, 50] | 0.012208 | 0.012568

After:

shape | fp32 / s | bf16 / s
-- | -- | --
[10, 128, 20, 20] | 7.44E-05 | 1.33E-04
[10, 128, 50, 50] | 0.000333 | 0.000469

Pull Request resolved: https://github.com/pytorch/pytorch/pull/88729
Approved by: https://github.com/mingfeima, https://github.com/jgong5, https://github.com/malfet"
pytorch/pytorch,e3383d296f47e10b8e7652c5ae2e181d207445b0,"[optim][fix] test_fused_optimizers did not test fused before (#91228)

I realized test_fused_optimizers used a helper that was written for foreach, so we were not testing fused at all. This PR fixes that test so we actually test fused adam.

The explicitly adding fused=False is to set the stage for my later changes (but should be a no-op here).
Pull Request resolved: https://github.com/pytorch/pytorch/pull/91228
Approved by: https://github.com/albanD, https://github.com/soulitzer"
pytorch/pytorch,c7f1974cf10019a43194b22e78421b67245db238,"Fix FastToLocals call by copy pasting (#91168)

Pull Request resolved: https://github.com/pytorch/pytorch/pull/91168
Approved by: https://github.com/ezyang"
pytorch/pytorch,8b617f813d86c348be368a72170ab0d319308b23,"[cuBLAS] Add an option to disable reduced precision reductions for BF16 GEMM (#89172)

Essentially the same change as #67946, except that the default is to disallow reduced precision reductions in `BFloat16` GEMMs (for now). If performance is severely regressed, we can change the default, but this option appears to be necessary to pass some `addmm` `BFloat16` tests on H100.

CC @ptrblck @ngimel
Pull Request resolved: https://github.com/pytorch/pytorch/pull/89172
Approved by: https://github.com/ngimel"
pytorch/pytorch,6d2b0cbb404fe43ae72423b850363eae6e7ea5dc,"[Re-landing 86706] [JIT] Frozen Graph Linear-BatchNormNd Folding (#91020)

Re-landing #86706

This PR adds linear-batchnormNd folding for JIT frozen graphs.

**Performance benchmark**
A preliminary benchmark with a simple model of linear+bn1d tested on first socket, physical cores of skylake machine.

**FP32, JIT**
without linear-bn folding
![Screenshot (1368)](https://user-images.githubusercontent.com/93151422/195168944-cfc5b920-bc82-4be1-a221-d194c8fa6c18.png)

with linear-bn folding
![Screenshot (1367)](https://user-images.githubusercontent.com/93151422/195168926-267b0515-45a1-4f08-922d-c150845199ae.png)
Pull Request resolved: https://github.com/pytorch/pytorch/pull/91020
Approved by: https://github.com/davidberard98"
pytorch/pytorch,e8bf7c21e444ecb7e835326c054df9a689c9ae5e,"Integrate apply_optim_in_backward with DDP (#89194)

Allow _apply_optim_in_backward to work with DDP.

Example:

```
dist.init_process_group(""nccl"", rank=rank, world_size=2)
    torch.cuda.set_device(rank)
    e = enc().cuda(rank)
    _apply_optimizer_in_backward(
        optimizer_class=torch.optim.SGD,
        params=e.parameters(),
        optimizer_kwargs={""lr"": 0.03},
    )
    e = DDP(e, device_ids=[rank])
    inp = torch.randn(1, 10, device=rank)
    e(inp).sum().backward()
```

Constraints:

1. Custom communication hook not yet supported
2. _apply_optim_in_backward needs to be called _before_ wrapping model in DDP.
3. DDP will remove the gradient hooks _apply_optim_in_backward registers, so these gradient hooks will not be fired and cannot be used.
4. All DDP managed parameters have grads set to None by default once optimizer is applied. There is no support for setting only some parameter grads to None, this must be done manually by user (and DDP_OVERLAPPED_OPTIM_SET_GRADS_TO_NONE=0 needs to be set.)

Differential Revision: [D41329694](https://our.internmc.facebook.com/intern/diff/D41329694/)

**NOTE FOR REVIEWERS**: This PR has internal Meta-specific changes or comments, please review them on [Phabricator](https://our.internmc.facebook.com/intern/diff/D41329694/)!
Pull Request resolved: https://github.com/pytorch/pytorch/pull/89194
Approved by: https://github.com/zhaojuanmao"
pytorch/pytorch,c7e7ea92e204ef6e184d4dea2920a8e6a3d9f5b2,"[NamedOptimizer][2/N] Prepare the enablement of state_dict for FSDP (#91147)

1. Add param_group check logic and unit test
2. Remove unnecessary check for conditional param update
3. Return the param_group from the inner optimizer so that when param_group is None or not all params are specified, we still return the expected result.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/91147
Approved by: https://github.com/fegin"
pytorch/pytorch,c43209db4db426fd7981a04bebf81a7790595bac,"use libdevice for tanh (#90889)

Per title
I see slight differences in perf with this implementation, where standalone tanh is slightly slower for a tensor of 4000000
 elements (20.4 us instead of 19.4us), other sizes are within noise.
 @bertmaher could you check if it affects your benchmarks?

Pull Request resolved: https://github.com/pytorch/pytorch/pull/90889
Approved by: https://github.com/bertmaher, https://github.com/anijain2305"
pytorch/pytorch,2f5759eaba555174dc4a92939e3260e6f814107d,"Disable non-deterministic models for optimizers (#91149)

These two models are non-deterministic even with constant inputs + weights and sometimes fail with variations between the fp64 and fp32 models in CI very rarely as a result.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/91149
Approved by: https://github.com/desertfire"
pytorch/pytorch,34717b3ea87b123b98e783d0331ec59375c3d748,"nn/test_convolution to run in serial (#91113)

unfortunately it takes 50 minutes on slow gradcheck but thats on periodic

ends up taking up >6000 MB of space (7440 available)
Pull Request resolved: https://github.com/pytorch/pytorch/pull/91113
Approved by: https://github.com/huydhn, https://github.com/ZainRizvi"
pytorch/pytorch,da9af9868e0bd8cb516690bcce34e29ea414cf5e,"[FSDP][4/N] Refactor func to share state/init handle attrs (#90871)

For `limit_all_gathers`, if we do not enforce that they all have the same value, then the entire semantics guaranteed by the `bool` can be violated. It could be as if none of them set that value to be `True`.

For `use_orig_params`, optimizer state dict assumes that the value is the same for all FSDP instances.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/90871
Approved by: https://github.com/mrshenli"
pytorch/pytorch,289f06434cd91cf3266bd9ae9c4d1822978510ae,"[dynamo] check buffers when checking accuracy (#91037)

Tested by running `python benchmarks/dynamo/torchbench.py --accuracy --float32 -dcuda --output=inductor_torchbench_float32_training_cuda_performance.csv --training --inductor --no-skip --dashboard --only mobilenet_v2 --cold_start_latency` and breakpointing after the changes to inspect buffers.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/91037
Approved by: https://github.com/anijain2305"
pytorch/pytorch,13dbad63696f0ad39d63e4457eeebf800fb80dff,"use scatter_add for index_add when dim is the most inner dim (#88729)

### Motivation
When dim is -1 and the slice of source or result is noncontiguous, original `index_add` is slow as it uses add for the sliced tensor, which is serial on index and parallel on sliced tensor to avoid write conflict. Doing parallel on the sliced tensor is not optimal as the size of sliced tensor may be not big enough to parallel and also causes multiple parallelizations.

`scatter_add ` is used to speedup for this case as `scatter_add ` parallels on the outer dimension of input and is serial on the inner dimension to avoid write conflict. `scatter_add ` only need one parallel and the size of outer dimensions is bigger to do parallel.

### Testing

- Single core:

Before:

shape | fp32 / s | bf16 / s
-- | -- | --
[10, 128, 20, 20] | 2.82E-03 | 2.11E-03
[10, 128, 50, 50] | 0.023604 | 0.023794

After:

shape | fp32 / s | bf16 / s
-- | -- | --
[10, 128, 20, 20] | 9.30E-04 | 1.66E-03
[10, 128, 50, 50] | 0.005995 | 0.010003

- Single socket (28 cores):

Before:

shape | fp32 / s | bf16 / s
-- | -- | --
[10, 128, 20, 20] | 2.96E-03 | 2.52E-03
[10, 128, 50, 50] | 0.012208 | 0.012568

After:

shape | fp32 / s | bf16 / s
-- | -- | --
[10, 128, 20, 20] | 7.44E-05 | 1.33E-04
[10, 128, 50, 50] | 0.000333 | 0.000469

Pull Request resolved: https://github.com/pytorch/pytorch/pull/88729
Approved by: https://github.com/mingfeima, https://github.com/jgong5, https://github.com/malfet"
pytorch/pytorch,0f57e7f2d9c9835fd504602dfb2fc12e6c08a74b,"Do not run inductor perf test with postnightly branch (#91133)

Inductor performance test job would be triggered every night associated with the pull request push event from https://github.com/pytorch/pytorch/pull/27167
Since we are already running three times a day the job, there is no need to run this test with postnightly branch. Plus, this postnightly branch currently fails dozens of tests due to ""docker argument too long error"".

Example workflow: https://github.com/pytorch/pytorch/actions/runs/3731250111
Pull Request resolved: https://github.com/pytorch/pytorch/pull/91133
Approved by: https://github.com/clee2000, https://github.com/malfet, https://github.com/seemethere, https://github.com/desertfire"
pytorch/pytorch,0148809131f494b842baf50d1f392f7404b87b44,"use libdevice for tanh (#90889)

Per title
I see slight differences in perf with this implementation, where standalone tanh is slightly slower for a tensor of 4000000
 elements (20.4 us instead of 19.4us), other sizes are within noise.
 @bertmaher could you check if it affects your benchmarks?

Pull Request resolved: https://github.com/pytorch/pytorch/pull/90889
Approved by: https://github.com/bertmaher, https://github.com/anijain2305"
pytorch/pytorch,1d3e7fcc3bd35d341701c5a945188286bfba6d05,"[pytorch profiler] Add step tracker logic to handle multiple sources of step increments (#90880)

Pull Request resolved: https://github.com/pytorch/pytorch/pull/90880

# Summary
Enables multiple step trackers. Currently we only had one place to mark that a step() has occurred in the program. This was via pytorch profiler step().
We are now working on adding an Optimizer step hook - https://github.com/pytorch/pytorch/issues/88446
- This could mean programs that already call profiler.step() every iteration can end up double incrementing steps
- If a model uses multiple optimizers we can also have double or more counting of the step.

## Solution
We fix this by adding a layer of abstraction before calling step() to the kineto library. The idea is to maintain steps per requester in a dictionary
```
{
   ""ProfilerStep"": 100,  # triggered by profiler step() call
   ""Optimizer1Step"": 100,   # Optimizer 1 or 2 are just examples, could be SGD, Adam etc
   ""Optimizer2Step"": 100,
}
```
To figure out the global step count just take max on the dict values (100).
```
{
   ""ProfilerStep"": 100,
   ""Optimizer1Step"": 101,   # Optimizer1 got incremented first say
   ""Optimizer2Step"": 100,
}
```
Then global step count is 101

## Calling kineto
We only call the kineto step() function when global count increments.

# Test Plan:
Added a unit test
   buck2 run mode/dev-nosan caffe2/test:profiler

Differential Revision: D41751157

Pull Request resolved: https://github.com/pytorch/pytorch/pull/90880
Approved by: https://github.com/chaekit"
pytorch/pytorch,7e9bf2ed860b8b60d252eead4cc457c3fe5f1667,"When nopython=True, Dynamo can't allow graph breaks. (#90970)

I count the number of sub-graphs (for tiny-GPT2 in huggingface) by
```
    class GraphCaptureCompiler:
        def __init__(self):
            self.captured_graphs = []
        def compile(self, gm, example_inputs):
            self.captured_graphs.append(gm)
            return gm
    compiler = GraphCaptureCompiler()
    torch._dynamo.optimize(compiler, nopython=True)(Wrapper(fn))(*args)
```

Although `len(compiler.captured_graphs)` is 2, no error was thrown during the compilation. This observation conflicts with `nopython=True`. After some digging, I found a check is missed before making graph break. This PR adds it.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/90970
Approved by: https://github.com/ezyang, https://github.com/jansel"
pytorch/pytorch,7ebc45eaddd7693fbf1a2983ce9333f188ff6a4f,"[dynamo] Better error message for bad timm model name (#91049)

Fixes https://github.com/pytorch/torchdynamo/issues/1995

Running `python benchmarks/dynamo/timm_models.py --performance --float32 -dcuda --output=out.csv --training --inductor --only bad_model_name` gives
```
Traceback (most recent call last):
  File ""benchmarks/dynamo/timm_models.py"", line 338, in <module>
    main(TimmRunnner())
  File ""/scratch/williamwen/work/pytorch/benchmarks/dynamo/common.py"", line 1660, in main
    return maybe_fresh_cache(run, args.cold_start_latency and args.only)(
  File ""/scratch/williamwen/work/pytorch/benchmarks/dynamo/common.py"", line 833, in inner
    return fn(*args, **kwargs)
  File ""/scratch/williamwen/work/pytorch/benchmarks/dynamo/common.py"", line 2000, in run
    ) = runner.load_model(device, model_name, batch_size=batch_size)
  File ""benchmarks/dynamo/timm_models.py"", line 215, in load_model
    raise RuntimeError(f""Failed to load model '{model_name}'"")
RuntimeError: Failed to load model 'bad_model_name'
```

Pull Request resolved: https://github.com/pytorch/pytorch/pull/91049
Approved by: https://github.com/ezyang"
pytorch/pytorch,731f417f60bfd5bb8d2ec756c23c0e6624ea3351,"Use scalar implementation to keep the precision in linspace of integral types (#89048)

Fixes #88652

In the CPU implementation of linspace of integral types, `base` type in vectorized implementation is `int64_t`, which will drop the precision when `base` comes from a floating number. Meanwhile, its vectorized implementation tends to suffer from the catastrophic cancellation of floating point arithemtic since both the `base (start + step * idx)` and the `step` are not exact. Its scalar implementation is fine since start is always an integer and the result would be truncated to integer as well.

Therefore, in this PR , we will skip the vectorized implementation since the vec doesn't contribute to performance anyway. And now the behaviors between CPU and GPU are the same. In some cases, the results are the same as numpy's. In some other cases, the results are different from numpy's, but it is not related to the devices (CPU and GPU). https://github.com/pytorch/pytorch/issues/81996#issuecomment-1192980485

Pull Request resolved: https://github.com/pytorch/pytorch/pull/89048
Approved by: https://github.com/mingfeima, https://github.com/jgong5, https://github.com/albanD"
pytorch/pytorch,ea49e769f65cfe16a2028fedb3a47f294e64c052,"[Quant] Add fused linear-tanh op for onednn backend (#88879)

**Summary**
Post op fusion can reduce data movement overhead and improve inference performance. This PR adds fused `linear-tanh` op for `onednn` backend, which will be used for int8 inference with `onednn` backend. Linear-tanh is found in models like CGAN.
Cannot call this op with other quantization backends otherwise an error is thrown.

**Test Plan**
python test_quantization.py TestQuantizedLinear

Pull Request resolved: https://github.com/pytorch/pytorch/pull/88879
Approved by: https://github.com/jgong5, https://github.com/jerryzh168"
pytorch/pytorch,3916d7a575c1811bc49eb45bc03b45f41f587f98,"Apply modernize-use-emplace to aten, c10, torch (#91077)

Apply clang-tidy check modernize-use-emplace. This is slightly more efficient by using an inplace constructor and is the recommended style in parts of the codebase covered by clang-tidy. This just manually applies the check to rest of the codebase. Pinging @ezyang as this is related to my other PRs he reviewed like #89000

Pull Request resolved: https://github.com/pytorch/pytorch/pull/91077
Approved by: https://github.com/ezyang"
pytorch/pytorch,1accd915a4652058f6f31fcbee76d5f023b2c83d,"Re-enable optimizers (#90709)

Fixes
https://github.com/pytorch/pytorch/issues/90165
https://github.com/pytorch/torchdynamo/issues/328

Re-enables optimizer capture + compilation now that the dynamo slowdowns have been fixed

and it has speedups, numbers to come soon

Pull Request resolved: https://github.com/pytorch/pytorch/pull/90709
Approved by: https://github.com/anijain2305, https://github.com/jansel, https://github.com/yanboliang"
pytorch/pytorch,7c524221ba8e8ac3698e2e877f410e4713d95937,"[reland3][dynamo] Revert ""Revert ""[reland][dynamo] use optimizers correctly in benchmar… (#90956)

…king (#87492)"" (#90746)""

This reverts commit ff1bbc2773a31ab839438966266ed8ee206cb8c5.

This should be okay to merge now. The flakiness of HF models will be fixed by seeding the rng (https://github.com/pytorch/pytorch/pull/90936), and the numeric mismatch was root-caused to three decomps (still investigating why those decomps cause this) see https://github.com/pytorch/torchdynamo/issues/1985 for more detail.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/90956
Approved by: https://github.com/desertfire"
pytorch/pytorch,4fa8d774b804c125b4597fb7bee5773c5da25608,"Add macro C10_AS_INTARRAYREF_SLOW (#90675)

This makes it easier to narrow down who is throwing the error,
instead of having to use gdb.

Signed-off-by: Edward Z. Yang <ezyang@fb.com>
Differential Revision: [D42088781](https://our.internmc.facebook.com/intern/diff/D42088781)"
pytorch/pytorch,67ef88af37b61080a0640d2a734f48685d516378,"Revert ""[Quant] onednn backend switch to ideep new api without affacting performance (#90354)""

This reverts commit 9b89ff0923251d2a30ceccf61120d051a687557c.

Reverted https://github.com/pytorch/pytorch/pull/90354 on behalf of https://github.com/osalpekar due to Breaking core pytorch contbuilds internally with function not found errors- more details in D42081737"
pytorch/pytorch,bd94ee66ea361e97158d37d6ddd8dbbeb8b624ef,"[quantized] [executorch] typo (#89960)

Summary: Inefficient impl in python

Test Plan: buck2 test mode/dev //caffe2/test/quantization:test_quantization -- --exact 'caffe2/test/quantization:test_quantization - test_quantized_embedding_byte (caffe2.test.quantization.core.test_quantized_tensor.TestQuantizedTensor)'

Differential Revision: D41627744

Pull Request resolved: https://github.com/pytorch/pytorch/pull/89960
Approved by: https://github.com/jerryzh168"
pytorch/pytorch,6bc6fb21db1b16392d21f40f19ad3326f966c474,"Revert ""[reland2][dynamo] Revert ""Revert ""[reland][dynamo] use optimizers correctly in benchmar… (#90956)""

This reverts commit 8bc38ae4e2037ae42813d552e5d412db77167bc0.

Reverted https://github.com/pytorch/pytorch/pull/90956 on behalf of https://github.com/desertfire due to Causing TIMM model failures"
pytorch/pytorch,a10b3ce8768c7f86feef38de4f17bcb7dda1134b,"generate device context managers in inductor code (#90934)

Fixes https://github.com/pytorch/torchdynamo/issues/1717, https://github.com/pytorch/torchdynamo/issues/1990

<s>TODO: add test with multiple devices, figure out extra context initialization</s>

Problems:
<s>It still initializes context on 0-th device that it shouldn't, I'll take a look where that happens and fix before landing</s>
It adds a python device context manages, that is absurdly slow and takes ~2.5 us (should be nanoseconds). That's not a problem for real models, because it'll be called just once, but it is a bit of an inconvenience for microbenchmarking, we should make that context manager more performant (won't fix in this PR)
It still can have bugs for graphs that run on multiple devices and can have buffers incorrectly shared between multiple device by memory reuse, if that happens that'll need to be solved separately.

Generated code:
```
def call(args):
    arg0_1, arg1_1 = args
    args.clear()
    with torch.cuda.device(1):
        buf0 = empty_strided((4, ), (1, ), device='cuda', dtype=torch.float32)
        stream1 = get_cuda_stream(1)
        triton_fused_div_0.run(arg0_1, arg1_1, buf0, 4, grid=grid(4), stream=stream1)
        del arg0_1
        del arg1_1
        return (buf0, )
```

Pull Request resolved: https://github.com/pytorch/pytorch/pull/90934
Approved by: https://github.com/wconstab"
pytorch/pytorch,8bc38ae4e2037ae42813d552e5d412db77167bc0,"[reland2][dynamo] Revert ""Revert ""[reland][dynamo] use optimizers correctly in benchmar… (#90956)

…king (#87492)"" (#90746)""

This reverts commit ff1bbc2773a31ab839438966266ed8ee206cb8c5.

This should be okay to merge now. The flakiness of HF models will be fixed by seeding the rng (https://github.com/pytorch/pytorch/pull/90936), and the numeric mismatch was root-caused to three decomps (still investigating why those decomps cause this) see https://github.com/pytorch/torchdynamo/issues/1985 for more detail.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/90956
Approved by: https://github.com/desertfire"
pytorch/pytorch,6ea93b2295cb77c8e21db03d70849ad2665977ec,"[Quant] Add fused LinearLeakyReLU module for onednn backend (#88661)

**Summary**
Post op fusion can reduce data movement overhead and improve inference performance. This PR adds fused `QLinearLeakyReLU` module for onednn backend, which will be used for int8 inference with onednn backend. Cannot call this module with other quantization backends otherwise an error is thrown.

**Test plan**
python test_quantization.py TestStaticQuantizedModule

Pull Request resolved: https://github.com/pytorch/pytorch/pull/88661
Approved by: https://github.com/jgong5, https://github.com/jerryzh168"
pytorch/pytorch,9d523616b3761136479bdc61ed894eaf2bd9fc29,"fix segfault for EmbeddingBag on CPU slow path when include_last_offset is true (#90358)

This PR is to fix the segfault reported at https://github.com/pytorch/pytorch/issues/89677, this is a `double free` issue caused by `invalid read`.

The reported issue broke at slow path for `EmbeddingBag` on float32, at [EmbeddingBag.cpp#L451](https://github.com/pytorch/pytorch/blob/master/aten/src/ATen/native/EmbeddingBag.cpp#L451)

Root cause is that `add_indices` has index which exceeds range of `output_data`, for the reported case.

The offsets are given as
```
{0,  6, 12, 15, 25, 32, 40, 42, 46, 53, 53}
```

The `indices` has 55 elements and `offsets[-1] != indices.size(0)`.

When `include_last_offset` is true, the `output` will be in the shape of {offsets.size(0) - 1, weight.sizes()[1]}, which will be {10, 5}.
Originally, `add_indices` will be (i re-arange the 1D tensor by rows, so here 10 rows in total)
```
### this is 55 elements
  0 0 0 0 0 0
  1 1 1 1 1 1
  2 2 2
  3 3 3 3 3 3 3 3 3 3
  4 4 4 4 4 4 4
  5 5 5 5 5 5 5 5
  6 6
  7 7 7 7
  8 8 8 8 8 8 8
  10 10
```
The last row has index of 10 which is out of range of output tensor whose size is [10, 5].

The reason is `make_offset2bag` at [EmbeddingBag.cpp#L66](https://github.com/pytorch/pytorch/blob/master/aten/src/ATen/native/EmbeddingBag.cpp#L66) would give the following `offset2bag`:
```
### this is 55 + 1 elements:
0 0 0 0 0 0 1
0 0 0 0 0 1
0 0 1
0 0 0 0 0 0 0 0 0 1
0 0 0 0 0 0 1
0 0 0 0 0 0 0 1
0 1
0 0 0 1
0 0 0 0 0 0 2
0 0
```

Notice for index 53, it is added twice.

The fix is ignore the last index from `offsets` when `include_last_offset` is true, also this behavior aligns with CUDA, quote from https://github.com/pytorch/pytorch/pull/57208#issuecomment-1021727378

Pull Request resolved: https://github.com/pytorch/pytorch/pull/90358
Approved by: https://github.com/ezyang"
pytorch/pytorch,353c2e7d39c2c4d0c3e1b8c4d7338e19c7b02f57,"[Quant] Add fused LinearLeakyReLU module for onednn backend (#88661)

**Summary**
Post op fusion can reduce data movement overhead and improve inference performance. This PR adds fused `QLinearLeakyReLU` module for onednn backend, which will be used for int8 inference with onednn backend. Cannot call this module with other quantization backends otherwise an error is thrown.

**Test plan**
python test_quantization.py TestStaticQuantizedModule

Pull Request resolved: https://github.com/pytorch/pytorch/pull/88661
Approved by: https://github.com/jgong5, https://github.com/jerryzh168"
pytorch/pytorch,140a3139d6c5fd48852865af5675ab533db51b37,"Revert ""Add macro C10_AS_INTARRAYREF_SLOW (#90675)""

This reverts commit 8090cb5386dccf4cf341aea585c793dfbb6c6002.

Reverted https://github.com/pytorch/pytorch/pull/90675 on behalf of https://github.com/osalpekar due to broke internal acc_tensor implementation in training_platform contbuild. See [D42052101](https://www.internalfb.com/diff/D42052101) for details."
pytorch/pytorch,9b89ff0923251d2a30ceccf61120d051a687557c,"[Quant] onednn backend switch to ideep new api without affacting performance (#90354)

**Summary**
Onednn quantization backend switch to new API in `third_party/ideep`.
- `struct forward_params` for conv/deconv are changed. Modify primitive cache accordingly.
- Use new versions of `prepare` and `compute` API. Fp32 and int8 paths separated. The old ones will be deprecated.
- Now `ideep::tensor::reorder_if_differ_in` supports block-to-block reorder. Use it instead of defining a util function `onednn_utils::try_reorder`.
- For new API of transposed convolution, we can use a flag to keep weight desc align with oneDNN thus needless to transpose it explicitly in PyTorch.
- Use `is_channels_last` flag to specify layout of src/dst when querying expected weight desc.

It won't impact correctness. Performance should be unaffected or slightly better.
FBGEMM and QNNPACK backends are not affected.

Performance results are given below.
1. End-to-end performance of static quantized models (from torchvision)
(throughput: fps, higher is better)
![image](https://user-images.githubusercontent.com/12522207/206105879-45c59996-9804-4531-aa1f-dc962e6db5ab.png)

2. Op benchmark of dynamic quantized linear
(Latency: ms, lower is better)
![image](https://user-images.githubusercontent.com/12522207/206124949-77352991-0fda-4285-a484-e20a5797262b.png)

Test method & env:
- Intel(R) Xeon(R) Platinum 8358 CPU @ 2.60GHz
- Run multi-instances on a single node. Use one core for each instance.
- Use Jemalloc and Intel OpenMP

**Test plan**
python test/test_quantization.py

Pull Request resolved: https://github.com/pytorch/pytorch/pull/90354
Approved by: https://github.com/jgong5"
pytorch/pytorch,eecd621f06d97d51072d924749a5d54b081295a0,"[cuDNN][cuDNN V8 API] (re-re-open) cuDNN V8 API on by default (#89022)

Testing V8 on by default again after fixes have been merged for e.g., https://github.com/pytorch/torchdynamo/issues/1833

One new failure that seems to be surfaced with V8 on appears in halonext + amp
```
RuntimeError: Internal Triton PTX codegen error:
Segmentation fault (core dumped)
```
But I'm not sure if this points to a V8 issue or a Triton issue CC @ngimel @ptrblck

Current dynamo benchmarks on A100:
v7 vs. v8
|dev |name                           |batch_size|abs_latency_v7|abs_latency_v8|
|----|-------------------------------|----------|--------------|--------------|
|cuda|adv_inception_v3               |128       |166.0240      |165.5798      |
|cuda|beit_base_patch16_224          |64        |123.5912      |123.0797      |
|cuda|botnet26t_256                  |128       |107.7343      |107.5948      |
|cuda|cait_m36_384                   |4         |184.5038      |184.0271      |
|cuda|coat_lite_mini                 |128       |142.3061      |140.5814      |
|cuda|convit_base                    |64        |165.2499      |161.0743      |
|cuda|convmixer_768_32               |32        |325.6984      |325.7094      |
|cuda|convnext_base                  |64        |237.4632      |238.0142      |
|cuda|crossvit_9_240                 |128       |72.2980       |72.4367       |
|cuda|cspdarknet53                   |64        |96.6862       |96.8308       |
|cuda|deit_base_distilled_patch16_224|64        |117.6045      |117.9616      |
|cuda|dla102                         |128       |182.3073      |182.2304      |
|cuda|dm_nfnet_f0                    |128       |133.6011      |133.6298      |
|cuda|dpn107                         |32        |148.5080      |148.5885      |
|cuda|eca_botnext26ts_256            |128       |113.8676      |113.1514      |
|cuda|eca_halonext26ts               |128       |119.2242      |119.1845      |
|cuda|ese_vovnet19b_dw               |128       |80.0217       |79.9438       |
|cuda|fbnetc_100                     |128       |91.4548       |91.4009       |
|cuda|fbnetv3_b                      |128       |115.4496      |115.5058      |
|cuda|gernet_l                       |128       |114.8365      |114.7870      |
|cuda|ghostnet_100                   |128       |58.5766       |58.5766       |
|cuda|gluon_inception_v3             |128       |165.5222      |165.7167      |
|cuda|gluon_xception65               |32        |165.8779      |165.7818      |
|cuda|gmixer_24_224                  |128       |116.3611      |113.4925      |
|cuda|gmlp_s16_224                   |128       |121.2607      |121.2534      |
|cuda|hrnet_w18                      |128       |246.5706      |246.7599      |
|cuda|inception_v3                   |128       |166.1096      |166.2034      |
|cuda|jx_nest_base                   |32        |93.6064       |93.4088       |
|cuda|lcnet_050                      |128       |21.4156       |21.4207       |
|cuda|levit_128                      |128       |27.2901       |27.2543       |
|cuda|mixer_b16_224                  |128       |157.8992      |158.2878      |
|cuda|mixnet_l                       |128       |197.3443      |197.2125      |
|cuda|mnasnet_100                    |128       |71.4604       |71.2997       |
|cuda|mobilenetv2_100                |128       |67.6080       |67.7515       |
|cuda|mobilenetv3_large_100          |128       |57.7224       |57.6591       |
|cuda|mobilevit_s                    |64        |93.0372       |93.0530       |
|cuda|nfnet_l0                       |128       |113.1664      |113.2853      |
|cuda|pit_b_224                      |64        |133.3333      |133.4153      |
|cuda|pnasnet5large                  |16        |238.9545      |238.8122      |
|cuda|poolformer_m36                 |64        |144.2353      |144.2375      |
|cuda|regnety_002                    |128       |32.8534       |32.9069       |
|cuda|repvgg_a2                      |128       |102.4150      |102.3827      |
|cuda|res2net101_26w_4s              |64        |120.8127      |120.8322      |
|cuda|res2net50_14w_8s               |128       |149.7052      |149.8969      |
|cuda|res2next50                     |128       |153.7439      |153.8215      |
|cuda|resmlp_12_224                  |128       |89.1918       |86.9226       |
|cuda|resnest101e                    |64        |159.4706      |159.3133      |
|cuda|rexnet_100                     |128       |88.0032       |88.0397       |
|cuda|sebotnet33ts_256               |64        |80.4635       |80.0120       |
|cuda|selecsls42b                    |128       |70.4430       |70.3663       |
|cuda|spnasnet_100                   |128       |78.0537       |78.1991       |
|cuda|swin_base_patch4_window7_224   |64        |212.9073      |213.0824      |
|cuda|swsl_resnext101_32x16d         |32        |193.0229      |193.0404      |
|cuda|tf_efficientnet_b0             |128       |97.1316       |97.0410       |
|cuda|tf_mixnet_l                    |128       |203.4956      |203.5340      |
|cuda|tinynet_a                      |128       |82.4038       |82.8733       |
|cuda|tnt_s_patch16_224              |128       |284.8576      |284.8867      |
|cuda|twins_pcpvt_base               |64        |118.3893      |119.2329      |
|cuda|visformer_small                |128       |126.0533      |126.0390      |
|cuda|vit_base_patch16_224           |64        |118.2873      |118.0573      |
|cuda|volo_d1_224                    |64        |108.7764      |108.2063      |
|cuda|xcit_large_24_p8_224           |5         |100.4656      |100.5209      |

v7 vs. v8 amp

|dev |name                           |batch_size|abs_latency_v7|abs_latency_v8|
|----|-------------------------------|----------|--------------|--------------|
|cuda|adv_inception_v3               |128       |104.9729      |105.1237      |
|cuda|beit_base_patch16_224          |64        |75.4330       |75.2039       |
|cuda|botnet26t_256                  |128       |74.5149       |74.8071       |
|cuda|cait_m36_384                   |4         |110.9788      |111.5170      |
|cuda|coat_lite_mini                 |128       |62.3618       |64.4965       |
|cuda|convit_base                    |64        |116.4054      |117.9129      |
|cuda|convmixer_768_32               |32        |264.4401      |264.4491      |
|cuda|convnext_base                  |64        |182.9009      |179.2136      |
|cuda|crossvit_9_240                 |128       |48.8586       |48.8359       |
|cuda|cspdarknet53                   |64        |80.0245       |80.0160       |
|cuda|deit_base_distilled_patch16_224|64        |66.5921       |66.7448       |
|cuda|dla102                         |128       |116.7780      |117.1683      |
|cuda|dm_nfnet_f0                    |128       |78.9322       |79.1135       |
|cuda|dpn107                         |32        |85.5206       |85.7514       |
|cuda|eca_botnext26ts_256            |128       |76.3672       |77.0050       |
|cuda|eca_halonext26ts               |128       |86.2458       |              |
|cuda|ese_vovnet19b_dw               |128       |43.2943       |43.3379       |
|cuda|fbnetc_100                     |128       |54.8479       |54.9251       |
|cuda|fbnetv3_b                      |128       |70.7504       |71.0188       |
|cuda|gernet_l                       |128       |66.1607       |66.0379       |
|cuda|ghostnet_100                   |128       |43.8882       |43.9336       |
|cuda|gluon_inception_v3             |128       |104.9297      |105.0204      |
|cuda|gluon_xception65               |32        |85.7118       |85.8370       |
|cuda|gmixer_24_224                  |128       |75.1214       |76.1170       |
|cuda|gmlp_s16_224                   |128       |76.4207       |76.6641       |
|cuda|hrnet_w18                      |128       |186.1326      |186.2435      |
|cuda|inception_v3                   |128       |105.0561      |105.0783      |
|cuda|jx_nest_base                   |32        |65.3066       |65.3245       |
|cuda|lcnet_050                      |128       |14.7991       |14.8687       |
|cuda|levit_128                      |128       |19.2893       |19.4772       |
|cuda|mixer_b16_224                  |128       |93.9826       |94.2056       |
|cuda|mixnet_l                       |128       |147.1245      |147.0435      |
|cuda|mnasnet_100                    |128       |39.1781       |39.2565       |
|cuda|mobilenetv2_100                |128       |42.3704       |42.3114       |
|cuda|mobilenetv3_large_100          |128       |37.2946       |37.2816       |
|cuda|mobilevit_s                    |64        |55.8930       |55.8934       |
|cuda|nfnet_l0                       |128       |64.0448       |64.4438       |
|cuda|pit_b_224                      |64        |80.6342       |80.2933       |
|cuda|pnasnet5large                  |16        |154.9611      |154.8654      |
|cuda|poolformer_m36                 |64        |101.7489      |101.8138      |
|cuda|regnety_002                    |128       |27.0939       |27.0309       |
|cuda|repvgg_a2                      |128       |60.9651       |61.2533       |
|cuda|res2net101_26w_4s              |64        |77.3291       |77.4739       |
|cuda|res2net50_14w_8s               |128       |93.6572       |93.7221       |
|cuda|res2next50                     |128       |112.4975      |112.3248      |
|cuda|resmlp_12_224                  |128       |59.5422       |60.7644       |
|cuda|resnest101e                    |64        |97.9894       |98.3358       |
|cuda|rexnet_100                     |128       |55.2218       |55.0718       |
|cuda|sebotnet33ts_256               |64        |60.4880       |60.8113       |
|cuda|selecsls42b                    |128       |41.4294       |41.5341       |
|cuda|spnasnet_100                   |128       |45.0037       |45.0304       |
|cuda|swin_base_patch4_window7_224   |64        |98.2561       |98.6925       |
|cuda|swsl_resnext101_32x16d         |32        |100.6179      |100.9195      |
|cuda|tf_efficientnet_b0             |128       |56.5344       |56.4591       |
|cuda|tf_mixnet_l                    |128       |153.0318      |152.9367      |
|cuda|tinynet_a                      |128       |54.1307       |53.9298       |
|cuda|tnt_s_patch16_224              |128       |142.4801      |142.6589      |
|cuda|twins_pcpvt_base               |64        |67.9027       |67.8325       |
|cuda|visformer_small                |128       |72.5589       |72.9427       |
|cuda|vit_base_patch16_224           |64        |71.4885       |71.7342       |
|cuda|volo_d1_224                    |64        |69.3539       |69.5910       |
|cuda|xcit_large_24_p8_224           |5         |59.9000       |59.9699       |

v7 vs. v8 float16
|dev |name                           |batch_size|abs_latency|abs_latency|
|----|-------------------------------|----------|-----------|-----------|
|cuda|adv_inception_v3               |128       |104.2544   |104.2677   |
|cuda|beit_base_patch16_224          |64        |85.3601    |85.3786    |
|cuda|botnet26t_256                  |128       |72.1476    |71.8277    |
|cuda|cait_m36_384                   |4         |108.3075   |108.5941   |
|cuda|coat_lite_mini                 |128       |61.2382    |61.6049    |
|cuda|convmixer_768_32               |32        |263.3818   |263.3598   |
|cuda|convnext_base                  |64        |172.6821   |173.8520   |
|cuda|crossvit_9_240                 |128       |44.6321    |44.6340    |
|cuda|cspdarknet53                   |64        |79.3165    |79.2964    |
|cuda|deit_base_distilled_patch16_224|64        |61.9816    |62.2109    |
|cuda|dla102                         |128       |115.7403   |115.9928   |
|cuda|dm_nfnet_f0                    |128       |77.5434    |77.7440    |
|cuda|dpn107                         |32        |83.6489    |83.5605    |
|cuda|eca_botnext26ts_256            |128       |73.9953    |74.1031    |
|cuda|eca_halonext26ts               |128       |81.7951    |81.7103    |
|cuda|ese_vovnet19b_dw               |128       |42.9618    |42.8853    |
|cuda|fbnetc_100                     |128       |54.3590    |54.3575    |
|cuda|fbnetv3_b                      |128       |69.7977    |70.1696    |
|cuda|gernet_l                       |128       |64.8684    |65.1726    |
|cuda|ghostnet_100                   |128       |43.2054    |43.1319    |
|cuda|gluon_inception_v3             |128       |104.1988   |104.3030   |
|cuda|gluon_xception65               |32        |84.2245    |84.5085    |
|cuda|gmixer_24_224                  |128       |82.0418    |82.7252    |
|cuda|gmlp_s16_224                   |128       |75.4792    |75.8374    |
|cuda|hrnet_w18                      |128       |184.1450   |184.1848   |
|cuda|inception_v3                   |128       |104.1203   |104.2536   |
|cuda|jx_nest_base                   |32        |58.2386    |58.4901    |
|cuda|lcnet_050                      |128       |14.6409    |14.5616    |
|cuda|levit_128                      |128       |22.3875    |22.4680    |
|cuda|mixer_b16_224                  |128       |98.9534    |98.4730    |
|cuda|mixnet_l                       |128       |146.1623   |146.1947   |
|cuda|mnasnet_100                    |128       |38.9208    |39.3463    |
|cuda|mobilenetv2_100                |128       |41.8946    |41.9847    |
|cuda|mobilenetv3_large_100          |128       |36.7810    |36.8264    |
|cuda|mobilevit_s                    |64        |55.3211    |55.3186    |
|cuda|nfnet_l0                       |128       |63.1302    |63.5544    |
|cuda|pit_b_224                      |64        |73.8752    |73.4602    |
|cuda|pnasnet5large                  |16        |151.6806   |151.6111   |
|cuda|poolformer_m36                 |64        |86.8341    |86.8021    |
|cuda|regnety_002                    |128       |26.6798    |26.5295    |
|cuda|repvgg_a2                      |128       |61.6652    |62.1482    |
|cuda|res2net101_26w_4s              |64        |75.8037    |75.7739    |
|cuda|res2net50_14w_8s               |128       |92.6362    |92.4338    |
|cuda|res2next50                     |128       |111.5371   |111.5832   |
|cuda|resmlp_12_224                  |128       |58.2349    |57.9807    |
|cuda|resnest101e                    |64        |96.1114    |96.2742    |
|cuda|rexnet_100                     |128       |54.8138    |54.7643    |
|cuda|sebotnet33ts_256               |64        |53.1524    |53.3823    |
|cuda|selecsls42b                    |128       |40.6070    |40.7104    |
|cuda|spnasnet_100                   |128       |44.5732    |44.4318    |
|cuda|swin_base_patch4_window7_224   |64        |98.6447    |98.8445    |
|cuda|swsl_resnext101_32x16d         |32        |97.0195    |97.2968    |
|cuda|tf_efficientnet_b0             |128       |56.0640    |56.0278    |
|cuda|tf_mixnet_l                    |128       |152.0958   |152.0874   |
|cuda|tinynet_a                      |128       |53.3694    |53.3762    |
|cuda|tnt_s_patch16_224              |128       |130.2981   |130.3726   |
|cuda|twins_pcpvt_base               |64        |62.5459    |62.6416    |
|cuda|visformer_small                |128       |68.8502    |69.1756    |
|cuda|vit_base_patch16_224           |64        |65.8587    |66.0285    |
|cuda|volo_d1_224                    |64        |64.5348    |64.6057    |

Pull Request resolved: https://github.com/pytorch/pytorch/pull/89022
Approved by: https://github.com/ngimel"
pytorch/pytorch,e585156c59767ff13306a31d8c31ffe7a33439dc,"[JIT] Frozen Graph Linear-BatchNormNd Folding (#86706)

This PR adds linear-batchnormNd folding for JIT frozen graphs.

**Performance benchmark**
A preliminary benchmark with a simple model of linear+bn1d tested on first socket, physical cores of skylake machine.

**FP32, JIT**
without linear-bn folding
![Screenshot (1368)](https://user-images.githubusercontent.com/93151422/195168944-cfc5b920-bc82-4be1-a221-d194c8fa6c18.png)

with linear-bn folding
![Screenshot (1367)](https://user-images.githubusercontent.com/93151422/195168926-267b0515-45a1-4f08-922d-c150845199ae.png)

Pull Request resolved: https://github.com/pytorch/pytorch/pull/86706
Approved by: https://github.com/davidberard98"
pytorch/pytorch,8090cb5386dccf4cf341aea585c793dfbb6c6002,"Add macro C10_AS_INTARRAYREF_SLOW (#90675)

This makes it easier to narrow down who is throwing the error,
instead of having to use gdb.

Signed-off-by: Edward Z. Yang <ezyang@fb.com>

Pull Request resolved: https://github.com/pytorch/pytorch/pull/90675
Approved by: https://github.com/ngimel, https://github.com/malfet, https://github.com/JackCaoG"
pytorch/pytorch,708108a9d3db164092b7b8bbb8598f6f99938012,"Optimized vertical flip using memcpy (#89414)

## Description

- Use memcpy for vertical flip
- Added bool type support for horizontal flip
  - channels last input with horizontal flip goes also into cpu_vflip_memcpy and has a speed-up

Previous PRs:
- https://github.com/pytorch/pytorch/pull/90013
- https://github.com/pytorch/pytorch/pull/88989

## Results

### Horizontal flip

- AVX2 (only cases with speed-up or same perfs for channels last input)
```
[------------------------------------------------------------------------- Horizontal flip -------------------------------------------------------------------------]
                                                                      |  torch (1.14.0a0+giteb3e189) PR  |    Pillow (9.3.0)   |  torch (1.14.0a0+gitb0bd5c4) nightly
1 threads: ----------------------------------------------------------------------------------------------------------------------------------------------------------
      channels=3, size=256, dtype=torch.int64, mf=channels_last       |        204.813 (+-1.018)         |                     |           308.070 (+-1.573)
      channels=3, size=520, dtype=torch.int64, mf=channels_last       |        844.523 (+-2.302)         |                     |           1226.801 (+-5.069)
      channels=3, size=712, dtype=torch.int64, mf=channels_last       |        2246.512 (+-8.935)        |                     |          2689.692 (+-22.654)

      channels=1, size=256, dtype=torch.int32, mf=channels_last       |         21.024 (+-0.083)         |   44.196 (+-0.131)  |            22.564 (+-0.066)
      channels=1, size=520, dtype=torch.int32, mf=channels_last       |         71.806 (+-0.150)         |  166.653 (+-0.789)  |            72.660 (+-0.160)
      channels=1, size=712, dtype=torch.int32, mf=channels_last       |        129.354 (+-0.385)         |  306.998 (+-0.819)  |           130.094 (+-0.274)

      channels=3, size=256, dtype=torch.uint8, mf=channels_last       |        177.250 (+-0.485)         |   44.232 (+-0.465)  |           289.201 (+-2.837)
      channels=3, size=520, dtype=torch.uint8, mf=channels_last       |        699.055 (+-1.940)         |  166.540 (+-0.903)  |           1172.747 (+-3.645)
      channels=3, size=712, dtype=torch.uint8, mf=channels_last       |        1302.968 (+-5.390)        |  307.210 (+-0.852)  |          2149.396 (+-23.570)

      channels=1, size=256, dtype=torch.int16, mf=channels_last       |         11.943 (+-0.079)         |                     |            12.451 (+-0.033)
      channels=1, size=520, dtype=torch.int16, mf=channels_last       |         39.830 (+-0.093)         |                     |            40.583 (+-0.070)
      channels=1, size=712, dtype=torch.int16, mf=channels_last       |         69.001 (+-0.078)         |                     |            69.590 (+-0.162)

      channels=3, size=256, dtype=torch.int8, mf=channels_last        |        177.378 (+-0.507)         |                     |           283.461 (+-2.957)
      channels=3, size=520, dtype=torch.int8, mf=channels_last        |        698.915 (+-1.840)         |                     |          1061.208 (+-10.449)
      channels=3, size=712, dtype=torch.int8, mf=channels_last        |        1299.365 (+-3.919)        |                     |          1957.424 (+-13.149)

      channels=3, size=256, dtype=torch.int8, mf=channels_first       |         17.955 (+-0.077)         |                     |            89.456 (+-0.285)
      channels=3, size=520, dtype=torch.int8, mf=channels_first       |         56.901 (+-0.081)         |                     |           339.802 (+-0.879)
      channels=3, size=712, dtype=torch.int8, mf=channels_first       |        103.629 (+-0.256)         |                     |           627.845 (+-1.185)

      channels=1, size=256, dtype=torch.float32, mf=channels_last     |         21.179 (+-0.077)         |   44.146 (+-0.260)  |            22.957 (+-0.138)
      channels=1, size=520, dtype=torch.float32, mf=channels_last     |         71.685 (+-0.155)         |  166.666 (+-0.730)  |            72.606 (+-0.124)
      channels=1, size=712, dtype=torch.float32, mf=channels_last     |        129.168 (+-0.288)         |  307.094 (+-1.571)  |           130.156 (+-0.453)

      channels=1, size=256, dtype=torch.float16, mf=channels_last     |         33.049 (+-0.089)         |                     |            33.056 (+-0.477)
      channels=1, size=520, dtype=torch.float16, mf=channels_last     |        116.635 (+-0.299)         |                     |           113.433 (+-0.891)
      channels=1, size=712, dtype=torch.float16, mf=channels_last     |        212.134 (+-0.413)         |                     |           204.394 (+-0.822)

      channels=3, size=256, dtype=torch.float64, mf=channels_last     |        207.214 (+-0.586)         |                     |           302.370 (+-0.670)
      channels=3, size=520, dtype=torch.float64, mf=channels_last     |        846.553 (+-2.301)         |                     |           1223.851 (+-5.280)
      channels=3, size=712, dtype=torch.float64, mf=channels_last     |        2251.687 (+-6.513)        |                     |          2711.557 (+-14.011)

      channels=1, size=256, dtype=torch.bfloat16, mf=channels_last    |         33.237 (+-0.072)         |                     |            33.101 (+-0.070)
      channels=1, size=520, dtype=torch.bfloat16, mf=channels_last    |        113.605 (+-0.337)         |                     |           117.067 (+-0.547)
      channels=1, size=712, dtype=torch.bfloat16, mf=channels_last    |        204.632 (+-0.487)         |                     |           212.590 (+-0.848)

      channels=1, size=256, dtype=torch.bool, mf=channels_last        |         7.950 (+-0.030)          |                     |            37.757 (+-0.080)
      channels=1, size=520, dtype=torch.bool, mf=channels_last        |         23.799 (+-0.080)         |                     |           136.571 (+-0.441)
      channels=1, size=712, dtype=torch.bool, mf=channels_last        |         37.970 (+-0.075)         |                     |           246.894 (+-0.926)

      channels=1, size=256, dtype=torch.bool, mf=channels_first       |         8.009 (+-0.077)          |                     |            37.800 (+-0.100)
      channels=1, size=520, dtype=torch.bool, mf=channels_first       |         23.861 (+-0.099)         |                     |           136.553 (+-0.519)
      channels=1, size=712, dtype=torch.bool, mf=channels_first       |         38.211 (+-0.104)         |                     |           246.939 (+-0.692)

Times are in microseconds (us).
```
[Source](https://gist.github.com/vfdev-5/c2ca615b522aeb1c4636dc8d948fec74#file-20221209-100405-pr_vs_nightly-md)

- AVX512 (only cases with speed-up or same perfs for channels last input)
```
[---------------------------------------------------------------------------- Horizontal flip ----------------------------------------------------------------------------]
                                                                      |  torch (1.14.0a0+giteb3e189) PR  |    Pillow (9.3.0)    |  torch (1.14.0.dev20221208+cu116) nightly
1 threads: ----------------------------------------------------------------------------------------------------------------------------------------------------------------
      channels=3, size=256, dtype=torch.int64, mf=channels_last       |        194.708 (+-9.566)         |                      |             372.067 (+-12.430)
      channels=3, size=520, dtype=torch.int64, mf=channels_last       |        765.151 (+-10.098)        |                      |            1524.231 (+-111.283)
      channels=3, size=712, dtype=torch.int64, mf=channels_last       |       1587.229 (+-88.117)        |                      |            2950.081 (+-92.322)

      channels=1, size=256, dtype=torch.int32, mf=channels_last       |         13.328 (+-0.375)         |   49.693 (+-1.193)   |              10.323 (+-0.333)
      channels=1, size=520, dtype=torch.int32, mf=channels_last       |         90.580 (+-0.812)         |  191.936 (+-4.369)   |              92.269 (+-0.980)
      channels=1, size=712, dtype=torch.int32, mf=channels_last       |        163.821 (+-3.174)         |  352.053 (+-10.909)  |             165.661 (+-4.436)

      channels=3, size=256, dtype=torch.uint8, mf=channels_last       |        206.862 (+-4.417)         |   49.336 (+-1.492)   |             287.373 (+-7.266)
      channels=3, size=520, dtype=torch.uint8, mf=channels_last       |        829.736 (+-15.857)        |  191.489 (+-5.645)   |            1166.126 (+-45.667)
      channels=3, size=712, dtype=torch.uint8, mf=channels_last       |       1540.953 (+-28.269)        |  352.171 (+-8.784)   |            2171.570 (+-82.740)

      channels=1, size=256, dtype=torch.int16, mf=channels_last       |         7.856 (+-0.131)          |                      |              7.943 (+-0.148)
      channels=1, size=520, dtype=torch.int16, mf=channels_last       |         34.750 (+-1.195)         |                      |              36.309 (+-0.716)
      channels=1, size=712, dtype=torch.int16, mf=channels_last       |         85.858 (+-0.729)         |                      |              87.306 (+-0.981)

      channels=3, size=256, dtype=torch.int8, mf=channels_last        |        206.896 (+-5.716)         |                      |             262.551 (+-6.598)
      channels=3, size=520, dtype=torch.int8, mf=channels_last        |        828.212 (+-13.441)        |                      |            1077.916 (+-28.810)
      channels=3, size=712, dtype=torch.int8, mf=channels_last        |       1542.748 (+-31.379)        |                      |            2003.661 (+-71.614)

      channels=3, size=256, dtype=torch.int8, mf=channels_first       |         11.038 (+-0.271)         |                      |             126.867 (+-5.590)
      channels=3, size=520, dtype=torch.int8, mf=channels_first       |         90.190 (+-1.185)         |                      |             501.446 (+-13.498)
      channels=3, size=712, dtype=torch.int8, mf=channels_first       |        165.797 (+-3.016)         |                      |             921.131 (+-20.500)

      channels=1, size=256, dtype=torch.float32, mf=channels_last     |         13.516 (+-0.578)         |   49.678 (+-1.966)   |              10.360 (+-0.256)
      channels=1, size=520, dtype=torch.float32, mf=channels_last     |         91.195 (+-0.830)         |  191.778 (+-4.742)   |              91.117 (+-0.855)
      channels=1, size=712, dtype=torch.float32, mf=channels_last     |        168.551 (+-3.352)         |  351.585 (+-8.230)   |             164.199 (+-3.725)

      channels=1, size=256, dtype=torch.float16, mf=channels_last     |         35.832 (+-0.840)         |                      |              35.087 (+-0.972)
      channels=1, size=520, dtype=torch.float16, mf=channels_last     |        133.624 (+-5.293)         |                      |             131.423 (+-6.002)
      channels=1, size=712, dtype=torch.float16, mf=channels_last     |        240.702 (+-5.213)         |                      |             236.876 (+-7.867)

      channels=3, size=256, dtype=torch.float64, mf=channels_last     |        192.351 (+-6.740)         |                      |             313.999 (+-12.141)
      channels=3, size=520, dtype=torch.float64, mf=channels_last     |        766.553 (+-16.669)        |                      |            1270.797 (+-49.828)
      channels=3, size=712, dtype=torch.float64, mf=channels_last     |       1501.700 (+-69.499)        |                      |            2427.303 (+-126.694)

      channels=1, size=256, dtype=torch.bfloat16, mf=channels_last    |         35.386 (+-0.801)         |                      |              34.539 (+-0.844)
      channels=1, size=520, dtype=torch.bfloat16, mf=channels_last    |        132.369 (+-4.107)         |                      |             130.926 (+-3.597)
      channels=1, size=712, dtype=torch.bfloat16, mf=channels_last    |        237.722 (+-6.680)         |                      |             237.072 (+-5.027)

      channels=1, size=256, dtype=torch.bool, mf=channels_last        |         6.796 (+-0.132)          |                      |              44.727 (+-0.905)
      channels=1, size=520, dtype=torch.bool, mf=channels_last        |         24.827 (+-0.669)         |                      |             166.758 (+-5.141)
      channels=1, size=712, dtype=torch.bool, mf=channels_last        |         42.392 (+-0.980)         |                      |             310.830 (+-6.130)

      channels=1, size=256, dtype=torch.bool, mf=channels_first       |         8.114 (+-0.141)          |                      |              44.776 (+-0.707)
      channels=1, size=520, dtype=torch.bool, mf=channels_first       |         24.787 (+-0.787)         |                      |             167.766 (+-5.004)
      channels=1, size=712, dtype=torch.bool, mf=channels_first       |         42.545 (+-0.636)         |                      |             313.715 (+-7.603)

Times are in microseconds (us).
```
[Source](https://gist.github.com/vfdev-5/c2ca615b522aeb1c4636dc8d948fec74#file-20221209-105633-pr_vs_nightly-avx512-md)

### Vertical flip

- AVX2 (all tested cases showing speed-up or same perfs)
```
[-------------------------------------------------------------------------- Vertical flip --------------------------------------------------------------------------]
                                                                      |  torch (1.14.0a0+giteb3e189) PR  |    Pillow (9.3.0)   |  torch (1.14.0a0+gitb0bd5c4) nightly
1 threads: ----------------------------------------------------------------------------------------------------------------------------------------------------------
      channels=3, size=256, dtype=torch.int64, mf=channels_last       |         93.125 (+-3.022)         |                     |           101.064 (+-0.436)
      channels=3, size=520, dtype=torch.int64, mf=channels_last       |        412.942 (+-57.066)        |                     |           461.463 (+-2.098)
      channels=3, size=712, dtype=torch.int64, mf=channels_last       |        1533.265 (+-4.071)        |                     |          1829.713 (+-14.311)

      channels=3, size=256, dtype=torch.int64, mf=channels_first      |        101.134 (+-0.924)         |                     |           102.858 (+-0.319)
      channels=3, size=520, dtype=torch.int64, mf=channels_first      |        421.679 (+-1.101)         |                     |           477.413 (+-1.809)
      channels=3, size=712, dtype=torch.int64, mf=channels_first      |        1550.418 (+-3.647)        |                     |           1877.143 (+-6.622)

      channels=1, size=256, dtype=torch.int32, mf=channels_last       |         20.961 (+-0.063)         |   19.515 (+-0.302)  |            21.980 (+-0.070)
      channels=1, size=520, dtype=torch.int32, mf=channels_last       |         71.199 (+-0.173)         |   70.199 (+-0.332)  |            95.262 (+-0.109)
      channels=1, size=712, dtype=torch.int32, mf=channels_last       |        128.532 (+-0.318)         |  127.325 (+-0.328)  |           167.190 (+-0.370)

      channels=1, size=256, dtype=torch.int32, mf=channels_first      |         21.206 (+-0.059)         |   19.471 (+-0.128)  |            21.469 (+-0.064)
      channels=1, size=520, dtype=torch.int32, mf=channels_first      |         71.284 (+-0.163)         |   70.124 (+-0.388)  |            94.988 (+-0.239)
      channels=1, size=712, dtype=torch.int32, mf=channels_first      |        129.017 (+-0.286)         |  128.088 (+-0.461)  |           167.115 (+-1.075)

      channels=3, size=256, dtype=torch.uint8, mf=channels_last       |         16.909 (+-0.057)         |   19.570 (+-0.353)  |            17.981 (+-0.072)
      channels=3, size=520, dtype=torch.uint8, mf=channels_last       |         55.163 (+-0.138)         |   70.218 (+-0.275)  |           107.938 (+-0.620)
      channels=3, size=712, dtype=torch.uint8, mf=channels_last       |         98.518 (+-0.121)         |  127.737 (+-0.486)  |           170.965 (+-0.436)

      channels=3, size=256, dtype=torch.uint8, mf=channels_first      |         18.150 (+-0.084)         |   19.758 (+-0.221)  |            18.122 (+-0.088)
      channels=3, size=520, dtype=torch.uint8, mf=channels_first      |         56.693 (+-0.200)         |   70.278 (+-0.386)  |            89.018 (+-0.206)
      channels=3, size=712, dtype=torch.uint8, mf=channels_first      |        100.409 (+-0.235)         |  127.772 (+-0.457)  |           168.072 (+-0.436)

      channels=1, size=256, dtype=torch.int16, mf=channels_last       |         12.817 (+-0.041)         |                     |            12.818 (+-0.049)
      channels=1, size=520, dtype=torch.int16, mf=channels_last       |         38.359 (+-0.081)         |                     |            63.378 (+-0.165)
      channels=1, size=712, dtype=torch.int16, mf=channels_last       |         68.246 (+-0.090)         |                     |           116.637 (+-0.583)

      channels=1, size=256, dtype=torch.int16, mf=channels_first      |         12.899 (+-0.054)         |                     |            12.649 (+-0.060)
      channels=1, size=520, dtype=torch.int16, mf=channels_first      |         38.404 (+-0.069)         |                     |            63.448 (+-0.108)
      channels=1, size=712, dtype=torch.int16, mf=channels_first      |         68.378 (+-0.104)         |                     |           116.415 (+-0.332)

      channels=3, size=256, dtype=torch.int8, mf=channels_last        |         17.071 (+-0.044)         |                     |            17.792 (+-0.050)
      channels=3, size=520, dtype=torch.int8, mf=channels_last        |         55.163 (+-0.100)         |                     |           108.539 (+-0.466)
      channels=3, size=712, dtype=torch.int8, mf=channels_last        |         98.537 (+-0.091)         |                     |           171.675 (+-0.553)

      channels=3, size=256, dtype=torch.int8, mf=channels_first       |         17.837 (+-0.071)         |                     |            18.355 (+-0.067)
      channels=3, size=520, dtype=torch.int8, mf=channels_first       |         56.051 (+-0.087)         |                     |            88.261 (+-0.129)
      channels=3, size=712, dtype=torch.int8, mf=channels_first       |        100.603 (+-0.245)         |                     |           169.067 (+-0.430)

      channels=1, size=256, dtype=torch.float32, mf=channels_last     |         21.204 (+-0.063)         |   19.607 (+-0.140)  |            22.202 (+-0.094)
      channels=1, size=520, dtype=torch.float32, mf=channels_last     |         71.356 (+-0.211)         |   69.844 (+-0.343)  |            94.614 (+-0.167)
      channels=1, size=712, dtype=torch.float32, mf=channels_last     |        129.087 (+-0.290)         |  127.065 (+-0.319)  |           166.513 (+-0.444)

      channels=1, size=256, dtype=torch.float32, mf=channels_first    |         21.196 (+-0.065)         |   19.156 (+-0.132)  |            21.516 (+-0.073)
      channels=1, size=520, dtype=torch.float32, mf=channels_first    |         71.422 (+-0.180)         |   70.296 (+-0.136)  |            94.913 (+-0.095)
      channels=1, size=712, dtype=torch.float32, mf=channels_first    |        129.045 (+-0.312)         |  128.023 (+-0.585)  |           166.089 (+-0.409)

      channels=1, size=256, dtype=torch.float16, mf=channels_last     |         12.770 (+-0.045)         |                     |            34.853 (+-0.089)
      channels=1, size=520, dtype=torch.float16, mf=channels_last     |         38.363 (+-0.064)         |                     |           131.969 (+-0.577)
      channels=1, size=712, dtype=torch.float16, mf=channels_last     |         67.954 (+-0.107)         |                     |           239.507 (+-0.835)

      channels=1, size=256, dtype=torch.float16, mf=channels_first    |         12.855 (+-0.067)         |                     |            35.124 (+-0.109)
      channels=1, size=520, dtype=torch.float16, mf=channels_first    |         38.725 (+-0.079)         |                     |           131.708 (+-0.586)
      channels=1, size=712, dtype=torch.float16, mf=channels_first    |         68.931 (+-0.086)         |                     |           239.022 (+-0.914)

      channels=3, size=256, dtype=torch.float64, mf=channels_last     |         90.277 (+-0.083)         |                     |           101.512 (+-0.285)
      channels=3, size=520, dtype=torch.float64, mf=channels_last     |        421.277 (+-1.030)         |                     |           471.913 (+-3.654)
      channels=3, size=712, dtype=torch.float64, mf=channels_last     |        1534.394 (+-7.572)        |                     |          1833.262 (+-12.185)

      channels=3, size=256, dtype=torch.float64, mf=channels_first    |        100.809 (+-0.328)         |                     |           103.166 (+-0.335)
      channels=3, size=520, dtype=torch.float64, mf=channels_first    |        425.535 (+-0.926)         |                     |           482.606 (+-1.450)
      channels=3, size=712, dtype=torch.float64, mf=channels_first    |        1550.832 (+-3.547)        |                     |           1859.098 (+-6.517)

      channels=1, size=256, dtype=torch.bfloat16, mf=channels_last    |         12.954 (+-0.051)         |                     |            12.744 (+-0.046)
      channels=1, size=520, dtype=torch.bfloat16, mf=channels_last    |         41.180 (+-0.064)         |                     |            63.362 (+-0.139)
      channels=1, size=712, dtype=torch.bfloat16, mf=channels_last    |         68.136 (+-0.142)         |                     |           117.009 (+-0.292)

      channels=1, size=256, dtype=torch.bfloat16, mf=channels_first   |         13.049 (+-0.052)         |                     |            12.792 (+-0.076)
      channels=1, size=520, dtype=torch.bfloat16, mf=channels_first   |         38.488 (+-0.092)         |                     |            63.451 (+-0.096)
      channels=1, size=712, dtype=torch.bfloat16, mf=channels_first   |         68.103 (+-0.091)         |                     |           116.693 (+-0.290)

      channels=1, size=256, dtype=torch.bool, mf=channels_last        |         7.572 (+-0.029)          |                     |            8.017 (+-0.071)
      channels=1, size=520, dtype=torch.bool, mf=channels_last        |         22.121 (+-0.061)         |                     |            23.614 (+-0.074)
      channels=1, size=712, dtype=torch.bool, mf=channels_last        |         36.896 (+-0.094)         |                     |            39.460 (+-0.084)

      channels=1, size=256, dtype=torch.bool, mf=channels_first       |         7.671 (+-0.028)          |                     |            8.034 (+-0.058)
      channels=1, size=520, dtype=torch.bool, mf=channels_first       |         21.989 (+-0.053)         |                     |            23.645 (+-0.063)
      channels=1, size=712, dtype=torch.bool, mf=channels_first       |         37.252 (+-0.072)         |                     |            39.477 (+-0.100)

      channels=1, size=256, dtype=torch.complex64, mf=channels_last   |         37.129 (+-0.052)         |                     |            37.801 (+-0.101)
      channels=1, size=520, dtype=torch.complex64, mf=channels_last   |        122.646 (+-0.230)         |                     |           139.074 (+-0.467)
      channels=1, size=712, dtype=torch.complex64, mf=channels_last   |        228.946 (+-0.736)         |                     |           257.589 (+-0.545)

      channels=1, size=256, dtype=torch.complex64, mf=channels_first  |         37.088 (+-0.070)         |                     |            37.894 (+-0.078)
      channels=1, size=520, dtype=torch.complex64, mf=channels_first  |        122.695 (+-0.268)         |                     |           138.933 (+-0.336)
      channels=1, size=712, dtype=torch.complex64, mf=channels_first  |        234.655 (+-0.454)         |                     |           255.787 (+-0.530)

Times are in microseconds (us).
```
[Source](https://gist.github.com/vfdev-5/c2ca615b522aeb1c4636dc8d948fec74#file-20221209-100440-pr_vs_nightly-md)

- AVX512 (all tested cases showing speed-up or same perfs)

```
[---------------------------------------------------------------------------- Vertical flip -----------------------------------------------------------------------------]
                                                                      |  torch (1.14.0a0+giteb3e189) PR  |    Pillow (9.3.0)   |  torch (1.14.0.dev20221208+cu116) nightly
1 threads: ---------------------------------------------------------------------------------------------------------------------------------------------------------------
      channels=3, size=256, dtype=torch.int64, mf=channels_last       |        122.544 (+-1.962)         |                     |             129.161 (+-1.809)
      channels=3, size=520, dtype=torch.int64, mf=channels_last       |        508.274 (+-4.790)         |                     |             533.872 (+-7.457)
      channels=3, size=712, dtype=torch.int64, mf=channels_last       |        951.176 (+-29.534)        |                     |            1073.603 (+-44.676)

      channels=3, size=256, dtype=torch.int64, mf=channels_first      |        127.872 (+-2.700)         |                     |             127.326 (+-2.666)
      channels=3, size=520, dtype=torch.int64, mf=channels_first      |        518.019 (+-4.157)         |                     |             538.094 (+-6.600)
      channels=3, size=712, dtype=torch.int64, mf=channels_first      |       1002.176 (+-42.545)        |                     |            1033.989 (+-42.137)

      channels=1, size=256, dtype=torch.int32, mf=channels_last       |         10.025 (+-0.135)         |   10.054 (+-0.369)  |              10.155 (+-0.285)
      channels=1, size=520, dtype=torch.int32, mf=channels_last       |         89.867 (+-0.994)         |   88.712 (+-0.622)  |             103.029 (+-2.254)
      channels=1, size=712, dtype=torch.int32, mf=channels_last       |        161.787 (+-2.080)         |  161.370 (+-1.801)  |             182.608 (+-7.031)

      channels=1, size=256, dtype=torch.int32, mf=channels_first      |         10.005 (+-0.277)         |   9.965 (+-0.338)   |              10.604 (+-0.334)
      channels=1, size=520, dtype=torch.int32, mf=channels_first      |         89.116 (+-0.996)         |   88.840 (+-0.608)  |             102.103 (+-2.111)
      channels=1, size=712, dtype=torch.int32, mf=channels_first      |        164.328 (+-3.284)         |  161.538 (+-2.739)  |             181.702 (+-3.770)

      channels=3, size=256, dtype=torch.uint8, mf=channels_last       |         8.853 (+-0.148)          |   10.292 (+-0.494)  |              8.961 (+-0.190)
      channels=3, size=520, dtype=torch.uint8, mf=channels_last       |         68.368 (+-1.158)         |   90.068 (+-1.780)  |              81.155 (+-0.945)
      channels=3, size=712, dtype=torch.uint8, mf=channels_last       |        125.458 (+-2.511)         |  163.150 (+-2.532)  |             147.039 (+-4.264)

      channels=3, size=256, dtype=torch.uint8, mf=channels_first      |         10.409 (+-0.435)         |   10.406 (+-0.351)  |              10.263 (+-0.252)
      channels=3, size=520, dtype=torch.uint8, mf=channels_first      |         69.077 (+-1.062)         |   90.057 (+-0.992)  |              79.910 (+-0.884)
      channels=3, size=712, dtype=torch.uint8, mf=channels_first      |        127.286 (+-2.789)         |  162.862 (+-2.953)  |             142.821 (+-2.119)

      channels=1, size=256, dtype=torch.int16, mf=channels_last       |         7.513 (+-0.143)          |                     |              7.364 (+-0.154)
      channels=1, size=520, dtype=torch.int16, mf=channels_last       |         33.140 (+-0.779)         |                     |              42.141 (+-0.820)
      channels=1, size=712, dtype=torch.int16, mf=channels_last       |         86.235 (+-1.187)         |                     |             104.205 (+-2.205)

      channels=1, size=256, dtype=torch.int16, mf=channels_first      |         7.410 (+-0.162)          |                     |              7.075 (+-0.126)
      channels=1, size=520, dtype=torch.int16, mf=channels_first      |         33.656 (+-0.914)         |                     |              40.991 (+-0.893)
      channels=1, size=712, dtype=torch.int16, mf=channels_first      |         86.087 (+-1.191)         |                     |             105.419 (+-1.801)

      channels=3, size=256, dtype=torch.int8, mf=channels_last        |         8.802 (+-0.196)          |                     |              8.627 (+-0.202)
      channels=3, size=520, dtype=torch.int8, mf=channels_last        |         66.348 (+-0.775)         |                     |              80.631 (+-1.832)
      channels=3, size=712, dtype=torch.int8, mf=channels_last        |        126.275 (+-2.318)         |                     |             144.597 (+-4.242)

      channels=3, size=256, dtype=torch.int8, mf=channels_first       |         10.255 (+-0.383)         |                     |              10.101 (+-0.335)
      channels=3, size=520, dtype=torch.int8, mf=channels_first       |         68.124 (+-0.849)         |                     |              79.286 (+-0.748)
      channels=3, size=712, dtype=torch.int8, mf=channels_first       |        127.118 (+-2.225)         |                     |             142.029 (+-2.507)

      channels=1, size=256, dtype=torch.float32, mf=channels_last     |         9.850 (+-0.453)          |   9.299 (+-0.253)   |              10.030 (+-0.234)
      channels=1, size=520, dtype=torch.float32, mf=channels_last     |         91.506 (+-1.319)         |   90.265 (+-0.824)  |             107.570 (+-2.093)
      channels=1, size=712, dtype=torch.float32, mf=channels_last     |        167.820 (+-3.883)         |  162.871 (+-2.397)  |             180.046 (+-8.952)

      channels=1, size=256, dtype=torch.float32, mf=channels_first    |         10.118 (+-0.359)         |   10.433 (+-0.479)  |              10.204 (+-0.344)
      channels=1, size=520, dtype=torch.float32, mf=channels_first    |         90.862 (+-1.486)         |   90.138 (+-0.969)  |             107.011 (+-1.801)
      channels=1, size=712, dtype=torch.float32, mf=channels_first    |        163.931 (+-3.653)         |  163.155 (+-2.673)  |             186.707 (+-2.248)

      channels=1, size=256, dtype=torch.float16, mf=channels_last     |         7.304 (+-0.134)          |                     |              24.141 (+-0.444)
      channels=1, size=520, dtype=torch.float16, mf=channels_last     |         35.186 (+-0.656)         |                     |             101.523 (+-1.465)
      channels=1, size=712, dtype=torch.float16, mf=channels_last     |         85.707 (+-0.841)         |                     |             192.640 (+-4.942)

      channels=1, size=256, dtype=torch.float16, mf=channels_first    |         7.286 (+-0.142)          |                     |              24.155 (+-0.555)
      channels=1, size=520, dtype=torch.float16, mf=channels_first    |         33.819 (+-1.009)         |                     |             101.620 (+-3.034)
      channels=1, size=712, dtype=torch.float16, mf=channels_first    |         84.811 (+-0.993)         |                     |             192.286 (+-4.707)

      channels=3, size=256, dtype=torch.float64, mf=channels_last     |        126.273 (+-2.519)         |                     |             128.831 (+-1.975)
      channels=3, size=520, dtype=torch.float64, mf=channels_last     |        551.861 (+-4.159)         |                     |             517.343 (+-4.501)
      channels=3, size=712, dtype=torch.float64, mf=channels_last     |       1102.465 (+-66.427)        |                     |            1224.532 (+-55.656)

      channels=3, size=256, dtype=torch.float64, mf=channels_first    |        129.965 (+-2.083)         |                     |             130.709 (+-2.261)
      channels=3, size=520, dtype=torch.float64, mf=channels_first    |        526.332 (+-5.354)         |                     |             515.399 (+-4.320)
      channels=3, size=712, dtype=torch.float64, mf=channels_first    |       1169.215 (+-78.889)        |                     |            1102.536 (+-51.178)

      channels=1, size=256, dtype=torch.bfloat16, mf=channels_last    |         7.478 (+-0.147)          |                     |              7.154 (+-0.162)
      channels=1, size=520, dtype=torch.bfloat16, mf=channels_last    |         33.836 (+-1.022)         |                     |              38.854 (+-0.648)
      channels=1, size=712, dtype=torch.bfloat16, mf=channels_last    |         85.483 (+-0.582)         |                     |              99.190 (+-2.202)

      channels=1, size=256, dtype=torch.bfloat16, mf=channels_first   |         7.416 (+-0.125)          |                     |              7.169 (+-0.121)
      channels=1, size=520, dtype=torch.bfloat16, mf=channels_first   |         34.958 (+-0.717)         |                     |              40.136 (+-0.784)
      channels=1, size=712, dtype=torch.bfloat16, mf=channels_first   |         85.505 (+-1.207)         |                     |              99.793 (+-2.065)

      channels=1, size=256, dtype=torch.bool, mf=channels_last        |         5.856 (+-0.178)          |                     |              5.824 (+-0.118)
      channels=1, size=520, dtype=torch.bool, mf=channels_last        |         12.030 (+-0.330)         |                     |              14.478 (+-0.554)
      channels=1, size=712, dtype=torch.bool, mf=channels_last        |         30.116 (+-0.639)         |                     |              31.163 (+-0.873)

      channels=1, size=256, dtype=torch.bool, mf=channels_first       |         5.804 (+-0.113)          |                     |              5.825 (+-0.102)
      channels=1, size=520, dtype=torch.bool, mf=channels_first       |         12.043 (+-0.363)         |                     |              14.240 (+-0.341)
      channels=1, size=712, dtype=torch.bool, mf=channels_first       |         30.001 (+-1.001)         |                     |              33.199 (+-0.430)

      channels=1, size=256, dtype=torch.complex64, mf=channels_last   |         29.941 (+-0.861)         |                     |              28.229 (+-0.904)
      channels=1, size=520, dtype=torch.complex64, mf=channels_last   |        173.244 (+-2.577)         |                     |             173.173 (+-2.260)
      channels=1, size=712, dtype=torch.complex64, mf=channels_last   |        323.548 (+-3.338)         |                     |             318.318 (+-2.764)

      channels=1, size=256, dtype=torch.complex64, mf=channels_first  |         29.001 (+-1.029)         |                     |              28.565 (+-2.074)
      channels=1, size=520, dtype=torch.complex64, mf=channels_first  |        173.078 (+-1.993)         |                     |             170.664 (+-1.722)
      channels=1, size=712, dtype=torch.complex64, mf=channels_first  |        324.782 (+-3.759)         |                     |             315.745 (+-2.600)

Times are in microseconds (us).

```

[Source](https://gist.github.com/vfdev-5/c2ca615b522aeb1c4636dc8d948fec74#file-20221209-105707-pr_vs_nightly-avx512-md)

Pull Request resolved: https://github.com/pytorch/pytorch/pull/89414
Approved by: https://github.com/peterbell10, https://github.com/lezcano"
pytorch/pytorch,ed050e7a18f3239a8083ff3cb1985a16af44722b,"Small fixes for better channels last performance (#89616)

1) don't codegen maxpool backward, it's exceedingly slow
2) better determine reduction variables for more accurate hints
3) deterministic iteration order for reduction arguments, take into account all full size reduction argument, for hints break ties to outer reduction

fixes #1653

Pull Request resolved: https://github.com/pytorch/pytorch/pull/89616
Approved by: https://github.com/jansel, https://github.com/Chillee"
pytorch/pytorch,d52f121dba96c8ec941b03e95630815fbfdeefca,"[Composable API]Common _State parent class for composable and wrapper FSDP (#89147)

**Why this PR?**

For the composable APIs implementation, sometimes the internal APIs may not have the application (FSDP, DDP) root module but only the local module. One example is the state_dict/optimizer_state_dict implementation of FSDP. These APIs  are designed to start with the root module of the model. It is tricky for these APIs to tell whether a random submodule is managed by either DDP or FSDP.

It will be useful to have APIs like:
`_get_module_state(module)`: return the composable state if this module is managed by composable API.
`_get_module_fsdp_state(module)`: return the FSDP state if this module is managed by FSDP.

**What does this PR propose?**
1. Make `_State` out of `_composable` module so that `FullyShardedDataParallel` can inherit from it.
2. A global `_module_state_mapping: Dict[nn.Module, _State]` that keeps the mapping of all submodules (not just root module) to the state.
3. Create `_get_module_state(module)` to look up `_module_state_mapping`.
4. Create `_get_module_fsdp_state(module)` that uses `_get_module_state(module)` to get the state then verifies if the state is `_FSDPState`.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/89147
Approved by: https://github.com/awgu"
pytorch/pytorch,65e762acc888277a6aee1e7c490299efaa347550,"[FSDP][optim_state_dict][5/N] Remove optim_inputs for sharded state_dict. (#89981)

The argument, `optim_inputs`, is being deprecated. Sharded optimizer state_dict APIs are not be used. It is safe to remove them.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/89981
Approved by: https://github.com/awgu"
pytorch/pytorch,043de8d1b1fefe24fde93b9fda584dad164efb3f,"[FSDP][optim_state_dict][3/N] Support use_orig_param optim_state_dict (non-broadcast version) (#89900)

**What:**
This PR add the optim state_dict support of `use_orig_params` with rank0_only is False. rank0_only support will be added in a following PR. The design of this PR focus on the simplicity and may not have good performance, especially for optim state_dict loading. Since optim state_dict loading is only called once in the beginning of the training, performance is not the major concern.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/89900
Approved by: https://github.com/awgu, https://github.com/rohan-varma"
pytorch/pytorch,1439ebd8998f6cca6025ff61bffaddb5f834415d,"Enable inductor perf test on GCP A100  (#90322)

This PR tries to enable inductor performance nightly testing on A100 runner provided by GCP. Currently these GCP runners were created and maintained using scripts in https://github.com/fairinternal/pytorch-gha-infra/pull/82.
For some reason the artifacts cannot (and does not need to) be uploaded to S3, so adding use-gha parameter to _linux-test.yml to avoid creating a new but mostly identical _linux-test.yml.

Workflow test results: https://github.com/pytorch/pytorch/actions/runs/3642340544/jobs/6149691109

Pull Request resolved: https://github.com/pytorch/pytorch/pull/90322
Approved by: https://github.com/anijain2305, https://github.com/seemethere, https://github.com/desertfire"
pytorch/pytorch,96a36c9a3b57ffca10ac4fa95d2a21f85cba78b6,"Fix: Apply clang-tidy to c10/core (#90699)

Enables clang-tidy on 'c10/core'. Request by @ezyang to extend coverage of clang-tidy for better performance linting.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/90699
Approved by: https://github.com/ezyang"
pytorch/pytorch,ff1bbc2773a31ab839438966266ed8ee206cb8c5,"Revert ""[reland][dynamo] use optimizers correctly in benchmarking (#87492)"" (#90746)

This reverts commit d91d7a322172da4d92672301f3cfa3344d544a9e.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/90746
Approved by: https://github.com/anijain2305"
pytorch/pytorch,eae0f3f5e3a4f8c5a37f1a869284f22f3dc30e0d,"Add mkl implementation for exponential on CPU (#69967)

### Description
Add mkl implementation for exponential on CPU to improve the performance of exponential.

### Testing
data type: float32
single socket (28cores):
```
before: torch.Size([10, 128, 10, 124])  0.065 s
        torch.Size([10, 128, 20, 124])  0.130 s

after:  torch.Size([10, 128, 10, 124])  5.9e-05 s
        torch.Size([10, 128, 20, 124])  0.000113 s
```
single core:
```
before: torch.Size([10, 128, 10, 124])  0.065 s
        torch.Size([10, 128, 20, 124])  0.130 s

after:  torch.Size([10, 128, 10, 124])  0.00117 s
        torch.Size([10, 128, 20, 124])  0.002347 s
```

Pull Request resolved: https://github.com/pytorch/pytorch/pull/69967
Approved by: https://github.com/frank-wei, https://github.com/jgong5"
pytorch/pytorch,06326a7721b3944d41641841719d3e012feec113,"[optim] skip .item calls in all optimizers when compiling with dynamo (#88173)

@mlazos: skips `item()` calls if compiling with dynamo, by defining a helper function `_get_value` which either returns the result of `.item()` or the scalar cpu tensor if compiling with dynamo. This was done because removing `item()` calls significantly regresses eager perf. Additionally, `_dispatch_sqrt` calls the appropriate sqrt function (math.sqrt, or torch.sqrt).

Fixes https://github.com/pytorch/torchdynamo/issues/1083

This PR will no longer be needed once symint support is default.

This PR closes all remaining graph breaks in the optimizers (!!)

Pull Request resolved: https://github.com/pytorch/pytorch/pull/88173
Approved by: https://github.com/albanD"
pytorch/pytorch,4ca2fc485c654e5855ada66d1d3d2d6688865fb5,"inductor(CPU): add Conv+binary+unary fusion filter (#90259)

For Conv+binary+unary fusion, we only support conv+add+relu, this PR adds a such check to fix TIMM failed models.
TODO: enable more Conv+binary+unary fusion to improve TIMM models' performance.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/90259
Approved by: https://github.com/EikanWang, https://github.com/jgong5, https://github.com/jansel"
pytorch/pytorch,8127724c3be0335adea9e9e771d22573eb992fef,"Skip some unittests (#90609)

* Skip a unittest that needs FFT if not built with FFT
* Mark a test with ""slow"": `python test/test_ops.py -k TestCompositeComplianceCUDA.test_forward_ad_svd_lowrank_cuda_float32` took >5min on my machine.
* Skip a flaky test that's marked ""expectedFailure"", similar to #90233
Pull Request resolved: https://github.com/pytorch/pytorch/pull/90609
Approved by: https://github.com/soumith"
pytorch/pytorch,181d37475da5cd85b6939d77197c1b5c7576a5c0,"Simple fix: add missing positional arg in init_optimizer() call (#90641)

Fixes #ISSUE_NUMBER

Pull Request resolved: https://github.com/pytorch/pytorch/pull/90641
Approved by: https://github.com/kit1980"
pytorch/pytorch,12671fe6202137a1ea8979f57a2f4d80cd92ebd1,"Reserve space for std::vector output in extract_tensors for nccl python bindings (#88203)

Optimizes the nccl python bindings to reserve space when converting PythonObject* into Tensors. This should reduce the number of unnecessary allocations in the nccl bindings as the std::vector grows.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/88203
Approved by: https://github.com/ezyang"
pytorch/pytorch,583d216c1a767fcb9a8fc33fc2f7a19ff20eba95,"Fix: [ATen] add more missing moves - part 2 (#89000)

Applies some more missing std::move found by static analysis. This should improve performance and reduce unnecessary copies. This PR only targets ATen for now.

And before you ask about the edits, std::move is optimal in a ternary operator as copy ellision cannot happen one. The best thing is probably rewriting it as an if else, but ultimately this should be performant enough.
Followup to #88512 and #88514

Pull Request resolved: https://github.com/pytorch/pytorch/pull/89000
Approved by: https://github.com/ezyang"
pytorch/pytorch,4a1633ca694b11ffcb1f44d802dbfa93bc25efc7,"[Inductor] GEMM Shape Padding Optimization (#90425)

Summary:
Optimize the shape padding in the following perspectives:
- Add BFloat16 support for AMP training and Float16 support for inference
- Optimize microbenchmark to avoid peak memory issue, and include profiling memory ops to make more accurate decision
- Add a flag to turn off/on padding dims N and M in `torch.bmm` due to expensive memory copy of `.contiguous` to avoid peak memory issues in internal models

Test Plan: CI

Differential Revision: D41724868

Pull Request resolved: https://github.com/pytorch/pytorch/pull/90425
Approved by: https://github.com/jianyuh"
pytorch/pytorch,912748e3b7e4cd02d8fb7b428ab083e444dde8df,"[SDP] Fix alignment check for efficient_attention (#90413)

Fixes a bug found using head_dim_size==100 on an a100 gpu. This PR contains stricter guards on the input shape. These constraints are taken from xformers: https://github.com/facebookresearch/xformers/blob/gh/danthe3rd/60/orig/xformers/ops/fmha/cutlass.py#L23
Pull Request resolved: https://github.com/pytorch/pytorch/pull/90413
Approved by: https://github.com/mikekgfb"
pytorch/pytorch,d91d7a322172da4d92672301f3cfa3344d544a9e,"[reland][dynamo] use optimizers correctly in benchmarking (#87492)

Reland https://github.com/pytorch/pytorch/pull/87311

mlazos: updated to use SGD to not add a bunch of additional memory allocations (like Adam)

Pull Request resolved: https://github.com/pytorch/pytorch/pull/87492
Approved by: https://github.com/desertfire"
pytorch/pytorch,aacafd2cba832a87eb9597537dbba3c32496fc21,"Fixed a couple of mistakes in type annotations in optim package (#90216)

Doing some tests with all Optimizer and LRScheduler classes in optim package, I noticed a couple of mistakes in type annotations, so created a pull request to fix them.

- In Optimizer class, incorrectly named parameter `default` instead of `defaults` in pyi file
- In SGD class, type for `maximize` and `differentiable` not available in either py or pyi files

I don't know if there is a plan to move all types from pyi to py files, so wasn't too sure where to fix what.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/90216
Approved by: https://github.com/janeyx99"
pytorch/pytorch,76f440f20afa95b41c1f261bdee1b4e00955921b,"[dynamo] Rewrite inplace addcdiv and inplace add (#90330)

Rewrite inplace addcdiv to a div, mul and inplace add to avoid graph break
Rewrite inplace add to a mul and inplace add to avoid graph break

Needed to close optimizer graph breaks

Pull Request resolved: https://github.com/pytorch/pytorch/pull/90330
Approved by: https://github.com/jansel"
pytorch/pytorch,e89685b0b541386825479bf120f6a1aa6d000238,"Revert ""[inductor] Use decomposition for _to_copy (#90314)""

This reverts commit 3fdb5f2dda7164f6282e80c39799843527d135e7.

Reverted https://github.com/pytorch/pytorch/pull/90314 on behalf of https://github.com/desertfire due to regresses performance on hf_Bert"
pytorch/pytorch,dc40b6d04320baf26b940225a65f40466ebf3664,"Upgrade oneDNN to v2.7.2 (#90051)

This PR is to upgrade oneDNN to v2.7.2.

### oneDNN v2.7.1 & 2.7.2 changes:
Fixes #89104
Updated ITT API version to 3.23.0

### Performance Benchmark
Use TorchBench test in ICX with 40 cores
Intel OpenMP & tcmalloc were preloaded
![image](https://user-images.githubusercontent.com/61222868/205240855-04e2d50f-8b3a-4097-9038-fdd0c0fc93b9.png)

Pull Request resolved: https://github.com/pytorch/pytorch/pull/90051
Approved by: https://github.com/XiaobingSuper, https://github.com/jgong5"
pytorch/pytorch,22a249e44e3fe5f92e6fad6567ca464158d95249,"Revert ""[Inductor] More robust stride and offset extraction from index expressions (#90184)""

This reverts commit 71f27f768839394ec226c37a763bd524d8589f07.

Reverted https://github.com/pytorch/pytorch/pull/90184 on behalf of https://github.com/ngimel due to catastrophically regresses performance"
pytorch/pytorch,b8b7480065d9aa78116a59e8e1a0dcb99ecfdda9,"[Checkpoint][2D][6/N] Add optimizer and update default_planner to core distributed (#90212)

This is the last PR for integrating 2D into core distributed.

This PR does the following:
1. Add optimizer.py: this adds ability to load a state_dict in conjunction with FSDP sharded optimzer state.
2. Update default_planner.py to support 2D checkpoint.
3. Add test_fsdp_optim_state.py as a unit test for No. 1.
4. Fix bug in torch/testing/_internal/distributed/checkpoint_utils.py
5. Rename the filename for the APIs that should be private. Will organize and cleanup further in following PRs. #90328

Docstring and integration test will be added in the following PRs.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/90212
Approved by: https://github.com/wanchaol"
pytorch/pytorch,4cdc96fb4f8649bc8291f14695e6a6d7381ac065,"Add hooks structure for passing around user provided hooks, add a new guard_failure_fn (#90371)

This PR introduces a new function we can pass to torch._dynamo.optimize - guard_failure_fn. Usage is in the PR, and the one stacked on top of it, but the gist of it is that it emits failed guard reason strings alongside code. This is useful for tests and debugging, as it gives far finer grained assertions and control than the compile counter alone.

This is a resubmit of https://github.com/pytorch/pytorch/pull/90129

Pull Request resolved: https://github.com/pytorch/pytorch/pull/90371
Approved by: https://github.com/ezyang"
pytorch/pytorch,22e363348c149788fd6a0bd870befc51b7a820bd,"[Vulkan] Partially fix and then disable copying of vulkan quantized tensors to cpu (#90275)

Summary:
Before this diff, copying of vulkan quantized tensors to cpu was broken. This was mainly caused because the shader only works properly with specific global and local work group sizes, and those specific sizes had been modified in earlier refactoring.

As part of this fix, an optimized version of the shader that performs the copying was written, to take advantage of the special case when the plane size (x*y) is multiple of 4).

After fixing this, and writing comprehensive tests, it was discovered that the copying still has issues on Android for specific input sizes, e.g. [1, 1, 11, 17]. These issues are currently unresolved, so, copying of quantized vulkan tensors to cpu has been disabled.

What is contained in this diff?
- Fix for existing issue
- New optimized shader (image_to_nchw_quantized_mul4)
- New comprehensive tests (which have been disabled)
- Disable the copying of quantized vulkan tensors to cpu until issues on Android are fixed.

Test Plan:
On Mac
```
cd ~/fbsource
buck1 run -c pt.vulkan_full_precision=1 //xplat/caffe2:pt_vulkan_quantized_api_test_binAppleMac\#macosx-arm64
```

On Android
```
cd ~/fbsource
buck1 build -c ndk.custom_libcxx=false -c pt.enable_qpl=0 -c pt.vulkan_full_precision=1 //xplat/caffe2:pt_vulkan_quantized_api_test_binAndroid\#android-arm64 --show-output
adb push buck-out/gen/xplat/caffe2/pt_vulkan_quantized_api_test_binAndroid\#android-arm64 /data/local/tmp/vulkan_quantized_api_test
adb shell ""/data/local/tmp/vulkan_quantized_api_test""
```

Reviewed By: kimishpatel

Differential Revision: D41047098

Pull Request resolved: https://github.com/pytorch/pytorch/pull/90275
Approved by: https://github.com/kimishpatel"
pytorch/pytorch,97e47a52b8872873533abc22f93a919222799864,"[Quant] Add fused linear-leaky_relu op for onednn backend (#88478)

**Summary**
Post op fusion can reduce data movement overhead and improve inference performance. This PR adds fused `linear-leaky_relu` op for `onednn` backend, which will be used for int8 inference with `onednn` backend. Cannot call this op with other quantization backends otherwise an error is thrown.

**Test Plan**
python test_quantization.py TestQuantizedLinear

Pull Request resolved: https://github.com/pytorch/pytorch/pull/88478
Approved by: https://github.com/jgong5, https://github.com/jerryzh168"
pytorch/pytorch,176b962f4b9a94586107885614667fe729e5f02a,"Revert ""[PT-D][Composability][1/N] Upstream NamedOptimizer from TorchRec (KeyedOptimizer in TR) (#89480)""

This reverts commit 31ec1a1ef7032508fc36f0b70692832acbeed72d.

Reverted https://github.com/pytorch/pytorch/pull/89480 on behalf of https://github.com/kit1980 due to Broke test_correct_module_names"
pytorch/pytorch,2d9267ba309bad1a30d73ac66a315c588859972f,"[dynamo] Rewrite addcdiv in dynamo to its constituent ops (#90227)

This avoids a graph break when `value` is used. This fixes a graph break in the variants of Adam and Adagrad optimizers.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/90227
Approved by: https://github.com/jansel"
pytorch/pytorch,31ec1a1ef7032508fc36f0b70692832acbeed72d,"[PT-D][Composability][1/N] Upstream NamedOptimizer from TorchRec (KeyedOptimizer in TR) (#89480)

In pytorch, the optim state_dict will always use number to index optimizer state_dict for parameters.

Now composability workstream need a FQN based way to index optimizer state_dict for parameters..

For example, SGD optimizer might have something in its `state_dict` like:

```
{'state':
  {0:
    {'momentum_buffer': tensor(...)},
  {1:
    {'momentum_buffer': tensor(...)},
  ...
}
'param_groups':
    [{'lr': 0.001, 'momentum': 0.9, 'dampening': 0, 'weight_decay': 0, 'nesterov': False, 'maximize': False, 'foreach': None, 'differentiable': False, 'params': [0, 1, 2, 3, 4, 5, 6, 7]}]
}
```

And in NamedOptimizer we want the `state_dict` can be:

```
{'state':
  {'net1.0.weight':
    {'momentum_buffer': tensor(...)},
  {'net1.0.bias':
    {'momentum_buffer': tensor(...)},
  ...
}
'param_groups':
    [{'lr': 0.001, 'momentum': 0.9, 'dampening': 0, 'weight_decay': 0, 'nesterov': False, 'maximize': False, 'foreach': None, 'differentiable': False, 'params': ['net1.0.weight', 'net1.0.bias', 'net2.0.weight', 'net2.0.bias', 'net3.weight', 'net3.bias', 'net4.1.weight', 'net4.1.bias']}]
}
```

We also want to support load_state_dict to enable optim `state_dict` override for NameOptimizer.

For the next couple PR/diffs, we also need to:
1. To make `NamedOptimizer` working with FSDP (like registering a hook for model wrapped with FSDP) and other PTD/PT components.
2. Make `NamedOptimizer` works well with apply_optim_in_backward
3. Upstream also `CombinedOptimizer`.

Differential Revision: [D41432088](https://our.internmc.facebook.com/intern/diff/D41432088/)

**NOTE FOR REVIEWERS**: This PR has internal Meta-specific changes or comments, please review them on [Phabricator](https://our.internmc.facebook.com/intern/diff/D41432088/)!
Pull Request resolved: https://github.com/pytorch/pytorch/pull/89480
Approved by: https://github.com/rohan-varma"
pytorch/pytorch,777ac632fb989119a95a95737d3d31c2ad1e52fe,"Added vectorized flip for uint8 (#90013)

Following https://github.com/pytorch/pytorch/pull/89414#discussion_r1036224613 just refactoring and adding `flip` method for `Vectorized<uint8>`. This should speed up torch.flip horizontal implementation similarly to what is reported in https://github.com/pytorch/pytorch/pull/89414

Pull Request resolved: https://github.com/pytorch/pytorch/pull/90013
Approved by: https://github.com/peterbell10, https://github.com/lezcano"
pytorch/pytorch,0bde8105727ab37cbba6d9989dc9012a379cda47,"Add more debug information for Inductor (#90008)

- Add graph index to the profile information of the Inductor kernel for better debugability.

  The generated code for different graphs could produce kernels with the same name. The side effect is that it is hard to identify the portion of E2E performance for these kernels because the profiler will aggregate the performance with the same kernel name regardless of different graphs. Hence, this PR added the graph index to the profile information to address this limitation.

- Label arbitrary code ranges for `eager` and `opt` modes for better debugability

  The profile information of dynamo benchmarks mixes the eager mode and opt mode. It is hard to separate the range for different modes. This PR added eager and opt marks to the profile information to address this limitation.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/90008
Approved by: https://github.com/jgong5, https://github.com/jansel"
pytorch/pytorch,6f4dea562de0a1388f3d63b5ae1ecc15a47d254f,"Implement post and pre hooks for optimizer (#89176)

Fixes #88446

Pull Request resolved: https://github.com/pytorch/pytorch/pull/89176
Approved by: https://github.com/albanD"
pytorch/pytorch,b058a0278630471f0591309cb7173eb76b0cf6c1,"TorchDynamo: enable convolution bn folding for functional bn (#89746)

Motivation: for Timm model, there is always use customer-defined BN which using F.batch_norm: https://github.com/rwightman/pytorch-image-models/blob/main/timm/models/layers/norm_act.py#L26, and the fx graph will be like:
```
-------------  ----------------------  ---------------------------------------  ---------------------------------------------------------------------------------------------------------  --------
placeholder    x                       x                                        ()                                                                                                         {}
call_module    self_conv               self_conv                                (x,)                                                                                                       {}
get_attr       self_bn_running_mean_1  self_bn_running_mean                     ()                                                                                                         {}
get_attr       self_bn_running_var     self_bn_running_var                      ()                                                                                                         {}
get_attr       self_bn_weight          self_bn_weight                           ()                                                                                                         {}
get_attr       self_bn_bias            self_bn_bias                             ()                                                                                                         {}
call_function  batch_norm              <function batch_norm at 0x7f07196cdf70>  (self_conv, self_bn_running_mean_1, self_bn_running_var, self_bn_weight, self_bn_bias, False, 0.1, 1e-05)  {}
call_module    self_bn_drop            self_bn_drop                             (batch_norm,)
```

the original conv+bn folding path doesn't work for **F.batch_norm**, but for **F.batch_norm** case, if its' parameters are const(attr of the module and will not be updated), we can also do the const folding's optimization. This PR will enable it and will improve the Timm models' performance.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/89746
Approved by: https://github.com/jgong5, https://github.com/jansel"
pytorch/pytorch,342d78d1a2dfccd058dac53ffe23f03f1bfdb1e8,"Cache guards once per variable tracker, rather than re-propagating them repeatedly (#89827)

This improves tracing performance of optimizer tracing significantly (2x). In essence this just removes the recursion from propagate because it is not necessary. ListVariables and ConstDictVariables already contain the guards from the items contained in them.

Adds two other optimizations for special cases of `recursively_contains`

helps with https://github.com/pytorch/torchdynamo/issues/1803

Pull Request resolved: https://github.com/pytorch/pytorch/pull/89827
Approved by: https://github.com/anijain2305, https://github.com/jansel"
pytorch/pytorch,c63afb283c39b8540b872cc0fa3d6fec6d50dcc8,"Disable dynamo on optimizer lazy initialization (#89902)

Helps with https://github.com/pytorch/torchdynamo/issues/1803

Separate out the group initialization and disable dynamo on it

Pull Request resolved: https://github.com/pytorch/pytorch/pull/89902
Approved by: https://github.com/soumith, https://github.com/albanD"
pytorch/pytorch,063bbeb3ba80f4308d4fba3e7b903b3d25a524f0,"Revert ""[quant] Explictly set default quantized engine instead of relying on the order of supported_qengines (#89804)""

This reverts commit 607ff6f4c10914a2a46bab90577cd083a6b3d46d.

Reverted https://github.com/pytorch/pytorch/pull/89804 on behalf of https://github.com/clee2000 due to breaking tests https://hud.pytorch.org/pytorch/pytorch/commit/607ff6f4c10914a2a46bab90577cd083a6b3d46d https://github.com/pytorch/pytorch/actions/runs/3596841274/jobs/6058297637 trunk label didnt kick off workflows fast enough"
pytorch/pytorch,768bd3fb4af298aaff29ad8febd281075b9a29d9,"Add `torch.compile` implementation (#89607)

`torch.compile` can be used either as decorator or to optimize model directly, for example:
```
@torch.compile
def foo(x):
  return torch.sin(x) + x.max()
```
or
```
mod = torch.nn.ReLU()
optimized_mod = torch.compile(mod, mode=""max-autotune"")
```

Pull Request resolved: https://github.com/pytorch/pytorch/pull/89607
Approved by: https://github.com/soumith"
pytorch/pytorch,8d333761a9fd7545f5b782b4b01893d4961231eb,"When dealing with dupe arguments, prefer leafifying if possible (#89896)

See code comment for details. I also had to do some extra fixes:

* `run_functionalized_fw_and_collect_metadata` now is able to handle duplicated arguments
* `aot_wrapper_dedupe` now always returns boxed compiled functions
* `aot_wrapper_dedupe` is now applied to inference compiler along with autograd compiler (preexisting)

Fixes https://github.com/pytorch/torchdynamo/issues/1939
Fixes DebertaV2ForQuestionAnswering DebertaForMaskedLM DebertaForQuestionAnswering DebertaV2ForMaskedLM

Repro command:

```
python benchmarks/dynamo/huggingface.py --performance --float32 -dcuda --training --inductor --no-skip --dashboard --only DebertaForQuestionAnswering --cold_start_latency
```

Signed-off-by: Edward Z. Yang <ezyang@fb.com>

Pull Request resolved: https://github.com/pytorch/pytorch/pull/89896
Approved by: https://github.com/bdhirsh"
pytorch/pytorch,6372f11d8d8c95038c11e4c6abe8e14dec8888a2,"RowwiseMoments: use float as acc type for bfloat16 inputs (#84405)

To fix https://github.com/pytorch/pytorch/issues/77507

Originally `utils::RowwiseMoments<BFloat16>` will still accululate on BFloat16,
which is not only slow but also introducing additional rounding errors.

This patch will do accumulation on float for the bfloat16 inputs:
each of bfloat16 vec (size 16) will be converted to two float vec (size 8),
and accumulated on m1(mean) and m2(rstd) vecs which are all float vecs.

No effect on float performance, will improve bfloat16 performance:
* avx512 single socket:
```
before: LayerNorm((1024,), eps=1e-05, elementwise_affine=True) : 32x128x1024: fp32: 0.210 ms; bf16: 0.770 ms
after:  LayerNorm((1024,), eps=1e-05, elementwise_affine=True) : 32x128x1024: fp32: 0.215 ms; bf16: 0.178 ms
```
* avx512 single core:
```
before: LayerNorm((1024,), eps=1e-05, elementwise_affine=True) : 32x128x1024: fp32: 2.661 ms; bf16: 12.267 ms
after:  LayerNorm((1024,), eps=1e-05, elementwise_affine=True) : 32x128x1024: fp32: 2.618 ms; bf16: 2.309 ms
```
* avx2 single socket:
```
before: LayerNorm((1024,), eps=1e-05, elementwise_affine=True) : 32x128x1024: fp32: 0.540 ms; bf16: 2.030 ms
after:  LayerNorm((1024,), eps=1e-05, elementwise_affine=True) : 32x128x1024: fp32: 0.527 ms; bf16: 0.458 ms
```
* avx2 single core:
```
before: LayerNorm((1024,), eps=1e-05, elementwise_affine=True) : 32x128x1024: fp32: 4.349 ms; bf16: 19.252 ms
after:  LayerNorm((1024,), eps=1e-05, elementwise_affine=True) : 32x128x1024: fp32: 4.416 ms; bf16: 3.524 ms
```

Pull Request resolved: https://github.com/pytorch/pytorch/pull/84405
Approved by: https://github.com/jgong5"
pytorch/pytorch,8a760ea922bb943c39fc9489127ce9cc3c318582,"Subscribing janeyx99 to optimizer PRs (#89943)

Adding myself to keep updated with what's up in the world of optimizers
Pull Request resolved: https://github.com/pytorch/pytorch/pull/89943
Approved by: https://github.com/albanD"
pytorch/pytorch,a6caa9c54bb5fce93a77e7407fd7e8993a767527,"Add a cpp wrapper for Inductor (#88167)

## Description
Implements https://github.com/pytorch/torchdynamo/issues/1556.
This PR adds a cpp wrapper to invoke the generated kernels. The cpp wrapper is turned off by default and can be turned on by setting:
```python
from torch._inductor import config
config.cpp_wrapper = True
```

### Example
The main part of the generated code:
```python
from torch.utils.cpp_extension import load_inline
wrapper = (
'''
#include <dlfcn.h>
#include <assert.h>
    std::tuple<at::Tensor, at::Tensor> call_0(std::tuple<at::Tensor, at::Tensor> args) {
    at::Tensor arg0_1, arg1_1;
    std::tie(arg0_1, arg1_1) = args;
    auto buf0 = at::empty_strided({8, 8}, {8, 1}, at::ScalarType::Float);
    auto buf1 = at::empty_strided({8, 8}, {1, 8}, at::ScalarType::Float);
    auto kernel0_lib = dlopen(""/tmp/torchinductor_user/kn/ckn7ubcn2qbkme2vx5r6antnh5sv6d3o3t6qwdfgfoupnxty6pnm.so"", RTLD_NOW);
    assert(kernel0_lib != nullptr);
    void (*kernel0)(const float*,const float*,float*,float*);
    *(void **) (&kernel0) = dlsym(kernel0_lib, ""kernel"");
    kernel0((float*)(arg0_1.data_ptr()), (float*)(arg1_1.data_ptr()), (float*)(buf0.data_ptr()), (float*)(buf1.data_ptr()));
    arg0_1.reset();
    arg1_1.reset();
    return std::make_tuple(buf0, buf1); }''' )

module = load_inline(
    name='inline_extension_c64wpbccpbre3th2k6oxwrjy5bhvxnmkdxkhcfxlsw7xpsg4eabu',
    cpp_sources=[wrapper],
    functions=['call_0'],
    extra_cflags=['-fPIC -Wall -std=c++14 -Wno-unused-variable -march=native -O3 -ffast-math -fno-finite-math-only -fopenmp'],
    extra_ldflags=['-shared  -lgomp'],
    extra_include_paths=['-I/home/user/pytorch/torch/include -I/home/user/pytorch/torch/include/torch/csrc/api/include -I/home/user/pytorch/torch/include/TH -I/home/user/pytorch/torch/include/THC -I/home/user/miniconda3/envs/pytorch/include/python3.7m'])

def _wrap_func(f):
    def g(args):
        return f(args)
    return g
call = _wrap_func(module.call_0)
```

### Next steps
The below items will be addressed in upcoming PRs.
- [x] Support Reduction: #88561
- [x] Support None: #88560
- [ ] Support ExternKernel
   - [x] ATen GEMM-related OPs: #88667
   - [ ] ATen Conv
   - [ ] Conv/GEMM fusion OPs
- [x] Cache the kernel loading part: #89742
- [ ] De-allocate input buffers when possible by leveraging CPython APIs
- [ ] Support Constant

Pull Request resolved: https://github.com/pytorch/pytorch/pull/88167
Approved by: https://github.com/jgong5, https://github.com/jansel, https://github.com/desertfire"
pytorch/pytorch,7cd6e6acad797608fd615eecc79d67f6cd023492,"add bf16 in fp32 out fast path for embedingbag in caffe2 perfkernel (#89198)

Add BF16 in FP32 out kernel into Caffe2 emb perfkernels. And also update the python code-gen files to generate the kernel.
The ut will be covered in the next PR(#89199) in this stack ( Tested by nn.EmbeddingBag with BF16 data type)

Pull Request resolved: https://github.com/pytorch/pytorch/pull/89198
Approved by: https://github.com/jgong5, https://github.com/kit1980"
pytorch/pytorch,296e1ba4d0d5a1766a4b31aa8a9474f4ba982506,"Row and column select support for block compressed sparse tensors (#88733)

As in the title:

- Support `select` and `select_copy` on block sparse compressed tensors
- Fixes incorrect results when selecting dense dimensions

The PR also improves the performance of indexing sparse compressed tensors considerably:

<details>

Before:

```python
In [3]: a=torch.rand((1000, 1000)).to_sparse_csr()

In [4]: %timeit a.select(0, 0)
606 µs ± 4.27 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)

In [5]: %timeit a.select(1, 0)
527 µs ± 57.7 ns per loop (mean ± std. dev. of 7 runs, 1000 loops each)

In [6]: %timeit a[0, 0]
617 µs ± 3.74 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)

In [7]: a = a.cuda()

In [8]: %timeit a.select(0, 0); torch.cuda.synchronize();
1.19 ms ± 137 ns per loop (mean ± std. dev. of 7 runs, 1000 loops each)

In [9]: %timeit a.select(1, 0); torch.cuda.synchronize();
1.2 ms ± 119 ns per loop (mean ± std. dev. of 7 runs, 1000 loops each)

In [10]: %timeit a[0, 0]; torch.cuda.synchronize();
1.23 ms ± 482 ns per loop (mean ± std. dev. of 7 runs, 1000 loops each)
```

This PR:

```python
In [3]: a=torch.rand((1000, 1000)).to_sparse_csr()

In [4]: %timeit a.select(0, 0)
4.75 µs ± 8.94 ns per loop (mean ± std. dev. of 7 runs, 100000 loops each)

In [5]: %timeit a.select(1, 0)
565 µs ± 156 ns per loop (mean ± std. dev. of 7 runs, 1000 loops each)

In [6]: %timeit a[0, 0]
13.1 µs ± 435 ns per loop (mean ± std. dev. of 7 runs, 100000 loops each)

In [7]: a = a.cuda()

In [8]: %timeit a.select(0, 0); torch.cuda.synchronize();
21.6 µs ± 23.9 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)

In [9]: %timeit a.select(1, 0); torch.cuda.synchronize();
1.15 ms ± 3.13 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)

In [10]: %timeit a[0, 0]; torch.cuda.synchronize();
63.7 µs ± 2.5 µs per loop (mean ± std. dev. of 7 runs, 10000 loops each)
```

</details>

Pull Request resolved: https://github.com/pytorch/pytorch/pull/88733
Approved by: https://github.com/nikitaved, https://github.com/amjames, https://github.com/cpuhrsch"
pytorch/pytorch,0cc0e5ef65c09983e6e7127de9d16176a1219e91,"[PT-D][Checkpoint]Add MultiThreaded FileSystemWriter for distributed checkpointing and Update tests  (#87987)

This PR includes:

Changes from @kumpera (https://github.com/pytorch/pytorch/pull/86327): adding MultiThreaded FileSystemWriter for distributed checkpointing, which adds two knobs to FileSystemWriter: thread_count and per_thread_copy_ahead. This increases up to 50% performance improvement on 32 GPUS workloads on AWS.
Add parametrize tests to /test/distributed/_shard/checkpoint/test_file_system_checkpoint.py and /test/distributed/_shard/checkpoint/test_file_system_checkpoint_cpu.py
Modify @with_comms in ShardedTensorTestBase to take in *args and **kwargs.
Tests:

```
python3 test/distributed/checkpoint/test_file_system_checkpoint_cpu.py
```

test/distributed/checkpoint/test_file_system_checkpoint.py(GPU tests) runs fine locally but would timeout on CI. We will use thread-based PG and update this test in following PR.

[T134844615]

## Add docstring and update comments in the following PRs.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/87987
Approved by: https://github.com/fduwjj"
pytorch/pytorch,447283752c760ae20bb35efb13e8cec4653f3c67,"Update DDP docs for Dynamo/DDPOptimizer (#89096)

Pull Request resolved: https://github.com/pytorch/pytorch/pull/89096
Approved by: https://github.com/msaroufim"
pytorch/pytorch,f4707ae00472de746ebe0d5e5f589ba7d299a49a,"Add arguments to collect_results (#89611)

Fixes https://github.com/pytorch/torchdynamo/issues/1901. Test script:
```python
import copy

import torch
import torch._dynamo as dynamo
import torch._dynamo.config

dynamo.config.repro_after = ""dynamo""
dynamo.config.repro_level = 4

def custom_backend(gm: torch.fx.GraphModule, example_inputs):
    gm = copy.deepcopy(gm)
    for node in gm.graph.nodes:
        if len(node.args) > 1:
            node.target = torch.add
            node.args = (node.args[0], 0)
    gm.recompile()
    return gm

inp = torch.ones(5)
inp.requires_grad_(True)

@dynamo.optimize(custom_backend)
def foo(x):
    x = x * x
    return x.sum()

y = foo(inp)
print(y)
y.backward()
print(inp.grad)
```
Before, the script will finish but output an incorrect gradient. After the change, the accuracy minifier is triggered.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/89611
Approved by: https://github.com/ezyang"
pytorch/pytorch,3d47c74cfec2b4804ba588d1ac8b42c7db555cbb,"Update code style for optimizer code (#89862)

Separating out whitespace-only changes
Pull Request resolved: https://github.com/pytorch/pytorch/pull/89862
Approved by: https://github.com/albanD, https://github.com/soumith"
pytorch/pytorch,c599cf24ad01a4befa58642f9e703792f53b79fa,"[FSDP] Another fix for `DTensor`, `use_orig_params=True` (#89845)

The issue for `test_2d_parallel.py` is that `DTensor` does not support the idiom `param.data = view` where `view` is a `DTensor`. To work around this, we do not preserve the parameter variable `param` and instead create a new parameter variable altogether via `nn.Parameter(view)`. Preserving the parameter variable when unsharded was not a strict requirement -- it just made sense to do that if we are already doing that when _sharded_, where it _is_ a strict requirement to support the optimizer step. The sharded case is not an issue for 2D because sharded implies local tensor, not `DTensor`.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/89845
Approved by: https://github.com/zhaojuanmao"
pytorch/pytorch,a029ec2c88edf430e682789684f1b27bfe607bf2,"Move gpu slow tests to sm86 (#87880)

NVFuser tests (which are slow tests) would be better to run on more
modern GPU hardware.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/87880
Approved by: https://github.com/malfet"
pytorch/pytorch,e1dbd9a288f6d9c487c537349aaf8d5b687486a8,"Revert ""[GHA] Decrease Windows test timeout to 120 minutes (#89694)""

This reverts commit faa032c5e58502de6ea461e531109d2acc22e56a.

Reverted https://github.com/pytorch/pytorch/pull/89694 on behalf of https://github.com/clee2000 due to broke periodic b/c they take ~2.5 hrs, also broke mem leak check b/c its slow, should probably look into having this be a parameter"
pytorch/pytorch,40dd03eeaaa42d55ff4c774facd9209e602e0529,"[dynamo] Don't copy the graph during checkpointing (copy_graphstate) (#89232)

copy_graphstate is called a ton, this makes copy_graphstate a lot faster, helps with https://github.com/pytorch/torchdynamo/issues/1803

tag each graph node with a timestamp, when checkpointing store the timestamp, when restoring remove nodes older than the timestamp stored in the state. This essentially has the same behavior as the original impl, just doesn't copy the whole graph.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/89232
Approved by: https://github.com/jansel"
pytorch/pytorch,91899a9ebd61040c0babcd4505b1984880c0e331,"add memory_tracker tool to help profiling memory usages (#88825)

Adding a memory_tracker API to show operator level memory traces for allocated_memory, active_memory and reserved memory stats, it gave the summary about top 20 operators that generate memories as well.

The implementation mainly uses torchDispatchMode and module hooks to get traces and add markers.

Will add following up PRs:
1. allow tracing more than 1 iteration
2. dump json data for visualization
3. add unit test for DDP training
4. add unit test for FSDP training
5. add unit test for activation checkpointing + DDP/FSDP training
6. add traces for activation memories and top operators that generate activation memories
7. print summaries for more breakdowns like model size, optimizer states, etc
8. add traces for temporary memories or memories consumed by cuda streams or nccl library if possible
9. connect the tool with OOM memory debugging
10. add dynamic programming (dp) algorithm to find best activation checkpointing locations based on the operator level activation memory traces
11. add same traces & dp algorithm for module level memory stats, as FSDP wrapping depends on module level memories, for some model users/not model authors, if they have to apply activation checkpointing on module level, they need module level memory traces as well

======================================================

Current test result for the memory_tracker_example.py on notebook:

Top 20 ops that generates memory are:
bn1.forward.cudnn_batch_norm.default_0: 98.0009765625MB
maxpool.forward.max_pool2d_with_indices.default_0: 74.5MB
layer1.0.conv1.backward.max_pool2d_with_indices_backward.default_0: 49.0MB
layer1.0.bn1.forward.cudnn_batch_norm.default_1: 24.5009765625MB
layer1.0.bn2.forward.cudnn_batch_norm.default_2: 24.5009765625MB
layer1.1.bn1.forward.cudnn_batch_norm.default_3: 24.5009765625MB
layer1.1.bn2.forward.cudnn_batch_norm.default_4: 24.5009765625MB
layer1.2.bn1.forward.cudnn_batch_norm.default_5: 24.5009765625MB
layer1.2.bn2.forward.cudnn_batch_norm.default_6: 24.5009765625MB
layer1.0.conv1.forward.convolution.default_1: 24.5MB
layer1.0.conv2.forward.convolution.default_2: 24.5MB
layer1.1.conv1.forward.convolution.default_3: 24.5MB
layer1.1.conv2.forward.convolution.default_4: 24.5MB
layer1.2.conv1.forward.convolution.default_5: 24.5MB
layer1.2.conv2.forward.convolution.default_6: 24.5MB
maxpool.backward.threshold_backward.default_32: 23.5MB
layer2.0.downsample.backward.convolution_backward.default_26: 12.2802734375MB
layer2.0.bn1.forward.cudnn_batch_norm.default_7: 12.2509765625MB
layer2.0.bn2.forward.cudnn_batch_norm.default_8: 12.2509765625MB
layer2.0.downsample.1.forward.cudnn_batch_norm.default_9: 12.2509765625MB

<img width=""1079"" alt=""Screen Shot 2022-11-10 at 10 03 06 AM"" src=""https://user-images.githubusercontent.com/48731194/201172577-ddfb769c-fb0f-4962-80df-92456b77903e.png"">

Pull Request resolved: https://github.com/pytorch/pytorch/pull/88825
Approved by: https://github.com/awgu"
pytorch/pytorch,7860fcc245980cc7feb78433ed4696ae6a9bb0cd,"Enable DDPOptimizer by default in dynamo (#88523)

Performance benchmarks on 6 popular models from 1-64 GPUs compiled with
torchinductor show performance gains or parity with eager, and showed
regressions without DDPOptimizer.  *Note: resnet50 with small batch size shows a regression with optimizer, in part due to failing to compile one subgraph due to input mutation, which will be fixed.
(hf_Bert, hf_T5_large, hf_T5, hf_GPT2_large, timm_vision_transformer, resnet50)

Correctness checks are implemented in CI (test_dynamo_distributed.py),
via single-gpu benchmark scripts iterating over many models
(benchmarks/dynamo/torchbench.py/timm_models.py/huggingface.py),
and via (multi-gpu benchmark scripts in torchbench)[https://github.com/pytorch/benchmark/tree/main/userbenchmark/ddp_experiments].

Pull Request resolved: https://github.com/pytorch/pytorch/pull/88523
Approved by: https://github.com/davidberard98"
pytorch/pytorch,2e0cd7c8bdbbb66daac72f9c23fbb0bdfa79c78a,"Add meta implementation for _efficientzerotensor (#88936)

`_efficientzerotensor` is used in several backwards formulas, so its
lack of meta implementation makes those functions untracable.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/88936
Approved by: https://github.com/anjali411"
pytorch/pytorch,b589e726d9abef2cf0e71dce67f0f3d9c5c176ea,"Refactor how AOTAutograd backends are defined (#89736)

There was a lot of strangeness in how AOTAutograd backends were previously defined. This refactor replaces the strangeness with something simple and straightforward. The improvements:

- There is no longer a footgun aot_autograd ""backend"" which doesn't actually work. No more mistyping `torch._dynamo.optimize(""aot_autograd"")` when you meant ""aot_eager""
- Deleted aot_print because it's annoying and anyway there's no uses of it
- Instead of having BOTH the backend Subgraph and AotAutogradStrategy, there is now only an aot_autograd function which takes the kwargs to configure AOTAutograd, and then gives you a compiler function that does AOTAutograd given those kwargs. Easy.
- The primary downside is that we are now eagerly populating all of the kwargs, and that can get us into import cycle shenanigans. Some cycles I resolved directly (e.g., we now no longer manually disable the forward function before passing it to aot_autograd; aot_autograd it does it for us), but for getting inductor decompositions I had to make it take a lambda so I could lazily populate the decomps later.

New code is 130 lines shorter!

Signed-off-by: Edward Z. Yang <ezyang@fb.com>

Pull Request resolved: https://github.com/pytorch/pytorch/pull/89736
Approved by: https://github.com/anjali411, https://github.com/albanD"
pytorch/pytorch,cf4969d9d6884d675ee79193e24b88e57566f1e9,"[ROCm] Replace layer_norm_grad_input_kernel with cuComputeGradInput for ROCm (#87726)

We observed that the native PyTorch LayerNormBackwardKernelImplInternal has suboptimal performance for certain input sizes on AMD GPUs especially when fs (=config_m in our benchmark script) is large and bs (=config_n in our benchmark script) is small (commonly seen in [the CvT model](https://arxiv.org/abs/2103.15808)) in the benchmark script of https://github.com/pytorch/pytorch/pull/68238#issue-1051621716 on AMD GPUs.

This PR is to replace layer_norm_grad_input_kernel with the Apex cuComputeGradInput kernel with some ROCm-specific parameter tuning when fs (=config_m) is larger than or equal to `32768` on AMD GPUs. Some of the code changes in LayerNormBackwardKernelImplInternal are from another PR: https://github.com/pytorch/pytorch/pull/87635

We used the same benchmark script in the previous PR and tested the optimized kernel with various input shapes on AMD MI100 GPU.

**At [the previous PR](https://github.com/pytorch/pytorch/pull/87635):**
<html xmlns:v=""urn:schemas-microsoft-com:vml""
xmlns:o=""urn:schemas-microsoft-com:office:office""
xmlns:x=""urn:schemas-microsoft-com:office:excel""
xmlns=""http://www.w3.org/TR/REC-html40"">

<head>

<meta name=ProgId content=Excel.Sheet>
<meta name=Generator content=""Microsoft Excel 15"">
<link id=Main-File rel=Main-File
href=""file:///C:/Users/hubertlu/AppData/Local/Temp/msohtmlclip1/01/clip.htm"">
<link rel=File-List
href=""file:///C:/Users/hubertlu/AppData/Local/Temp/msohtmlclip1/01/clip_filelist.xml"">
<!--table
	{mso-displayed-decimal-separator:""\."";
	mso-displayed-thousand-separator:""\,"";}
@page
	{mso-header-data:""&L&\0022Arial\0022&10&K0000FF \[AMD Official Use Only - General\]&1\#\000D"";
	margin:.75in .7in .75in .7in;
	mso-header-margin:.3in;
	mso-footer-margin:.3in;}
tr
	{mso-height-source:auto;}
col
	{mso-width-source:auto;}
br
	{mso-data-placement:same-cell;}
td
	{padding-top:1px;
	padding-right:1px;
	padding-left:1px;
	mso-ignore:padding;
	color:black;
	font-size:11.0pt;
	font-weight:400;
	font-style:normal;
	text-decoration:none;
	font-family:Calibri, sans-serif;
	mso-font-charset:0;
	mso-number-format:General;
	text-align:general;
	vertical-align:bottom;
	border:none;
	mso-background-source:auto;
	mso-pattern:auto;
	mso-protection:locked visible;
	white-space:nowrap;
	mso-rotate:0;}
.xl65
	{color:windowtext;}
-->
</head>

<body link=""#0563C1"" vlink=""#954F72"">

M | N | fwd (half) | fwdbwd (half) | fwd (float) | fwdbwd (float)
-- | -- | -- | -- | -- | --
50432 | 384 | 0.38589 | 0.92603 | 0.38367 | 1.15148
50176 | 384 | 0.38719 | 0.91579 | 0.37815 | 1.13761
200704 | 192 | 0.99787 | 2.39954 | 0.98996 | 2.54284
802816 | 64 | 3.66525 | 7.96952 | 3.61293 | 7.69946
200 | 256 | 0.06578 | 0.34613 | 0.06966 | 0.35449
1000 | 256 | 0.07837 | 0.37631 | 0.07725 | 0.37758
6000 | 256 | 0.09318 | 0.3788 | 0.09202 | 0.37989
6272 | 256 | 0.08694 | 0.36267 | 0.08703 | 0.3615
200 | 512 | 0.06975 | 0.34506 | 0.06973 | 0.34208
1000 | 512 | 0.07012 | 0.36363 | 0.07307 | 0.36741
6000 | 512 | 0.09725 | 0.36251 | 0.09908 | 0.37078
6272 | 512 | 0.09899 | 0.36519 | 0.10068 | 0.37514
200 | 1024 | 0.07188 | 0.33896 | 0.0712 | 0.34683
1000 | 1024 | 0.07357 | 0.3625 | 0.0734 | 0.3598
6000 | 1024 | 0.12642 | 0.38949 | 0.12973 | 0.5035
6272 | 1024 | 0.12901 | 0.40759 | 0.13609 | 0.51871
200 | 1536 | 0.06998 | 0.34782 | 0.07419 | 0.3514
1000 | 1536 | 0.07987 | 0.37915 | 0.07888 | 0.37264
6000 | 1536 | 0.15401 | 0.47524 | 0.15416 | 0.68609
6272 | 1536 | 0.15286 | 0.48843 | 0.17681 | 0.72997
200 | 2048 | 0.07054 | 0.34791 | 0.07289 | 0.35138
1000 | 2048 | 0.07767 | 0.37954 | 0.08554 | 0.37464
6000 | 2048 | 0.18744 | 0.5811 | 0.25004 | 0.93338
6272 | 2048 | 0.20037 | 0.63398 | 0.26918 | 0.97018
200 | 3072 | 0.07687 | 0.36739 | 0.08917 | 0.37845
1000 | 3072 | 0.09323 | 0.38901 | 0.09739 | 0.39823
6000 | 3072 | 0.24314 | 0.89029 | 0.38093 | 1.30719
6272 | 3072 | 0.26079 | 0.92023 | 0.38352 | 1.51012
128 | 2097152 | 6.17775 | 23.876 | 10.27952 | 30.10848
256 | 1048576 | 4.51855 | 19.47637 | 10.07609 | 29.42678
512 | 524288 | 4.13615 | 18.80888 | 10.07853 | 32.29804
1024 | 262144 | 4.47397 | 17.88388 | 9.50367 | 31.15699
2048 | 131072 | 4.2458 | 16.70852 | 9.17979 | 30.51708
4096 | 65536 | 4.24412 | 16.43098 | 8.97651 | 30.1617
8192 | 32768 | 4.24556 | 16.09038 | 8.77001 | 30.3643
16384 | 16384 | 4.14642 | 15.80355 | 8.82402 | 30.35291
32768 | 8192 | 4.12599 | 15.68897 | 8.82605 | 30.43423

</body>

</html>

----

**At this PR:**

<html xmlns:v=""urn:schemas-microsoft-com:vml""
xmlns:o=""urn:schemas-microsoft-com:office:office""
xmlns:x=""urn:schemas-microsoft-com:office:excel""
xmlns=""http://www.w3.org/TR/REC-html40"">

<head>

<meta name=ProgId content=Excel.Sheet>
<meta name=Generator content=""Microsoft Excel 15"">
<link id=Main-File rel=Main-File
href=""file:///C:/Users/hubertlu/AppData/Local/Temp/msohtmlclip1/01/clip.htm"">
<link rel=File-List
href=""file:///C:/Users/hubertlu/AppData/Local/Temp/msohtmlclip1/01/clip_filelist.xml"">
<!--table
	{mso-displayed-decimal-separator:""\."";
	mso-displayed-thousand-separator:""\,"";}
@page
	{mso-header-data:""&L&\0022Arial\0022&10&K0000FF \[AMD Official Use Only - General\]&1\#\000D"";
	margin:.75in .7in .75in .7in;
	mso-header-margin:.3in;
	mso-footer-margin:.3in;}
tr
	{mso-height-source:auto;}
col
	{mso-width-source:auto;}
br
	{mso-data-placement:same-cell;}
td
	{padding-top:1px;
	padding-right:1px;
	padding-left:1px;
	mso-ignore:padding;
	color:black;
	font-size:11.0pt;
	font-weight:400;
	font-style:normal;
	text-decoration:none;
	font-family:Calibri, sans-serif;
	mso-font-charset:0;
	mso-number-format:General;
	text-align:general;
	vertical-align:bottom;
	border:none;
	mso-background-source:auto;
	mso-pattern:auto;
	mso-protection:locked visible;
	white-space:nowrap;
	mso-rotate:0;}
.xl65
	{color:windowtext;}
.xl66
	{background:yellow;
	mso-pattern:black none;}
-->
</head>

<body link=""#0563C1"" vlink=""#954F72"">

M | N | fwd (half) | fwdbwd (half) | fwd (float) | fwdbwd (float)
-- | -- | -- | -- | -- | --
50432 | 384 | 0.38667 | 0.84133 | 0.37916 | 1.01222
50176 | 384 | 0.3814 | 0.87266 | 0.37858 | 1.04399
200704 | 192 | 0.99902 | 2.14386 | 0.98973 | 2.33265
802816 | 64 | 3.66578 | 6.85376 | 3.6092 | 7.00331
200 | 256 | 0.06607 | 0.34176 | 0.07009 | 0.34548
1000 | 256 | 0.06947 | 0.36461 | 0.07902 | 0.37851
6000 | 256 | 0.09319 | 0.37432 | 0.09342 | 0.36927
6272 | 256 | 0.09544 | 0.37565 | 0.09476 | 0.37377
200 | 512 | 0.07935 | 0.364 | 0.07891 | 0.36894
1000 | 512 | 0.07676 | 0.37552 | 0.07957 | 0.37564
6000 | 512 | 0.10472 | 0.37504 | 0.1051 | 0.38782
6272 | 512 | 0.1069 | 0.36662 | 0.10062 | 0.38506
200 | 1024 | 0.07793 | 0.36561 | 0.08023 | 0.35019
1000 | 1024 | 0.07426 | 0.36729 | 0.07345 | 0.35851
6000 | 1024 | 0.12729 | 0.39219 | 0.12974 | 0.51526
6272 | 1024 | 0.13622 | 0.41627 | 0.14252 | 0.52926
200 | 1536 | 0.07615 | 0.36621 | 0.0797 | 0.3695
1000 | 1536 | 0.08327 | 0.38174 | 0.07938 | 0.37573
6000 | 1536 | 0.14894 | 0.46197 | 0.15268 | 0.63814
6272 | 1536 | 0.15368 | 0.48818 | 0.16309 | 0.71441
200 | 2048 | 0.06935 | 0.36691 | 0.07258 | 0.35548
1000 | 2048 | 0.07738 | 0.36388 | 0.08036 | 0.36452
6000 | 2048 | 0.18757 | 0.58573 | 0.23701 | 0.92915
6272 | 2048 | 0.1938 | 0.61628 | 0.26475 | 0.96896
200 | 3072 | 0.07884 | 0.3673 | 0.07724 | 0.37869
1000 | 3072 | 0.09342 | 0.38193 | 0.09822 | 0.38646
6000 | 3072 | 0.24452 | 0.86776 | 0.38251 | 1.3036
6272 | 3072 | 0.25971 | 0.91053 | 0.38744 | 1.39039
128 | 2097152 | 6.06752 | 23.26379 | 9.87466 | 29.81851
256 | 1048576 | 4.50336 | 19.4614 | 10.11239 | 29.25554
512 | 524288 | 4.12649 | 18.72831 | 10.054 | 32.26784
1024 | 262144 | 4.40855 | 17.77993 | 9.38856 | 31.18679
2048 | 131072 | 4.18716 | 16.74615 | 9.14487 | 30.24603
4096 | 65536 | 4.17374 | 16.34444 | 8.94894 | 30.0326
8192 | 32768 | 4.19095 | 16.05751 | 8.70358 | 30.14669
16384 | 16384 | 4.15404 | 15.83771 | 8.80042 | 30.5022
32768 | 8192 | 4.12515 | 15.5657 | 8.66138 | 28.87386

</body>

</html>

---

**Performance Improvement (%)**

<html xmlns:v=""urn:schemas-microsoft-com:vml""
xmlns:o=""urn:schemas-microsoft-com:office:office""
xmlns:x=""urn:schemas-microsoft-com:office:excel""
xmlns=""http://www.w3.org/TR/REC-html40"">

<head>

<meta name=ProgId content=Excel.Sheet>
<meta name=Generator content=""Microsoft Excel 15"">
<link id=Main-File rel=Main-File
href=""file:///C:/Users/hubertlu/AppData/Local/Temp/msohtmlclip1/01/clip.htm"">
<link rel=File-List
href=""file:///C:/Users/hubertlu/AppData/Local/Temp/msohtmlclip1/01/clip_filelist.xml"">
<!--table
	{mso-displayed-decimal-separator:""\."";
	mso-displayed-thousand-separator:""\,"";}
@page
	{mso-header-data:""&L&\0022Arial\0022&10&K0000FF \[AMD Official Use Only - General\]&1\#\000D"";
	margin:.75in .7in .75in .7in;
	mso-header-margin:.3in;
	mso-footer-margin:.3in;}
tr
	{mso-height-source:auto;}
col
	{mso-width-source:auto;}
br
	{mso-data-placement:same-cell;}
td
	{padding-top:1px;
	padding-right:1px;
	padding-left:1px;
	mso-ignore:padding;
	color:black;
	font-size:11.0pt;
	font-weight:400;
	font-style:normal;
	text-decoration:none;
	font-family:Calibri, sans-serif;
	mso-font-charset:0;
	mso-number-format:General;
	text-align:general;
	vertical-align:bottom;
	border:none;
	mso-background-source:auto;
	mso-pattern:auto;
	mso-protection:locked visible;
	white-space:nowrap;
	mso-rotate:0;}
.xl65
	{color:windowtext;}
.xl66
	{mso-number-format:""0\.000"";}
-->
</head>

<body link=""#0563C1"" vlink=""#954F72"">

M | N | fwdbwd, torch.float16 | fwdbwd, torch.float32
-- | -- | -- | --
50432 | 384 | 9.147 | 12.094
50176 | 384 | 4.710 | 8.230
200704 | 192 | 10.655 | 8.266
802816 | 64 | 14.000 | 9.042
200 | 256 | 1.263 | 2.542
1000 | 256 | 3.109 | -0.246
6000 | 256 | 1.183 | 2.796
6272 | 256 | -3.579 | -3.394
200 | 512 | -5.489 | -7.852
1000 | 512 | -3.270 | -2.240
6000 | 512 | -3.456 | -4.596
6272 | 512 | -0.392 | -2.644
200 | 1024 | -7.862 | -0.969
1000 | 1024 | -1.321 | 0.359
6000 | 1024 | -0.693 | -2.336
6272 | 1024 | -2.130 | -2.034
200 | 1536 | -5.287 | -5.151
1000 | 1536 | -0.683 | -0.829
6000 | 1536 | 2.792 | 6.989
6272 | 1536 | 0.051 | 2.132
200 | 2048 | -5.461 | -1.167
1000 | 2048 | 4.126 | 2.701
6000 | 2048 | -0.797 | 0.453
6272 | 2048 | 2.792 | 0.126
200 | 3072 | 0.024 | -0.063
1000 | 3072 | 1.820 | 2.956
6000 | 3072 | 2.531 | 0.275
6272 | 3072 | 1.054 | 7.929
128 | 2097152 | 2.564 | 0.963
256 | 1048576 | 0.077 | 0.582
512 | 524288 | 0.428 | 0.094
1024 | 262144 | 0.581 | -0.096
2048 | 131072 | -0.225 | 0.888
4096 | 65536 | 0.527 | 0.428
8192 | 32768 | 0.204 | 0.717
16384 | 16384 | -0.216 | -0.492
32768 | 8192 | 0.786 | 5.127

</body>

</html>

CC: @jeffdaily

Pull Request resolved: https://github.com/pytorch/pytorch/pull/87726
Approved by: https://github.com/ngimel"
pytorch/pytorch,f45fe7de3378047d4c3b34af67ecfbd4934ff537,"Add mypy checking for a few files in torch/_dynamo (#89731)

It's kind of intractable to enable mypy everywhere at the moment,
because there are a lot of errors, and also mypy is really slow
for some reason.  I just want enough types to explain the public
types for user compiler calls, going through typing the _C.dynamo
bindings along the way.  This is a first step for this.

Signed-off-by: Edward Z. Yang <ezyang@fb.com>

Pull Request resolved: https://github.com/pytorch/pytorch/pull/89731
Approved by: https://github.com/suo"
pytorch/pytorch,1f95f24d3003a35568a00b5e5e18439846089b0f,"Factor input deduplication into a separate function (#89701)

It turns out that instead of having a giant blobby aot_dispatch_autograd
function, we can factor it into a series of wrapper functions, each
of which successively guarantees more invariants on the inner
compilation function until the final inner function is quite trivial.
How exactly you have to wrap the input user functions and the output
compiled functions can be expressed concisely in Haskell, so I've
included the Haskell formulation in code comments.

This PR shows how to do this for input deduplication.  Dealing with the
rest of the view handling is left to future work.

This PR should also be a slight performance improvement as deduplicating
is skipped entirely when there are no duplicate inputs.

Signed-off-by: Edward Z. Yang <ezyang@fb.com>
Pull Request resolved: https://github.com/pytorch/pytorch/pull/89701
Approved by: https://github.com/bdhirsh"
pytorch/pytorch,143d2881a844934c95c4ada63b38179d97e65af3,"[Profiler] Memory profiler part 10: Mark optimizer state (#88925)

This is also a fairly simple pass, since we're simply collecting values from the python tracer.

Differential Revision: [D40868664](https://our.internmc.facebook.com/intern/diff/D40868664/)
Pull Request resolved: https://github.com/pytorch/pytorch/pull/88925
Approved by: https://github.com/chaekit"
pytorch/pytorch,0435894bb3b2d60e5da9f993c2a56d95fb03a971,"[Profiler] Memory profiler part 8: Mark parameters. (#87568)

Following the pattern of earlier PRs, we use two methods to extract parameters. The primary one is the Python tracer; both nn.Module and optim.Optimizer collect parameters and in most cases that is sufficient. As a fallback we can analyze the data flow graph and deduce likely parameters based on gradient computation and updates.

Parameter identification has a circular interaction with input identification. Inputs are defined as ""not part of the core forward-backward-update loop"", but we need inputs for the parameter identification fallback to give us a proxy for the forward pass. Thus, we mark parameters from the python tracer which limits which Tensors get marked as inputs. While not necessary, it adds a bit of robustness. (As shown by the strengthening of the input unit tests.)

Differential Revision: [D40238619](https://our.internmc.facebook.com/intern/diff/D40238619/)
Pull Request resolved: https://github.com/pytorch/pytorch/pull/87568
Approved by: https://github.com/chaekit"
pytorch/pytorch,07151a6bd62e308b6b32e2e0edfc4d5f0563576e,"TorchDynamo: weight prepack for onednn convolution external call (#88988)

This PR is about enabled weight prepack using the MKLDNN tensor:
1.  enable fake tensor mode for MKLDNN tensor input.
2.  make convolution fusion kernel support MKLDNN tensor input.
3. do the weight prepack at FX fusion step.

For better performance, we always use channels_last for CPU convolution path. because we test that the channels_last path can get a better performance than block input path, and also avoid the activation's layout conversion(plain to block, block to plain), currently, there only need plain to plain format conversion.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/88988
Approved by: https://github.com/jgong5, https://github.com/jansel"
pytorch/pytorch,fc7dcb684aa38da5b1534fc701657ee63af8909c,"Run optimizer tests with fake tensors (#89643)

This is a slight regression: RAdam and Adagrad don't appear to
trace at all under fake tensors.  But I think this is a more accurate
reflection of the current state of affairs.

Along the way fix some problems on the fake tensor path.

Signed-off-by: Edward Z. Yang <ezyang@fb.com>

Pull Request resolved: https://github.com/pytorch/pytorch/pull/89643
Approved by: https://github.com/anjali411"
pytorch/pytorch,1588ea0dbf16f37ce14cfc8764666985c16ccbf9,"Added log1p for complex in c10 (#89214)

One PR towards #89205.
The content is mostly from PR #38465, but slightly changed the expression to make it faster.

Here are some benchmarking code:
```c++
#include <complex>
#include <iostream>
#include <chrono>

// main.cc

template<typename T> inline std::complex<T> log1p_v0(const std::complex<T> &z) {
    // this PR
    T x = z.real();
    T y = z.imag();
    T theta = std::atan2(y, x + T(1));
    T r = x * (x + T(2)) + y * y;
    return {T(0.5) * std::log1p(r), theta};
}

template<typename T> inline std::complex<T> log1p_v1(const std::complex<T> &z) {
    // PR #38465
    T x = z.real();
    T y = z.imag();
    std::complex<T> p1 = z + T(1);
    T r = std::abs(p1);
    T a = std::arg(p1);
    T rm1 = (x * x + y * y + x * T(2)) / (r + 1);
    return {std::log1p(rm1), a};
}

template<typename T>
inline std::complex<T> log1p_v2(const std::complex<T> &z) {
    // naive, but numerically inaccurate
    return std::log(T(1) + z);
}

int main() {
    int n = 1000000;
    std::complex<float> res(0.0, 0.0);
    std::complex<float> input(0.5, 2.0);
    auto start = std::chrono::system_clock::now();
    for (int i = 0; i < n; i++) {
        res += log1p_v0(input);
    }
    auto end = std::chrono::system_clock::now();
    auto elapsed = end - start;
    std::cout << ""time for v0: "" << elapsed.count() << '\n';

    start = std::chrono::system_clock::now();
    for (int i = 0; i < n; i++) {
        res += log1p_v1(input);
    }
    end = std::chrono::system_clock::now();
    elapsed = end - start;
    std::cout << ""time for v1: "" << elapsed.count() << '\n';

    start = std::chrono::system_clock::now();
    for (int i = 0; i < n; i++) {
        res += log1p_v2(input);
    }
    end = std::chrono::system_clock::now();
    elapsed = end - start;
    std::cout << ""time for v2: "" << elapsed.count() << '\n';
    std::cout << res << '\n';
}
```

Compiling the script with command `g++ main.cc` produces the following results:
```
time for v0: 237812271
time for v1: 414524941
time for v2: 360585994
```

Pull Request resolved: https://github.com/pytorch/pytorch/pull/89214
Approved by: https://github.com/lezcano"
pytorch/pytorch,903ae4570e401e5c4e42dc4a44cae37f805044a4,"Disable optimizer tracing, enable for tests only (#89500)

Disabling optimizer tracing before launch until it can be added to the benchmark suites without increasing compile times

Pull Request resolved: https://github.com/pytorch/pytorch/pull/89500
Approved by: https://github.com/anijain2305"
pytorch/pytorch,b8d3afd88665de5f01f696333d0ff291bd94a57b,"Skip upload test stats for test reports from rerun disabled tests workflow (#89548)

I have found the reason why uploading tests stats fails for rerun disabled workflow, for example https://github.com/pytorch/pytorch/actions/runs/3522896778/jobs/5917765699.  The problem is that the pytest XML file is now too big to be processed quickly (x50 bigger). Unlike unittest, `pytest-flakefinder` used by rerun disabled tests for test_ops includes skipped messages multiple times (50 times by default, retrying and skipping).  This slows down the upload test stats script too much (O(n)) because it tries to gather all the stats. On the other hand, `check_disabled_tests` doesn't suffer from the same issue because it ignores all these skipped messages.

This is a quick fix to skip test reports from rerun disabled tests workflow when trying to upload test stats.

I'll try to fix this properly later in the way we use pytest-flakefinder. From what I see, a zipped test report from rerun disabled test is only few MB ([example](https://gha-artifacts.s3.amazonaws.com/pytorch/pytorch/3521687954/1/artifact/test-reports-test-default-1-2-linux.2xlarge_9636028803.zip)), but will balloon up to a much bigger XML file after extracting from a dozen to a few hundred MB (text).  The size of the zipped file is not a big immediate problem

### Testing

[3521687954](https://github.com/pytorch/pytorch/actions/runs/3521687954) is an example workflow with rerun disabled tests and mem leak check.  The script can now finish when running locally:

* `upload_test_stats` finishes around 3+ minutes
```
time python -m tools.stats.upload_test_stats --workflow-run-id 3521687954 --workflow-run-attempt 1 --head-branch master
...
Writing 8925 documents to S3
Done!
Writing 1760 documents to S3
Done!
Writing 1675249 documents to S3
Done!
python3 -m tools.stats.upload_test_stats --workflow-run-id 3521687954  1    185.69s user 12.89s system 75% cpu 4:22.82 total
```

* `check_disabled_tests` finishes within 3 minutes
```
time python -m tools.stats.check_disabled_tests --workflow-run-id 3521687954 --workflow-run-attempt 1 --repo pytorch/pytorch
...
python -m tools.stats.check_disabled_tests --workflow-run-id 3521687954  1    154.19s user 4.17s system 97% cpu 2:42.50 total
```

Pull Request resolved: https://github.com/pytorch/pytorch/pull/89548
Approved by: https://github.com/clee2000"
pytorch/pytorch,795473ff5e861f21c6b5e0611177fcfa9e1c4e0c,"Call `symint::sizes()` instead of `sizes()` on convolution error messages. (#89549)

This PR fixes convolution when using `torchdynamo` with dynamic shapes.

**Problem:** there are some `tensor.sizes()` calls in a few error messages. As a result, an uninformative error message was being displayed.

```python
@torch._dynamo.optimize(""eager"")
def foo(inp, w):
    return F.conv2d(inp, w)

inp = torch.rand((1, 1, 32, 32))
w = torch.rand((1, 2, 3, 3))
#                  |
#                  |--------- incorrect shape!

foo(inp, w)
```

-----
**Before this PR:**
```python
Traceback (most recent call last):
  File ""torch/_dynamo/utils.py"", line 1076, in run_node
    return node.target(*args, **kwargs)
  File ""torch/_subclasses/fake_tensor.py"", line 867, in __torch_dispatch__
    op_impl_out = op_impl(self, func, *args, **kwargs)
  File ""torch/_subclasses/fake_tensor.py"", line 445, in conv
    conv_backend = torch._C._select_conv_backend(**kwargs)
RuntimeError: Cannot call sizes() on tensor with symbolic sizes/strides
```

**After this PR:**
```python
Traceback (most recent call last):
  File ""torch/_dynamo/utils.py"", line 1076, in run_node
    return node.target(*args, **kwargs)
  File ""torch/_subclasses/fake_tensor.py"", line 867, in __torch_dispatch__
    op_impl_out = op_impl(self, func, *args, **kwargs)
  File ""torch/_subclasses/fake_tensor.py"", line 445, in conv
    conv_backend = torch._C._select_conv_backend(**kwargs)
RuntimeError: Given groups=1, weight of size [1, s1, s2, s2], expected input[1, 1, s0, s0] to have s1 channels, but got 1 channels instead
```

Pull Request resolved: https://github.com/pytorch/pytorch/pull/89549
Approved by: https://github.com/ezyang"
pytorch/pytorch,74e62a1fefb7100689169dc12fd70095de54079d,"[ROCm] Optimize layer norm backward kernel for ROCm (#87635)

We observed that the native PyTorch LayerNormBackwardKernelImplInternal has suboptimal performance for certain input sizes on AMD GPUs especially when `fs`  (=`config_m` in our benchmark script) is large and `bs`  (=`config_n` in our benchmark script) is small (commonly seen in [the CvT model](https://arxiv.org/abs/2103.15808)) in the benchmark script of [PR #68238](https://github.com/pytorch/pytorch/pull/68238#issue-1051621716) on AMD GPUs.

This PR is to replace `GammaBetaBackwardCUDAKernel` with the Apex layernorm backward kernel with some ROCm-specific parameter tuning when `fs`  (=`config_m`) is larger than 512 on AMD GPUs.

There are a few PRs for LayerNorm kernel:
- https://github.com/pytorch/pytorch/pull/26201
- https://github.com/pytorch/pytorch/pull/27634
- https://github.com/pytorch/pytorch/pull/68238

Therefore, we have tested and compared the kernel before and at this PR with the input shapes in the last two PRs along with those commonly used in the CvT model on AMD MI100.

---
**Current**
<html xmlns:v=""urn:schemas-microsoft-com:vml""
xmlns:o=""urn:schemas-microsoft-com:office:office""
xmlns:x=""urn:schemas-microsoft-com:office:excel""
xmlns=""http://www.w3.org/TR/REC-html40"">

<head>

<meta name=ProgId content=Excel.Sheet>
<meta name=Generator content=""Microsoft Excel 15"">
<link id=Main-File rel=Main-File
href=""file:///C:/Users/hubertlu/AppData/Local/Temp/msohtmlclip1/01/clip.htm"">
<link rel=File-List
href=""file:///C:/Users/hubertlu/AppData/Local/Temp/msohtmlclip1/01/clip_filelist.xml"">
<!--table
	{mso-displayed-decimal-separator:""\."";
	mso-displayed-thousand-separator:""\,"";}
@page
	{mso-header-data:""&L&\0022Arial\0022&10&K0000FF \[AMD Official Use Only - General\]&1\#\000D"";
	margin:.75in .7in .75in .7in;
	mso-header-margin:.3in;
	mso-footer-margin:.3in;}
tr
	{mso-height-source:auto;}
col
	{mso-width-source:auto;}
br
	{mso-data-placement:same-cell;}
td
	{padding-top:1px;
	padding-right:1px;
	padding-left:1px;
	mso-ignore:padding;
	color:black;
	font-size:11.0pt;
	font-weight:400;
	font-style:normal;
	text-decoration:none;
	font-family:Calibri, sans-serif;
	mso-font-charset:0;
	mso-number-format:General;
	text-align:general;
	vertical-align:bottom;
	border:none;
	mso-background-source:auto;
	mso-pattern:auto;
	mso-protection:locked visible;
	white-space:nowrap;
	mso-rotate:0;}
-->
</head>

<body link=""#0563C1"" vlink=""#954F72"">

M | N | fwd (half) | fwdbwd (half) | fwd (float) | fwdbwd (float)
-- | -- | -- | -- | -- | --
50432 | 384 | 0.387256 | 1.372758 | 0.378975 | 1.47892
50176 | 384 | 0.38231 | 1.362416 | 0.378084 | 1.473886
200704 | 192 | 0.997859 | 4.315875 | 0.989306 | 4.560827
802816 | 64 | 3.671828 | 16.68013 | 3.613515 | 16.827946
200 | 256 | 0.066503 | 0.332096 | 0.071422 | 0.325349
1000 | 256 | 0.071848 | 0.333355 | 0.073038 | 0.334753
6000 | 256 | 0.086334 | 0.345139 | 0.086834 | 0.347429
6272 | 256 | 0.088601 | 0.347906 | 0.087855 | 0.351245
200 | 512 | 0.071626 | 0.329726 | 0.073798 | 0.326878
1000 | 512 | 0.073975 | 0.330226 | 0.074166 | 0.332751
6000 | 512 | 0.099617 | 0.362367 | 0.100095 | 0.378313
6272 | 512 | 0.100378 | 0.358066 | 0.099857 | 0.395982
200 | 1024 | 0.072954 | 0.326382 | 0.073899 | 0.333007
1000 | 1024 | 0.0743 | 0.325532 | 0.071126 | 0.330991
6000 | 1024 | 0.127025 | 0.390084 | 0.128692 | 0.471504
6272 | 1024 | 0.130704 | 0.403536 | 0.135244 | 0.487133
200 | 1536 | 0.070331 | 0.339169 | 0.070086 | 0.331015
1000 | 1536 | 0.075085 | 0.330042 | 0.076295 | 0.328778
6000 | 1536 | 0.148889 | 0.44949 | 0.155781 | 0.659987
6272 | 1536 | 0.154939 | 0.478871 | 0.17673 | 0.716025
200 | 2048 | 0.070269 | 0.335585 | 0.072804 | 0.334655
1000 | 2048 | 0.080094 | 0.326991 | 0.080426 | 0.32685
6000 | 2048 | 0.187888 | 0.623023 | 0.245762 | 0.981635
6272 | 2048 | 0.195431 | 0.65244 | 0.262574 | 1.008141
200 | 3072 | 0.068205 | 0.339428 | 0.073068 | 0.344034
1000 | 3072 | 0.087554 | 0.328899 | 0.09218 | 0.346433
6000 | 3072 | 0.240352 | 0.905058 | 0.368135 | 1.280462
6272 | 3072 | 0.26179 | 0.959387 | 0.387782 | 1.476524
128 | 2097152 | 5.905976 | 22.724793 | 10.287974 | 30.242092
256 | 1048576 | 4.561596 | 19.554308 | 10.223171 | 29.42371
512 | 524288 | 4.146751 | 22.7247 | 11.404285 | 39.175902
1024 | 262144 | 5.193135 | 23.403325 | 11.334512 | 38.947192
2048 | 131072 | 4.992907 | 23.377801 | 11.400286 | 40.889191
4096 | 65536 | 5.429488 | 24.275701 | 11.196778 | 41.4751
8192 | 32768 | 5.35758 | 21.360312 | 10.535418 | 42.875646
16384 | 16384 | 5.44947 | 20.852605 | 10.357685 | 34.603408
32768 | 8192 | 4.688925 | 17.379392 | 9.635596 | 31.188271

</body>

</html>

---------
**At this PR**
<html xmlns:v=""urn:schemas-microsoft-com:vml""
xmlns:o=""urn:schemas-microsoft-com:office:office""
xmlns:x=""urn:schemas-microsoft-com:office:excel""
xmlns=""http://www.w3.org/TR/REC-html40"">

<head>

<meta name=ProgId content=Excel.Sheet>
<meta name=Generator content=""Microsoft Excel 15"">
<link id=Main-File rel=Main-File
href=""file:///C:/Users/hubertlu/AppData/Local/Temp/msohtmlclip1/01/clip.htm"">
<link rel=File-List
href=""file:///C:/Users/hubertlu/AppData/Local/Temp/msohtmlclip1/01/clip_filelist.xml"">

<!--table
	{mso-displayed-decimal-separator:""\."";
	mso-displayed-thousand-separator:""\,"";}
@page
	{mso-header-data:""&L&\0022Arial\0022&10&K0000FF \[AMD Official Use Only - General\]&1\#\000D"";
	margin:.75in .7in .75in .7in;
	mso-header-margin:.3in;
	mso-footer-margin:.3in;}
tr
	{mso-height-source:auto;}
col
	{mso-width-source:auto;}
br
	{mso-data-placement:same-cell;}
td
	{padding-top:1px;
	padding-right:1px;
	padding-left:1px;
	mso-ignore:padding;
	color:black;
	font-size:11.0pt;
	font-weight:400;
	font-style:normal;
	text-decoration:none;
	font-family:Calibri, sans-serif;
	mso-font-charset:0;
	mso-number-format:General;
	text-align:general;
	vertical-align:bottom;
	border:none;
	mso-background-source:auto;
	mso-pattern:auto;
	mso-protection:locked visible;
	white-space:nowrap;
	mso-rotate:0;}
.xl63
	{color:windowtext;}
-->
</head>

<body link=""#0563C1"" vlink=""#954F72"">

M | N | fwd (half) | fwdbwd (half) | fwd (float) | fwdbwd (float)
-- | -- | -- | -- | -- | --
50432 | 384 | 0.38797 | 0.93103 | 0.37966 | 1.15283
50176 | 384 | 0.3874 | 0.96417 | 0.38462 | 1.18595
200704 | 192 | 1.00002 | 2.40876 | 0.99224 | 2.55579
802816 | 64 | 3.67348 | 7.98658 | 3.61871 | 7.72404
200 | 256 | 0.07292 | 0.35119 | 0.07195 | 0.32602
1000 | 256 | 0.07354 | 0.33325 | 0.07237 | 0.33742
6000 | 256 | 0.08819 | 0.33283 | 0.08453 | 0.3279
6272 | 256 | 0.0886 | 0.33446 | 0.08774 | 0.33426
200 | 512 | 0.0701 | 0.33505 | 0.07072 | 0.33018
1000 | 512 | 0.07042 | 0.33442 | 0.074 | 0.33206
6000 | 512 | 0.09931 | 0.34956 | 0.09895 | 0.3572
6272 | 512 | 0.10103 | 0.32976 | 0.10041 | 0.36635
200 | 1024 | 0.07144 | 0.33579 | 0.07209 | 0.33216
1000 | 1024 | 0.0736 | 0.32803 | 0.07286 | 0.32936
6000 | 1024 | 0.12584 | 0.38916 | 0.12852 | 0.48273
6272 | 1024 | 0.13053 | 0.38804 | 0.13464 | 0.49545
200 | 1536 | 0.07159 | 0.3396 | 0.07062 | 0.33545
1000 | 1536 | 0.07443 | 0.33239 | 0.07366 | 0.33204
6000 | 1536 | 0.14959 | 0.45043 | 0.15826 | 0.69119
6272 | 1536 | 0.1542 | 0.47644 | 0.18249 | 0.72208
200 | 2048 | 0.07258 | 0.33982 | 0.07412 | 0.33859
1000 | 2048 | 0.0793 | 0.32816 | 0.07864 | 0.32583
6000 | 2048 | 0.18973 | 0.571 | 0.25506 | 0.91796
6272 | 2048 | 0.19719 | 0.64208 | 0.26445 | 0.95055
200 | 3072 | 0.07092 | 0.33867 | 0.07104 | 0.34695
1000 | 3072 | 0.08727 | 0.33144 | 0.09144 | 0.36633
6000 | 3072 | 0.24683 | 0.87275 | 0.37761 | 1.3289
6272 | 3072 | 0.26437 | 0.91178 | 0.38496 | 1.53694
128 | 2097152 | 6.27936 | 23.69425 | 10.40004 | 30.13699
256 | 1048576 | 4.5404 | 19.47675 | 10.28494 | 29.36936
512 | 524288 | 4.13951 | 18.78771 | 10.09557 | 32.67083
1024 | 262144 | 4.47576 | 18.00411 | 9.56488 | 31.47117
2048 | 131072 | 4.28026 | 16.95619 | 9.40297 | 30.82845
4096 | 65536 | 4.2653 | 16.5018 | 9.03315 | 30.08392
8192 | 32768 | 4.25613 | 16.13583 | 8.9258 | 30.75296
16384 | 16384 | 4.20256 | 16.38207 | 9.52587 | 31.31113
32768 | 8192 | 4.20231 | 16.19452 | 9.31478 | 31.03514

</body>

</html>

---------

**Performance Improvement (%)**
<html xmlns:o=""urn:schemas-microsoft-com:office:office""
xmlns:dt=""uuid:C2F41010-65B3-11d1-A29F-00AA00C14882""
xmlns=""http://www.w3.org/TR/REC-html40"">

<head>

<meta name=ProgId content=OneNote.File>
<meta name=Generator content=""Microsoft OneNote 15"">
</head>

<body lang=en-US style='font-family:Calibri;font-size:11.0pt'>
<!--StartFragment-->

<div style='direction:ltr'>

M | N | fwdbwd,   torch.float16 | fwdbwd,   torch.float32
-- | -- | -- | --
50432 | 384 | 32.178 | 22.049
50176 | 384 | 29.231 | 19.536
200704 | 192 | 44.188 | 43.962
802816 | 64 | 52.119 | 54.100
200 | 256 | -5.750 | -0.206
1000 | 256 | 0.031 | -0.797
6000 | 256 | 3.566 | 5.621
6272 | 256 | 3.865 | 4.836
200 | 512 | -1.615 | -1.010
1000 | 512 | -1.270 | 0.208
6000 | 512 | 3.534 | 5.581
6272 | 512 | 7.905 | 7.483
200 | 1024 | -2.883 | 0.254
1000 | 1024 | -0.767 | 0.493
6000 | 1024 | 0.237 | -2.381
6272 | 1024 | 3.840 | -1.707
200 | 1536 | -0.127 | -1.340
1000 | 1536 | -0.711 | -0.992
6000 | 1536 | -0.209 | -4.728
6272 | 1536 | 0.508 | -0.846
200 | 2048 | -1.262 | -1.176
1000 | 2048 | -0.358 | 0.312
6000 | 2048 | 8.350 | 6.487
6272 | 2048 | 1.588 | 5.713
200 | 3072 | 0.223 | -0.848
1000 | 3072 | -0.773 | -5.743
6000 | 3072 | 3.570 | -3.783
6272 | 3072 | 4.962 | -4.092
128 | 2097152 | -4.266 | 0.348
256 | 1048576 | 0.397 | 0.185
512 | 524288 | 17.325 | 16.605
1024 | 262144 | 23.070 | 19.195
2048 | 131072 | 27.469 | 24.605
4096 | 65536 | 32.023 | 27.465
8192 | 32768 | 24.459 | 28.274
16384 | 16384 | 21.439 | 9.514
32768 | 8192 | 6.818 | 0.491

</div>

<!--EndFragment-->
</body>

</html>

---------
**Benchmark script of this PR**
```
# Ref:
#       1. https://github.com/pytorch/pytorch/pull/26201
#       2. https://github.com/pytorch/pytorch/pull/68238

from distutils.command.config import config
import torch
from torch.nn import LayerNorm
import timeit

number_runs = 1000  # TODO: Modify this to save time!
def test_forward(layer_norm_cuda, input_cuda):
    layer_norm_cuda(input_cuda); torch.cuda.synchronize()

def test_backward(out_cuda, layer_norm_grad_cuda, create_graph):
    out_cuda.backward(layer_norm_grad_cuda, retain_graph=True, create_graph=create_graph); torch.cuda.synchronize()

def test_fwdbwd(input_cuda, layer_norm_cuda, gO):
    input_cuda.grad = None
    layer_norm_cuda.zero_grad(set_to_none=True)
    out = layer_norm_cuda(input_cuda)
    out.backward(gO)
    torch.cuda.synchronize()

def benchmark(config_m, config_n):

    print(""M | N | fwd (half) | fwdbwd (half) | fwd (float) | fwdbwd (float)"")
    if len(config_m) != len(config_n):
        print(""Please make sure the lengths of config_m and config_m are the same."")

    for i in range(len(config_m)):
        normalized_shape = config_n[i]
        results = [config_m[i], config_n[i]]
        for dtype in (torch.half, torch.float):
            if dtype == torch.half:
                layer_norm_cuda = LayerNorm(normalized_shape).half().cuda()
            else:
                layer_norm_cuda = LayerNorm(normalized_shape).cuda()

            input_cuda = torch.randn(config_m[i], config_n[i], device='cuda', dtype=dtype, requires_grad=True)

            # print(""cuda forward:"")
            result_fwd = timeit.timeit(lambda: test_forward(layer_norm_cuda, input_cuda), number=number_runs)
            results.append(result_fwd / number_runs * 1000)

            gO = torch.rand_like(input_cuda)

            result_fwdbwd = timeit.timeit(lambda: test_fwdbwd(input_cuda, layer_norm_cuda, gO), number=number_runs)
            results.append(result_fwdbwd / number_runs * 1000)

        print('{:09d}|{:09d}|{:9.5f}|{:9.5f}|{:9.5f}|{:9.5f}'.format(results[0], results[1], results[2], results[3], results[4], results[5]))

    print(""Times are in microseconds (us)."")

# CVT
config_m_cvt = [50432, 50176, 200704, 802816]
config_n_cvt = [384, 384, 192, 64]

# https://github.com/pytorch/pytorch/pull/68238#issue-1051621716
config_m_68238 = [200, 1000, 6000, 6272, 200, 1000, 6000, 6272, 200, 1000, 6000, 6272, 200, 1000, 6000, 6272, 200, 1000, 6000, 6272, 200, 1000, 6000, 6272]
config_n_68238 = [256,256,256,256,512,512,512,512,1024,1024,1024,1024,1536,1536,1536,1536,2048,2048,2048,2048,3072,3072,3072,3072]

# https://github.com/pytorch/pytorch/pull/27634
config_m_27634 = [128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768]
config_n_27634 = [2097152, 1048576, 524288, 262144, 131072, 65536, 32768, 16384, 8192]

config_m = config_m_cvt + config_m_68238 + config_m_27634
config_n = config_n_cvt + config_n_68238 + config_n_27634

benchmark(config_m, config_n)
```

CC: @jeffdaily

Pull Request resolved: https://github.com/pytorch/pytorch/pull/87635
Approved by: https://github.com/jataylo, https://github.com/jeffdaily, https://github.com/ezyang"
pytorch/pytorch,40cf214f2d18b3b8af5354ddc5dad8156ea32520,"Support masked_fill to address the GPT2 performance issue (#89274)

Pull Request resolved: https://github.com/pytorch/pytorch/pull/89274
Approved by: https://github.com/jgong5, https://github.com/jansel"
pytorch/pytorch,e545caa50f3cd893ca0419543e57af08a7de85b5,"dynamo/torchxla integration: trace on xla rather than eager (#88904)

In #87741 we added the inference support for dynamo/torchxla integration. Later on in #88449 we attempt to add the training support. That attempt is not smooth because
- we try 2 things together
   1. let dynamo trace the model on xla rather than eager
   2. enable training
- It turns out neither of these two tasks are trivial enough.

Furthermore, item 2 (enable training) depends on item 1 (tracing on xla). We enable training via AOTAutograd. AOTAutograd lift all model parameters/buffers as graph inputs. Without item 1 being done, we would need copy all graph inputs (including model parameters/buffers) from eager device to xla devices. That hurts performance a lot. Have a cache to map eager parameter to XLA parameter does not solve the problem since the update on either will not sync automatically to the other. They will easily go out of sync.

This PR let dynamo trace the model on XLA rather than eager. This is a preparation step to enabling training.

Also, tracing on XLA makes the data movement more efficient. We see 1.5x geomean speedup compared to previous 1.38x.
```
+-------------------------+--------------------+-------------------------+
| Model                   |   XLA (trace once) |   XLA (trace everytime) |
+=========================+====================+=========================+
| resnet18                |            1.38    |                 1.008   |
+-------------------------+--------------------+-------------------------+
| resnet50                |            1.227   |                 0.998   |
+-------------------------+--------------------+-------------------------+
| resnext50_32x4d         |            1.544   |                 1.008   |
+-------------------------+--------------------+-------------------------+
| alexnet                 |            1.085   |                 1.045   |
+-------------------------+--------------------+-------------------------+
| mobilenet_v2            |            2.028   |                 1.013   |
+-------------------------+--------------------+-------------------------+
| mnasnet1_0              |            1.516   |                 0.995   |
+-------------------------+--------------------+-------------------------+
| squeezenet1_1           |            0.868   |                 1.01    |
+-------------------------+--------------------+-------------------------+
| vgg16                   |            1.099   |                 1.008   |
+-------------------------+--------------------+-------------------------+
| BERT_pytorch            |            3.26    |                 1.027   |
+-------------------------+--------------------+-------------------------+
| timm_vision_transformer |            2.182   |                 1.015   |
+-------------------------+--------------------+-------------------------+
| geomean                 |            1.50389 |                 1.01261 |
+-------------------------+--------------------+-------------------------+
```

Example command
```
GPU_NUM_DEVICES=1 python benchmarks/dynamo/torchbench.py --randomize-input --performance --trace-on-xla --only resnet18 --backend=torchxla_trace_once
```

Pull Request resolved: https://github.com/pytorch/pytorch/pull/88904
Approved by: https://github.com/wconstab, https://github.com/JackCaoG, https://github.com/jansel"
pytorch/pytorch,ce342ed2d3a4a0dd8151abe80bfe0bb06a7b0ae9,"Fix retrying logic for successful unittest tests under --rerun-disabled-tests mode (#89454)

When looking into Rockset data for disabled test unittest, for example `testAdd`, I see that it's re-run only 3 times instead of 50+ times as expected under rerun-disabled -test mode

```
[
  {
    ""name"": ""testAdd"",
    ""classname"": ""TestLazyReuseIr"",
    ""filename"": ""lazy/test_reuse_ir.py"",
    ""flaky"": false,
    ""num_green"": 3,
    ""num_red"": 0
  }
]
```

It turns out that I made a mistake mixing `RERUN_DISABLED_TESTS` and `report_only` into `(RERUN_DISABLED_TESTS or report_only) and num_retries_left < MAX_NUM_RETRIES` in https://github.com/pytorch/pytorch/pull/88646.  The retrying logic for successful tests under rerun-disabled-tests mode is never executed because num_retries_left would be equal to MAX_NUM_RETRIES (not smaller) if the very first run successes. Thus, the sample test `testAdd` finishes right away (1 success count)

* `report_only` and `RERUN_DISABLED_TESTS` are 2 different things and shouldn't be mixed together. RERUN_DISABLED_TESTS has the higher priority.
* We also don't want to retry skipped tests under rerun-disabled-tests mode because they are only skipped due to `check_if_enable` check `Test is enabled but --rerun-disabled-tests verification mode is set, so only disabled tests are run`

### Testing

* CI https://github.com/pytorch/pytorch/actions/runs/3518228784 generates https://gha-artifacts.s3.amazonaws.com/pytorch/pytorch/3518228784/1/artifact/test-reports-test-default-4-4-linux.4xlarge.nvidia.gpu_9627285587.zip in which `testAdd` is correctly called multiple times and `TestLazyReuseIr` is skipped correctly
* Locally

```
# export CI=1
# export PYTORCH_RETRY_TEST_CASES=1
# export PYTORCH_OVERRIDE_FLAKY_SIGNAL=1
# export PYTORCH_TEST_RERUN_DISABLED_TESTS=1
$ python test/run_test.py --verbose -i lazy/test_reuse_ir
Ignoring disabled issues:  []
Selected tests:
 lazy/test_reuse_ir
Prioritized test from test file changes.
reordering tests for PR:
prioritized: []
the rest: ['lazy/test_reuse_ir']

Downloading https://raw.githubusercontent.com/pytorch/test-infra/generated-stats/stats/slow-tests.json to /Users/huydo/Storage/mine/pytorch/test/.pytorch-slow-tests.json
Downloading https://raw.githubusercontent.com/pytorch/test-infra/generated-stats/stats/disabled-tests-condensed.json to /Users/huydo/Storage/mine/pytorch/test/.pytorch-disabled-tests.json
parallel (file granularity) tests:
 lazy/test_reuse_ir
serial (file granularity) tests:

Ignoring disabled issues:  []
Ignoring disabled issues:  []
Running lazy/test_reuse_ir ... [2022-11-21 13:21:07.165877]
Executing ['/Users/huydo/miniconda3/envs/py3.9/bin/python', '-bb', 'lazy/test_reuse_ir.py', '-v', '--import-slow-tests', '--import-disabled-tests', '--rerun-disabled-tests'] ... [2022-11-21 13:21:07.166279]

Expand the folded group to see the log file of lazy/test_reuse_ir
##[group]PRINTING LOG FILE of lazy/test_reuse_ir (/Users/huydo/Storage/mine/pytorch/test/test-reports/lazy-test_reuse_ir_6cf_dxa1)

Running tests...
----------------------------------------------------------------------
Test results will be stored in test-reports/python-unittest/lazy.test_reuse_ir
  testAdd (__main__.TestLazyReuseIr) ... ok (1.215s)
  testAdd (__main__.TestLazyReuseIr) ...     testAdd succeeded - num_retries_left: 50
ok (0.001s)
  testAdd (__main__.TestLazyReuseIr) ...     testAdd succeeded - num_retries_left: 49
ok (0.001s)
  testAdd (__main__.TestLazyReuseIr) ...     testAdd succeeded - num_retries_left: 48
ok (0.001s)
  testAdd (__main__.TestLazyReuseIr) ...     testAdd succeeded - num_retries_left: 47
ok (0.001s)
  testAdd (__main__.TestLazyReuseIr) ...     testAdd succeeded - num_retries_left: 46
ok (0.001s)
  testAdd (__main__.TestLazyReuseIr) ...     testAdd succeeded - num_retries_left: 45
ok (0.001s)
  testAdd (__main__.TestLazyReuseIr) ...     testAdd succeeded - num_retries_left: 44
ok (0.001s)
  testAdd (__main__.TestLazyReuseIr) ...     testAdd succeeded - num_retries_left: 43
ok (0.001s)
  testAdd (__main__.TestLazyReuseIr) ...     testAdd succeeded - num_retries_left: 42
ok (0.001s)
  testAdd (__main__.TestLazyReuseIr) ...     testAdd succeeded - num_retries_left: 41
ok (0.001s)
  testAdd (__main__.TestLazyReuseIr) ...     testAdd succeeded - num_retries_left: 40
ok (0.001s)
  testAdd (__main__.TestLazyReuseIr) ...     testAdd succeeded - num_retries_left: 39
ok (0.001s)
  testAdd (__main__.TestLazyReuseIr) ...     testAdd succeeded - num_retries_left: 38
ok (0.001s)
  testAdd (__main__.TestLazyReuseIr) ...     testAdd succeeded - num_retries_left: 37
ok (0.001s)
  testAdd (__main__.TestLazyReuseIr) ...     testAdd succeeded - num_retries_left: 36
ok (0.001s)
  testAdd (__main__.TestLazyReuseIr) ...     testAdd succeeded - num_retries_left: 35
ok (0.001s)
  testAdd (__main__.TestLazyReuseIr) ...     testAdd succeeded - num_retries_left: 34
ok (0.001s)
  testAdd (__main__.TestLazyReuseIr) ...     testAdd succeeded - num_retries_left: 33
ok (0.001s)
  testAdd (__main__.TestLazyReuseIr) ...     testAdd succeeded - num_retries_left: 32
ok (0.001s)
  testAdd (__main__.TestLazyReuseIr) ...     testAdd succeeded - num_retries_left: 31
ok (0.001s)
  testAdd (__main__.TestLazyReuseIr) ...     testAdd succeeded - num_retries_left: 30
ok (0.001s)
  testAdd (__main__.TestLazyReuseIr) ...     testAdd succeeded - num_retries_left: 29
ok (0.001s)
  testAdd (__main__.TestLazyReuseIr) ...     testAdd succeeded - num_retries_left: 28
ok (0.001s)
  testAdd (__main__.TestLazyReuseIr) ...     testAdd succeeded - num_retries_left: 27
ok (0.001s)
  testAdd (__main__.TestLazyReuseIr) ...     testAdd succeeded - num_retries_left: 26
ok (0.001s)
  testAdd (__main__.TestLazyReuseIr) ...     testAdd succeeded - num_retries_left: 25
ok (0.001s)
  testAdd (__main__.TestLazyReuseIr) ...     testAdd succeeded - num_retries_left: 24
ok (0.001s)
  testAdd (__main__.TestLazyReuseIr) ...     testAdd succeeded - num_retries_left: 23
ok (0.001s)
  testAdd (__main__.TestLazyReuseIr) ...     testAdd succeeded - num_retries_left: 22
ok (0.001s)
  testAdd (__main__.TestLazyReuseIr) ...     testAdd succeeded - num_retries_left: 21
ok (0.001s)
  testAdd (__main__.TestLazyReuseIr) ...     testAdd succeeded - num_retries_left: 20
ok (0.001s)
  testAdd (__main__.TestLazyReuseIr) ...     testAdd succeeded - num_retries_left: 19
ok (0.001s)
  testAdd (__main__.TestLazyReuseIr) ...     testAdd succeeded - num_retries_left: 18
ok (0.001s)
  testAdd (__main__.TestLazyReuseIr) ...     testAdd succeeded - num_retries_left: 17
ok (0.001s)
  testAdd (__main__.TestLazyReuseIr) ...     testAdd succeeded - num_retries_left: 16
ok (0.001s)
  testAdd (__main__.TestLazyReuseIr) ...     testAdd succeeded - num_retries_left: 15
ok (0.001s)
  testAdd (__main__.TestLazyReuseIr) ...     testAdd succeeded - num_retries_left: 14
ok (0.001s)
  testAdd (__main__.TestLazyReuseIr) ...     testAdd succeeded - num_retries_left: 13
ok (0.001s)
  testAdd (__main__.TestLazyReuseIr) ...     testAdd succeeded - num_retries_left: 12
ok (0.001s)
  testAdd (__main__.TestLazyReuseIr) ...     testAdd succeeded - num_retries_left: 11
ok (0.001s)
  testAdd (__main__.TestLazyReuseIr) ...     testAdd succeeded - num_retries_left: 10
ok (0.001s)
  testAdd (__main__.TestLazyReuseIr) ...     testAdd succeeded - num_retries_left: 9
ok (0.001s)
  testAdd (__main__.TestLazyReuseIr) ...     testAdd succeeded - num_retries_left: 8
ok (0.001s)
  testAdd (__main__.TestLazyReuseIr) ...     testAdd succeeded - num_retries_left: 7
ok (0.001s)
  testAdd (__main__.TestLazyReuseIr) ...     testAdd succeeded - num_retries_left: 6
ok (0.001s)
  testAdd (__main__.TestLazyReuseIr) ...     testAdd succeeded - num_retries_left: 5
ok (0.001s)
  testAdd (__main__.TestLazyReuseIr) ...     testAdd succeeded - num_retries_left: 4
ok (0.001s)
  testAdd (__main__.TestLazyReuseIr) ...     testAdd succeeded - num_retries_left: 3
ok (0.001s)
  testAdd (__main__.TestLazyReuseIr) ...     testAdd succeeded - num_retries_left: 2
ok (0.001s)
  testAdd (__main__.TestLazyReuseIr) ...     testAdd succeeded - num_retries_left: 1
ok (0.001s)
  testAddSub (__main__.TestLazyReuseIr) ...     testAdd succeeded - num_retries_left: 0
skip: Test is enabled but --rerun-disabled-tests verification mode is set, so only disabled tests are run (0.001s)
  testAddSubFallback (__main__.TestLazyReuseIr) ... skip: Test is enabled but --rerun-disabled-tests verification mode is set, so only disabled tests are run (0.001s)
  testBatchNorm (__main__.TestLazyReuseIr) ... skip: Test is enabled but --rerun-disabled-tests verification mode is set, so only disabled tests are run (0.001s)

----------------------------------------------------------------------
Ran 54 tests in 1.264s

OK (skipped=3)
```

Here is the sample rockset query

```
WITH added_row_number AS (
  SELECT
    *,
    ROW_NUMBER() OVER(PARTITION BY name, classname, filename ORDER BY _event_time DESC) AS row_number
  FROM
    commons.rerun_disabled_tests
)
SELECT
  name,
  classname,
  filename,
  flaky,
  num_green,
  num_red
FROM
  added_row_number
WHERE
  row_number = 1
  AND name = 'testAdd'
```
Pull Request resolved: https://github.com/pytorch/pytorch/pull/89454
Approved by: https://github.com/clee2000"
pytorch/pytorch,120d200620159597f416f9142f1d5708182ca047,"Revert ""Added conv constraint that infers layouts (#89031)"" (#89451)

This reverts commit 716f70f19a4b63268da2a753afdbe9b385a831ab.

Fixes performance regression and compilation latency increase.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/89451
Approved by: https://github.com/soumith, https://github.com/jansel"
pytorch/pytorch,dbc354b262f7f5e49aa781785cfce6299fdc2aa8,"Mitigate flaky test_ops_fwd_gradients on macOS (#89410)

This has been flaky on macOS for a while ([hud](https://hud.pytorch.org/failure/RuntimeError%3A%20test_ops_fwd_gradients%20failed)) and I can reproduce this locally. The issue was raised by https://github.com/pytorch/pytorch/issues/66033 and it seems to point to macos itself https://github.com/graphia-app/graphia/issues/33.  So switching to single thread when running `test_ops_fwd_gradients` on macOS as a mitigation for the flaky tests.

### Testing

`pytest test_ops_fwd_gradients.py -k test_fn_fwgrad_bwgrad -vv --flake-finder` to run all `test_fn_fwgrad_bwgrad` tests 50 times to make sure they all pass (no flaky anymore)

https://hud.pytorch.org/tests shows that `test_ops_fwd_gradients` on macOS takes about 15m to finish or 8 minute if using 2 shards like in the test.  There is no obvious difference in the test duration:

```
2022-11-21T21:34:18.6078080Z Running test_ops_fwd_gradients ... [2022-11-21 21:34:18.600663]
2022-11-21T21:34:21.6805770Z Executing ['/Users/runner/work/_temp/conda_environment_3517515737/bin/python', '-bb', 'test_ops_fwd_gradients.py', '-v', '--use-pytest', '-vv', '-rfEX', '-x', '--reruns=2', '--shard-id=0', '--num-shards=2', '-k=not _linalg_cholesky_', '--import-slow-tests', '--import-disabled-tests'] ... [2022-11-21 21:34:21.680156]
2022-11-21T21:34:21.6806380Z Ignoring disabled issues:  []
2022-11-21T21:34:21.6815250Z Executing ['/Users/runner/work/_temp/conda_environment_3517515737/bin/python', '-bb', 'test_ops_fwd_gradients.py', '-v', '--use-pytest', '-vv', '-rfEX', '-x', '--reruns=2', '--shard-id=1', '--num-shards=2', '-k=not _linalg_cholesky_', '--import-slow-tests', '--import-disabled-tests'] ... [2022-11-21 21:34:21.681174]
2022-11-21T21:34:21.6815830Z Ignoring disabled issues:  []
.....
2022-11-21T21:40:42.2422700Z =============================== warnings summary ===============================
.....
2022-11-21T21:40:42.2424670Z - generated xml file: /Users/runner/work/pytorch/pytorch/test/test-reports/python-pytest/test_ops_fwd_gradients/test_ops_fwd_gradients-47b619449ea7db1f.xml -
2022-11-21T21:40:42.2424850Z = 831 passed, 596 skipped, 5 deselected, 17 xfailed, 1 warning in 374.54s (0:06:14) =
.....
2022-11-21T21:42:00.1923310Z =============================== warnings summary ===============================
.....
2022-11-21T21:42:00.1925370Z - generated xml file: /Users/runner/work/pytorch/pytorch/test/test-reports/python-pytest/test_ops_fwd_gradients/test_ops_fwd_gradients-d24ee6419a602a6e.xml -
2022-11-21T21:42:00.1925540Z = 828 passed, 603 skipped, 7 deselected, 20 xfailed, 1 warning in 452.94s (0:07:32) =
....
2022-11-21T21:42:09.9035670Z FINISHED PRINTING LOG FILE of test_ops_fwd_gradients (/Users/runner/work/pytorch/pytorch/test/test-reports/test_ops_fwd_gradients_ha_3rfhb)
```

Pull Request resolved: https://github.com/pytorch/pytorch/pull/89410
Approved by: https://github.com/soulitzer"
pytorch/pytorch,e4d9dbd7d236e86fac0055feb7dd8f64516d375e,"Port torchdynamo's torchbench script to userbenchmark (#89239)

Summary:
This Diff ports the torchbench.py script from torchdynamo to torchbench to support the development of internal models.

Currently, only works with the `--only` option, and can only test one model at a time.

Note that the noisy logs are from upstream model code, not the benchmark code.
In the internal environment, `torch._dynamo.config.base_dir` is not writable, so we add an option to specify the output directory.

Test Plan:
```
$ buck2 run mode/opt //caffe2/benchmarks/dynamo:torchbench -- --performance --only ads_dhen_5x --part over --output-directory /tmp/tb-test/
cuda eval  ads_dhen_5x
  1/  1 +0 frames   2s  1 graphs  1 graph calls  412/ 411 = 100% ops 100% time
```

```
$  buck2 run mode/opt //caffe2/benchmarks/dynamo:torchbench -- --performance --only cmf_10x --part over --output-directory /tmp/tb-test/
cuda eval  cmf_10x
  1/  1 +0 frames   1s  1 graphs  1 graph calls  306/ 305 = 100% ops 100% time
```

Reviewed By: jansel

Differential Revision: D41294311

Pull Request resolved: https://github.com/pytorch/pytorch/pull/89239
Approved by: https://github.com/jansel"
pytorch/pytorch,6796979ee1063890fd04bbf21f298f669129df8f,"[Inductor] Limit the number of compile threads to the available cpu cores (#89377)

`config.compile_threads` gets the number of compile threads via `min(32,os.cpu_count())` while `os.cpu_count()` is the total number of cpu cores in the system, not the available ones. This would cause compile thread contention when the available cpu cores are less than `min(32,os.cpu_count())`, e.g., available cpu cores are limited with numactl or taskset, making the compilation very slow. This PR tries to use `len(os.sched_getaffinity(0))` if `os.sched_getaffinity` is available which returns the available number of cpu cores.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/89377
Approved by: https://github.com/soumith"
pytorch/pytorch,a80e5e78137fb8adea6e7d638be483f866fe26e8,"Update ideep for future performance improvement (#87966)

**Summary**
The update includes API changes and optimzations to reduce framework overhead, which will benefit all mkldnn (onednn) ops in JIT mode and inductor CPU backend, etc. These benefits will be seen after switching to new ideep API by future PRs.

**Test plan**
For correctness, all UTs that call mkldnn ops, including test_ops.py, test_mkldnn*.py, test_quantization.py, etc.
For performance, TorchBench has been run and no regression is found. Results are shown below.
- Intel (R) Xeon (R) IceLake with 40 cores
- Use multi-instance
- Using tcmalloc & Intel OMP

![image](https://user-images.githubusercontent.com/12522207/201631004-bb77468d-953b-4757-a001-94d44615b5f6.png)

Pull Request resolved: https://github.com/pytorch/pytorch/pull/87966
Approved by: https://github.com/jgong5, https://github.com/XiaobingSuper"
pytorch/pytorch,31708a731076b7feed3051b81d309a9babb4efc0,"TorchDynamo: enable conv+silu fusion (#89278)

This PR will improve the tf_efficientnet_b0 performance by fusing conv+silu.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/89278
Approved by: https://github.com/jgong5, https://github.com/jansel"
pytorch/pytorch,79770d3636626b2130e58d5acdf1d6a56953329d,"TorchDynamo: enable conv+relu6 fusion (#89265)

This PR is about enabled conv+relu6 which improves mobilenet'e performance.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/89265
Approved by: https://github.com/jgong5, https://github.com/jansel"
pytorch/pytorch,e1d58b1928a9427f05e3f44ab9b8119000bced09,"Revert ""Update sdp dispatch logic to enable fused backward (#89154)""

This reverts commit 2e72ec79823111e8dd8c5e82c5d1b56197cd52d3.

Reverted https://github.com/pytorch/pytorch/pull/89154 on behalf of https://github.com/huydhn due to Sorry for reverting your PR but the new test_sdp_math_gradcheck test breaks periodic slow gradcheck, i.e. https://hud.pytorch.org/pytorch/pytorch/commit/419ef2cdcfe84442de5232739284c6a51a18632f"
pytorch/pytorch,7c811efab70a3546f997e37178c93d1de24e0444,"Add support for dynamic kwarg to torch._dynamo.optimize (#89290)

This is an easier way to enable dynamic shapes for a region.

Signed-off-by: Edward Z. Yang <ezyang@fb.com>

Pull Request resolved: https://github.com/pytorch/pytorch/pull/89290
Approved by: https://github.com/soumith, https://github.com/jansel, https://github.com/voznesenskym"
pytorch/pytorch,808bdbab89e875abbbe9652bde675b4402eed532,"Fix try/except flow where DataDependentOutputException is getting wrapped in a RuntimeError (#89314)

Repro fixed

```
def fn(a):
    return a.repeat_interleave(14, dim=0).repeat_interleave(14, dim=1)

x = torch.ones(14, 14).to(dtype=torch.int64)
opt_fn = torch._dynamo.optimize(""eager"")(fn)
opt_fn(x)
```

Fixes [#1886](https://github.com/pytorch/torchdynamo/issues/1886)

Pull Request resolved: https://github.com/pytorch/pytorch/pull/89314
Approved by: https://github.com/anijain2305, https://github.com/eellison"
pytorch/pytorch,85a87e635c677e1c6d992bb9ea21c634e8b1d58f,"[dynamo] mutable local caching to make dynamo faster at tracing mutation (#89170)

Make mutation faster to speed up tracing optimizers, helps with https://github.com/pytorch/torchdynamo/issues/1803

`replace_all` no longer iterates over the entire variable tracker data structure  every time a mutation is performed

Each variable tracker internally keeps a set of contained mutable variable trackers, to provide a hint to `replace_all`. This is populated with a call to `apply` from `__post_init__` in the base `VariableTracker`

Pull Request resolved: https://github.com/pytorch/pytorch/pull/89170
Approved by: https://github.com/jansel"
pytorch/pytorch,7ec8a4d2a26f717d0a4073e6005f9edfdd7ab641,"Vectorized horizontal flip implementation (#88989)

When we benchmarked image processing transforms in torchvision : tensor vs pillow we saw that horizontal flip on uint8 data `(3, X, X)` is 2-3x slower.

Due to the fact that output's first stride is negative, implementation does a simple data copy using [`basic_loop`](https://github.com/pytorch/pytorch/blob/8371bb8a3dddbead709bc1e9d26715818a34fa8a/aten/src/ATen/native/cpu/Loops.h#L286). In this PR, a vectorized path is added for horizontal flip op for dtypes: uint8, int, float32, long and double and there is a speed-up that reduces the gap between PIL and tensor ops

```
CPU capability usage: AVX2

[----------------------------------------------------------------- Horizontal flip -----------------------------------------------------------------]
                                                 |  torch (1.14.0a0+git2ed1d29) PR  |    Pillow (9.3.0)   |  torch (1.14.0.dev20221116+cu116) nightly
1 threads: ------------------------------------------------------------------------------------------------------------------------------------------
      channels=3, size=256, dtype=torch.int64    |        101.307 (+-0.904)         |                     |             111.364 (+-0.328)
      channels=3, size=520, dtype=torch.int64    |        462.369 (+-2.184)         |                     |             505.602 (+-0.541)
      channels=3, size=712, dtype=torch.int64    |        1855.441 (+-6.528)        |                     |             1828.370 (+-8.600)

      channels=1, size=256, dtype=torch.int32    |         22.282 (+-0.130)         |   44.218 (+-0.936)  |              34.651 (+-0.162)
      channels=1, size=520, dtype=torch.int32    |         72.180 (+-0.076)         |  166.639 (+-1.180)  |             118.820 (+-0.210)
      channels=1, size=712, dtype=torch.int32    |        129.621 (+-0.649)         |  307.140 (+-2.221)  |             216.104 (+-0.793)

      channels=3, size=256, dtype=torch.uint8    |         51.685 (+-0.200)         |   44.171 (+-0.818)  |             361.611 (+-0.276)
      channels=3, size=520, dtype=torch.uint8    |        223.320 (+-0.726)         |  166.607 (+-2.256)  |             1462.012 (+-4.917)
      channels=3, size=712, dtype=torch.uint8    |        423.298 (+-1.156)         |  307.067 (+-1.999)  |             2738.481 (+-1.715)

      channels=1, size=256, dtype=torch.float32  |         22.281 (+-0.056)         |   44.149 (+-0.808)  |              35.316 (+-0.028)
      channels=1, size=520, dtype=torch.float32  |         72.268 (+-0.106)         |  166.631 (+-1.212)  |             119.504 (+-0.340)
      channels=1, size=712, dtype=torch.float32  |        129.777 (+-0.632)         |  307.078 (+-1.909)  |             216.987 (+-0.185)

      channels=1, size=256, dtype=torch.float16  |         32.789 (+-0.081)         |                     |              34.044 (+-0.039)
      channels=1, size=520, dtype=torch.float16  |        112.693 (+-0.478)         |                     |             117.445 (+-0.125)
      channels=1, size=712, dtype=torch.float16  |        203.644 (+-0.791)         |                     |             213.283 (+-0.397)

      channels=3, size=256, dtype=torch.float64  |        102.058 (+-0.333)         |                     |             108.404 (+-0.346)
      channels=3, size=520, dtype=torch.float64  |        473.139 (+-1.327)         |                     |             503.265 (+-0.365)
      channels=3, size=712, dtype=torch.float64  |        1854.489 (+-9.513)        |                     |             1844.345 (+-1.371)

      channels=1, size=256, dtype=torch.int16    |         11.927 (+-0.056)         |                     |              33.993 (+-0.037)
      channels=1, size=520, dtype=torch.int16    |         39.724 (+-0.148)         |                     |             117.577 (+-0.153)
      channels=1, size=712, dtype=torch.int16    |         68.264 (+-0.133)         |                     |             213.118 (+-0.157)

Times are in microseconds (us).

```

```
CPU capability usage: AVX512

[----------------------------------------------------------------- Horizontal flip ------------------------------------------------------------------]
                                                 |  torch (1.14.0a0+git2ed1d29) PR  |    Pillow (9.3.0)    |  torch (1.14.0.dev20221118+cu116) nightly
1 threads: -------------------------------------------------------------------------------------------------------------------------------------------
      channels=3, size=256, dtype=torch.int64    |        131.244 (+-1.954)         |                      |             135.649 (+-4.066)
      channels=3, size=520, dtype=torch.int64    |        522.032 (+-4.660)         |                      |             539.822 (+-10.420)
      channels=3, size=712, dtype=torch.int64    |       1041.111 (+-53.575)        |                      |            1322.411 (+-80.017)

      channels=1, size=256, dtype=torch.int32    |         10.108 (+-0.414)         |   49.164 (+-1.000)   |              34.606 (+-0.865)
      channels=1, size=520, dtype=torch.int32    |         93.218 (+-1.417)         |  191.985 (+-5.047)   |             133.664 (+-5.372)
      channels=1, size=712, dtype=torch.int32    |        167.919 (+-2.854)         |  353.574 (+-6.568)   |             246.162 (+-5.753)

      channels=3, size=256, dtype=torch.uint8    |         34.710 (+-0.541)         |   49.005 (+-0.923)   |             136.603 (+-2.339)
      channels=3, size=520, dtype=torch.uint8    |        154.873 (+-3.049)         |  191.729 (+-4.997)   |             534.329 (+-10.754)
      channels=3, size=712, dtype=torch.uint8    |        290.319 (+-4.819)         |  351.619 (+-6.978)   |             997.119 (+-33.086)

      channels=1, size=256, dtype=torch.float32  |         10.345 (+-0.338)         |   49.105 (+-0.942)   |              35.478 (+-0.733)
      channels=1, size=520, dtype=torch.float32  |         81.131 (+-5.281)         |  191.697 (+-4.555)   |             133.554 (+-4.193)
      channels=1, size=712, dtype=torch.float32  |        169.581 (+-3.476)         |  352.995 (+-10.792)  |             251.089 (+-7.485)

      channels=1, size=256, dtype=torch.float16  |         35.259 (+-0.612)         |                      |              35.154 (+-0.924)
      channels=1, size=520, dtype=torch.float16  |        132.407 (+-1.980)         |                      |             131.850 (+-5.611)
      channels=1, size=712, dtype=torch.float16  |        240.192 (+-5.479)         |                      |             239.555 (+-7.273)

      channels=3, size=256, dtype=torch.float64  |        129.649 (+-2.349)         |                      |             130.429 (+-6.240)
      channels=3, size=520, dtype=torch.float64  |        548.534 (+-5.179)         |                      |             622.568 (+-25.720)
      channels=3, size=712, dtype=torch.float64  |       1208.091 (+-77.095)        |                      |            1679.204 (+-316.292)

      channels=1, size=256, dtype=torch.int16    |         7.801 (+-0.115)          |                      |              34.517 (+-0.482)
      channels=1, size=520, dtype=torch.int16    |         36.010 (+-0.855)         |                      |             131.001 (+-1.686)
      channels=1, size=712, dtype=torch.int16    |         87.395 (+-1.355)         |                      |             237.731 (+-4.181)

Times are in microseconds (us).
```

[Source](https://gist.github.com/vfdev-5/c0421f54c8aed655b042dd1ce4cb621e)

Pull Request resolved: https://github.com/pytorch/pytorch/pull/88989
Approved by: https://github.com/lezcano, https://github.com/datumbox, https://github.com/peterbell10, https://github.com/ngimel"
pytorch/pytorch,30c3e5afb0c0ad22c1084a2064ebdc09f7808ecc,"Disable tracing `zero_grad()` (#88731)

Tracing through zero grad is slow, and doesn't provide any benefits.

Helps https://github.com/pytorch/torchdynamo/issues/1803

Pull Request resolved: https://github.com/pytorch/pytorch/pull/88731
Approved by: https://github.com/anijain2305"
pytorch/pytorch,2b131b1d43b10a2a005f3f042f920a62501e4e2d,"Support masked_fill (#88736)

Support `masked_fill` to address the GPT2 performance issue.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/88736
Approved by: https://github.com/jansel, https://github.com/jgong5"
pytorch/pytorch,3beccbc29939f7a34346ed1a3646f6464086eeb4,"Add BFloat16 support and optimization for mish, hardtanh backward, and silu on CPU (#82460)

### Description
* add BFloat16 support for mish and hardtanh backward on CPU.
* optimize the performance for silu

### Testing

- optimize the performance for silu: bfloat16

single socket (28 cores):
```
before: 1x128x1024  forward 0.090 s  backward  0.218 s
        10x128x1024 forward 0.146 s  backward  0.314 s

after:  1x128x1024   forward  0.064 s backward  0.100 s
        10x128x1024  forward  0.085 s backward  0.133 s
```
single core:
```
before: 1x128x1024   forward 0.300 s  backward  0.606 s
        10x128x1024  forward 2.825 s  backward  5.834 s

after:  1x128x1024   forward 0.156 s backward   0.239 s
        10x128x1024  forward 1.447 s backward   2.165 s
```

- Add BFloat16 support for mish and backward of hardtanh on CPU.

single socket (20 cores):
op | shape | fp32 / s | fp32 / s | bf16 / s |  bf16 / s
-- | -- | -- | -- | -- | --
  |   | forward | backward | forward | backward
silu | [10, 128, 10, 10] | 4.41E-05 | 7.67E-05 | 5.32E-05 | 9.38E-05
  | [10, 128, 80, 80] | 0.0008 | 0.001788 | 0.00067 | 0.001031
mish | [10, 128, 10, 10] | 0.000356 | 0.000427 | 0.000367 | 0.000436
  | [10, 128, 80, 80] | 0.004527 | 0.005807 | 0.004757 | 0.005393
hardtanh | [10, 128, 10, 10] | / | 3.97E-05 | / | 4.45E-05
  | [10, 128, 80, 80] | / | 0.001748 | / | 0.000645

single core:
op | shape | fp32 / s | fp32 / s | bf16 / s |  bf16 / s
-- | -- | -- | -- | -- | --
  |   | forward | backward | forward | backward
silu | [10, 128, 10, 10] | 1.17E-04 | 1.91E-04 | 1.35E-04 | 2.23E-04
  | [10, 128, 80, 80] | 0.007434 | 0.013141 | 0.008464 | 0.013044
mish | [10, 128, 10, 10] | 0.00103 | 0.00122 | 0.00106 | 0.001227
  | [10, 128, 80, 80] | 0.065629 | 0.078418 | 0.067779 | 0.077214
hardtanh | [10, 128, 10, 10] | / | 1.18E-04 | / | 9.30E-05
  | [10, 128, 80, 80] | / | 0.010773 | / | 0.005834

Pull Request resolved: https://github.com/pytorch/pytorch/pull/82460
Approved by: https://github.com/mingfeima, https://github.com/malfet"
pytorch/pytorch,37c85cf5f2215da13d5836de46f44af72ed079ba,"Add warning if tensor cores are not used (#88844)

Fixes https://github.com/pytorch/torchdynamo/issues/1839

Should I do this for all backends or just inductor?

## Test
On a V100 I got from AWS

```python
from torch._dynamo import optimize
import torch

def fn(x, y):
    a = torch.cos(x)
    b = torch.sin(y)
    return a + b

new_fn = optimize(""inductor"")(fn)

a = new_fn(torch.Tensor(1),torch.Tensor(1))
print(a)
```

## New logs
```
(sourcetorch) ubuntu@ip-172-31-31-152:~/test$ python test.py
/home/ubuntu/pytorch/torch/_dynamo/eval_frame.py:318: UserWarning: Tensor cores are available but not enabled. Consider setting torch.backends.cuda.matmul.allow_tf32 == True in your python script for speedups
  warnings.warn(
tensor([1.3717])
```

Pull Request resolved: https://github.com/pytorch/pytorch/pull/88844
Approved by: https://github.com/ngimel, https://github.com/mlazos, https://github.com/anijain2305"
pytorch/pytorch,b72f5b9ae3f7d1de74d9d2d40236fd09d606be0e,"[Dynamo] Support typing.Mapping & Support function as argument (#88963)

These missing features come from https://github.com/pytorch/benchmark/pull/1302, where we'd like to enable E2E hf_bert dynamo train/eval. The dependent [HuggingFace accelerate library](https://huggingface.co/docs/accelerate/index) requires these improvements.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/88963
Approved by: https://github.com/jansel"
pytorch/pytorch,54fca6a9da77b56b1a82373c814e61378b5d04c2,"Fix: prefer .is_none() over .is(py::none()) for pybind11 in caffe2 (#88199)

Follow up to #88051 . I noticed that I missed a few spots in the caffe2 folder. Prefer `.is_none()` over `.is(py::none())` as `.is_none()` is more efficient since it avoid reference counting increments and decrements.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/88199
Approved by: https://github.com/albanD, https://github.com/kit1980"
pytorch/pytorch,ee1d375bf98f6e4c69b2d6f3aa1c702cb652d2f2,"[FSDP] Add fast path for `NO_SHARD` `clip_grad_norm_()` (#89137)

Pull Request resolved: https://github.com/pytorch/pytorch/pull/89137
Approved by: https://github.com/rohan-varma"
pytorch/pytorch,46ba0150cbfb8d86c378f0f3ce2d816e530a933b,"Increase slow grad check timeout (#89079)

Now that periodic jobs are run under `mem_leak_check` mode with parallelization turning off. It's very easy for `linux-bionic-cuda11.6-py3-gcc7-slow-gradcheck / test` to timeout because one of the shards is very close to the 4h mark.

* https://hud.pytorch.org/pytorch/pytorch/commit/2452e3f99a072760fc46d3f9025aaa37ca7ea2ab
* https://hud.pytorch.org/pytorch/pytorch/commit/35e668b5ced25e735b6e523d557ed7fd60267914

Pull Request resolved: https://github.com/pytorch/pytorch/pull/89079
Approved by: https://github.com/clee2000"
pytorch/pytorch,a13433940c4e8d7cc54d4fa5b3a9c0ff28fc0e8b,"allow loading model from a path in torchbench (#89028)

Sometimes it's really convenient to run simple models thru the torchbench.py script rather than those from pytorch/benchmark. This PR add the ability to run any model from a specified path by overloading the --only argument.

This PR is split out from #88904

Here is the usage:

        Specify the path and class name of the model in format like:
        --only=path:<MODEL_FILE_PATH>,class:<CLASS_NAME>

        Due to the fact that dynamo changes current working directory,
        the path should be an absolute path.

        The class should have a method get_example_inputs to return the inputs
        for the model. An example looks like
        ```
        class LinearModel(nn.Module):
            def __init__(self):
                super().__init__()
                self.linear = nn.Linear(10, 10)

            def forward(self, x):
                return self.linear(x)

            def get_example_inputs(self):
                return (torch.randn(2, 10),)
        ```

Test command:
```
# python benchmarks/dynamo/torchbench.py --performance --only=path:/pytorch/myscripts/model_collection.py,class:LinearModel --backend=eager
WARNING:common:torch.cuda.is_available() == False, using CPU
cpu  eval  LinearModel                        0.824x p=0.00
```

Content of model_collection.py
```
from torch import nn
import torch

class LinearModel(nn.Module):
    """"""
    AotAutogradStrategy.compile_fn ignore graph with at most 1 call nodes.
    Make sure this model calls 2 linear layers to avoid being skipped.
    """"""
    def __init__(self, nlayer=2):
        super().__init__()
        layers = []
        for _ in range(nlayer):
            layers.append(nn.Linear(10, 10))
        self.layers = nn.Sequential(*layers)

    def forward(self, x):
        return self.layers(x)

    def get_example_inputs(self):
        return (torch.randn(2, 10),)
```

Pull Request resolved: https://github.com/pytorch/pytorch/pull/89028
Approved by: https://github.com/jansel"
pytorch/pytorch,da2afcb1e0006354f78d5e56d2933382d7af9ebf,"Add test for out-of-bounds Tensor access on GPU (#39211)

Since CUDA context can not recover safely from on-device assert, use `torch.multiprocessing.spawn` to execute a method in another context and verify that it raises unrecoverable error.

As those types of tests are pretty slow (6 seconds on powerful linux box with one GPU) run it only in the slow shard.

Closes https://github.com/pytorch/pytorch/issues/38944

Pull Request resolved: https://github.com/pytorch/pytorch/pull/39211
Approved by: https://github.com/ezyang"
pytorch/pytorch,ff6d2a6d1b8245563c8122849144dddaa276483a,"Add mem efficient backward (#88856)

# Registers the derivative for mem efficient backward

- Use gradcheck to test correctness. The kernel is not implemented for fp64 so run checks with bumped tolerances in fp32
- I also made updates based off of Xformer main branch and flash-attention cutlass branch.
- This will enable the fused backward to be called for scaled dot product attention

Pull Request resolved: https://github.com/pytorch/pytorch/pull/88856
Approved by: https://github.com/cpuhrsch"
pytorch/pytorch,f5df68509097c65263ccf100e5df6b1057e9a2fa,"Enable channels_last_3d on SyncBatchNorm (#88401)

This PR enabled the use of fast channels_last kernels on SyncBatchNorm with channels_last_3d memory format.

With a small benchmark script here https://github.com/pytorch/pytorch/issues/88021#issuecomment-1299059859, on V100, I got

master:
```
DDP channels_last=False, run_forward_backward, time: 0.8945400714874268 sec
DDP channels_last=True, run_forward_backward, time: 1.4736433029174805 sec
```

This PR:
```
DDP channels_last=False, run_forward_backward, time: 0.8927242755889893 sec
DDP channels_last=True, run_forward_backward, time: 0.48697471618652344 sec
```

This PR is a follow-up of https://github.com/pytorch/pytorch/pull/46906

Close https://github.com/pytorch/pytorch/issues/88021
Pull Request resolved: https://github.com/pytorch/pytorch/pull/88401
Approved by: https://github.com/ngimel"
pytorch/pytorch,1db0f735e8fe14245e98e875c15ecf95ed2142ce,"[Profiler] Account for caching when assigning IDs (#88917)

The python tracer caches information about module and optimizer state. That means that for subsequent calls, the presence of a Tensor in these fields does not imply that the Tensor is still live; just that it was live during the first call. (I should perhaps rename the fields to something like `stale_parameters` to convey this.) Unless we discard subsequent calls ID assignment get tripped up when it see's a Tensor that was already released.

Differential Revision: [D41226827](https://our.internmc.facebook.com/intern/diff/D41226827/)
Pull Request resolved: https://github.com/pytorch/pytorch/pull/88917
Approved by: https://github.com/chaekit"
pytorch/pytorch,50c18217a3849c56a0fe5bdb923bd67fa70da31c,"Revert ""Add mem efficient backward (#88856)""

This reverts commit 35e668b5ced25e735b6e523d557ed7fd60267914.

Reverted https://github.com/pytorch/pytorch/pull/88856 on behalf of https://github.com/DanilBaibak due to breaking internal builds"
pytorch/pytorch,35e668b5ced25e735b6e523d557ed7fd60267914,"Add mem efficient backward (#88856)

# Registers the derivative for mem efficient backward

- Use gradcheck to test correctness. The kernel is not implemented for fp64 so run checks with bumped tolerances in fp32
- I also made updates based off of Xformer main branch and flash-attention cutlass branch.
- This will enable the fused backward to be called for scaled dot product attention

Pull Request resolved: https://github.com/pytorch/pytorch/pull/88856
Approved by: https://github.com/cpuhrsch"
pytorch/pytorch,072920c281bb4d9ca899c6c781a8374ab42a9a3f,"TorchDynamo: Add convolution binary+unary fusion for cpu in inference mode (#88412)

This PR is about enabling the fusion of **conv+binary+relu**, which will improve the vision model's performance.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/88412
Approved by: https://github.com/jgong5, https://github.com/jansel"
pytorch/pytorch,48dc24ddceb5d048ceb38f00f6d4ec0cfc3e71d0,"Fix: [ATen] Add some missing moves (#88514)

Related to #88512 , but for ATen. This should reduce a number of copies and inefficient atomic smart pointer increments.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/88514
Approved by: https://github.com/jgong5, https://github.com/ezyang"
pytorch/pytorch,c83348597b195f2da1cca0e8318c878b104bce5d,"[dynamo][api] Better support of torch.nn.Module (#88629)

This is an API change, so please review carefully.

With this PR, torchdynamo returns an `OptimizedModule` class object, a subclass of `torch.nn.Module`, when asked to optimize a `nn.Module` object. Most of the methods are redirected to the original `nn.Module`, which is installed as `_mod` in the `OptimizedModule`.

This is helpful for many cases

```
mod = MockModule()

opt_mod = torch._dynamo.optimize()(mod)

print(opt_mod) # Works

opt_mod = opt_mod.to(device=""cuda"")
print(opt_mod) # Works
opt_mod(input) # Triggers recompile if necessary, earlier we were shedding the TorchDynamo wrapper

opt_mod.parameters() # Refers to the original module

```

Topics unclear to me
* I have overridden many methods to raise NotImplementedError. A careful review of those will be good.
* hooks
* For the optimized forward, should we call torchdynamo optimization on `__call__` or `forward`
* What else to test

Pull Request resolved: https://github.com/pytorch/pytorch/pull/88629
Approved by: https://github.com/Chillee, https://github.com/jansel, https://github.com/msaroufim"
pytorch/pytorch,a7fa423f48af8af220e9286a6b4c374d533f77e0,"copy_: Short-circuit when self and src view the same data (#88884)

This comes up if you use inplace operators on a slice, e.g.
```python
import torch
a = torch.rand(1000000, device=""cuda"")
a[::2] *= 2
```

The last line looks as if it should be fully inplace, but is actually
equivalent to:

```python
tmp = a[::2]
tmp *= 2
a[::2] = tmp
```

Which results in `mul_` and `copy_` being called. With this PR, the
redundant copy becomes a no-op and the above example is 2x faster.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/88884
Approved by: https://github.com/ngimel"
pytorch/pytorch,fbc1878265374a159639993269d40a6e08503278,"[ONNX] Pretty print diagnostic logging (#88261)

Adds pretty print diagnostic logging. For example
```python
import io
import torch
from torch.onnx._internal import diagnostics

class CustomAdd(torch.autograd.Function):
    @staticmethod
    def forward(ctx, x, y):
        return x + y

    @staticmethod
    def symbolic(g, x, y):
        return g.op(""custom::CustomAdd"", x, y)

class M(torch.nn.Module):
    def forward(self, x):
        return CustomAdd.apply(x, x)

# trigger warning for missing shape inference.
# rule = diagnostics.rules.node_missing_onnx_shape_inference
torch.onnx.export(M(), torch.randn(3, 4), io.BytesIO())
```

By default, observe minimum summary of diagnostics
```
========= Diagnostic Run torch.onnx.export version 1.14.0a0+git90a69c5 =========
verbose: False, log level: Level.ERROR
======================= 0 NONE 0 NOTE 3 WARNING 0 ERROR ========================
3 WARNING were not printed due to the log level.
```

Adjusting the `verbose` and `level` argument.
```python
diagnostics.engine.pretty_print(verbose=True, level=diagnostics.levels.WARNING)
```

Prints full log.
```
=============================== 1 Diagnostic Run ===============================
========= Diagnostic Run torch.onnx.export version 1.14.0a0+git90a69c5 =========
verbose: True, log level: Level.WARNING
======================= 0 NONE 0 NOTE 3 WARNING 0 ERROR ========================
WARNING: node-missing-onnx-shape-inference
==========================================
The shape inference of custom::CustomAdd type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.
--------------------------- Stack: Python call stack ---------------------------
frame: diagnostic = ExportDiagnostic(rule, level, message, **kwargs) /home/bowbao/pytorch_dev/torch/onnx/_internal/diagnostics/_diagnostic.py:151
frame: n, utils._params_dict, GLOBALS.export_onnx_opset_version /home/bowbao/pytorch_dev/torch/onnx/_patch_torch.py:82
frame: <@beartype(torch.onnx._patch_torch._graph_op) at 0x7f62184b6710>:78
frame: return beartyped(*args, **kwargs) /home/bowbao/pytorch_dev/torch/onnx/_internal/_beartype.py:81
frame: return function(*args, **kwargs) /home/bowbao/pytorch_dev/torch/onnx/_deprecation.py:30
frame: return g.op(""custom::CustomAdd"", x, y) test_pretty_print.py:14
frame: return symbolic_fn(g, *args) /home/bowbao/pytorch_dev/torch/onnx/utils.py:1716
frame: return beartyped(*args, **kwargs) /home/bowbao/pytorch_dev/torch/onnx/_internal/_beartype.py:81
frame: graph = _C._jit_pass_onnx(graph, operator_export_type) /home/bowbao/pytorch_dev/torch/onnx/utils.py:663
frame: <@beartype(torch.onnx.utils._optimize_graph) at 0x7f62180e05f0>:85
frame: return beartyped(*args, **kwargs) /home/bowbao/pytorch_dev/torch/onnx/_internal/_beartype.py:81
frame: module=module, /home/bowbao/pytorch_dev/torch/onnx/utils.py:1123
frame: return beartyped(*args, **kwargs) /home/bowbao/pytorch_dev/torch/onnx/_internal/_beartype.py:81
frame: dynamic_axes=dynamic_axes, /home/bowbao/pytorch_dev/torch/onnx/utils.py:1539
frame: return beartyped(*args, **kwargs) /home/bowbao/pytorch_dev/torch/onnx/_internal/_beartype.py:81
frame: export_modules_as_functions=export_modules_as_functions, /home/bowbao/pytorch_dev/torch/onnx/utils.py:519
frame: <@beartype(torch.onnx.utils.export) at 0x7f62180e0170>:347
frame: return beartyped(*args, **kwargs) /home/bowbao/pytorch_dev/torch/onnx/_internal/_beartype.py:81
frame: torch.onnx.export(M(), torch.randn(3, 4), io.BytesIO()) test_pretty_print.py:22
---------------------------- Stack: C++ call stack -----------------------------
frame: (<unknown frame>)
frame: (<unknown function> + 0x88411b (0x7f625b36011b in /home/bowbao/pytorch_dev/torch/lib/libtorch_python.so))
frame: (torch::jit::UpdateReliable(torch::jit::Value*, std::pair<bool, bool> const&) + 0x7d3 (0x7f625b351743 in /home/bowbao/pytorch_dev/torch/lib/libtorch_python.so))
frame: (torch::jit::UpdateReliable(torch::jit::Node*) + 0x4f (0x7f625b35198f in /home/bowbao/pytorch_dev/torch/lib/libtorch_python.so))
frame: (torch::jit::ONNXShapeTypeInference(torch::jit::Node*, std::map<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, c10::IValue, std::less<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::allocator<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const, c10::IValue> > > const&, int) + 0xac9 (0x7f625b357179 in /home/bowbao/pytorch_dev/torch/lib/libtorch_python.so))
frame: (<unknown function> + 0xabd026 (0x7f625b599026 in /home/bowbao/pytorch_dev/torch/lib/libtorch_python.so))
frame: (<unknown function> + 0x3c0fda (0x7f625ae9cfda in /home/bowbao/pytorch_dev/torch/lib/libtorch_python.so))
frame: (<unknown frame>)

WARNING: node-missing-onnx-shape-inference
==========================================
The shape inference of custom::CustomAdd type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.
--------------------------- Stack: Python call stack ---------------------------
frame: diagnostic = ExportDiagnostic(rule, level, message, **kwargs) /home/bowbao/pytorch_dev/torch/onnx/_internal/diagnostics/_diagnostic.py:151
frame: graph, params_dict, GLOBALS.export_onnx_opset_version /home/bowbao/pytorch_dev/torch/onnx/utils.py:688
frame: <@beartype(torch.onnx.utils._optimize_graph) at 0x7f62180e05f0>:85
frame: return beartyped(*args, **kwargs) /home/bowbao/pytorch_dev/torch/onnx/_internal/_beartype.py:81
frame: module=module, /home/bowbao/pytorch_dev/torch/onnx/utils.py:1123
frame: return beartyped(*args, **kwargs) /home/bowbao/pytorch_dev/torch/onnx/_internal/_beartype.py:81
frame: dynamic_axes=dynamic_axes, /home/bowbao/pytorch_dev/torch/onnx/utils.py:1539
frame: return beartyped(*args, **kwargs) /home/bowbao/pytorch_dev/torch/onnx/_internal/_beartype.py:81
frame: export_modules_as_functions=export_modules_as_functions, /home/bowbao/pytorch_dev/torch/onnx/utils.py:519
frame: <@beartype(torch.onnx.utils.export) at 0x7f62180e0170>:347
frame: return beartyped(*args, **kwargs) /home/bowbao/pytorch_dev/torch/onnx/_internal/_beartype.py:81
frame: torch.onnx.export(M(), torch.randn(3, 4), io.BytesIO()) test_pretty_print.py:22
---------------------------- Stack: C++ call stack -----------------------------
frame: (<unknown frame>)
frame: (<unknown function> + 0x88411b (0x7f625b36011b in /home/bowbao/pytorch_dev/torch/lib/libtorch_python.so))
frame: (torch::jit::UpdateReliable(torch::jit::Value*, std::pair<bool, bool> const&) + 0x7d3 (0x7f625b351743 in /home/bowbao/pytorch_dev/torch/lib/libtorch_python.so))
frame: (torch::jit::UpdateReliable(torch::jit::Node*) + 0x4f (0x7f625b35198f in /home/bowbao/pytorch_dev/torch/lib/libtorch_python.so))
frame: (torch::jit::ONNXShapeTypeInference(torch::jit::Node*, std::map<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, c10::IValue, std::less<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::allocator<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const, c10::IValue> > > const&, int) + 0xac9 (0x7f625b357179 in /home/bowbao/pytorch_dev/torch/lib/libtorch_python.so))
frame: (<unknown function> + 0x87d6d1 (0x7f625b3596d1 in /home/bowbao/pytorch_dev/torch/lib/libtorch_python.so))
frame: (torch::jit::ONNXShapeTypeInference(std::shared_ptr<torch::jit::Graph>&, std::map<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, c10::IValue, std::less<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::allocator<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const, c10::IValue> > > const&, int) + 0x33 (0x7f625b359cf3 in /home/bowbao/pytorch_dev/torch/lib/libtorch_python.so))
frame: (<unknown function> + 0xabdbae (0x7f625b599bae in /home/bowbao/pytorch_dev/torch/lib/libtorch_python.so))
frame: (<unknown function> + 0x3c0fda (0x7f625ae9cfda in /home/bowbao/pytorch_dev/torch/lib/libtorch_python.so))
frame: (<unknown frame>)

WARNING: node-missing-onnx-shape-inference
==========================================
The shape inference of custom::CustomAdd type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.
--------------------------- Stack: Python call stack ---------------------------
frame: diagnostic = ExportDiagnostic(rule, level, message, **kwargs) /home/bowbao/pytorch_dev/torch/onnx/_internal/diagnostics/_diagnostic.py:151
frame: graph, params_dict, GLOBALS.export_onnx_opset_version /home/bowbao/pytorch_dev/torch/onnx/utils.py:1179
frame: return beartyped(*args, **kwargs) /home/bowbao/pytorch_dev/torch/onnx/_internal/_beartype.py:81
frame: dynamic_axes=dynamic_axes, /home/bowbao/pytorch_dev/torch/onnx/utils.py:1539
frame: return beartyped(*args, **kwargs) /home/bowbao/pytorch_dev/torch/onnx/_internal/_beartype.py:81
frame: export_modules_as_functions=export_modules_as_functions, /home/bowbao/pytorch_dev/torch/onnx/utils.py:519
frame: <@beartype(torch.onnx.utils.export) at 0x7f62180e0170>:347
frame: return beartyped(*args, **kwargs) /home/bowbao/pytorch_dev/torch/onnx/_internal/_beartype.py:81
frame: torch.onnx.export(M(), torch.randn(3, 4), io.BytesIO()) test_pretty_print.py:22
---------------------------- Stack: C++ call stack -----------------------------
frame: (<unknown frame>)
frame: (<unknown function> + 0x88411b (0x7f625b36011b in /home/bowbao/pytorch_dev/torch/lib/libtorch_python.so))
frame: (torch::jit::UpdateReliable(torch::jit::Value*, std::pair<bool, bool> const&) + 0x7d3 (0x7f625b351743 in /home/bowbao/pytorch_dev/torch/lib/libtorch_python.so))
frame: (torch::jit::UpdateReliable(torch::jit::Node*) + 0x4f (0x7f625b35198f in /home/bowbao/pytorch_dev/torch/lib/libtorch_python.so))
frame: (torch::jit::ONNXShapeTypeInference(torch::jit::Node*, std::map<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, c10::IValue, std::less<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::allocator<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const, c10::IValue> > > const&, int) + 0xac9 (0x7f625b357179 in /home/bowbao/pytorch_dev/torch/lib/libtorch_python.so))
frame: (<unknown function> + 0x87d6d1 (0x7f625b3596d1 in /home/bowbao/pytorch_dev/torch/lib/libtorch_python.so))
frame: (torch::jit::ONNXShapeTypeInference(std::shared_ptr<torch::jit::Graph>&, std::map<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, c10::IValue, std::less<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::allocator<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const, c10::IValue> > > const&, int) + 0x33 (0x7f625b359cf3 in /home/bowbao/pytorch_dev/torch/lib/libtorch_python.so))
frame: (<unknown function> + 0xabdbae (0x7f625b599bae in /home/bowbao/pytorch_dev/torch/lib/libtorch_python.so))
frame: (<unknown function> + 0x3c0fda (0x7f625ae9cfda in /home/bowbao/pytorch_dev/torch/lib/libtorch_python.so))
frame: (<unknown frame>)
```

Pull Request resolved: https://github.com/pytorch/pytorch/pull/88261
Approved by: https://github.com/abock, https://github.com/justinchuby"
pytorch/pytorch,ea0ec9d71ca5428bedfcaf74990c109af8cb9a64,"[tourch] BatchBoxCox - fix numerical issue in vectorized code (#88875)

Summary:
Usage of fast math in BatchBoxCox kernel provided different math results between dev and optimized versions which cause few internal test to fail.
For now disabling the compiler optimized version and relying on ATEN vectors

Differential Revision: D41211784

Pull Request resolved: https://github.com/pytorch/pytorch/pull/88875
Approved by: https://github.com/hyuen"
pytorch/pytorch,adfbd831cf59111c3d3a4a50ba6372bba94b63d1,"Revert ""[Autograd] Use in-place input accumulation fast path for dense Tensors. (#88339)""

This reverts commit 8f66ae413f8c9d7f2418d7f0b9f69d409c455b46.

Reverted https://github.com/pytorch/pytorch/pull/88339 on behalf of https://github.com/mehtanirav due to Internal test failures"
pytorch/pytorch,396c3b1d88d7624938a2bb0b287f2a19f1e89bb4,"Use `atomicAdd` for `bfloat16` in Ampere and above (#84981)

WIP to fix extremely slow `scatter_add` issue vs. fp16. The current changes seem to improve performance, but it still appears to lag behind the fp16 equivalent.

CC @ngimel @ptrblck
Pull Request resolved: https://github.com/pytorch/pytorch/pull/84981
Approved by: https://github.com/ngimel"
pytorch/pytorch,9d09968bbe05fc6d7d7c3d8b1acfbe1b1b1413a8,"Disable check for dropout in MultiheadAttention fast_path (#88831)

Since we already enforce eval mode for the fast_path, we do not need to also check for a falsy dropout value, as a model trained with dropout will have a non-zero dropout during eval mode, even though it won't be applied.

Fixes #88806

Pull Request resolved: https://github.com/pytorch/pytorch/pull/88831
Approved by: https://github.com/drisspg"
pytorch/pytorch,1ae772a663f772171f0c5d6d7d311792f331206a,"[inductor] Remove import check for fast_flush (#88812)

https://github.com/pytorch/pytorch/pull/88557/ has a guard to make sure that triton's `do_bench` includes the `fast_flush` argument.  Since we've updated Triton to a sufficiently recent revision, we can remove that guard.

Differential Revision: [D41185280](https://our.internmc.facebook.com/intern/diff/D41185280/)

Pull Request resolved: https://github.com/pytorch/pytorch/pull/88812
Approved by: https://github.com/soumith"
pytorch/pytorch,6bf2776ac1d16692778f052ba6796d3308ea97c6,"[FSDP][Perf] Do not call `pad` in no-padding case (#88769)

- Calling `F.pad()` issues a pad kernel from the CPU even if there is no padding needed, which can incur some non-negligible overhead. This PR removes that unnecessary call for the no-padding case.
- This PR also does not zero the newly-allocated sharded gradient tensor before the reduce-scatter if `use_orig_params=True` because there is no need. The reduce-scatter will fill the tensor anyway, and we do not care about the values in the padding. For `use_orig_params=False`, the padding is exposed to the user, so we preserve the existing semantics of zeroing it. I left a to-do to follow-up since we may optimize that.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/88769
Approved by: https://github.com/zhaojuanmao"
pytorch/pytorch,7ad87f63e248b629d435a199cb61f4ed1f3dfcab,"Support src_mask and src_key_padding_mask for Better Transformer (#88488)

Fixes T135842750 (follow-up for #87377)

## Description

At present, having both `src_key_padding_mask` and `src_mask` at the same time is not supported on the fastpath in Transformer and Multi-Head Attention.

This PR enables using both masks on the fastpath on CPU and GPU: if both masks are passed, we merge them into a 4D mask in Python and change mask type to 2 before passing downstream.

Downstream processing in native code is not changed, as it already supports 4D mask. Indeed, it is done depending on the device:
- on CUDA, by `SoftMax.cu::masked_softmax_cuda`. When mask type is 2, it calls either `dispatch_softmax_forward` -> `softmax_warp_forward` or `at::softmax` (depending on the input size). In both cases 4D mask is supported.
- on CPU, by `SoftMax.cpp::masked_softmax_cpp`. It calls `hosted_softmax` which supports 4D mask.

## Tests
- Extended `test_mask_check_fastpath` to check that fast path is indeed taken in Transformer when two masks are passed
- Added `test_multihead_self_attn_two_masks_fast_path_mock` to check that fast path is taken in MHA when two masks are passed
- Added `test_multihead_self_attn_two_masks_fast_path` to check that fast and slow paths give the same result when two masks are passed in MHA
- `test_masked_softmax_mask_types` now covers mask type 2
- `test_transformerencoderlayer_fast_path` (CPU smoke test) is expanded to the case of both masks provided simultaneously
- `test_masked_softmax_devices_parity` checks that mask type 2 is accepted by CPU and CUDA paths

Pull Request resolved: https://github.com/pytorch/pytorch/pull/88488
Approved by: https://github.com/mikekgfb"
pytorch/pytorch,dcefea2706fb35ece5e49fc138d952a2acd15824,"[caffe2][tourch] Optimize BatchBoxCox (#87585)

Differential Revision: D40215424

Pull Request resolved: https://github.com/pytorch/pytorch/pull/87585
Approved by: https://github.com/hyuen"
pytorch/pytorch,6e3555edea3ec2f453d6dc2ddcba9c6313d5ced5,"Add absolute latency to dashboard (#88790)

Add absolute latency to dashboard, as requested by https://github.com/pytorch/torchdynamo/issues/1833#issuecomment-1302742914

Tested by setting `run.sh` to
```
# Setup the output directory
rm -rf ../test-dynamo-runner-logs-7/
mkdir ../test-dynamo-runner-logs-7/

# Commands for torchbench for device=cuda, dtype=float32 for training and for performance testing
python benchmarks/dynamo/torchbench.py --performance --float32 -dcuda --output=../test-dynamo-runner-logs-7//inductor_torchbench_float32_training_cuda_performance.csv --training --inductor   --no-skip --dashboard --only mobilenet_v2 --cold_start_latency

# Commands for torchbench for device=cuda, dtype=float32 for training and for accuracy testing
python benchmarks/dynamo/torchbench.py --accuracy --float32 -dcuda --output=../test-dynamo-runner-logs-7//inductor_torchbench_float32_training_cuda_accuracy.csv --training --inductor   --no-skip --dashboard --only mobilenet_v2
```
and running `python benchmarks/dynamo/runner.py --output-dir ../test-dynamo-runner-logs-7/ --dashboard-archive-path /data/home/williamwen/dynamo-runner-logs-copy --training --run --compilers inductor --flag-compilers inductor --suites torchbench --update-dashboard`  (need to comment out the `generate_commands` line and change the github issue ID from 681 to something else).

Sample comment: https://github.com/pytorch/torchdynamo/issues/1831#issuecomment-1309645562

NOTE: this change breaks processing old logs.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/88790
Approved by: https://github.com/anijain2305"
pytorch/pytorch,fb5c6ae61f1f622ec388ae9fa00e7683ce1729ce,"[cuDNN][cuDNN V8 API] Match V7 API behavior for `channels_last` stride coercion for cuDNN (#88699)

For ConvNeXt failure in https://github.com/pytorch/torchdynamo/issues/1833

cuDNN V7 has some stride ""fixing"" code to coerce cuDNN to use channels-last in cases when allowed by size 1 strides that was omitted in V8, which seems to seems to lead to performance regressions. This PR patches in the same fix for V8.

CC @ngimel @ptrblck
Pull Request resolved: https://github.com/pytorch/pytorch/pull/88699
Approved by: https://github.com/ngimel"
pytorch/pytorch,ab9a19a95b628132bf0ad6474f245b4e596b9d74,"[BE] Move `setup-ssh` step ahead of clone PyTorch (#88715)

It allows one to SSH faster rather than having to wait for repo clone to
finish.

I.e. right now one usually have to wait for a few minutes fore PyTorch clone is finished, but with this change you can SSH ahead of time (thanks to `setup-ssh` being a composite action

Pull Request resolved: https://github.com/pytorch/pytorch/pull/88715
Approved by: https://github.com/clee2000, https://github.com/izaitsevfb"
pytorch/pytorch,8f66ae413f8c9d7f2418d7f0b9f69d409c455b46,"[Autograd] Use in-place input accumulation fast path for dense Tensors. (#88339)

There is a fast path in InputBuffer to steal memory when use count is zero, however it is only used for sparse Tensors. According to Natalia, this is just because it wasn't obvious that there would be a benefit for dense Tensors so there was no reason to live dangerously. However I've noticed large Tensors in internal models which would benefit from this optimization as well.

Differential Revision: [D40946601](https://our.internmc.facebook.com/intern/diff/D40946601/)

Pull Request resolved: https://github.com/pytorch/pytorch/pull/88339
Approved by: https://github.com/ngimel"
pytorch/pytorch,eb3f975c6e29104014fa9bbffe12ab32709672d9,"Fix segfault in has_torch_function (#88559)

Fixes #83908

`PySequence_Fast` may return `NULL` to indicate an error was raised, in which
case `sequence_has_torch_function` will dereference a null pointer.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/88559
Approved by: https://github.com/ezyang, https://github.com/Skylion007, https://github.com/hameerabbasi"
pytorch/pytorch,4796e23bbbdcbfa9110338af3c445ca366bd0b2b,"Fix pull docs build running with a schedule and increase cpp doc timeout to 4h (#88589)

* After https://github.com/pytorch/pytorch/pull/88373, pull workflow can now be triggered with a schedule. This changes the assumption in the doc build workflow when schedule event is used to determine if the docs should be pushed
* I'll create a follow-up issue to see if it's possible to improve the performance of cpp doc build job.  At the moment, it uses a linux.12xlarge runner and still couldn't finish the job after 3h

Pull Request resolved: https://github.com/pytorch/pytorch/pull/88589
Approved by: https://github.com/seemethere, https://github.com/ZainRizvi"
pytorch/pytorch,0e67b2f7dd13db1fea421d860ede65a653738dfe,"Dynamo Dashboard Improvements (#88516)

Implement various features in https://github.com/pytorch/torchdynamo/issues/1644:
- Upload nightly run logs to /fsx before parsing - for backing up parsing failures.
- Flag models with (1) < 0.95x speedup, (2) > 2min compile time, (3) < 0.9x compression ratio
- Flag models that were passing yesterday but failed today.
- Other small bug fixes.

See https://github.com/pytorch/torchdynamo/issues/1831 for sample outputs.
Also tested by running run.sh:
```bash
# Setup the output directory
rm -rf ../test-dynamo-runner-logs-3/
mkdir ../test-dynamo-runner-logs-3/

# Commands for torchbench for device=cuda, dtype=float32 for training and for performance testing
python benchmarks/dynamo/torchbench.py --performance --float32 -dcuda --output=../test-dynamo-runner-logs-3//inductor_torchbench_float32_training_cuda_performance.csv --training --inductor   --no-skip --dashboard --only mobilenet_v2 --cold_start_latency

# Commands for torchbench for device=cuda, dtype=float32 for training and for accuracy testing
python benchmarks/dynamo/torchbench.py --accuracy --float32 -dcuda --output=../test-dynamo-runner-logs-3//inductor_torchbench_float32_training_cuda_accuracy.csv --training --inductor   --no-skip --dashboard --only mobilenet_v2
```

with the command
`python benchmarks/dynamo/runner.py --output-dir ../test-dynamo-runner-logs-3/ --dashboard-archive-path /data/home/williamwen/dynamo-runner-logs-copy --training --run --compilers inductor --flag-compilers inductor --suites torchbench --update-dashboard` (need to comment out the `generate_commands` line and change the github issue ID from 681 to something else).

Pull Request resolved: https://github.com/pytorch/pytorch/pull/88516
Approved by: https://github.com/anijain2305"
pytorch/pytorch,a9d37ce8f50a3111cc9eaf4f633decd092b9d726,"Support reduction vectorization (#87356)

This PR is to optimize reduction implementation by `at::vec`. The main idea is as same as the aten implementation.
- Step1: Parallelize and vectorize the reduction implementation
- Step2: Invoke `at::vec::vec_reduce_all` to reduce the vector generated at step 1 to a single scalar
- Step3: Handle the tail elements

For the implementation, we create two kernels - `CppVecKernel` and `CppKernel`. The code block generation is as follows step by step.

- Gen the non-reduction loop - [Code](https://github.com/pytorch/pytorch/blob/gh/EikanWang/9/head/torch/_inductor/codegen/cpp.py#L1008-L1010)
- Gen the reduction initialization both for vectorization and non-vectorization kernel - [Code](https://github.com/pytorch/pytorch/blob/gh/EikanWang/9/head/torch/_inductor/codegen/cpp.py#L1015)
- Gen the reduction loop for the vectorization kernel - [Code](https://github.com/pytorch/pytorch/blob/gh/EikanWang/9/head/torch/_inductor/codegen/cpp.py#L1021-L1023)
- Gen the code to reduce the vector to scalar - [Code](https://github.com/pytorch/pytorch/blob/gh/EikanWang/9/head/torch/_inductor/codegen/cpp.py#L1033)
- Gen the reduction loop for the non-vectorization kernel - [Code](https://github.com/pytorch/pytorch/blob/gh/EikanWang/9/head/torch/_inductor/codegen/cpp.py#L1042)
- Do some post-reduction things like store reduction value - [Code](https://github.com/pytorch/pytorch/blob/gh/EikanWang/9/head/torch/_inductor/codegen/cpp.py#L1049)

```python
# Gen the non-reduction loop
for loop in CppVecKernel.NoneReductionLoop:
    # Gen the reduction initialization both for vectorization and non-vectorization kernel
    CppVecKernel.ReductionPrefix
    # Gen the reduction loop for the vectorization kernel
    for loop in CppVecKernel.ReductionLoop
        CppVecKernel.Loads
        CppVecKernel.Compute
        CppVecKernel.Stores
    # Gen the code to reduce the vector to scalar
    CppVecKernel.ReductionSuffix
    # Gen the reduction loop for the non-vectorization kernel
    for loop in CppKernel.ReductionLoop
        CppKernel.Loads
        CppKernel.Compute
        CppKernel.Stores
    # The reduction is almost finished. To do some post-reduction things like store reduction value.
    CppKernel.ReductionSuffix
```
The code snippet for maximum reduction exemplifies the idea. More detailed comments are inlined.

```C++
    {
        // Declare reduction for at::vec::Vectorized since it is not built-in data type.
        #pragma omp declare reduction(+:at::vec::Vectorized<float>:omp_out += omp_in) initializer(omp_priv={{0}})

        float tmp4 = 0;
        // tmp4_vec is used to vectorize the sum reduction for tmp4
        auto tmp4_vec = at::vec::Vectorized<float>(tmp4);
        float tmp6 = 0;
        // tmp6_vec is used to vectorize the sum reduction for tmp6
        auto tmp6_vec = at::vec::Vectorized<float>(tmp6);
        #pragma omp parallel num_threads(48)
        {
            // Parallelize the vectorized reduction
            #pragma omp for reduction(+:tmp4_vec) reduction(+:tmp6_vec)
            for(long i0=0; i0<192; i0+=1)
            {
                auto tmp0 = at::vec::Vectorized<float>::loadu(in_ptr0 + 8*i0);
                auto tmp1 = at::vec::Vectorized<float>::loadu(in_ptr1 + 8*i0);
                auto tmp2 = tmp0 - tmp1;
                auto tmp3 = tmp2.abs();
                auto tmp5 = tmp2 * tmp2;
                tmp4_vec += tmp3;
                tmp6_vec += tmp5;
            }
            // Reduce the tmp4_vec as a scalar and store at tmp4
            tmp4 = at::vec::vec_reduce_all<float>([](at::vec::Vectorized<float>& x, at::vec::Vectorized<float>&y) {return x + y;}, tmp4_vec);
            // Reduce the tmp6_vec as a scalar and store at tmp6
            tmp6 = at::vec::vec_reduce_all<float>([](at::vec::Vectorized<float>& x, at::vec::Vectorized<float>&y) {return x + y;}, tmp6_vec);
            // Handle the tail elements that could not be vectorized by aten.
            #pragma omp for simd simdlen(4) reduction(+:tmp4) reduction(+:tmp6)
            for(long i0=1536; i0<1536; i0+=1)
            {
                auto tmp0 = in_ptr0[i0];
                auto tmp1 = in_ptr1[i0];
                auto tmp2 = tmp0 - tmp1;
                auto tmp3 = std::abs(tmp2);
                auto tmp5 = tmp2 * tmp2;
                tmp4 += tmp3;
                tmp6 += tmp5;
            }
        }
        out_ptr0[0] = tmp4;
        out_ptr1[0] = tmp6;
    }
```

Performance(Measured by operatorbench and the base line of speedup ratio is aten operator performance):
Softmax (1,16,384,384,dim=3) | Speedup ratio (simdlen=None) |  Speedup ratio (simdlen=8) + this PR
-- | -- | --
24c | 0.37410838067524177 | 0.9036240100351164
4c | 0.24655829520907663 | 1.0255329993674518
1c | 0.21595768114988007 | 1.000587368005134

HW Configuration:
SKU: SKX Intel(R) Xeon(R) Platinum 8260 CPU @ 2.40GHz
MemTotal:       196708148 kB
MemFree:        89318532 kB
MemBandwidth:  112195.1MB/S

Pull Request resolved: https://github.com/pytorch/pytorch/pull/87356
Approved by: https://github.com/jgong5, https://github.com/jansel"
pytorch/pytorch,6541e51ffd84b044cfde81bb2ea241a75a87952d,"Explicit vectorization support for TorchInductor (#87068)

In this PR, we replace OMP SIMD with `aten::vec` to optimize TorchInductor vectorization performance. Take `res=torch.exp(torch.add(x, y))` as the example. The generated code is as follows if `config.cpp.simdlen` is 8.

```C++
extern ""C"" void kernel(const float* __restrict__ in_ptr0,
                       const float* __restrict__ in_ptr1,
                       float* __restrict__ out_ptr0,
                       const long ks0,
                       const long ks1)
{
    #pragma omp parallel num_threads(48)
    {
        #pragma omp for
        for(long i0=0; i0<((ks0*ks1) / 8); ++i0)
        {
            auto tmp0 = at::vec::Vectorized<float>::loadu(in_ptr0 + 8*i0);
            auto tmp1 = at::vec::Vectorized<float>::loadu(in_ptr1 + 8*i0);
            auto tmp2 = tmp0 + tmp1;
            auto tmp3 = tmp2.exp();
            tmp3.store(out_ptr0 + 8*i0);
        }
        #pragma omp for simd simdlen(4)
        for(long i0=8*(((ks0*ks1) / 8)); i0<ks0*ks1; ++i0)
        {
            auto tmp0 = in_ptr0[i0];
            auto tmp1 = in_ptr1[i0];
            auto tmp2 = tmp0 + tmp1;
            auto tmp3 = std::exp(tmp2);
            out_ptr0[i0] = tmp3;
        }
    }
}

```

The major pipeline is as follows.
- Check whether the loop body could be vectorized by `aten::vec`. The checker consists of two parts. [One ](https://github.com/pytorch/pytorch/blob/bf66991fc4860724368c5289d3db81de591b4cb2/torch/_inductor/codegen/cpp.py#L702)is to check whether all the `ops` have been supported. The [other one](https://github.com/pytorch/pytorch/blob/355326faa35405565ddb6ff8a2a945c7fce83db8/torch/_inductor/codegen/cpp.py#L672) is to check whether the data access could be vectorized.
  - [`CppSimdVecKernelChecker`](https://github.com/pytorch/pytorch/blob/355326faa35405565ddb6ff8a2a945c7fce83db8/torch/_inductor/codegen/cpp.py#L655)
- Create the `aten::vec` kernel and original omp simd kernel. Regarding the original omp simd kernel, it serves for the tail loop when the loop is vectorized.
  - [`CppSimdVecKernel`](https://github.com/pytorch/pytorch/blob/355326faa35405565ddb6ff8a2a945c7fce83db8/torch/_inductor/codegen/cpp.py#L601)
  - [`CppSimdVecOverrides`](https://github.com/pytorch/pytorch/blob/355326faa35405565ddb6ff8a2a945c7fce83db8/torch/_inductor/codegen/cpp.py#L159): The ops that we have supported on the top of `aten::vec`
  - Create kernel
    - [`aten::vec` kernel](https://github.com/pytorch/pytorch/blob/355326faa35405565ddb6ff8a2a945c7fce83db8/torch/_inductor/codegen/cpp.py#L924)
    - [`Original CPP kernel - OMP SIMD`](https://github.com/pytorch/pytorch/blob/355326faa35405565ddb6ff8a2a945c7fce83db8/torch/_inductor/codegen/cpp.py#L929)
- Generate code
  - [`CppKernelProxy`](https://github.com/pytorch/pytorch/blob/355326faa35405565ddb6ff8a2a945c7fce83db8/torch/_inductor/codegen/cpp.py#L753) is used to combine the `aten::vec` kernel and original cpp kernel
    - [Vectorize the most inner loop](https://github.com/pytorch/pytorch/blob/355326faa35405565ddb6ff8a2a945c7fce83db8/torch/_inductor/codegen/cpp.py#L753)
    - [Generate code](https://github.com/pytorch/pytorch/blob/355326faa35405565ddb6ff8a2a945c7fce83db8/torch/_inductor/codegen/cpp.py#L821)

Next steps:
- [x] Support reduction
- [x] Vectorize the tail loop with `aten::vec`
- [ ] Support BF16
- [ ] Optimize the loop condition and loop index calculation by replacing `div` with `add`

Pull Request resolved: https://github.com/pytorch/pytorch/pull/87068
Approved by: https://github.com/jgong5, https://github.com/jansel"
pytorch/pytorch,a95419b47e43b0064cf92a6ffd7e459a463d00e3,"use faster cache flush in triton benchmarking (#88557)

Speeds up autotuning a little bit more (about 90s -> 75s for coat_lite_mini)

@bertmaher, I've put in workaround so that internal doesn't break, but it can be removed once triton is updated internally.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/88557
Approved by: https://github.com/anijain2305"
pytorch/pytorch,eda247ee6ce2f8bc29d86ec94f3863f929a2ea6e,"[Dynamo] fix torchdynamo's TVM meta schedule backend (#88249)

Note that the previous `optimize_torch` functionality of pytorch is not working with default pytorch release with  CXX11 ABI off as TVM by default needs CXX11 ABI for builds. Source: [1](https://discuss.tvm.apache.org/t/can-someone-please-give-me-the-steps-to-use-pt-tvmdsoop/12525), [2](https://discuss.pytorch.org/t/undefined-symbol-when-import-lltm-cpp-extension/32627). It would be easier for user to tune with meta schedule instead of finding a CXX11-compatible pytorch, turning on the `pt-tvmdsoop` flag in TVM and rebuilding it. This could be revisited once the `pt-tvmdsoop` flag is updated and tuned on by default in TVM.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/88249
Approved by: https://github.com/jansel"
pytorch/pytorch,791d9ee2533d394dc26cff64de74df72d45835e4,"[inductor] Add lowering for as_strided_scatter (#88379)

Ref pytorch/torchdynamo#327

The use of as_strided does require in-memory manipulations, however this
 lowering allows those memory ops to be fused with any preceding calculations.
e.g.

```
def f(a, b):
    return torch.as_strided_scatter(
        a * 8 + 10,
        b * 2 - 4,
        size=(a.numel() // 2,),
        stride=(2,))
```

Before this compiles to two kernels and a call to `aten.as_strided_scatter` and
with this PR it compiles to just two kernels and no additional operator calls.

In theory I think this could be a decomposition, but in practice I saw the
`output_view.copy_(src)` being optimized out in some cases when this was
implemented as a decomposition.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/88379
Approved by: https://github.com/jansel"
pytorch/pytorch,81042d3a53335259c60e5aa8c9b9614c3d87b05f,"Revert ""Reenable optimizer overlap tests (#88439)""

This reverts commit da452bcadbc6f34989c6b3b0db6075a272aa9891.

Reverted https://github.com/pytorch/pytorch/pull/88439 on behalf of https://github.com/huydhn due to This change breaks trunk due to a land race missing reason parameter to sandcastle_skip_if https://hud.pytorch.org/pytorch/pytorch/commit/da452bcadbc6f34989c6b3b0db6075a272aa9891"
pytorch/pytorch,da452bcadbc6f34989c6b3b0db6075a272aa9891,"Reenable optimizer overlap tests (#88439)

Closes https://github.com/pytorch/pytorch/issues/73259. Not sure the root cause but CI seems fine with these tests.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/88439
Approved by: https://github.com/awgu"
pytorch/pytorch,4bb5c2c2051371bfed09f9ec46416f3dba550c14,"Add docstring to DDPOptimizer (#88521)

Pull Request resolved: https://github.com/pytorch/pytorch/pull/88521
Approved by: https://github.com/aazzolini"
pytorch/pytorch,3fd0729bb663d039204cbcea0726e028541a25ad,"DDPOptimizer replace debug=True/False with using torchdynamo logger (#88480)

Example output:

```
2022-11-04 05:09:29,525] torch._dynamo.optimizations.distributed: [INFO]
DDPOptimizer bucket assignments
┌─────────┬────────────┬───────────────────┐
│   Index │   Size (b) │ Param Names       │
├─────────┼────────────┼───────────────────┤
│       0 │  100120020 │ self_net_6_weight │
├─────────┼────────────┼───────────────────┤
│         │            │ self_net_6_bias   │
├─────────┼────────────┼───────────────────┤
│         │            │ self_net_4_weight │
├─────────┼────────────┼───────────────────┤
│         │            │ self_net_4_bias   │
├─────────┼────────────┼───────────────────┤
│       1 │  100020000 │ self_net_2_weight │
├─────────┼────────────┼───────────────────┤
│         │            │ self_net_2_bias   │
├─────────┼────────────┼───────────────────┤
│       2 │     220000 │ self_net_0_weight │
├─────────┼────────────┼───────────────────┤
│         │            │ self_net_0_bias   │
└─────────┴────────────┴───────────────────┘
[2022-11-04 05:09:29,527] torch._dynamo.optimizations.distributed: [DEBUG]
---orig graph---
graph():
    %inputs : torch.Tensor [#users=1] = placeholder[target=inputs]
    %self_net_0 : [#users=1] = call_module[target=self_net_0](args = (%inputs,), kwargs = {})
    %self_net_1 : [#users=1] = call_module[target=self_net_1](args = (%self_net_0,), kwargs = {})
    %self_net_2 : [#users=1] = call_module[target=self_net_2](args = (%self_net_1,), kwargs = {})
    %self_net_3 : [#users=1] = call_module[target=self_net_3](args = (%self_net_2,), kwargs = {})
    %self_net_4 : [#users=1] = call_module[target=self_net_4](args = (%self_net_3,), kwargs = {})
    %self_net_5 : [#users=1] = call_module[target=self_net_5](args = (%self_net_4,), kwargs = {})
    %self_net_6 : [#users=1] = call_module[target=self_net_6](args = (%self_net_5,), kwargs = {})
    %self_net_7 : [#users=1] = call_module[target=self_net_7](args = (%self_net_6,), kwargs = {})
    return (self_net_7,)

---split graph---
graph():
    %inputs : torch.Tensor [#users=1] = placeholder[target=inputs]
    %submod_0 : [#users=1] = call_module[target=submod_0](args = (%inputs,), kwargs = {})
    %submod_1 : [#users=1] = call_module[target=submod_1](args = (%submod_0,), kwargs = {})
    %submod_2 : [#users=1] = call_module[target=submod_2](args = (%submod_1,), kwargs = {})
    return (submod_2,)

---submod_0 graph---
graph():
    %inputs : [#users=1] = placeholder[target=inputs]
    %self_net_0 : [#users=1] = call_module[target=self_net_0](args = (%inputs,), kwargs = {})
    %self_net_1 : [#users=1] = call_module[target=self_net_1](args = (%self_net_0,), kwargs = {})
    return self_net_1

---submod_1 graph---
graph():
    %self_net_1 : [#users=1] = placeholder[target=self_net_1]
    %self_net_2 : [#users=1] = call_module[target=self_net_2](args = (%self_net_1,), kwargs = {})
    %self_net_3 : [#users=1] = call_module[target=self_net_3](args = (%self_net_2,), kwargs = {})
    return self_net_3

---submod_2 graph---
graph():
    %self_net_3 : [#users=1] = placeholder[target=self_net_3]
    %self_net_4 : [#users=1] = call_module[target=self_net_4](args = (%self_net_3,), kwargs = {})
    %self_net_5 : [#users=1] = call_module[target=self_net_5](args = (%self_net_4,), kwargs = {})
    %self_net_6 : [#users=1] = call_module[target=self_net_6](args = (%self_net_5,), kwargs = {})
    %self_net_7 : [#users=1] = call_module[target=self_net_7](args = (%self_net_6,), kwargs = {})
    return self_net_7

---------------
```

Pull Request resolved: https://github.com/pytorch/pytorch/pull/88480
Approved by: https://github.com/anj-s, https://github.com/davidberard98"
pytorch/pytorch,678d038001b0bd61501739ea97989d28f758343e,"Support DDP ignored parameters in DDPOptimizer (#88460)

Pull Request resolved: https://github.com/pytorch/pytorch/pull/88460
Approved by: https://github.com/aazzolini"
pytorch/pytorch,ead36e5a907c9fbcd837835e52ce448d428f228e,"Add dep on Accelerate framework to torch podspecs (#88422)

A dep on Accelerate was added in https://github.com/pytorch/pytorch/pull/80449 We need to declare this dep in our podspec, otherwise users will have to add the Accelerate framework to their projects manually.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/88422
Approved by: https://github.com/kimishpatel, https://github.com/malfet"
pytorch/pytorch,657f2e12f0e212b2f4afd89ab2c824c409dcc951,"[MPS] Add native `cumsum` implementation (#88319)

Using https://developer.apple.com/documentation/metalperformanceshadersgraph/mpsgraph/4057333-cumulativesumwithtensor?language=objc

Fall back to CPU if running on older MacOS versions
In `unary_op` add output tensor dims/dtype to the graph key (as even in default op we check output graph type)
Also, upcast int16 to int32 as MPS cumsum op on Ventura returns incorrect results for Int16 type (and it makes total sense for int8, as chances for overflow are very high)
Pull Request resolved: https://github.com/pytorch/pytorch/pull/88319
Approved by: https://github.com/kulinseth"
pytorch/pytorch,893f8e3790df47b165025c9e6b5b37b85bdfd501,"[PyTorch][Vulkan] Add template based codegen for shader generation (#88323)

We would like to be able to parameterize kernels such that a parameterized
algorithm can be implemented via templates. We can then profile performance of
a kernel with different parameter values. This enables us to determine what
parameters may work the best for a given kernel or a given device.

In this diff one such kernel added in 1x1 conv which parameters across size of
the tile being produced by each invocation.

Few other options for parameters can be:
- One can imagine dtype can also be a parameter such that we can do compute in
fp16 or int8/int16.
- Register blocking for input channels

Differential Revision: [D40280336](https://our.internmc.facebook.com/intern/diff/D40280336/)

**NOTE FOR REVIEWERS**: This PR has internal Meta-specific changes or comments, please review them on [Phabricator](https://our.internmc.facebook.com/intern/diff/D40280336/)!
Pull Request resolved: https://github.com/pytorch/pytorch/pull/88323
Approved by: https://github.com/jmdetloff"
pytorch/pytorch,a689502275529f78ba4a88c2e62ab897a96a040a,"[FSDP] Do not include empty state in `_flatten_optim_state_dict()` (#88353)

https://github.com/pytorch/pytorch/blob/983c0e7f3101f1543bed6c4ec1539a4d590a94c0/torch/optim/adam.py#L163
The above line requires that a candidate optimizer state dict being loaded via `load_state_dict()` has non-empty state for its 0th parameter (via `state_values[0]`). This PR changes FSDP to only include non-empty mappings in the state returned by `_flatten_optim_state_dict()`, which is the subroutine for both `shard_full_optim_state_dict()` and `flatten_sharded_optim_state_dict()`.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/88353
Approved by: https://github.com/fegin"
pytorch/pytorch,5fb9c113aee76dd0465a6ee7067eeb018929b922,"Update pybind11 to v2.10.1 (#88332)

I am one of the maintainers of pybind11, and a frequent PyTorch user. We added quite a lot of bugfixes and performance improvements in 2.10.1 (see the changelog for full details) and I wanted to upstream them to PyTorch.

Our releases is tested throughout Google's codebase including on their global builds of PyTorch so there should be no surprises.

The main new feature is optin in Eigen Tensor to Numpy casters.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/88332
Approved by: https://github.com/soumith"
pytorch/pytorch,4c20c0509d5cf8d4dea83cc330056044a6277b1b,"Split out forward AD tests from test_ops_gradients and reenable slow gradcheck CI (#88216)

Fixes: https://github.com/pytorch/pytorch/issues/88010

This PR does a couple things to stop slow gradcheck from timing out:
- Splits out test_ops_fwd_gradients from test_ops_gradients, and factors out TestFwdGradients and TestBwdGradients which both inherit from TestGradients, now situated in common_utils (maybe there is a better place?)
- Skips CompositeCompliance (and several other test files) for slow gradcheck CI since they do not use gradcheck
- because test times for test_ops_fwd_gradients and test_ops_gradients are either unknown or wrong, we hardcode them for now to prevent them from being put together. We can undo the hack after we see actual test times are updated. (""def calculate_shards"" randomly divides tests with unknown test times in a round-robin fashion.)
- Updates references to test_ops_gradients and TestGradients
- Test files that are skipped for slow gradcheck CI are now centrally located in in run_tests.py, this reduces how fine-grained we can be with the skips, so for some skips (one so far) we still use the old skipping mechanism, e.g. for test_mps

Pull Request resolved: https://github.com/pytorch/pytorch/pull/88216
Approved by: https://github.com/albanD"
pytorch/pytorch,ef4ce6d4c6ce1bd5ec26d7f6f71f1c053da46945,"Add [[noreturn]] attribute to operator() in DispatchKeyExtractor.h (#88333)

Originally D40537408. Submitting this through the diff train workflow to
get it merged faster.

Test Plan:
- Build PyTorch
Pull Request resolved: https://github.com/pytorch/pytorch/pull/88333
Approved by: https://github.com/ezyang"
pytorch/pytorch,dcbcf5b90e56dfb30d4f87d607f3f4b361f52077,"[profiler] Expose experimental performance events to python (#87905)

Reports total counts (includes time spent in all children), self counts can be calculated manully.

Differential Revision: [D40282770](https://our.internmc.facebook.com/intern/diff/D40282770/)
Pull Request resolved: https://github.com/pytorch/pytorch/pull/87905
Approved by: https://github.com/SS-JIA"
pytorch/pytorch,03346296dbfd1033cb0898983eebcd4c0af32afb,"[edge profiler] Add support for performance events counting (#87876)

* Add support in lite_predictor benchmark binary to select event lists
* Uses Linux perf through Kineto profiler

Differential Revision: [D39837216](https://our.internmc.facebook.com/intern/diff/D39837216/)

**NOTE FOR REVIEWERS**: This PR has internal Meta-specific changes or comments, please review them on [Phabricator](https://our.internmc.facebook.com/intern/diff/D39837216/)!
Pull Request resolved: https://github.com/pytorch/pytorch/pull/87876
Approved by: https://github.com/SS-JIA"
pytorch/pytorch,bc1e9a07a3644f300e3d27b377a152c330ca6dd9,"[profiler] Add Performance events support in Kineto profiler (#87874)

* Wiring to allow user to pass event names to profiler and reflect the count to the chrometrace
* If not used, the runtime and size overhead should be neglegible
* For now, primary user will be KinetoEdgeCPUProfiler but the impl does not assume that
* Not exposed to python yet

Differential Revision: [D40238032](https://our.internmc.facebook.com/intern/diff/D40238032/)

**NOTE FOR REVIEWERS**: This PR has internal Meta-specific changes or comments, please review them on [Phabricator](https://our.internmc.facebook.com/intern/diff/D40238032/)!
Pull Request resolved: https://github.com/pytorch/pytorch/pull/87874
Approved by: https://github.com/SS-JIA"
pytorch/pytorch,bc9caafc7898a6df534f566f599f8f5a78d207d1,"record_function: update to use custom_class API (#76420)

Re-submit of gh-72302

This still has a small performance hit, but it much smaller. On my
machine I see `_record_fucntion_exit._RecordFunction` takes 1.05 us
compared to the `Tensor` overload taking 0.79 us.

In an overall comparison, I see a 0.7 us slowdown from 6.0 us to
6.7 us for this timeit benchmark
```python
import torch

def foo():
  with torch.profiler.record_function(""foo""):
    return torch.eye(3)

%timeit foo()
```

Pull Request resolved: https://github.com/pytorch/pytorch/pull/76420
Approved by: https://github.com/robieta"
pytorch/pytorch,ffd54def8fa52653d3a68bc00f0583e8d16d6acb,"[GHF] Remove CC line from commit message (#88252)

This line is added by autoCCBot, but is not really meaningful as commit
message

Test Plan:
```
>>> from trymerge import GitHubPR, RE_PR_CC_LINE
>>> import re
>>> pr=GitHubPR(""pytorch"", ""pytorch"", 87809)
>>> re.sub(RE_PR_CC_LINE, """", pr.get_body())
'Fixes #ISSUE_NUMBER\r\n\n\n'
>>> pr=GitHubPR(""pytorch"", ""pytorch"", 87913)
>>> re.sub(RE_PR_CC_LINE, """", pr.get_body())
'Parallel compilation warms the Threadpool when we call `torch._dynamo.optimize()`. In current benchmarks, we were setting up the TRITON_CACHE_DIR much later. Because of this parallel compilation artifacts were not used and compilation latency improvements were not visible in dashboard. This PR just prepones the setup of TRITON_CACHE_DIR.\n\n'
>>> pr=GitHubPR(""pytorch"", ""pytorch"", 85692)
>>> re.sub(RE_PR_CC_LINE, """", pr.get_body())
'This PR sets CUDA_MODULE_LOADING if it\'s not set by the user. By default, it sets it to ""LAZY"".\r\n\r\nIt was tested using the following commands:\r\n```\r\npython -c ""import torch; tensor=torch.randn(20, 16, 50, 100).cuda(); free, total = torch.cuda.cudart().cudaMemGetInfo(0); print(total-free)""\r\n```\r\nwhich shows a memory usage of: 287,047,680 bytes\r\n\r\nvs\r\n\r\n```\r\nCUDA_MODULE_LOADING=""DEFAULT"" python -c ""import torch; tensor=torch.randn(20, 16, 50, 100).cuda(); free, total = torch.cuda.cudart().cudaMemGetInfo(0); print(total-free)""\r\n```\r\nwhich shows 666,632,192 bytes. \r\n\r\nC++ implementation is needed for the libtorch users (otherwise it could have been a pure python functionality).\r\n\r\n'
```

Pull Request resolved: https://github.com/pytorch/pytorch/pull/88252
Approved by: https://github.com/xuzhao9, https://github.com/izaitsevfb"
pytorch/pytorch,73c9911fc001991809a6c90e2d61f71fc69ffde6,"always realize output regardless of the number of reads (#88046)

This improves hf_Bert 1.139x->1.21x, currently lowmem dropout doesn't work for nn.Dropout module, and before this change we were recomputing all the dropout masks in a very inefficient kernel. This change pushes dropout masks to be saved in the dropout kernels where they are first computed.

cc @mlazos @soumith @voznesenskym @yanboliang @penguinwu @anijain2305 @EikanWang @jgong5 @Guobing-Chen @chunyuan-w @XiaobingSuper @zhuhaozhe @blzheng @Xia-Weiwen @wenzhe-nrv @jiayisunx
Pull Request resolved: https://github.com/pytorch/pytorch/pull/88046
Approved by: https://github.com/Chillee"
pytorch/pytorch,23fe6c8ca15ec2cf6ea74f93aa91cae343ea534f,"[Static Runtime] Fix ReplaceWithMaybeCopy test in OSS (#88099)

Summary: `ReplaceWithMaybeCopy` is guarded by `FBCODE_CAFFE` in `OptimizeGraph`. Run the pass manually to ensure it does the replacement.

Test Plan: Existing tests

Differential Revision: D40858743

Pull Request resolved: https://github.com/pytorch/pytorch/pull/88099
Approved by: https://github.com/huydhn"
pytorch/pytorch,4c78c7c82af08872f901076c5daaf8148f03b096,"Enable `src_mask` in fast path of `TransformerEncoderLayer ` (#87377)

## Issues
Fixes https://github.com/pytorch/pytorch/issues/81129#issuecomment-1179435674

## Description

Passing a 2D attention mask `src_mask` into the fast path of `TransformerEncoderLayer` in CPU was causing an error and so was disabled in https://github.com/pytorch/pytorch/pull/81277. This PR unrolls this fix, enabling `src_mask` on the fast path:

- Either attention mask `src_mask` of shape `(L, L)` or padding mask `src_key_padding_mask` of shape `(B, L)` are now allowed on the CPU fast path. If softmax is applied along the last dimension (as in multi-head attention), these masks are processed without expanding them to 4D. Instead, when iterating through the input, `Softmax.cpp::host_softmax` converts the index to match the mask dimensions, depending on the type.
- If softmax is applied along the dimension other than the last, `Softmax.cpp::masked_softmax_cpu` expands masks to 4D, converting them to `mask_type=2`. Theoretically one could also add special optimized cases for `dim=0, 1, 2` and process them without mask expansion, but I don't know how often is that used

## Tests:
- `test_transformerencoderlayer_fast_path` is extended to cover both attention mask and padding mask
- `test_masked_softmax_mask_types_0_1` is added to ensure results from CPU softmax with attention and padding masks match the explicit slow calculation
- `test_masked_softmax_devices_parity` is added to ensure results from masked softmax on CPU and CUDA match

## Note
I had to replace `float` with `torch.get_default_dtype()` in a couple of tests for the following reason:
- `test_nn.py` [sets the default type to `torch.double`](https://github.com/pytorch/pytorch/blob/master/test/test_nn.py#L24-L26)
- If I execute `test_nn.py` and `test_transformers.py` in one `pytest` run, this default still holds for transformer tests
- Some tests in `test_transformers.py` which were previously following the slow path now switched to fast path, and hard-coded `float` started clashing with default `double`

Let me know if there is a better way around it - or maybe I'm not supposed to run tests with `pytest` like this

Pull Request resolved: https://github.com/pytorch/pytorch/pull/87377
Approved by: https://github.com/mikekgfb, https://github.com/weiwangmeta, https://github.com/malfet"
pytorch/pytorch,ceddcf5434c41ba6e96d36f9e727bde0ee191220,"istft: Use unfold_backward instead of col2im (#88060)

`unfold_backward` implements the same operation as `col2im` but without support
for 2d kernels or dilation. However, `istft` doesn't use any of those features
and `unfold_backward` actually has a faster `TensorIterator` based
implementation so we should use it here instead.

In the example from #87353 I see a 2x speedup on both CPU and CUDA.

On a wider variety of sizes and inputs I still see speedups across the board, especially
on CPU since `col2im` isn't parallelized but `unfold_backward` is:

| device | shape           | hop_length | Master (us) | This PR (us) | Speedup |
|--------|-----------------|------------|-------------|--------------|---------|
| CUDA   | (1, 129, 33)    | 256        | 147         | 136          | 1.08    |
|        |                 | 128        | 153         | 128          | 1.20    |
|        | (100, 129, 20)  | 256        | 181         | 147          | 1.23    |
|        |                 | 128        | 171         | 137          | 1.25    |
|        | (1000, 129, 10) | 256        | 681         | 443          | 1.55    |
|        |                 | 128        | 632         | 446          | 1.42    |
| CPU    | (1, 129, 33)    | 256        | 106         | 104          | 1.02    |
|        |                 | 128        | 103         | 81           | 1.27    |
|        | (100, 129, 20)  | 256        | 2400        | 399          | 6.02    |
|        |                 | 128        | 2150        | 313          | 6.87    |
|        | (1000, 129, 10) | 256        | 13800       | 3740         | 3.69    |
|        |                 | 128        | 12700       | 2110         | 6.02    |
Pull Request resolved: https://github.com/pytorch/pytorch/pull/88060
Approved by: https://github.com/albanD"
pytorch/pytorch,59fe272c1e698989228af5ad197bdd2985e4e9b9,"Fix: prefer .is_none() over .is(py::none()) for pybind11 (#88051)

Fixes minor perf regression I saw in #85688 and replaced throughout the code base. `obj == Py_None` is directly equivalent to is_none(). Constructing a temporary py::none() object needlessly incref/decref the refcount of py::none, this method avoids that and therefore is more efficient.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/88051
Approved by: https://github.com/albanD"
pytorch/pytorch,75dbe3790938c30716463604ccfa68c0f9f6a7f5,"make autocast cache global instead of thread-local (#86492)

Summary:

There is a memory leak because `torch.clear_autocast_cache()` clears
the autocast cache from the main thread, but autograd can write to
this cache from a background thread, so whatever autograd writes
will leak.

With some offline discussion we decided that a global cache is a
practical way to deal with this, and the performance impact of the
lock should be negligible.

Test Plan:

I don't have a local repro of the original issue, need to look into how to get
that.

A toy example
(https://gist.github.com/vkuzo/0d6318fe7f7cb1c505e370cd5c1a643b)
does cache clearing as expected on forward and backward pass.

local testing:
```
python test/test_cuda.py -k autocast
python test/test_autocast.py
```

Reviewers:

Subscribers:

Tasks:

Tags:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/86492
Approved by: https://github.com/ezyang"
pytorch/pytorch,e24ce484ed52f6441db159ef0479ff06c72f2efd,"Use scaled_dot_product_attention within attention.cpp (#87312)

# Summary
Use the private _scaled_dot_product_attention to support _native_multiheaded_attention. _SDP provides access to fused kernels when certain conditions are meant enabling a speed up for MHA.

cc @cpuhrsch @jbschlosser @bhosmer @mikaylagawarecki
Pull Request resolved: https://github.com/pytorch/pytorch/pull/87312
Approved by: https://github.com/cpuhrsch"
pytorch/pytorch,d67b2edec34ef27956de0b2ebb5d7e50dbba9de3,"[dynamo][dashboard] minor fixes for a clean Dashboard (#88056)

* better check for cold start latency
* sort on inductor column for better readability.

cc @mlazos @soumith @voznesenskym @yanboliang @penguinwu @EikanWang @jgong5 @Guobing-Chen @chunyuan-w @XiaobingSuper @zhuhaozhe @blzheng @Xia-Weiwen @wenzhe-nrv @jiayisunx
Pull Request resolved: https://github.com/pytorch/pytorch/pull/88056
Approved by: https://github.com/ngimel"
pytorch/pytorch,e4a8661ab84022c1bff622c6d2f6e679180b1df5,"torchdynamo and xla integration (#87741)

# Motivation
- torchdynamo and torchxla uses different strategies to be a sound graph capture technique. The former relies on guards; the latter relies on retracing
- guard system is quite low overhead but torchxla tracing overhead is quite high

The main idea is to leverage guard system in torchdynamo to avoid retracing in torchxla so that
- we can integration torchdynamo with XLA
- we reduce or even completely avoid tracing overhead of torchxla

# Technique details
## XLA baseline
We found that different frameworks do not generate numerically identical results for the SAME model with the SAME input. By default, torchdynamo uses eager as baseline so the model will run with PyTorch. It would be tricky to compare a model running on XLA with this baseline: it's hard to check correctness. To make the comparison easier, we add a flag `--use-xla-baseline`. When it's enabled, the baseline will be run on XLA.

## New dynamo backends added
We add 2 new dynamo backends torchxla_trivial and trochxla_trace_once to control the optimization targets.

torchxla_trivial simply moves inputs/model parameters to XLA and run the model on XLA. There is tracing overhead for each run. We should expect that result to be mostly neutral compared to the XLA baseline.

torchxla_trace_once only traces once during AOT compiling time. Here are the steps:
1. dynamo capture guards and the subgraph
2. torchxla_trace_once backend trace the graph with torchxla, lowering the graph and record a hash of the graph for later lookup
3. at inference time, the hash is used directly to lookup the optimized graph and run it.

# Limitations
We can not handle LTC/torchxla fall back right now. If a op misses LTC kernel, we raise and exception and that will results in dynamo fallback (or try another compiler). People have brainstormed the idea of graph breaking and stitching the subgraphs together. But maybe it's easier to add those missing LTC kernels for those models.

# Results
The models we tested are those not causing LTC fallback. We run the tests on **GPU**. We see **1.38x** geomean speedup for trochxla_trace_once  and torchxla_trivial is mostly neutral as expected.
```
| Model                   |   XLA (trace once) |   XLA (trace everytime) |
+=========================+====================+=========================+
| resnet18                |            1.346   |                 1.045   |
+-------------------------+--------------------+-------------------------+
| resnet50                |            1.153   |                 1.007   |
+-------------------------+--------------------+-------------------------+
| resnext50_32x4d         |            1.381   |                 1.039   |
+-------------------------+--------------------+-------------------------+
| alexnet                 |            1.045   |                 1.018   |
+-------------------------+--------------------+-------------------------+
| mobilenet_v2            |            1.562   |                 1.021   |
+-------------------------+--------------------+-------------------------+
| mnasnet1_0              |            1.303   |                 1.069   |
+-------------------------+--------------------+-------------------------+
| squeezenet1_1           |            1.278   |                 1.025   |
+-------------------------+--------------------+-------------------------+
| vgg16                   |            1.076   |                 1.008   |
+-------------------------+--------------------+-------------------------+
| BERT_pytorch            |            2.224   |                 0.978   |
+-------------------------+--------------------+-------------------------+
| timm_vision_transformer |            1.81    |                 1.025   |
+-------------------------+--------------------+-------------------------+
| geomean                 |            1.38101 |                 1.02324 |
+-------------------------+--------------------+-------------------------+
```

The speedup is similar to what we see from previous work for LTC's TorchScript backend (we see 1.40 geomean speedup there):
https://docs.google.com/presentation/d/1G09X8v41u_cLKLtSdf7v6R8G19-iZTPcW_VAdOnvYBI/edit#slide=id.g11bf989cb6b_1_5

# Next steps
- Use AOT autograd to enable training
- Share results on XLA devices
- Do more extensive tests on torchbench models

Example command
```
GPU_NUM_DEVICES=1 python benchmarks/dynamo/torchbench.py --randomize-input --performance --use-xla-baseline --only resnet18 --backend=torchxla_trace_once
```

Thanks @JackCaoG from torchxla team to help debugging various perf issues and merging the torchxla PR! That's super critical for us to get the results above. torchxla side PR: https://github.com/pytorch/xla/pull/4119

topic: not user facing

cc @mlazos @soumith @voznesenskym @yanboliang @penguinwu @anijain2305 @EikanWang @jgong5 @Guobing-Chen @chunyuan-w @XiaobingSuper @zhuhaozhe @blzheng @Xia-Weiwen @wenzhe-nrv @jiayisunx @jansel

Pull Request resolved: https://github.com/pytorch/pytorch/pull/87741
Approved by: https://github.com/wconstab"
pytorch/pytorch,af0c339f00094c4c2f3c260b55e04e0e3654776a,"Disable slow-gradcheck tests (#88008)

Disable because slow-gradcheck tests take > 4 hrs and time out. Will need to figure out if and how to re-enable later.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/88008
Approved by: https://github.com/seemethere, https://github.com/huydhn"
pytorch/pytorch,35c611d30f6024fc6fc94b437372ab4ee1b3544d,"Add mem efficient backend flag (#87946)

# Summary
Add in a torch.backends.cuda flag and update context manager to pic between the three implementations of the scaled_dot_product_attention.

cc @cpuhrsch @jbschlosser @bhosmer @mikaylagawarecki
Pull Request resolved: https://github.com/pytorch/pytorch/pull/87946
Approved by: https://github.com/cpuhrsch"
pytorch/pytorch,07f7c4615bc858a8822c05aa310310446fc78836,"[MKLDNN] Replace pooling algorithm `pooling_avg` with `pooling_avg_exclude_padding` for future oneDNN upgrades (#87851)

**Description**
Replace pooling algorithm `pooling_avg` with `pooling_avg_exclude_padding` in implementation of mkldnn pooling. It's only a change of names, not algorithm. The former is an alias of the latter and it will be removed in future oneDNN library upgrades.
This change has no effect on functionality or performance.

**Validation**
Covered by UT.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/87851
Approved by: https://github.com/jgong5, https://github.com/XiaobingSuper"
pytorch/pytorch,2cb7c3f865ac8305f0af2806082b3bc8ec29a640,"[dynamo][benchmarks] Prepone Cold start setup (#87913)

Parallel compilation warms the Threadpool when we call `torch._dynamo.optimize()`. In current benchmarks, we were setting up the TRITON_CACHE_DIR much later. Because of this parallel compilation artifacts were not used and compilation latency improvements were not visible in dashboard. This PR just prepones the setup of TRITON_CACHE_DIR.

cc @jansel @mlazos @soumith @voznesenskym @yanboliang @penguinwu @EikanWang @jgong5 @Guobing-Chen @chunyuan-w @XiaobingSuper @zhuhaozhe @blzheng @Xia-Weiwen @wenzhe-nrv @jiayisunx
Pull Request resolved: https://github.com/pytorch/pytorch/pull/87913
Approved by: https://github.com/wconstab"
pytorch/pytorch,03d6af4db3974fcbf1ce7d3b3be46c1134c72e6e,"add nesting to TORCH_SHOW_DISPATCH_TRACE (#87751)

Added indents to `TORCH_SHOW_DISPATCH_TRACE` so that you more easily see the call tree from the dispatcher. Definitely slower, but it's all guarded under the `DEBUG` build. Example output:

I know we have the PyDispatcher now, but I still found this helpful for debugging

```
 [call] op=[aten::ones], key=[BackendSelect]
  [redispatch] op=[aten::ones], key=[CPU]
   [call] op=[aten::empty.memory_format], key=[BackendSelect]
    [redispatch] op=[aten::empty.memory_format], key=[CPU]
   [call] op=[aten::fill_.Scalar], key=[CPU]
 [call] op=[aten::clone], key=[AutogradCPU]
  [redispatch] op=[aten::clone], key=[CPU]
   [call] op=[aten::empty_strided], key=[BackendSelect]
    [redispatch] op=[aten::empty_strided], key=[CPU]
   [call] op=[aten::copy_], key=[CPU]
 [call] op=[aten::view], key=[PythonTLSSnapshot]
  [redispatchBoxed] op=[aten::view], key=[AutogradCPU]
   [redispatch] op=[aten::view], key=[ADInplaceOrView]
    [redispatch] op=[aten::view], key=[Functionalize]
     [call] op=[aten::view], key=[PythonTLSSnapshot]
      [redispatchBoxed] op=[aten::view], key=[Meta]
     [call] op=[aten::view], key=[PythonTLSSnapshot]
      [redispatchBoxed] op=[aten::view], key=[Python]
       [callBoxed] op=[aten::view], key=[CPU]
 [call] op=[aten::clone], key=[PythonTLSSnapshot]
  [redispatchBoxed] op=[aten::clone], key=[AutogradCPU]
   [redispatch] op=[aten::clone], key=[Functionalize]
    [callBoxed] op=[aten::clone], key=[PythonTLSSnapshot]
     [redispatchBoxed] op=[aten::clone], key=[Python]
      [callBoxed] op=[aten::clone], key=[CPU]
       [call] op=[aten::empty_strided], key=[BackendSelect]
        [redispatch] op=[aten::empty_strided], key=[CPU]
       [call] op=[aten::copy_], key=[CPU]
```

Pull Request resolved: https://github.com/pytorch/pytorch/pull/87751
Approved by: https://github.com/ezyang, https://github.com/zou3519"
pytorch/pytorch,a12d3d6b49cb4c9fdc325b0952ac748f55ae72a2,"[profiler] Standard performance event names for the profiler (#87538)

Summary: The goal is to create a hardware/backend independent event abstraction on which a standard set of tooling can be developed.

Test Plan: CI

Reviewed By: kimishpatel

Differential Revision: D40238034

Pull Request resolved: https://github.com/pytorch/pytorch/pull/87538
Approved by: https://github.com/salilsdesai, https://github.com/kirklandsign"
pytorch/pytorch,5dbd80a605c5c0f12a57de464e9f89f55e6f8f97,"[pytorch] Layer norm backward speed gain with warp shuffles (#87814)

Summary:
Improved native layer norm backward performance.

Rewrote `GammaBetaBackwardCUDAKernel` to use shared memory only for the reduction step, but not for loading `mean` and `rstd`. The previous implementation used only `threadIdx.x = 0` to load `mean` and `rstd` into shared memory, and then all threads would access the values in order to do loop unrolling. This approached increased register usage and decreased occupancy, without much benefit from using shared memory (this is because the values were already cached in L1). The new implementation is simpler and register usage is smaller, thus occupancy is better.

Added another implementation called `GammaBetaBackwardCUDAKernel_32x32` which is only for shapes dividing exactly to a (32 x 32) block. This permits using warp shuffles for speeding up loading `mean` and `rstd` as well as for the final reduction stage. The effective bandwidth of this implementation is equal to STREAM Triad.

Observed that we can get additional benefit if we lower the threshold for calling `GammaBetaBackwardSimpleCUDAKernel` (simple col-wise reduction implementation) from `512` to `128`.

Test Plan:
Wrote a simple CUDA app that calls the previous implementation of `GammaBetaBackwardCUDAKernel` and the current one, using FP32 values and compares the results. The epsilon value we used for FP comparison is 0.00001 for the weight and 0.0001 for the bias.
Ran the benchmark for various sizes A100 GPU and got the results below. Almost all sizes show good speedup.

```
Size (32, 32); Mismatches: dg = 0 db = 0 out of 32. reference = 0.0073 (ms); optimized = 0.0071 (ms); bw_opt = 1.14 GB/s; speedup = 2.68%
Size (64, 32); Mismatches: dg = 0 db = 0 out of 32. reference = 0.0107 (ms); optimized = 0.0107 (ms); bw_opt = 1.50 GB/s; speedup = 0.22%
Size (256, 128); Mismatches: dg = 0 db = 0 out of 128. reference = 0.0323 (ms); optimized = 0.0075 (ms); bw_opt = 32.89 GB/s; speedup = 330.16%
Size (512, 1024); Mismatches: dg = 0 db = 0 out of 1024. reference = 0.0103 (ms); optimized = 0.0089 (ms); bw_opt = 440.54 GB/s; speedup = 15.82%
Size (1024, 2048); Mismatches: dg = 0 db = 0 out of 2048. reference = 0.0197 (ms); optimized = 0.0136 (ms); bw_opt = 1151.44 GB/s; speedup = 44.91%
Size (2048, 2048); Mismatches: dg = 0 db = 0 out of 2048. reference = 0.0416 (ms); optimized = 0.0283 (ms); bw_opt = 1105.31 GB/s; speedup = 47.01%
Size (4096, 16384); Mismatches: dg = 0 db = 0 out of 16384. reference = 0.4420 (ms); optimized = 0.3915 (ms); bw_opt = 1277.58 GB/s; speedup = 12.90%
Size (70000, 64); Mismatches: dg = 0 db = 0 out of 64. reference = 0.5908 (ms); optimized = 0.6850 (ms); bw_opt = 49.49 GB/s; speedup = -13.75%
Size (131072, 512); Mismatches: dg = 0 db = 0 out of 512. reference = 1.1961 (ms); optimized = 0.9234 (ms); bw_opt = 542.54 GB/s; speedup = 29.53%
Size (1000, 520); Mismatches: dg = 0 db = 0 out of 520. reference = 0.0132 (ms); optimized = 0.0113 (ms); bw_opt = 343.83 GB/s; speedup = 16.88%
Size (4005, 4005); Mismatches: dg = 0 db = 0 out of 4005. reference = 0.1441 (ms); optimized = 0.1054 (ms); bw_opt = 1134.36 GB/s; speedup = 36.71%
Size (10000, 1000); Mismatches: dg = 0 db = 0 out of 1000. reference = 0.1293 (ms); optimized = 0.1248 (ms); bw_opt = 597.71 GB/s; speedup = 3.63%
Size (1024, 10000); Mismatches: dg = 0 db = 0 out of 10000. reference = 0.0738 (ms); optimized = 0.0735 (ms); bw_opt = 1039.40 GB/s; speedup = 0.45%
Size (8192, 4096); Mismatches: dg = 0 db = 0 out of 4096. reference = 0.2673 (ms); optimized = 0.2223 (ms); bw_opt = 1125.01 GB/s; speedup = 20.25%
Size (10000, 10000); Mismatches: dg = 0 db = 0 out of 10000. reference = 0.7331 (ms); optimized = 0.8940 (ms); bw_opt = 833.54 GB/s; speedup = -18.00%
Size (3072, 10000); Mismatches: dg = 0 db = 0 out of 10000. reference = 0.2087 (ms); optimized = 0.2364 (ms); bw_opt = 968.64 GB/s; speedup = -11.71%
Size (6144, 10000); Mismatches: dg = 0 db = 0 out of 10000. reference = 0.4197 (ms); optimized = 0.5118 (ms); bw_opt = 894.63 GB/s; speedup = -18.00%
Size (1024, 20000); Mismatches: dg = 0 db = 0 out of 20000. reference = 0.1480 (ms); optimized = 0.1297 (ms); bw_opt = 1177.68 GB/s; speedup = 14.12%
Size (1024, 20000); Mismatches: dg = 0 db = 0 out of 20000. reference = 0.1483 (ms); optimized = 0.1278 (ms); bw_opt = 1195.26 GB/s; speedup = 16.04%
Size (512, 1536); Mismatches: dg = 0 db = 0 out of 1536. reference = 0.0104 (ms); optimized = 0.0091 (ms); bw_opt = 646.72 GB/s; speedup = 14.44%
Size (512, 6144); Mismatches: dg = 0 db = 0 out of 6144. reference = 0.0219 (ms); optimized = 0.0156 (ms); bw_opt = 1506.30 GB/s; speedup = 40.52%
Size (512, 10240); Mismatches: dg = 0 db = 0 out of 10240. reference = 0.0424 (ms); optimized = 0.0370 (ms); bw_opt = 1057.84 GB/s; speedup = 14.63%
Size (1000, 1000); Mismatches: dg = 0 db = 0 out of 1000. reference = 0.0139 (ms); optimized = 0.0119 (ms); bw_opt = 627.51 GB/s; speedup = 16.83%
Size (2000, 2000); Mismatches: dg = 0 db = 0 out of 2000. reference = 0.0421 (ms); optimized = 0.0412 (ms); bw_opt = 724.10 GB/s; speedup = 2.20%
Size (10240, 10240); Mismatches: dg = 0 db = 0 out of 10240. reference = 0.7210 (ms); optimized = 0.6098 (ms); bw_opt = 1281.40 GB/s; speedup = 18.24%
Size (384, 128); Mismatches: dg = 0 db = 0 out of 128. reference = 0.0449 (ms); optimized = 0.0089 (ms); bw_opt = 41.50 GB/s; speedup = 403.48%
Size (2048, 1024); Mismatches: dg = 0 db = 0 out of 1024. reference = 0.0208 (ms); optimized = 0.0169 (ms); bw_opt = 925.70 GB/s; speedup = 23.13%
Size (267, 513); Mismatches: dg = 0 db = 0 out of 513. reference = 0.0342 (ms); optimized = 0.0090 (ms); bw_opt = 114.18 GB/s; speedup = 280.64%
Size (67, 123479); Mismatches: dg = 0 db = 0 out of 123479. reference = 0.0562 (ms); optimized = 0.0552 (ms); bw_opt = 1133.46 GB/s; speedup = 1.81%
Size (1024, 123479); Mismatches: dg = 0 db = 0 out of 123479. reference = 0.8573 (ms); optimized = 0.9245 (ms); bw_opt = 1020.02 GB/s; speedup = -7.27%
Size (2048, 66679); Mismatches: dg = 0 db = 0 out of 66679. reference = 0.8778 (ms); optimized = 0.8590 (ms); bw_opt = 1185.05 GB/s; speedup = 2.19%
Size (200, 256); Mismatches: dg = 0 db = 0 out of 256. reference = 0.0215 (ms); optimized = 0.0066 (ms); bw_opt = 58.49 GB/s; speedup = 226.81%
Size (1000, 256); Mismatches: dg = 0 db = 0 out of 256. reference = 0.0109 (ms); optimized = 0.0092 (ms); bw_opt = 208.27 GB/s; speedup = 18.65%
Size (6000, 256); Mismatches: dg = 0 db = 0 out of 256. reference = 0.0394 (ms); optimized = 0.0301 (ms); bw_opt = 381.90 GB/s; speedup = 30.98%
Size (6272, 256); Mismatches: dg = 0 db = 0 out of 256. reference = 0.0403 (ms); optimized = 0.0300 (ms); bw_opt = 400.48 GB/s; speedup = 34.34%
Size (200, 512); Mismatches: dg = 0 db = 0 out of 512. reference = 0.0218 (ms); optimized = 0.0066 (ms); bw_opt = 116.33 GB/s; speedup = 229.96%
Size (1000, 512); Mismatches: dg = 0 db = 0 out of 512. reference = 0.0110 (ms); optimized = 0.0094 (ms); bw_opt = 407.29 GB/s; speedup = 17.26%
Size (6000, 512); Mismatches: dg = 0 db = 0 out of 512. reference = 0.0535 (ms); optimized = 0.0594 (ms); bw_opt = 386.05 GB/s; speedup = -9.95%
Size (6272, 512); Mismatches: dg = 0 db = 0 out of 512. reference = 0.0573 (ms); optimized = 0.0387 (ms); bw_opt = 619.62 GB/s; speedup = 48.06%
Size (200, 1024); Mismatches: dg = 0 db = 0 out of 1024. reference = 0.0221 (ms); optimized = 0.0069 (ms); bw_opt = 222.78 GB/s; speedup = 220.76%
Size (1000, 1024); Mismatches: dg = 0 db = 0 out of 1024. reference = 0.0113 (ms); optimized = 0.0097 (ms); bw_opt = 787.79 GB/s; speedup = 16.46%
Size (6000, 1024); Mismatches: dg = 0 db = 0 out of 1024. reference = 0.0723 (ms); optimized = 0.0715 (ms); bw_opt = 640.95 GB/s; speedup = 1.10%
Size (6272, 1024); Mismatches: dg = 0 db = 0 out of 1024. reference = 0.0751 (ms); optimized = 0.0572 (ms); bw_opt = 837.57 GB/s; speedup = 31.30%
Size (200, 1536); Mismatches: dg = 0 db = 0 out of 1536. reference = 0.0232 (ms); optimized = 0.0071 (ms); bw_opt = 323.97 GB/s; speedup = 226.51%
Size (1000, 1536); Mismatches: dg = 0 db = 0 out of 1536. reference = 0.0125 (ms); optimized = 0.0114 (ms); bw_opt = 1005.84 GB/s; speedup = 9.62%
Size (6000, 1536); Mismatches: dg = 0 db = 0 out of 1536. reference = 0.0807 (ms); optimized = 0.0830 (ms); bw_opt = 828.02 GB/s; speedup = -2.76%
Size (6272, 1536); Mismatches: dg = 0 db = 0 out of 1536. reference = 0.0836 (ms); optimized = 0.0695 (ms); bw_opt = 1033.62 GB/s; speedup = 20.27%
Size (200, 2048); Mismatches: dg = 0 db = 0 out of 2048. reference = 0.0224 (ms); optimized = 0.0075 (ms); bw_opt = 408.58 GB/s; speedup = 198.10%
Size (1000, 2048); Mismatches: dg = 0 db = 0 out of 2048. reference = 0.0165 (ms); optimized = 0.0135 (ms); bw_opt = 1132.42 GB/s; speedup = 22.26%
Size (6000, 2048); Mismatches: dg = 0 db = 0 out of 2048. reference = 0.0993 (ms); optimized = 0.0989 (ms); bw_opt = 926.35 GB/s; speedup = 0.41%
Size (6272, 2048); Mismatches: dg = 0 db = 0 out of 2048. reference = 0.1033 (ms); optimized = 0.0826 (ms); bw_opt = 1159.55 GB/s; speedup = 25.09%
Size (200, 3072); Mismatches: dg = 0 db = 0 out of 3072. reference = 0.0230 (ms); optimized = 0.0076 (ms); bw_opt = 605.09 GB/s; speedup = 202.51%
Size (1000, 3072); Mismatches: dg = 0 db = 0 out of 3072. reference = 0.0207 (ms); optimized = 0.0213 (ms); bw_opt = 1076.45 GB/s; speedup = -2.69%
Size (6000, 3072); Mismatches: dg = 0 db = 0 out of 3072. reference = 0.1198 (ms); optimized = 0.1274 (ms); bw_opt = 1078.58 GB/s; speedup = -5.95%
Size (6272, 3072); Mismatches: dg = 0 db = 0 out of 3072. reference = 0.1293 (ms); optimized = 0.1189 (ms); bw_opt = 1207.95 GB/s; speedup = 8.76%

Average speedup = 52.88%
```

For additional numerical validation used the following script:

```
def run_model_on_device(fs, X, gO, device_string, numeric_type):
    ln = torch.nn.LayerNorm((fs,), device=device_string, dtype=numeric_type)
    ln.reset_parameters()
    X.grad = None
    ln.zero_grad(set_to_none=True)
    out = ln(X)
    out.backward(gO)
    return (ln.weight.grad, ln.bias.grad)

def run_correctness_test(eps_weight, eps_bias):
    dtype = torch.float
    for fs in (512, 1024, 2048, 4096, 8192, 10000, 500, 1000, 2001, 4005, 8117):
        for bs in (512, 1024, 2048, 4096, 525, 1033, 2064, 3000):
            mean_adjustment = torch.randn(fs, device=""cpu"", dtype=torch.float)
            X = mean_adjustment * torch.randn(
                bs, fs, device=""cpu"", dtype=torch.float, requires_grad=True
            )

            X = X.detach().requires_grad_()
            gO = torch.rand_like(X)
            X_gpu = X.to(""cuda"")
            X_gpu = X_gpu.detach().requires_grad_()
            gO_gpu = gO.to(""cuda"")
            gO_gpu = gO_gpu.detach().requires_grad_()

            grad_cpu_ref = run_model_on_device(fs, X, gO, ""cpu"", dtype)
            grad_gpu = run_model_on_device(fs, X_gpu, gO_gpu, ""cuda"", dtype)
            weight_grad_gpu_target = grad_gpu[0].detach().to(""cpu"")
            bias_grad_gpu_target = grad_gpu[1].detach().to(""cpu"")

            weight_delta = torch.abs(grad_cpu_ref[0] - weight_grad_gpu_target)
            weight_mismatches = (weight_delta >= eps_weight).nonzero()
            weight_mismatch_pct = len(weight_mismatches) / len(weight_delta) * 100

            bias_delta = torch.abs(grad_cpu_ref[1] - bias_grad_gpu_target)
            bias_mismatches = (bias_delta >= eps_bias).nonzero()
            bias_mismatch_pct = len(bias_mismatches) / len(bias_delta) * 100

            print(
                ""Size ({} x {}) mismatch percentage: weight {:3.2f} bias {:3.2f}"".format(
                    fs, bs, weight_mismatch_pct, bias_mismatch_pct
                )
            )
```

`NVFuserTest.FusionMagicSchedulerLayerNormBackward_CUDA` test also does additional numerical validation and it passes.

Differential Revision: D40730981

Pull Request resolved: https://github.com/pytorch/pytorch/pull/87814
Approved by: https://github.com/weiwangmeta"
pytorch/pytorch,1522946882fee9e4d8c20e143a58d7074cc2efd4,"Simplify installation instruction in contributing file (#87460)

Simplification of one of the installation instructions in CONTRIBUTING.md that I found tricky to parse at first.

Also adds a link to the ""Make no-op build fast"" section to make it easier to navigate to.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/87460
Approved by: https://github.com/ngimel"
pytorch/pytorch,123b103bf101682e670c96ab505b6eb8475e8657,"Add dynamo_optimize_ddp arg to dist bench (#87768)

cc @jansel @mlazos @soumith @voznesenskym @yanboliang @penguinwu @anijain2305
Pull Request resolved: https://github.com/pytorch/pytorch/pull/87768
Approved by: https://github.com/davidberard98"
pytorch/pytorch,ebe5aad466fa7d1a25903be04ab7b15bdb6dcdf2,"[inductor] Revert channels-last support (#87588)

We witnessed slow compilation times last week. Earlier, I thought it was due to parallel compilation. But, after git bisect, I found the source of extra time to be my PR - https://github.com/pytorch/pytorch/pull/87049

For 1x1 kernel, the current striding check incorrectly declares channels-first 1x1 convs to channels last. I am not sure why it caused so much compilation time jump.  Or why it did not fail? There was no change in performance speedup. cc @jansel @lezcano @fdrocha @mlazos @soumith @voznesenskym @yanboliang @penguinwu to identify what could be source of this compilation time increase, so that we can manually check that part of the stack.

With this `res2next50` compilation time went back to 96 seconds (which was raised to 900 seconds with my earlier PR) for single thread. And parallel-compilation brings it down to ~30 seconds.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/87588
Approved by: https://github.com/soumith, https://github.com/jansel, https://github.com/ngimel"
pytorch/pytorch,0cba7888c5eeb66535e72bad852c3ca3dc3ac681,"Performance improvment to cumulative seq len (#87530)

# Summary
Performance improvement to calculating metadata needed for gluing in nested tensors to fused kernels.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/87530
Approved by: https://github.com/cpuhrsch"
pytorch/pytorch,fd60b818b9d5b9ff6c7e33b7c2ba15d5b2fe97cd,"[Python] refactor slices on sorted (#86995)

Sometimes you want to query the small element of a set of elements and use `sorted(elements)[0]` without a second thought. However, this is not optimal, since the entire list must be sorted first `O(n log n)`. It would be better to use the `min(elements)` method provided for this purpose `O(n)`.
Furthermore `sorted(elements)[::-1]` is not very efficient, because it would be better to use `sorted(elements, reverse=True)` to save the slice operation.

**TLDR: using `sorted(elements)[0]` is slow and can be replaced with `min(elements)`.**

I stumbled across these code snippets while playing around with CodeQL (see https://lgtm.com/query/4148064474379348546/).
Pull Request resolved: https://github.com/pytorch/pytorch/pull/86995
Approved by: https://github.com/jansel"
pytorch/pytorch,c4fecff97d5b5405bcac6c6f8dc34bcb1d2cb020,"[inductor] Prevent aggressive fusion during inductor lowering (#87447)

Fixes https://github.com/pytorch/torchdynamo/issues/1599

Inductor performs aggressive fusion of ops during the lowering of Fx graph into IR nodes. Note that this fusion is different from the fusion that we typically discuss in the context of Inductor, which refers to the fusion of SchedulerNodes (way after lowering). This PR, instead, ensures that we don't accumulate too many ops in the IR node to begin with.

In the case of hf_t5_large backward graph, earlier we would generate a kernel with 100s of operators, causing that kernel to take ~350 seconds of compilation time. With this PR, we get it down from 350 seconds to 50 seconds.

Note that this could affect performance. I doubt that it will lead to really large dip though. In my toy examples, even if the lowering creates multiple IR nodes, if its a simple fusion, later fusion still creates one node.

I would like (1) test_torchinductor.py, (2) test_torchinductor_info.py, and (3) atleast HF models to be enabled in CI before merging this one.

@ngimel @jansel @Chillee

cc @jansel @lezcano @fdrocha @mlazos @soumith @voznesenskym @yanboliang @penguinwu
Pull Request resolved: https://github.com/pytorch/pytorch/pull/87447
Approved by: https://github.com/jansel"
pytorch/pytorch,233305a852e1cd7f319b15b5137074c9eac455f6,"Improvements for DDP Optimizer (#87549)

- adds support for 'first_bucket_cap' arg, to align bucketing more precisely
  with DDP, which may start a smaller first bucket
- refactors the bucket splitting logic to be cleaner
- adds pretty-print for bucket info, and a way to access bucket info
  from the DDPOptimizer class from a test case or benchmark
- dumps debug logs to stdout

cc @jansel @lezcano @fdrocha @mlazos @soumith @voznesenskym @yanboliang
Pull Request resolved: https://github.com/pytorch/pytorch/pull/87549
Approved by: https://github.com/soumith"
pytorch/pytorch,b0e10292faf947fe08589392c3731bbbcf3b2a05,"[Profiler] Tensor IDs for Module and Optimizer variables (#86754)

More sophisticated profiling will increasingly rely on python tracer to contextualize observed results. This PR adds Tensors which are observed by the python tracer to the identity assignment loop.

Differential Revision: [D39852885](https://our.internmc.facebook.com/intern/diff/D39852885/)
Pull Request resolved: https://github.com/pytorch/pytorch/pull/86754
Approved by: https://github.com/slgong-fb, https://github.com/aaronenyeshi"
pytorch/pytorch,be2d647ea690cf302926a67b38d980841e403178,"[Profiler] Use parameter as key for optimizer state recording. (#86753)

While optimizer can store state however it likes, in practice most optimizer state corresponds to a particular parameter. (This is the case for all `torch.optim` optimizers.) Thus, it turns out to be ergonomic to collect using that structure. Note that this doesn't lock us into anything; we can always collect state with non Tensor keys if the use case arises.

One simplification that arises is that Module and Optimizer collection has very similar structure. So similar, in fact, that it is possible to use a common template for config. I also found that a lot of the `check_and_store` logic could be simplified and inlined by this joining of collected optimizer state.

Differential Revision: [D40210703](https://our.internmc.facebook.com/intern/diff/D40210703/)
Pull Request resolved: https://github.com/pytorch/pytorch/pull/86753
Approved by: https://github.com/slgong-fb, https://github.com/aaronenyeshi"
pytorch/pytorch,fc3beef5ac11a88c7f538efcb7c60c5971393f38,"Fix stupid N^2 naming behavior in FX and removed assert that slows things a lot sometimes (#87533)

cc @jansel @lezcano @fdrocha @mlazos @soumith @voznesenskym @yanboliang
Pull Request resolved: https://github.com/pytorch/pytorch/pull/87533
Approved by: https://github.com/ezyang, https://github.com/voznesenskym"
pytorch/pytorch,0ef0a78196cf8726b0327c9f370615c8889ed676,"Revert ""Improvements for DDP Optimizer (#87525)""

This reverts commit cf693a02e0f6a022d10fd882af20efacfe7ecb76.

Reverted https://github.com/pytorch/pytorch/pull/87525 on behalf of https://github.com/ZainRizvi due to The macos error messages look like they were indeed caused by this PR"
pytorch/pytorch,cf693a02e0f6a022d10fd882af20efacfe7ecb76,"Improvements for DDP Optimizer (#87525)

- adds support for 'first_bucket_cap' arg, to align bucketing more precisely
  with DDP, which may start a smaller first bucket
- refactors the bucket splitting logic to be cleaner
- adds pretty-print for bucket info, and a way to access bucket info
  from the DDPOptimizer class from a test case or benchmark
- dumps debug logs to stdout

cc @jansel @lezcano @fdrocha @mlazos @soumith @voznesenskym @yanboliang
Pull Request resolved: https://github.com/pytorch/pytorch/pull/87525
Approved by: https://github.com/davidberard98"
pytorch/pytorch,8461460d55c2474b236a5d7198067ed299631b76,"Unified debug directory for dynamo/inductor tools (#87438)

Fixes https://github.com/pytorch/torchdynamo/issues/1705
Fixes https://github.com/pytorch/torchdynamo/issues/1383

Adds a debug directory by default called `torchdynamo_debug` in the current working directory.
In the debug directory for each run of dynamo (an enter and exit of optimize) folder run_\<timestamp\> is created which contains any minifier/inductor/torchdynamo artifacts under respective folders.

Updated the minifier, record replay, and inductor tracing to use this directory

cc @jansel @lezcano @fdrocha @soumith @voznesenskym @yanboliang
Pull Request resolved: https://github.com/pytorch/pytorch/pull/87438
Approved by: https://github.com/soumith"
pytorch/pytorch,62d30f5a8ab6816874c7f1d43402bb7e1d1eb6ec,"Remove unused cold_start experiment (#87470)

- this `--cold_start` experiment didn't end up being used
- there is a new `--cold_start_latency` flag that is used
- this experiment was only hooked up for nvfuser anyway

cc @jansel @lezcano @fdrocha @mlazos @soumith @voznesenskym @yanboliang
Pull Request resolved: https://github.com/pytorch/pytorch/pull/87470
Approved by: https://github.com/anijain2305"
pytorch/pytorch,620dbc43d8e3c836ad8d934987ee2f87fefbad7a,"Slowly introduce ops to be tested by test_numpy_ref on MPS backend (#87342)

Enable a test that would have caught https://github.com/pytorch/pytorch/issues/86239

Prior to the fix for that bug, this test fails with

```
_____________________________ TestCommonMPS.test_numpy_ref_mps_where_mps_float32 _____________________________
Traceback (most recent call last):
  File ""/Users/alex/git/pytorch/test/test_ops.py"", line 197, in test_numpy_ref_mps
    self.compare_with_reference(
  File ""/Users/alex/git/pytorch/torch/testing/_internal/common_utils.py"", line 2366, in compare_with_reference
    actual = torch_fn(t_inp, *t_args, **t_kwargs)
  File ""/Users/alex/git/pytorch/torch/testing/_internal/opinfo/core.py"", line 1068, in __call__
    return self.op(*args, **kwargs)
  File ""/Users/alex/git/pytorch/torch/testing/_internal/common_methods_invocations.py"", line 15167, in <lambda>
    op=lambda self, condition, other: torch.where(condition, self, other),
RuntimeError: 0'th index 3 of x tensor does not match the other tensors
```
Pull Request resolved: https://github.com/pytorch/pytorch/pull/87342
Approved by: https://github.com/albanD"
pytorch/pytorch,1285542f9b54972089655f91146e277c004762a2,"OpInfo: Add test that sample_inputs_func returns a generator (#84567)

This also includes a small list exception for single element lists since none of the memory usage or performance implications of lists apply there.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/84567
Approved by: https://github.com/lezcano, https://github.com/mruberry"
pytorch/pytorch,bd1e95ce306956a748915a217c3ae9012469b0fa,"Improve the performance of validate_non_overlapping_shards_metadata (#85639)

`validate_non_overlapping_shards_metadata()` uses a quadratic algorithm to verify the overlapping. However, in some cases (only one dimension is sharded), we a O(nlogn) algorithm can easily be implemented. This PR changes the implementation of `validate_non_overlapping_shards_metadata()`.

Differential Revision: [D39681725](https://our.internmc.facebook.com/intern/diff/D39681725/)
Pull Request resolved: https://github.com/pytorch/pytorch/pull/85639
Approved by: https://github.com/wanchaol"
pytorch/pytorch,f38a88c4dd8ce006b9934d0d2f121fb93564479b,"Revert ""[dynamo] use optimizers correctly in benchmarking (#87311)""

This reverts commit 703c19008df4700b6a522b0ae5c4b6d5ffc0906f.

Reverted https://github.com/pytorch/pytorch/pull/87311 on behalf of https://github.com/anijain2305 due to Bin (desertfire) is trying to get torchbench models in CI, and this PR prevents that. I will bring this back after models are in CI."
pytorch/pytorch,c16b7b41f76233ba930ce7dce6d31f1d362f7e86,"[Profiler][Trivial] Small style and safety fixes (#86752)

I noticed a couple abbreviations in the new optimizer capture code that are worth expanding. I also made the RawTensorMetadata a bit safer.

Differential Revision: [D40210702](https://our.internmc.facebook.com/intern/diff/D40210702/)
Pull Request resolved: https://github.com/pytorch/pytorch/pull/86752
Approved by: https://github.com/slgong-fb, https://github.com/aaronenyeshi"
pytorch/pytorch,703c19008df4700b6a522b0ae5c4b6d5ffc0906f,"[dynamo] use optimizers correctly in benchmarking (#87311)

We were not setting optimizers correctly

* This hid the issue that we see here - https://github.com/pytorch/torchdynamo/issues/1687
* This has also revealed that we are activating profilers for every dynamo optimized model call. This could affect speedup

cc @jansel @lezcano @fdrocha
Pull Request resolved: https://github.com/pytorch/pytorch/pull/87311
Approved by: https://github.com/mlazos, https://github.com/yanboliang"
pytorch/pytorch,cc64863d71dd31e74c42b190a6a2dfd5de0305e6,"Clean Inductor complication cache during dynamo dashboard run (#87246)

Implement improvement from https://github.com/pytorch/torchdynamo/issues/1644.

Tested by running `python benchmarks/dynamo/runner.py --print_run_commands --training` and inspecting the generated `run.sh` file for the `--cold_start_latency` flag, e.g.
```
python benchmarks/dynamo/torchbench.py --performance --float32 -dcuda --output=benchmark_logs/inductor_torchbench_float32_training_cuda_performance.csv --training --inductor   --no-skip --dashboard -x fambench_xlmr -x detectron2_fasterrcnn_r_50_c4 -x detectron2_fasterrcnn_r_50_dc5 -x detectron2_maskrcnn_r_101_fpn -x detectron2_maskrcnn_r_50_fpn -x detectron2_fasterrcnn_r_50_fpn -x detectron2_maskrcnn -x detectron2_fasterrcnn_r_101_dc5 -x opacus_cifar10 -x detectron2_maskrcnn_r_101_c4 -x pyhpc_turbulent_kinetic_energy -x maml -x detectron2_fasterrcnn_r_101_fpn -x pyhpc_equation_of_state -x detectron2_fasterrcnn_r_101_c4 -x pyhpc_isoneutral_mixing --cold_start_latency
```
Pull Request resolved: https://github.com/pytorch/pytorch/pull/87246
Approved by: https://github.com/anijain2305, https://github.com/jansel"
pytorch/pytorch,232fbd90ff6d93362120d955befeeb297179ddad,"[TorchDynamo]: fused bias for cpu convolution path (#87050)

For aten.convolution CPU path, the bias always can be fused, so this PR adds a device check: if inputs' device is CPU, we will fuse it for a good performance.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/87050
Approved by: https://github.com/jgong5, https://github.com/jansel"
pytorch/pytorch,c471c29fdccc3fe48a78083c638a4a88559488b4,"Update sdp guards for performance (#87241)

# Summary

Makes the contiguous check for the nt input more strict/correct as well as makes some performance improvements to the checks

Pull Request resolved: https://github.com/pytorch/pytorch/pull/87241
Approved by: https://github.com/cpuhrsch"
pytorch/pytorch,80790ecee4f04de4bf1675fec8a2593d7a2b32c0,"[einsum] Call view instead of sum to remediate MPS regression (#87135)

Fixes #87010.

It turns out that squeeze is much faster than sum, and view is faster than squeeze, so we should default to that whenever possible.

Benchmarking results show that, on MPS, we would be going from the following code taking **29.89ms instead of the current 1466ms, almost a 50x speedup**.
```
q = torch.rand(16, 4096, 40, device='mps', dtype=torch.float)
k = torch.rand(16, 4096, 40, device='mps', dtype=torch.float)
torch.einsum('b i d, b j d -> b i j', q, k).max().item()
```
And a regular einsum will now take **.506ms instead of 2.76ms.**
```
q = torch.rand(16, 4096, 40, device='mps', dtype=torch.float)
k = torch.rand(16, 4096, 40, device='mps', dtype=torch.float)
torch.einsum('b i d, b j d -> b i j', q, k)
```

Special thanks to @soulitzer for helping me experiment + figure out how to squash the remaining 5x regression due to squeeze being slower than view!!
Pull Request resolved: https://github.com/pytorch/pytorch/pull/87135
Approved by: https://github.com/soulitzer, https://github.com/malfet, https://github.com/albanD"
pytorch/pytorch,e4285f09b9993d4a17b755c74b68bed69f7473d0,"[inductor] new way to compile f64 libdevice calls (#87189)

Porting over [torchdynamo/#1633](https://github.com/pytorch/torchdynamo/pull/1633)

`torch/_inductor/codegen/triton.py` now defines `libdevice_<function>` variants
of some functions. You can request dispatch to those for
float64 dtypes when using `register_pointwise` by setting
`use_libdevice_for_f64=True`.

Other minor changes:
    - In triton, sigmoid now codegens tl.sigmoid
    - silu now comes from decomp, not lowering
    - Some test skips no longer necessary, removed or made xfails

Switching to `tl.sigmoid` has exactly same performance.
Moving `silu` to decomp does not change anything, same triton code is generated.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/87189
Approved by: https://github.com/ngimel"
pytorch/pytorch,c56be31d2ec838f29c46d8b585b31b5e47f478e8,"Upgrade oneDNN to v2.7 (#87061)

This PR is to upgrade oneDNN to v2.7.

### oneDNN v2.7 changes:

**Performance Optimizations**
- Improved performance for future Intel Xeon Scalable processors (code name Sapphire Rapids).
- Introduced performance optimizations for [bf16 floating point math mode](http://oneapi-src.github.io/oneDNN/group_dnnl_api_mathmode.html) on Intel Xeon Scalable processors (code name Sapphire Rapids). The bf16 math mode allows oneDNN to use bf16 arithmetic and Intel AMX instructions in computations on fp32 data.

Please go to https://github.com/oneapi-src/oneDNN/releases/tag/v2.7 for more detailed changes.

### oneDNN v2.6.1 & 2.6.2 changes:

**Functionality**

- Updated ITT API to 3.22.5
- Fixed correctness issue in fp32 convolution implementation for cases with large spatial size (https://github.com/pytorch/pytorch/issues/84488)

### Performance Benchmark
Use TorchBench test in ICX with 40 cores
Intel OpenMP & tcmalloc were preloaded
![image](https://user-images.githubusercontent.com/61222868/196121957-656faebc-9f4a-49f0-9ef0-0784416c3a47.png)

Pull Request resolved: https://github.com/pytorch/pytorch/pull/87061
Approved by: https://github.com/jgong5, https://github.com/XiaobingSuper, https://github.com/weiwangmeta"
pytorch/pytorch,d36c284d1446cb250178f8e89fff9b342ee1a5a9,"[triton] allow cuda properties to be queried from workers (#87101)

Fixes https://github.com/pytorch/pytorch/pull/87048 by saving the needed properties before fork.

Actually attempting to get CUDA to load in the workers is probably not desired: cuda initialization takes O(seconds). Having multiple processes using the same device will slow things down.

This just moves the needed properties from the main trainer process to the workers.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/87101
Approved by: https://github.com/soumith"
pytorch/pytorch,5fb687182dba781d9c95388d19f4784b98cb8b20,"Enable sdp_forward for NestedTensors (#86720)

# Summary
This PR implements a sdp_forward for NestedTensors. This impl will call into flash and mem_efficient_attention when possible.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/86720
Approved by: https://github.com/cpuhrsch"
pytorch/pytorch,4fd98dfe69287914fd29b38fbccaf7ac4d7261ee,"Don't only apply DDP optimizer on forward frames (#87097)

Previously a check would only apply DDP optimizer on frames named ""forward"".

But on hf_T5_large, a graph break causes some frames like:

```
<graph break in _shift_right>
<graph break in forward>
```

So instead, apply DDP optimizer on all frames.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/87097
Approved by: https://github.com/wconstab"
pytorch/pytorch,1704256b107500c1ebc2e803b55e31e11104e618,"Enables `where` to have cpu scalar args (#87022)

This is for decompositions only, no attempt made to have good performance for this case.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/87022
Approved by: https://github.com/ezyang, https://github.com/eellison, https://github.com/mruberry"
pytorch/pytorch,0379af681b4b20475589189251aafbb2e6bb91ca,"[inductor] Disable parallel compile (#87048)

https://github.com/pytorch/pytorch/pull/87032 seems to have an issue that breaks our benchmark script, it might have to do with the benchmark script also using subprocess.

Before this PR:
```
$ ./benchmarks/dynamo/torchbench.py --performance --inductor --raise --training --float16
...
Traceback (most recent call last):
  File ""/home/jansel/conda/envs/pytorch/lib/python3.9/concurrent/futures/process.py"", line 246, in _process_worker
    r = call_item.fn(*call_item.args, **call_item.kwargs)
  File ""/home/jansel/pytorch/torch/_inductor/codecache.py"", line 239, in _worker_compile
    kernel = TritonCodeCache.load(source_code)
  File ""/home/jansel/pytorch/torch/_inductor/codecache.py"", line 234, in load
    mod = PyCodeCache.load(source_code)
  File ""/home/jansel/pytorch/torch/_inductor/codecache.py"", line 212, in load
    exec(code, mod.__dict__, mod.__dict__)
  File ""/tmp/torchinductor_jansel/ij/cij7smji4sw2a56i4yz45bjkrosd2sb2raqnxzsxxpg4kwzuo2ta.py"", line 5, in <module>
    from torch._inductor.triton_ops.autotune import reduction
  File ""/home/jansel/pytorch/torch/_inductor/triton_ops/__init__.py"", line 3, in <module>
    if has_triton():
  File ""/home/jansel/pytorch/torch/_inductor/utils.py"", line 38, in has_triton
    return triton is not None and torch.cuda.get_device_capability() >= (7, 0)
  File ""/home/jansel/pytorch/torch/cuda/__init__.py"", line 368, in get_device_capability
    prop = get_device_properties(device)
  File ""/home/jansel/pytorch/torch/cuda/__init__.py"", line 382, in get_device_properties
    _lazy_init()  # will define _get_device_properties
  File ""/home/jansel/pytorch/torch/cuda/__init__.py"", line 228, in _lazy_init
    raise RuntimeError(
RuntimeError: Cannot re-initialize CUDA in forked subprocess. To use CUDA with multiprocessing, you must use the 'spawn' start method
```

cc @zdevito
Pull Request resolved: https://github.com/pytorch/pytorch/pull/87048
Approved by: https://github.com/soumith"
pytorch/pytorch,2b7236a0e1c0d2339165103b2cd42e25debee99d,"[torchdynamo] Use ProcessPoolExecutor for triton compiles (#87032)

This patch significantly improves the parallel compilation performance for cThis patch significantly improves the parallel compilation performance for compiling triton kernels
by using ProcessPoolExecutor to create persistent pool of compilation
workers.

Previously os.fork overhead and GIL contention limited the achieved
parallelism. This patch replaces
the worker threads with a pool of processes to do the raw compilation,
and does serial work on the main thread
for everything else. This other work couldn't be parallelized anyway
since it is mostly in python.

In cold start situations, the time to get the worker threads started can
be significant portion of the time.
This patch starts the workers earlier so they are ready to perform
compilation (see code comments) when dynamo
gets to that point.

Just tested this on one example benchmark (tf_efficientnet_b0), but the
results are significant, almost eliminating the difference between a
warm and cold compilation.

```
39.613s - warm
41.290s - cold, this patch

2m53.197s - cold, single threaded:
1m7.092s - cold, old setup n = 8 (its best config)
```
 (cold compilation is done after running `rm -rf
/tmp/torchinductor_$USER`).ompiling triton kernels
by using ProcessPoolExecutor to create persistent pool of compilation workers.

Previously os.fork overhead and GIL contention limited the achieved parallelism. This patch replaces
the worker threads with a pool of processes to do the raw compilation, and does serial work on the main thread
for everything else. This other work couldn't be parallelized anyway since it is mostly in python.

In cold start situations, the time to get the worker threads started can be significant portion of the time.
This patch starts the workers earlier so they are ready to perform compilation (see code comments) when dynamo
gets to that point.

Just tested this on one example benchmark (tf_efficientnet_b0), but the results are significant, almost eliminating the difference between a warm and cold compilation.

```
39.613s - warm
41.290s - cold, this patch

2m53.197s - cold, single threaded:
1m7.092s - cold, old setup n = 8 (its best config)
```
 (cold compilation is done after running `rm -rf /tmp/torchinductor_$USER`).
Pull Request resolved: https://github.com/pytorch/pytorch/pull/87032
Approved by: https://github.com/soumith, https://github.com/jansel"
pytorch/pytorch,66979fbfaa2af227a6834157fa6f532979b2d23b,"Improve complex lerp performance (#84844)

The complex lerp kernel uses `std::abs(z) < 0.5` which involves
computing a sqrt. Instead compare the square against 0.25 has much
lower latency and so performs much better overall.

In a simple timeit benchmark I see more than 10x speedup on CPU for a 4096
element complex lerp, from 84 us to 6.7 us.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/84844
Approved by: https://github.com/ngimel"
pytorch/pytorch,974ad8fa6cc63b89234beb5ebff54c2d42711932,"Add BFloat16 dtype support for oneDNN Graph JIT fuser (#85591)

## BFloat16 dtype support for faster inference with TorchScript using oneDNN Graph

Intel Xeon Cooper Lake platform & beyond support the `AVX512_BF16` ISA, which is essentially native BFloat16 support.
oneDNN Graph delivers high inference performance with BFloat16 on such machines.

While oneDNN Graph can still be used with BFloat16 on older machines that lack `avx512_bf16` ISA but support `avx512bw`, `avx512vl` & `avx512dq` ISAs, the BF16 performance on these older machines will be significantly poorer (probably even poorer than Float32), as they lack native BF16 support.

Currently, [AMP support for eager mode & JIT mode is divergent in PyTorch](https://github.com/pytorch/pytorch/issues/75956).
So, for using oneDNN Graph with BFloat16, eager-mode AMP should be leveraged by turning off AMP for JIT mode, using `torch._C._jit_set_autocast_mode(False)` in python code, so as to avoid conflicts.

Please use the following environment variable to view JIT logs -
`PYTORCH_JIT_LOG_LEVEL="">>graph_helper:>>graph_fuser:>>kernel:>>interface""`

## Changes being made in this PR
1. This PR does NOT change the `oneDNN` commit or the `ideep` files. While the `ideep` commit is being updated, only files pertaining to oneDNN Graph are being updated. oneDNN Graph is being upgraded to version 0.5.2 (alpha patch release 2).
To put things into perspective, `ideep` is a git submodule of PyTorch. `oneDNN Graph` is a git submodule of `ideep` (`ideep/mkl-dnn`), and oneDNN is a git submodule of oneDNN Graph (`ideep/mkl-dnn/third_party/oneDNN`).
2. Unit-tests are being updated. We now use the [existing dtypes decorator](https://github.com/pytorch/pytorch/blob/master/torch/testing/_internal/common_device_type.py#L123-L131).
3. Suggestions made by @eellison in the [FP32 PR](https://github.com/pytorch/pytorch/pull/68111#pullrequestreview-896719477) are being incorporated/addressed -

| Action-item | Status |
| :---                                             |          ---: |
|checkInputCompatibility follow up | Fixed |
|the mayConvertScalarInputToTensor logic we can consider | Added type promotion code |
|fix up fixConvOptionalBias| The current approach seems correct |
|Use opinfo tests| using dtypes decorator. Will use `OpInfo` in a subsequent PR, if that'd be possible. Should we create a list of ops from opDB that are supported by oneDNN Graph, and add it to `common_methods_invocations.py`? |
|inferDevice torch_check call | not necessary now, perhaps, as only CPU is supported, for now? We'd add it by the beta release of oneDNN Graph, though, so that by then, users might be able to use other fusers with oneDNN Graph (NNC/TensorExpr are already compatible with the oneDNN Graph fuser). We can still add it, if you'd insist. |
|not checking shapes of input mkldnn tensor to llga guard | Those checks should not be present because oneDNN Graph may use blocked or channels-last layout, so those strides would be different. They're only skipped if an LLGA subgraph's output is input to another LLGA subgraph, which enables LLGA to choose an optimal layout between them. |
|fix test failures with respect to unsupported inputs | We'll address them with the upcoming release of oneDNN Graph beta version|

4. More PyTorch ops are being been mapped to oneDNN Graph

## Example of using oneDNN Graph with BFloat16

```python
# Assuming we have a model of the name 'model'

example_input = torch.rand(1, 3, 224, 224)

# enable oneDNN Graph
torch.jit.enable_onednn_fusion(True)
# Disable AMP for JIT
torch._C._jit_set_autocast_mode(False)
with torch.no_grad(), torch.cpu.amp.autocast():
    model = torch.jit.trace(model, (example_input))
    model = torch.jit.freeze(model)
     # 2 warm-ups (2 for tracing/scripting with an example, 3 without an example)
    model(example_input)
    model(example_input)

    # speedup would be observed in subsequent runs.
    model(example_input)
```

## TorchBench based Benchmarks
**URL:** https://github.com/sanchitintel/benchmark/tree/onednn_graph_benchmark (instructions present at URL).
**Batch-size(s):** TorchBench-default for each model
**Baseline :** PyTorch JIT OFI FP32
**Machine:** Intel(R) Xeon(R) Platinum 8371HC (Cooper Lake)
**Sockets used**: 1
**Number of cores on one socket**: 26
Intel OpenMP & tcmalloc were preloaded

#### Benchmark results with single thread
| name                                             | latency of PyTorch JIT OFI FP32 (s) |   Latency of oneDNN Graph BF16 (s) |   % change |
| :---                                             |          ---: |            ---: |       ---: |
| test_eval[alexnet-cpu-jit]                       |      1.063851 |        0.509820 |     -52.1% |
| test_eval[mnasnet1_0-cpu-jit]                    |      0.218435 |        0.107100 |     -51.0% |
| test_eval[mobilenet_v2-cpu-jit]                  |      0.114467 |        0.058359 |     -49.0% |
| test_eval[mobilenet_v3_large-cpu-jit]            |      0.233873 |        0.117614 |     -49.7% |
| test_eval[resnet18-cpu-jit]                      |      0.160584 |        0.075854 |     -52.8% |
| test_eval[resnet50-cpu-jit]                      |      1.652846 |        0.713373 |     -56.8% |
| test_eval[resnext50_32x4d-cpu-jit]               |      0.471174 |        0.209431 |     -55.6% |
|test_eval[shufflenet_v2_x1_0-cpu-jit] | 0.310306 | 0.167090 | -46.2% |
| test_eval[squeezenet1_1-cpu-jit]                 |      0.161247 |        0.045684 |     -71.7% |
| test_eval[timm_efficientnet-cpu-jit]             |      1.643772 |        0.800099 |     -51.3% |
| test_eval[timm_regnet-cpu-jit]                   |      5.732272 |        2.333417 |     -59.3% |
| test_eval[timm_resnest-cpu-jit]                  |      1.366464 |        0.715252 |     -47.7% |
| test_eval[timm_vision_transformer-cpu-jit]       |      0.508521 |        0.271598 |     -46.6% |
| test_eval[timm_vovnet-cpu-jit]                   |      2.756692 |        1.125033 |     -59.2% |
| test_eval[vgg16-cpu-jit]                         |      0.711533 |        0.312344 |     -56.1% |

#### Benchmark results with 26 threads:
| name                                             | latency of PyTorch JIT OFI FP32 (s) |   Latency of oneDNN Graph BF16 (s) |   % change |
| :---                                             |          ---: |            ---: |       ---: |
| test_eval[alexnet-cpu-jit]                       |      0.062871 |        0.034198 |     -45.6% |
| test_eval[mnasnet1_0-cpu-jit]                    |      0.022490 |        0.008172 |     -63.7% |
| test_eval[mobilenet_v2-cpu-jit]                  |      0.012730 |        0.005866 |     -53.9% |
| test_eval[mobilenet_v3_large-cpu-jit]            |      0.025948 |        0.010346 |     -60.1% |
| test_eval[resnet18-cpu-jit]                      |      0.011194 |        0.005726 |     -48.9% |
| test_eval[resnet50-cpu-jit]                      |      0.124662 |        0.045599 |     -63.4% |
| test_eval[resnext50_32x4d-cpu-jit]               |      0.034737 |        0.015214 |     -56.2% |
|test_eval[shufflenet_v2_x1_0-cpu-jit] | 0.028820 | 0.012517 | -56.6% |
| test_eval[squeezenet1_1-cpu-jit]                 |      0.012557 |        0.003876 |     -69.1% |
| test_eval[timm_efficientnet-cpu-jit]             |      0.203177 |        0.051879 |     -74.5% |
| test_eval[timm_regnet-cpu-jit]                   |      0.452050 |        0.151113 |     -66.6% |
| test_eval[timm_resnest-cpu-jit]                  |      0.117072 |        0.052848 |     -54.9% |
| test_eval[timm_vision_transformer-cpu-jit]       |      0.046048 |        0.023275 |     -49.5% |
| test_eval[timm_vovnet-cpu-jit]                   |      0.213187 |        0.077482 |     -63.7% |
| test_eval[vgg16-cpu-jit]                         |      0.044726 |        0.021998 |     -50.8% |

Pull Request resolved: https://github.com/pytorch/pytorch/pull/85591
Approved by: https://github.com/jgong5, https://github.com/frank-wei, https://github.com/chunyuan-w"
pytorch/pytorch,0feccda7d74a23509e1b1edd0c5c76d5f67fa813,"fix aliasing bug in pixel shuffle/unshuffle (#86608)

Fixes https://github.com/pytorch/pytorch/issues/82235

cc @albanD - `at::pixel_shuffle` and `at::pixel_unshuffle` advertise as being non-aliasing, but they have a C++ decomposition that internally uses reshape(), which means that it might return an alias.

I happened to notice this because a bunch of tests in `test/test_ops.py` failed when I ran locally with a `DEBUG=1` build.

(P.S.: when are we finally gonna get a debug build test in CI? 😃)

I fixed by adding an extra clone, which... is going to be an unnecessary perf hit in the case where the `reshape()` already properly cloned the input. My hope is that this is fine, because this only impacts the composite kernel- we already have a ""fast"" CPU kernel that does the right thing. Is `pixel_shuffle/unshuffle` commonly used with cuda? Maybe we should just add a fast cuda kernel for it if that's the case.

Alternatively, it seems like it would be nice if `reshape()` accepted an optional argument to unconditionally return a copy. That seems like a rabbit hole that isn't worth going down for now though - I remember a discussion a while ago about making `reshape()` copy-on-write

Pull Request resolved: https://github.com/pytorch/pytorch/pull/86608
Approved by: https://github.com/albanD"
pytorch/pytorch,aacb9f3ac63d9a31d064c76ff3d328037355b28e,"Make `Adadelta`,`Adagrad` & `Adamax` differentiable (#86096)

Continuing the differentiable optimizers support

Pull Request resolved: https://github.com/pytorch/pytorch/pull/86096
Approved by: https://github.com/janeyx99"
pytorch/pytorch,61a5898675d2b18bea1009305ce1b1f7042b7d64,"use cff standard for citation information (#86200)

GH picks up on our `CITATION` file in the root of the repository.

![Screenshot from 2022-10-04 11-34-54](https://user-images.githubusercontent.com/6849766/193811617-b71ef606-a043-498b-bb2d-14b6c05e79e7.png)

However, [the preferred way](https://docs.github.com/en/repositories/managing-your-repositorys-settings-and-features/customizing-your-repository/about-citation-files) is use a `CITATION.cff` file instead since GH supports the [citation file format (CFF) standard](https://github.com/citation-file-format/citation-file-format). With this PR, the prompt changes to

![Screenshot from 2022-10-04 13-48-21](https://user-images.githubusercontent.com/6849766/193812010-026bfad7-7c4e-4b59-a90a-1d3ad47303d0.png)

with the following auto-generated bibtex entry:

```bibtex
@inproceedings{Paszke_PyTorch_An_Imperative_2019,
author = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and Desmaison, Alban and Kopf, Andreas and Yang, Edward and DeVito, Zachary and Raison, Martin and Tejani, Alykhan and Chilamkurthy, Sasank and Steiner, Benoit and Fang, Lu and Bai, Junjie and Chintala, Soumith},
booktitle = {Advances in Neural Information Processing Systems 32},
pages = {8024--8035},
publisher = {Curran Associates, Inc.},
title = {{PyTorch: An Imperative Style, High-Performance Deep Learning Library}},
url = {http://papers.neurips.cc/paper/9015-pytorch-an-imperative-style-high-performance-deep-learning-library.pdf},
year = {2019}
}
```

Comparing with what we currently have the only significant difference is that the editors are no longer listed although the metadata is there. This is an issue with GH's automatic conversion and might be fixed in the future. Plus, the cite key was changed from `NEURIPS2019_9015` to `Paszke_PyTorch_An_Imperative_2019`, but this has no effect on the rendered result.

Do we also want to adopt the CFF standard?
Pull Request resolved: https://github.com/pytorch/pytorch/pull/86200
Approved by: https://github.com/dagitses"
pytorch/pytorch,c4f0b93f8653505584bbd71162f82d4e7633da0c,"Disable autocast in aot autograd (#86515)

Fix for https://github.com/pytorch/torchdynamo/issues/1368

From comment:
> When we invoke a Composite Implicit autograd operator that has an autocast rule, such as Einsum,
autocast is disabled during its invocation. When we trace out the operators in an implicit op,
re-applying on autocast rules on those operators might yield divergence from what was executed at runtime.
This pass checks for divergence. If divergence is found, we will disable autocast.
We would like to avoid disabling autocast if possible because accessing TLS is slow.

Concretely, the problem found was when invoked `sum` in `einsum`:

As seen by the following divergence:
```
>>> with torch.cuda.amp.autocast(enabled=True):
...     print(torch.ops.aten.sum.dim_IntList(torch.rand([2, 2, 2], device=""cuda"", dtype=torch.half), [1, 2]).dtype)
...
torch.float32
>>> print(torch.ops.aten.sum.dim_IntList(torch.rand([2, 2, 2], device=""cuda"", dtype=torch.half), [1, 2]).dtype)
torch.float16
```

Edit: we've decided to accept the overhead of universally disabling autocast instead
Pull Request resolved: https://github.com/pytorch/pytorch/pull/86515
Approved by: https://github.com/bdhirsh, https://github.com/Chillee"
pytorch/pytorch,92562046e9d6ef32b14e17b2b06433cfe7990912,"Optimize __dlpack_device__ performance (#86665)

This can be critical when processing a large number of tensors

```bash
python -m timeit --setup 'import torch; t = torch.empty(1000, device=""cuda"")' 't.__dlpack_device__()'
```

based on 1.12.1:
before:
100000 loops, best of 5: 2.32 usec per loop
after:
500000 loops, best of 5: 844 nsec per loop

Pull Request resolved: https://github.com/pytorch/pytorch/pull/86665
Approved by: https://github.com/SunDoge, https://github.com/soulitzer"
pytorch/pytorch,b645c237bc88c441792d83f19575d0fd3284dcb4,"make g2p ~30% faster on mobile by suppressing a log (#85907)

Summary: using the tool from D39559248 i was able to make g2p faster on mobile by taking a look at profiles on stella frames. It turned out that the pytorch interpreter code does some logging that ends up being a pretty big bottleneck.

Differential Revision: D39901455

Pull Request resolved: https://github.com/pytorch/pytorch/pull/85907
Approved by: https://github.com/dzdang"
pytorch/pytorch,be682befbc836a07d5d070bb569450429526a64b,"[FSDP] Add `use_orig_params` (#84911)

**Overview**
This PR adds the option to use the original parameters via `use_orig_params=True` in the FSDP constructor.
- This exposes the original parameters rather than the `FlatParameter`s from `named_parameters()`, which means that the optimizer runs on the original parameters. Hence, users may assign original parameters from the same `FlatParameter` to different parameter groups.
- This enables decoupling the original parameter variables from their storage without changing the variables themselves, which is critical for our upcoming execution-order-based non-recursive wrapping policy.

For more detailed design explanation, refer to the Quip shared internally.

**Follow-Ups**
See 85831 (removing link to avoid spamming the issue whenever I update this PR).

`test_fsdp_use_orig_params.py` adds ~4 min 46 seconds to the TTS on the AWS cluster.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/84911
Approved by: https://github.com/rohan-varma"
pytorch/pytorch,9b2ea41f481bac297c8e1e88c431c03127a35759,"COO intersection primitives : fusing value selection with value intersection. (#86269)

As per title. This one fuses 3 kernels into 1 with about 20-10% performance improvement.
This kernel is also useful for union-like operations.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/86269
Approved by: https://github.com/amjames, https://github.com/cpuhrsch"
pytorch/pytorch,70c6a988d6b5cabed84c686316b6bbeb235cc05c,"Fix the performance issue that the for-loop before ExternallCall could not be parallelized. (#85056)

Currently, NNC only parallelizes the loop statement of the graph outputs. The logic could bypass some loop statements that could be parallelized. Take an example as follows and suppose the output of `ExternallCall` is also the output of NNC fusion group. Current [parallel logic](https://github.com/pytorch/pytorch/pull/85056/files#diff-9a11174c26e4b57ab73e819520122bc314467c72962f3a5b79e7400ea3c4bbe5L781-L785) only tries to parallel the `ExternalCall` and bypass `stmt1` and `stmt2`.

```c++
stmt1: For:
stmt2:   For:
stmt3: ExternalCall
```
Pull Request resolved: https://github.com/pytorch/pytorch/pull/85056
Approved by: https://github.com/frank-wei, https://github.com/bertmaher"
pytorch/pytorch,2232db7fc12301a2226d1921948917d5b23b6888,"Replacement is irrelevant for 1-sample multinomial (#86342)

So use fast path, both on CPU and on MPS

Also, remove some spurious copy-n-paste checks from MPS codepath

CUDA already has this optimization, see
https://github.com/pytorch/pytorch/blob/dc9c507d24d0c833cb09105177326f1f6bbe99c4/aten/src/ATen/native/cuda/MultinomialKernel.cu#L355-L356

Pull Request resolved: https://github.com/pytorch/pytorch/pull/86342
Approved by: https://github.com/ngimel"
pytorch/pytorch,fa799132d82c3c48253aaf7d3ee3a8c5e007350d,"[MPS] Better error message for `slow_conv2d_forward` (#86303)

Error `Could not run 'aten::_slow_conv2d_forward' with arguments from the 'MPS' backend.` is very misleading as usually this method is only invoked if input is on CPU but weights are on MPS device.
Raise a more user friendly error in this case

Add test to `test_invalid_conv2d` to check for those conditions.

Fixes https://github.com/pytorch/pytorch/issues/77931

Pull Request resolved: https://github.com/pytorch/pytorch/pull/86303
Approved by: https://github.com/kulinseth"
pytorch/pytorch,a117fde86febc2b1c27e7a0e809ae22d46e33849,"[Profiler] Apply TensorMetadata for Optimizer and nnModule (#86047)

Summary: - Use `TensorMetadat` struct in saving tensor info from Optimizer and nnModule.

Test Plan: buck run mode/opt //caffe2/test:profiler

Reviewed By: chaekit

Differential Revision: D39682205

Pull Request resolved: https://github.com/pytorch/pytorch/pull/86047
Approved by: https://github.com/chaekit, https://github.com/robieta"
pytorch/pytorch,45f03d69486e45b67bfcab9e60a2c24aa5f1ea8d,"Add at::symint:: namespace for ease of templated functions (#86329)

Our prevailing strategy for symbolic shapes in C++ is to only
write the SymInt version of the code, and pay a slight performance
tax from not knowing if it is symbolic or not.  However, there are
some fastpath functions where this tax is unacceptable, and we want
to specialize for the int case.  Sometimes, it is easy to template
the function; but when the function involves Tensors, it is not,
because the functions you may want to call are not templated,
e.g., t.view vs t.view_symint

This PR adds an at::symint:: namespace which contains templated
functions for all functions in PyTorch which you can use in this
way.  To show this works, I refactored sum_to to stop incorrectly
reinterpret casting and instead use a template.  Instead of
t.sizes(), we call at::symint::sizes<T>(t), and so forth.

The template functions are SFINAE'd using a template argument that
is not otherwise used. As such, deduction is impossible. Typically, deduction
is hard anyway, because many of the constructors are ambiguous (this
is why we split foo and foo_symint in the first place). So you must pass
a template argument to these functions.

These functions are codegened into Functions.h so they are subject
to per-operator headers.  This matters most for methods, which likely
didn't include the per-operator header, so you will have to add an
include in that case.  We never generate method variants for these.

Signed-off-by: Edward Z. Yang <ezyang@fb.com>
Pull Request resolved: https://github.com/pytorch/pytorch/pull/86329
Approved by: https://github.com/bdhirsh, https://github.com/voznesenskym"
pytorch/pytorch,adf5919720c02dcf8c1ff32c890dd1c4e54d6fe7,"Add option to record C++ backtraces in _record_memory_history (#86145)

I used this to debug https://github.com/pytorch/pytorch/issues/86136 so it is useful. The implementation is not so fast so it is not enabled by default.

Signed-off-by: Edward Z. Yang <ezyang@fb.com>
Pull Request resolved: https://github.com/pytorch/pytorch/pull/86145
Approved by: https://github.com/albanD, https://github.com/zdevito"
pytorch/pytorch,e8b0bea677b44206f663788e3a9d6a85b3779ed2,"Rename fromIntArrayRef to fromIntArrayRefSlow, audit call sites (#86235)

Some of them are known non-negative, I've revised them accordingly.

Signed-off-by: Edward Z. Yang <ezyang@fb.com>
Pull Request resolved: https://github.com/pytorch/pytorch/pull/86235
Approved by: https://github.com/albanD"
pytorch/pytorch,3b1ec7511e6d616fbe2e9f8721ff9be6c55d3d42,"Optimize is_symbolic test and some refactor (#86230)

Our SymInt rep can be represented more efficiently as just a greater than test, but the compiler doesn't seem to figure it out. Help it out.

There is also some refactoring to simplify the code and add more debugging.

Signed-off-by: Edward Z. Yang <ezyang@fb.com>
Pull Request resolved: https://github.com/pytorch/pytorch/pull/86230
Approved by: https://github.com/albanD"
pytorch/pytorch,3ec71fce79f4e568c48796da4b18a3e6f2c6fc29,"Improve make_tensor performance for float and complex types (#85473)

For floating types, `make_tensor` calls `rand` and then does a linear
interpolation from `low` to `high`. This instead calls `uniform_(low,
high)` to cut out the interpolation step.

For complex types, `make_tensor` does the `rand` + interpolation step
twice and calls `torch.complex(real, imag)` at the end. This instead
uses `view_as_real` and `uniform_(low, high)` to fuse it all into one
operation.

My benchmarks show significant speedups in all cases for float32 and
complex64.

| Device | dtype     | Size  | Master (us) | This PR (us) | Speedup |
|--------|-----------|-------|-------------|--------------|---------|
| CPU    | float32   | 8     | 19.4        | 6.34         | 3.1     |
|        |           | 4096  | 36.8        | 21.3         | 1.7     |
|        |           | 2**24 | 167,000     | 80,500       | 2.1     |
|        | complex32 | 8     | 37.0        | 7.57         | 4.9     |
|        |           | 4096  | 73.1        | 37.6         | 1.9     |
|        |           | 2**24 | 409,000     | 161,000      | 2.5     |
| CUDA   | float32   | 8     | 40.4        | 11.7         | 3.5     |
|        |           | 4096  | 38.7        | 11.7         | 3.3     |
|        |           | 2**24 | 2,300       | 238          | 9.7     |
|        | complex32 | 8     | 78.7        | 14           | 5.6     |
|        |           | 4096  | 82.7        | 13.8         | 6.0     |
|        |           | 2**24 | 5,520       | 489          | 11.3    |
Pull Request resolved: https://github.com/pytorch/pytorch/pull/85473
Approved by: https://github.com/mruberry"
pytorch/pytorch,97d2e1df5565b7f3a5358178b8f3a2a039c7f976,"[MPS] Fix GELU for `torch.half` (#86218)

Also, make sure it raises catcheable errors if invoked with integral types

Otherwise, it used to fail with following fatal error  invoked for `torch.half` and with similar signatures if invoked for integral types
```
loc(""mps_multiply""(""(mpsFileLoc): /AppleInternal/Library/BuildRoots/4883e71d-37bd-11ed-b0ef-b25c5e9b9057/Library/Caches/com.apple.xbs/Sources/MetalPerformanceShadersGraph/mpsgraph/MetalPerformanceShadersGraph/Core/Files/MPSGraphUtilities.mm"":228:0)): error: input types 'tensor<2xf16>' and 'tensor<1xf32>' are not broadcast compatible
LLVM ERROR: Failed to infer result type(s).
```

Modified `test_gelu_simple` to check both fwd and backward gradients for gelu"
pytorch/pytorch,67eb2d5952741f2024c826d008ed35b8a1cc56d9,"[FSDP] Dequeue one instead of flush (#86165)

For the rate limiter, I initially implemented the approach of only dequeueing a single event, but there was concern about blocking the CPU _every_ iteration. The landed approach instead blocks every `_max_num_inflight_all_gathers` iterations and flushes the entire queue.

However, upon further analysis, the approach of dequeueing a single event should be more performant with the same memory usage -- as the name suggests, both have `_max_num_inflight_all_gathers` concurrently inflight all-gathers. The cost of blocking the CPU thread is not important compared to the duration the CPU thread is actually blocked. This PR's approach reduces the latter quantity.

**Fast Communication; Slow Computation**
<img width=""1235"" alt=""Screen Shot 2022-10-04 at 4 15 13 PM"" src=""https://user-images.githubusercontent.com/31054793/193917536-f1491803-9578-45ea-ba6e-e735c1bf7784.png"">

**Slow Communication; Fast Computation**
<img width=""718"" alt=""Screen Shot 2022-10-04 at 4 34 15 PM"" src=""https://user-images.githubusercontent.com/31054793/193921508-f2a4fd22-2b03-4a8e-b6ca-634c584c70e2.png"">

**T5-11B**
2 nodes / 16 40 GB A100s with EFA and batch size 6:
- [Old] 5.81 s / batch; 24 and 20 CUDA malloc retries on local rank 0s; 35.234 GB peak active; 38.806 GB peak reserved
- [New] 5.10 s / batch; 25 and 29 CUDA malloc retries on local rank 0s; 35.234 GB peak active; 38.868 GB peak reserved

4 nodes / 32 40 GB A100s with EFA and batch size 7:
- [Old] 5.21 s / batch; 0, 0, 0, 0 CUDA malloc retries on local rank 0s; 33.695 GB peak active; 38.494 GB peak reserved
- [New] 4.93 s / batch; 1, 0, 0, 0 CUDA malloc retries on local rank 0s; 33.678 GB peak active; 38.792 GB peak reserved

The new version changes the fragmentation in the allocator. It is possible that by blocking the CPU thread more in the old approach, the initial blocks used to serve the all-gather stream allocations are different compared to the new approach. Even though the number of CUDA malloc retries increases slightly, the net result is a speedup with the new approach.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/86165
Approved by: https://github.com/zhaojuanmao"
pytorch/pytorch,a348975e00081334ac96d855932d2753a62f1e77,"Add opteinsum backend to give users control (#86219)

This achieves the same things as https://github.com/pytorch/pytorch/pull/85908 but using backends instead of kwargs (which breaks torchscript unfortunately). This also does mean we let go of numpy compatibility BUT the wins here are that users can control what opt einsum they wanna do!

The backend allows for..well you should just read the docs:
```
.. attribute::  torch.backends.opteinsum.enabled

    A :class:`bool` that controls whether opt_einsum is enabled (on by default). If so,
    torch.einsum will use opt_einsum (https://optimized-einsum.readthedocs.io/en/stable/path_finding.html)
    to calculate an optimal path of contraction for faster performance.

.. attribute::  torch.backends.opteinsum.strategy

    A :class:`str` that specifies which strategies to try when `torch.backends.opteinsum.enabled` is True.
    By default, torch.einsum will try the ""auto"" strategy, but the ""greedy"" and ""optimal"" strategies are
    also supported. Note that the ""optimal"" strategy is factorial on the number of inputs as it tries all
    possible paths. See more details in opt_einsum's docs
    (https://optimized-einsum.readthedocs.io/en/stable/path_finding.html).
```

In trying (and failing) to land 85908, I discovered that jit script does NOT actually pull from python's version of einsum (because it cannot support variadic args nor kwargs). Thus I learned that jitted einsum does not subscribe to the new opt_einsum path calculation. Overall, this is fine since jit script is getting deprecated, but where is the best place to document this?

## Test plan:
- added tests to CI
- locally tested that trying to set the strategy to something invalid will error properly
- locally tested that tests will pass even if you don't have opt-einsum
- locally tested that setting the strategy when opt-einsum is not there will also error properly
Pull Request resolved: https://github.com/pytorch/pytorch/pull/86219
Approved by: https://github.com/soulitzer, https://github.com/malfet"
pytorch/pytorch,1432b9978b9e3838a7940700fb54f89b63fc72e5,"Add ref for cumsum (#86229)

As noted in the comment, this decomposition may not be as efficient as
specific implementations of it in different backends. Added here to then
benchmark it. Note that this is needed by TorchInductor https://github.com/pytorch/torchdynamo/issues/883
Pull Request resolved: https://github.com/pytorch/pytorch/pull/86229
Approved by: https://github.com/ngimel"
pytorch/pytorch,75c0e3a471c19b883feca15fd4ecfabedf746691,"[MPS] Improve memory usage and performance utilizing garbage collector and adaptive commit (#86119)

- Improve memory usage and performance utilizing garbage collector and adaptive commit
- Enable low watermark limit to detect memory pressure.
- Enable garbage collection and adaptive commit strategies when under memory pressure.
- More efficient resource management by splitting large heaps (instead of reusing oversized buffers for smaller allocation requests)
- Introduce Extra Large heaps to improve performance by avoiding numerous costly allocation of smaller heaps
- Fix purgeability when releasing the Metal heaps
- Fix the race condition when deferring the heap's size update

Fixes #79283

Pull Request resolved: https://github.com/pytorch/pytorch/pull/86119
Approved by: https://github.com/kulinseth, https://github.com/malfet"
pytorch/pytorch,2067b768fc8ffa181d6e9dc9d62e1696e9cf4ef8,"[FSDP] Delay moving tensor to CPU until necessary for optim_state_dict() (#85761)

Optimizer state_dict currently move tensors to CPU() immediately after allgather(). However, for sharded optimizer state_dict, this moving is duplicated. We should wait until all the sharding are done. This PR may slightly reduce the performance of full optimizer state_dict as it has to allocate more memory than w/o this PR. But the benchmark shows the memory allocation is pretty light.

Differential Revision: [D39855912](https://our.internmc.facebook.com/intern/diff/D39855912/)

**NOTE FOR REVIEWERS**: This PR has internal Meta-specific changes or comments, please review them on [Phabricator](https://our.internmc.facebook.com/intern/diff/D39855912/)!
Pull Request resolved: https://github.com/pytorch/pytorch/pull/85761
Approved by: https://github.com/rohan-varma"
pytorch/pytorch,463283e016ffa7d8a0da35a1d28c8b8ab0db2ea7,"[c10d] Start deprecating *_coalesced APIs (#85959)

- We consider that general users need not to use the `*_coalesced` APIs unless there is an extreme concern about performance.

- We are investigating using a context manager named `coalescing_manager` which wrap around multiple individual collectives to compose the coalescing hint, rather than giving each collective a *_coalesced variant.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/85959
Approved by: https://github.com/XilunWu, https://github.com/H-Huang"
pytorch/pytorch,24fc680ee4228225c01fb6699210056ca2603a3f,"[Quant] Enable XNNPACK ops in QNNPACK BackendConfig (#85863)

**Summary:** This commit enforces the following constraints on the
QNNPACK BackendConfig:

- `quant_min_lower_bound` = -127 for qint8 weight
- `quant_max_upper_bound` = 127 for qint8 weight
- `scale_min_lower_bound` = 2 ** -12 for qint8 activations and weight

These constraints will enable users to use this BackendConfig with
faster XNNPACK quantized ops. They are also consistent with the
existing settings in `default_symmetric_qnnpack_qconfig` and its
per_channel and QAT variants. For more detail on why these exact
values were chosen, please see the description of
https://github.com/pytorch/pytorch/pull/74396.

Note that there are currently no restrictions on the qscheme in
DTypeConfig. This should be added in the future to further enforce
the restriction that the weights must be quantized with either
per_tensor_symmetric or per_channel_symmetric.

Existing default QConfigs such as `get_default_qconfig(""qnnpack"")`
and `get_default_qat_qconfig(""qnnpack"")` will continue to be
supported, but only for the existing dtypes, e.g. quint8 activations
for weighted ops like linear and conv. In the future, we should
revisit whether to enable XNNPACK ops using these QConfigs as well.

**Test Plan:**

python test/test_quantization.py TestQuantizeFx.test_qnnpack_backend_config

**Reviewers:** jerryzh168, vkuzo

**Subscribers:** jerryzh168, vkuzo
Pull Request resolved: https://github.com/pytorch/pytorch/pull/85863
Approved by: https://github.com/jerryzh168"
pytorch/pytorch,1c97084685f19435759f785d33fde7ea3a61afa7,"[BE] Generate names of known device from array (#85982)

Rather than hardcoding list of device names, generate it from list of known types.
Performance is not important at the error codepath, as it will not be evaluated during normal codepath.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/85982
Approved by: https://github.com/kit1980"
pytorch/pytorch,71eb04403ca46e19a3efcde454cedbc2f990dc12,"Revert ""[CUBLAS][CUDA GRAPHS] (re-re-open of #83461) Explicitly set the workspace for cuBLAS handles (#85447)""

This reverts commit b04b2fa9aa52cacbdc9aaaf477d55b0af845ce81.

Reverted https://github.com/pytorch/pytorch/pull/85447 on behalf of https://github.com/seemethere due to Caused a CUDA memory leak, detected by our performance benchmark suite"
pytorch/pytorch,4c04fa9587fb534fa7c9848e06141bb862a56bb4,"Remove `optim_mt` from `test/test_optim.py` (#83549)

As per title, this updates `test_optim.py` so that `foreach` optimizers are constructed using the `foreach` keyword argument of `torch.optim` optimizers.

Also, this makes some cosmetic changes to remove `torch.autograd.Variable`, `.data` calls, and `torch._six`.

Related: https://github.com/pytorch/pytorch/pull/81705#discussion_r939440776

Pull Request resolved: https://github.com/pytorch/pytorch/pull/83549
Approved by: https://github.com/ngimel"
pytorch/pytorch,6db3539e700ce7a81be356700f0803b2002bc63c,"Revert ""Improve make_tensor performance for float and complex types (#85473)""

This reverts commit a76995e584b880910f0724be98eb21773e8ed6e9.

Reverted https://github.com/pytorch/pytorch/pull/85473 on behalf of https://github.com/huydhn due to Sorry for revert your PR, but it seems to cause a bunch of flaky test in pull an periodic"
pytorch/pytorch,a76995e584b880910f0724be98eb21773e8ed6e9,"Improve make_tensor performance for float and complex types (#85473)

For floating types, `make_tensor` calls `rand` and then does a linear
interpolation from `low` to `high`. This instead calls `uniform_(low,
high)` to cut out the interpolation step.

For complex types, `make_tensor` does the `rand` + interpolation step
twice and calls `torch.complex(real, imag)` at the end. This instead
uses `view_as_real` and `uniform_(low, high)` to fuse it all into one
operation.

My benchmarks show significant speedups in all cases for float32 and
complex64.

| Device | dtype     | Size  | Master (us) | This PR (us) | Speedup |
|--------|-----------|-------|-------------|--------------|---------|
| CPU    | float32   | 8     | 19.4        | 6.34         | 3.1     |
|        |           | 4096  | 36.8        | 21.3         | 1.7     |
|        |           | 2**24 | 167,000     | 80,500       | 2.1     |
|        | complex32 | 8     | 37.0        | 7.57         | 4.9     |
|        |           | 4096  | 73.1        | 37.6         | 1.9     |
|        |           | 2**24 | 409,000     | 161,000      | 2.5     |
| CUDA   | float32   | 8     | 40.4        | 11.7         | 3.5     |
|        |           | 4096  | 38.7        | 11.7         | 3.3     |
|        |           | 2**24 | 2,300       | 238          | 9.7     |
|        | complex32 | 8     | 78.7        | 14           | 5.6     |
|        |           | 4096  | 82.7        | 13.8         | 6.0     |
|        |           | 2**24 | 5,520       | 489          | 11.3    |
Pull Request resolved: https://github.com/pytorch/pytorch/pull/85473
Approved by: https://github.com/mruberry"
pytorch/pytorch,ad87365e54e7b20b49ac23ee325f1da732655808,"[qat]A more stable conv_bn fusion for qat training. (#85744)

Summary:
A more stable conv_bn fusion for qat training:
* Existing implementation may cause QAT training loss become NaN. This could happen when the fused conv for qat (torch/nn/intrinsic/qat/modules/conv_fused.py) is used and is independent of if fake_quant is enabled.
  * This is caused by the unscaling for the conv output (`conv_orig = conv / scale_factor` where `scale_factor = bn.weight / running_std`) when there is 0 in `bn.weight`.

* This implementation follows the [white paper](https://arxiv.org/pdf/1806.08342.pdf) better and fixed the issue by scaling `running_std / std_Y` instead and compute the fused output accordingly (see comments in conv_fused.py for more details):
  * It comes at the cost of running conv twice (one to update bn statistics and one to compute fake quant for fused weights).
  * It does not need to use conv bias for back prop.
  * It uses the bn statistics computed with the current input batch, while the existing code uses the statistics without the current batch.
* The implementation could be enabled by setting the flag `_enable_slow_path_for_better_numerical_stability` to True after the model is prepared for QAT.

* Unit test
  * Added test case for zero `bn.weight`.
  * Added test case for conv to has bias.

Test Plan: buck run mode/dev-nosan //caffe2/test:quantization -- -r quantization.eager.test_quantize_eager_qat

Differential Revision: D29506778

Pull Request resolved: https://github.com/pytorch/pytorch/pull/85744
Approved by: https://github.com/vkuzo"
pytorch/pytorch,3cfc61b84659cea435411a546eca6a891584247f,"[Profiler][trivial] Optimizer states (part 4 of Record Optimizer) (#85840)

Summary: - add states into OptInfo and update unit testcase

Test Plan: buck run mode/opt //caffe2/test:profiler

Reviewed By: chaekit

Differential Revision: D39406540

Pull Request resolved: https://github.com/pytorch/pytorch/pull/85840
Approved by: https://github.com/robieta"
pytorch/pytorch,dfdfaec3fc599c2bb7a8ffaf1215e0284a2f4aa8,"[Profiler] Don't assign in AppendOnlyList::emplace_back (#85716)

It turns out that we're invoking the copy assign operator in AppendOnlyList. While copy elision is expected to mostly hide any costs it does present issues for types with deleted copy assign operators. (It also seems to produce slightly worse assembly: https://godbolt.org/z/o4Gvz1fKs)

Calling new at the correct position seems to be a better way to go about this. (At least from looking at other high performance containers like SmallVector.)

Differential Revision: [D39852804](https://our.internmc.facebook.com/intern/diff/D39852804/)
Pull Request resolved: https://github.com/pytorch/pytorch/pull/85716
Approved by: https://github.com/chaekit"
pytorch/pytorch,3a3e2002d88ca2491170065c47cc50ce435fb92f,"[Quant] Add unified x86 quant backend (#84329)

## Description

Implement unified quantization backend 'X86' for x86 platforms. It combines the advantages of FBGEMM and ONEDNN. It selects kernels during weight prepacking and hide the details from end users. It will be the default backend in place of FBGEMM.

For details, please refer to this RFC: [[RFC] Unified quantization backend for x86 CPU platforms](https://github.com/pytorch/pytorch/issues/83888)

## Validation
**Correctness**
Covered by UT

**Accuracy**
By running torchvision models on imagenet, no accuracy difference is found between FBGEMM and the unified X86 backend:
[torchvision_accuracy_comparison_fbgemm_vs_x86.xlsx](https://github.com/pytorch/pytorch/files/9598114/torchvision_accuracy_comparison_fbgemm_vs_x86.xlsx)

**Performance**
Depends on https://github.com/pytorch/pytorch/pull/84470 which improves performance.
For early PoC results, please refer to https://github.com/pytorch/pytorch/files/9399202/unified_qengine_poc_performance_bechmark.xlsx

With the two PRs combined, we collected some data on Intel(R) Xeon(R) Platinum 8358 CPU @ 2.60GHz
Method: Run multi-instances with 4 cores per instance on whole socket. Using JeMalloc and Intel OMP.
Models/throughput | fbgemm | x86 | improvement
-- | -- | -- | --
wide_resnet101_2 | 173.5675 | 241.815 | 39.32%
resnext101_32x8d | 174.365 | 339.8175 | 94.89%
resnet50 | 573.155 | 1174.14 | 104.86%
vgg19_bn | 260.335 | 337.92 | 29.80%
vgg19 | 257.935 | 333.265 | 29.21%
inception_v3 | 601.1175 | 1309.33 | 117.82%
densenet161 | 296.645 | 435.5625 | 46.83%
mnasnet1_0 | 1216.7 | 4057.515 | 233.49%
squeezenet1_0 | 1220.085 | 5153.3875 | 322.38%
alexnet | 2294.91 | 2624.6375 | 14.37%
fbnetc_100 | 976.2825 | 3110.1825 | 218.57%
shufflenet_v2_x0_5 | 1555.76 | 3026.125 | 94.51%
spnasnet_100 | 1059.065 | 3502.0975 | 230.68%
pytorch-unet | 192.76 | 246.77 | 28.02%
acgan | 257.32 | 333.7325 | 29.70%
cgan | 7790.6925 | 7803.1025 | 0.16%
sgan | 257.565 | 338.8875 | 31.57%
se_resnet50 | 492.3725 | 916.5175 | 86.14%
vggm | 300.2875 | 316.2075 | 5.30%

Environment:
- PyTorch version: 1.13.0a0+gitcdd625b
- Is debug build: False
- CUDA used to build PyTorch: None
- ROCM used to build PyTorch: N/A
- OS: Ubuntu 20.04.3 LTS (x86_64)
- GCC version: (Ubuntu 9.3.0-17ubuntu1~20.04) 9.3.0
- Clang version: Could not collect
- CMake version: version 3.22.5
- Libc version: glibc-2.31
- Python version: 3.9.12 (main, Jun  1 2022, 11:38:51)  [GCC 7.5.0] (64-bit runtime)
- Python platform: Linux-5.11.0-27-generic-x86_64-with-glibc2.31
- Is CUDA available: False
- CUDA runtime version: No CUDA
- GPU models and configuration: No CUDA
- Nvidia driver version: No CUDA
- cuDNN version: No CUDA
- HIP runtime version: N/A
- MIOpen runtime version: N/A
- Is XNNPACK available: True

Versions of relevant libraries:
- [pip3] intel-extension-for-pytorch==1.13.0+cpu
- [pip3] numpy==1.23.3
- [pip3] pytorch-widedeep==0.3.7
- [pip3] torch==1.13.0a0+git48b423b
- [pip3] torchvision==0.14.0a0+ebb68f3
- [conda] blas                      1.0                         mkl
- [conda] intel-extension-for-pytorch 1.13.0+cpu               pypi_0    pypi
- [conda] mkl                       2021.4.0           h06a4308_640
- [conda] mkl-include               2022.1.0                 pypi_0    pypi
- [conda] mkl-service               2.4.0            py39h7f8727e_0
- [conda] mkl-static                2022.1.0                 pypi_0    pypi
- [conda] mkl_fft                   1.3.1            py39hd3c417c_0
- [conda] mkl_random                1.2.2            py39h51133e4_0
- [conda] numpy                     1.23.3                   pypi_0    pypi
- [conda] numpy-base                1.22.3           py39hf524024_0
- [conda] torch                     1.13.0a0+git48b423b          pypi_0    pypi
- [conda] torchvision               0.14.0a0+ebb68f3          pypi_0    pypi

Pull Request resolved: https://github.com/pytorch/pytorch/pull/84329
Approved by: https://github.com/jerryzh168"
pytorch/pytorch,d776693701bef4283858961a4c597edd73d1fc6d,"[Profiler] Optimizer param_groups (part 3 of Record Optimizer) (#85784)

Summary:
- use TensorMetadata struct
- check_and_store util as overloading
- param_groups
- clean up unit test cases

Test Plan: buck run mode/opt //caffe2/test:profiler

Reviewed By: chaekit

Differential Revision: D39406072

Pull Request resolved: https://github.com/pytorch/pytorch/pull/85784
Approved by: https://github.com/aaronenyeshi, https://github.com/robieta"
pytorch/pytorch,b2311192e6c4745aac3fdd774ac9d56a36b396d4,"[NN module] speed up _load_from_state_dict (#85743)

Fixes #61398

The original implementation is very slow when the state_dict.keys() is long. This PR only passes relevant keys to the child module.

existing test passes: `pytest test/test_nn.py -k state_dict`
I couldn't figure out a good way to write a new test for this new behavior. Had a new snippet, but it will be flaky if integrated into the main CI because it's a timing based check.
But I can verify that the test took 30s to run, after this PR it only takes 0.5s.

```python
    def test_load_state_dict_large(self):
        # construct a module with 4 levels of module, 10 linear each, leads to 10k items in the dictionary
        import copy
        import time
        base_module = nn.Linear(1,1)
        model = base_module
        for level in range(4):
           model = nn.Sequential(*[copy.deepcopy(model) for _ in range(10)])
        state_dict = model.state_dict()
        self.assertEqual(len(state_dict), 20000)
        st = time.time()
        model.load_state_dict(state_dict, strict=True)
        strict_load_time = time.time() - st
        # it took 0.5 seconds to
        self.assertLess(strict_load_time, 10)
```
Pull Request resolved: https://github.com/pytorch/pytorch/pull/85743
Approved by: https://github.com/albanD"
pytorch/pytorch,f80ef73d1c0da4938a264a1ac1c903c78ee3fc6a,"[Profiler] tracking Optimizer (part 2 of Record Optimizer) (#84920)

Summary:
Part 2 of Record Optimizer param_groups and states (https://github.com/pytorch/pytorch/pull/84063)
- hooking from optimizer step
- PyOptCall Type
- declare data type for collection
- python binding
- simple unit test case

Test Plan: buck run mode/opt //caffe2/test:profiler

Differential Revision: D39402667

Pull Request resolved: https://github.com/pytorch/pytorch/pull/84920
Approved by: https://github.com/robieta"
pytorch/pytorch,6cfe555f4fe54c8df5a07af39a335d0f4d914d95,"[ONNX] Apply Common Subexpression Elimination pass to ONNX export (#85665)

## Summary
Exporting graphs with Autocast may fail due to a limitation on JIT tracer. By disabling Autocast cache, tracer works, but there can be performance hit when there is reuse of weights in convolution, for example

By applying CSE, such performance loss can be reverted.

ps: As a comment at #84092 mentioned, disabling Autocast cache is an acceptable workaround and used throughout PyTorch code.

Fixes #84092

## Examples of before and after CSE being applied:

### Example: eliminating `%17` and reusing `%16` instead

```python
# BEFORE
graph(%0 : Float(requires_grad=0, device=cpu)):
  %3 : Scalar = aten::ScalarImplicit(%0), scope: test_onnx_opset.TestONNXOpset.test_full.<locals>.MyModule::
  %13 : int = prim::Constant[value=3](), scope: test_onnx_opset.TestONNXOpset.test_full.<locals>.MyModule:: # /home/thiagofc/dev/github/pytorch/test/onnx/test_onnx_opset.py:347:0
  %14 : int = prim::Constant[value=4](), scope: test_onnx_opset.TestONNXOpset.test_full.<locals>.MyModule:: # /home/thiagofc/dev/github/pytorch/test/onnx/test_onnx_opset.py:347:0
  %15 : int[] = prim::ListConstruct(%13, %14), scope: test_onnx_opset.TestONNXOpset.test_full.<locals>.MyModule::
  %16 : NoneType = prim::Constant(), scope: test_onnx_opset.TestONNXOpset.test_full.<locals>.MyModule::
  %17 : NoneType = prim::Constant(), scope: test_onnx_opset.TestONNXOpset.test_full.<locals>.MyModule::
  %18 : Device = prim::Constant[value=""cpu""](), scope: test_onnx_opset.TestONNXOpset.test_full.<locals>.MyModule:: # /home/thiagofc/dev/github/pytorch/test/onnx/test_onnx_opset.py:347:0
  %19 : bool = prim::Constant[value=0](), scope: test_onnx_opset.TestONNXOpset.test_full.<locals>.MyModule:: # /home/thiagofc/dev/github/pytorch/test/onnx/test_onnx_opset.py:347:0
  %20 : Float(3, 4, strides=[4, 1], requires_grad=0, device=cpu) = aten::full(%15, %3, %16, %17, %18, %19), scope: test_onnx_opset.TestONNXOpset.test_full.<locals>.MyModule:: # /home/thiagofc/dev/github/pytorch/test/onnx/test_onnx_opset.py:347:0
  return (%20)

AFTER
graph(%0 : Float(requires_grad=0, device=cpu)):
  %3 : Scalar = aten::ScalarImplicit(%0), scope: test_onnx_opset.TestONNXOpset.test_full.<locals>.MyModule::
  %13 : int = prim::Constant[value=3](), scope: test_onnx_opset.TestONNXOpset.test_full.<locals>.MyModule:: # /home/thiagofc/dev/github/pytorch/test/onnx/test_onnx_opset.py:347:0
  %14 : int = prim::Constant[value=4](), scope: test_onnx_opset.TestONNXOpset.test_full.<locals>.MyModule:: # /home/thiagofc/dev/github/pytorch/test/onnx/test_onnx_opset.py:347:0
  %15 : int[] = prim::ListConstruct(%13, %14), scope: test_onnx_opset.TestONNXOpset.test_full.<locals>.MyModule::
  %16 : NoneType = prim::Constant(), scope: test_onnx_opset.TestONNXOpset.test_full.<locals>.MyModule::
  %18 : Device = prim::Constant[value=""cpu""](), scope: test_onnx_opset.TestONNXOpset.test_full.<locals>.MyModule:: # /home/thiagofc/dev/github/pytorch/test/onnx/test_onnx_opset.py:347:0
  %19 : bool = prim::Constant[value=0](), scope: test_onnx_opset.TestONNXOpset.test_full.<locals>.MyModule:: # /home/thiagofc/dev/github/pytorch/test/onnx/test_onnx_opset.py:347:0
  %20 : Float(3, 4, strides=[4, 1], requires_grad=0, device=cpu) = aten::full(%15, %3, %16, %16, %18, %19), scope: test_onnx_opset.TestONNXOpset.test_full.<locals>.MyModule:: # /home/thiagofc/dev/github/pytorch/test/onnx/test_onnx_opset.py:347:0
  return (%20)
```
Pull Request resolved: https://github.com/pytorch/pytorch/pull/85665
Approved by: https://github.com/ngimel, https://github.com/AllenTiTaiWang, https://github.com/BowenBao"
pytorch/pytorch,9f1468ae6c8e6e3938a3f1cfb9378e11af2fd0cd,"CyclicLR memory leak fix (#85462)

Hi, we noticed in our team that by using CyclicLR, there is a problem with memory clearance on GPU (probably it will be the case without the GPU as well, but that was our use case) After initializing CyclicLR, GPU memory is not cleared even after the model, optimizer and scheduler are out of scope (e.g. reference count is zero). This is because `__init__` method inside `CyclicLR` creates reference to its own methods and it will not get removed until `gc.collect()` is called manually. This is a problem if people want to test multiple models in one run of a script, after testing the first model, second one will fail on `CUDA out of memory error` because the first one is not cleared from the memory.

I propose a simple fix by using `weakref`, similarly as in `_LRScheduler` base class, but if you have any comments I am happy to change it.

Here is the code to reproduce the bug:

```
import torch
import weakref
from transformers import DetrForObjectDetection

class X:
    def __init__(self, optimizer):
        self.optimizer = optimizer

        # Will cause cyclic reference.
        self.func = self.dummy

        # Will work as expected, memory cleared after instance count is zero.
        # self.func = weakref.WeakMethod(self.dummy)

    def dummy(self, x):
        return 1.

def test():
    model = DetrForObjectDetection.from_pretrained('facebook/detr-resnet-50')
    model.to('cuda')
    optimizer = torch.optim.Adam(model.parameters())
    x = X(optimizer)

test()
print(f'{torch.cuda.memory_reserved()}, {torch.cuda.memory_allocated()}')  # Should print (<some memory>, 0), but with cyclic reference, it will print (<some memory>, <some memory>).
```
Pull Request resolved: https://github.com/pytorch/pytorch/pull/85462
Approved by: https://github.com/albanD"
pytorch/pytorch,45be74cc63aad66f496475a9513e3cc36cace5b6,"Optimize to if the datatyep of the source tensor is as same as the dest datatype (#85140)

The AMP inserts `_autocast_to_reduced_precision` and `_autocast_to_full_precision` automatically. The aten implementation provides a fast path to bypass the conversion if the tensor data type has been the reduced/full precision. But NNC always does the conversion which could bring >5% E2E performance regression.

This PR is to address the performance issue like aten. We will not pull `_autocast_to_reduced_precision` and `_autocast_to_full_precision` into NNC fusion group and fallback to aten to trigger its fast path if the tensor data type has been the reduced/full precision.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/85140
Approved by: https://github.com/frank-wei"
pytorch/pytorch,755b39ba6668d2ce8c0e3ba4ee032ff0dfc827b7,"[LRD] Allowing using dedicated iteration counter for learning rate (#85195)

Summary: So that we could manipulate the iteration counter for lrarning rate separately (for learning rate decay or learning rate re-warming up etc), without affecting other techniques relying on iterations (such as EMA)

Test Plan:
Unit tests:
```
    ✓ Pass: caffe2/caffe2/python:optimizer_test - testSparse (caffe2.caffe2.python.optimizer_test.TestAdagradWithDedicatedLRIteration) (46.475)
    ✓ Pass: caffe2/caffe2/python:optimizer_test - test_global_norm_based_gradient_clipping (caffe2.caffe2.python.optimizer_test.TestAdagradWithDedicatedLRIteration) (46.475)
    ✓ Pass: caffe2/caffe2/python:optimizer_test - test_lr_injection (caffe2.caffe2.python.optimizer_test.TestAdagradWithDedicatedLRIteration) (46.475)
    ✓ Pass: caffe2/caffe2/python:optimizer_test - main (46.475)
Summary
  Pass: 5
  Skip: 1
    ↻ caffe2/caffe2/python:optimizer_test - testGPUDense (caffe2.caffe2.python.optimizer_test.TestAdagradWithDedicatedLRIteration)
  ListingSuccess: 1
```

Reviewed By: liangming168

Differential Revision: D38747417

Pull Request resolved: https://github.com/pytorch/pytorch/pull/85195
Approved by: https://github.com/liangming168, https://github.com/eellison"
pytorch/pytorch,d5ce2bbed26a175e7bf69480759c2cfe73f42a75,"[primTorch] decompositions for upsample_bicubic2d (#85403)

FYI, this decomposition seems to be significantly slower than the lowering in torchinductor:

```
------------------------------------- upsample_bicubic2d -------------------------------------]
                                                              |  lowering  |  Inductor  |  Eager
32 threads: ------------------------------------------------------------------------------------
      (torch.Size([16, 4, 128, 256]),), ((512, 1024), True)   |    1.8     |   3.880    |   1.4
      (torch.Size([16, 4, 128, 256]),), ((512, 1024), False)  |    1.9     |   3.887    |   1.4
```

This seems related to the fact that in the lowering we can use int32s as the indices and in the decomp we can only use int64s (see https://github.com/pytorch/torchdynamo/issues/1293).

Pull Request resolved: https://github.com/pytorch/pytorch/pull/85403
Approved by: https://github.com/ngimel"
pytorch/pytorch,a8ca0d4849111fd6c83e5ae90517f83bf1ceb6b3,"fix segmentation fault in QTensor.choose_qparams_optimized (#85552)

Summary:

Fixes segmentation fault in `QTensor.choose_qparams_optimized`, this
guards against the user passing in a value of `numel` which does not
make sense.

Fixes https://github.com/pytorch/pytorch/issues/85212

Test plan:

Probably not worth it to add a test for this, so testing manually.

```
import torch

input = torch.full((64,), 1, dtype=torch.float32, requires_grad=False)
numel = 1250999896764
n_bins = 0
ratio = 0
bit_width = 0
torch.choose_qparams_optimized(input, numel, n_bins, ratio, bit_width)
// RuntimeError is thrown
```
Pull Request resolved: https://github.com/pytorch/pytorch/pull/85552
Approved by: https://github.com/jerryzh168"
pytorch/pytorch,e7e1cd945fe218fde228cedbdb1509f1750f70ea,"Add path optimize kwarg to einsum (#84890)

## This PR seeks to:
- [x] add c++ support for an optimize path
- [x] add python opt_einsum path passthrough
- [x] add opt_einsum to OSS requirements, but a soft one
- [x] show benchmark results here

Additional things I've explored + their conclusions:
- **Delaying the summing over dimensions** => added!
    - The idea here is to not incur kernel calls to `sum` as we try to early sum out in einsum. Thus, we collect all the dimensions that need to be summed together in one contraction + sum at the end instead of summing as we go. While this optimization didn't feel like it made things faster for the random cases we've selected (they all summed 1 dim per contraction), it is a good principle and would help more common use cases that would reduce multiple dimensions at a time (like `bxy,xyi,xyj->bij`).
- **Caching contract_path based on equation and tensor sizes** => dropped :(
    - The benchmarks were strictly worse for all the cases, and, from scanning the use cases, I observed people do not often call einsum on the same equation/tensor order enough for caching to be justified. I do think caching can be effective in the future, but it would require further investigation.

## Not a part of this PR (but are next steps):
- adding opt_einsum package to OSS CI
- adding it to internal CI
- potentially adding a kwarg path argument to the python API -- if the path is given, we wouldn't have to spend time calculating it, but there would be some time lost validating user input.

## Testing:
- Added more tests to CI

## Benchmarking:
**TL;DRs**
- **torch.einsum with opt_einsum is a definite win for the production case**.
- **torch.einsum with opt_einsum installed is consistently fast, but has an overhead** of needing to find the path. If the path is already found/optimal, it will be slightly slower.
- The einsum overhead decreases for bigger dimensions.
- **torch.einsum without opt_einsum installed is comparable to before this commit**, with occasional slowness potentially due to not reshaping/squeezing as we contract until the end.
- For many of the random generated cases, the dimensions were too similar and small where an optimal order wasn't that much more optimal than just going left to right. However, in production, dimensions are commonly quite distinct (batch size will be small, but the data will be huge).
- **torch.einsum opt is comparable (slightly faster overall) compared to numpy.einsum opt for the cpu case**. This is interesting given that torch.einsum currently spends time computing the path, but numpy.einsum takes it as input.
- **torch.einsum opt is significantly faster than numpy.einsum opt for the gpu case**. This is because numpy doesn't take advantage of GPUs.

The following benchmarks were done on an A100 GPU and Linux CPUs. The line in the first chart separates GPU (on top) from CPU, and the line in the second graph separates CPU (on top) and then GPU. Sorry it's flipped 😛 .

Production example (see [colab benchmark](https://colab.research.google.com/drive/1V2s4v1dOOKwRvp5T_DC-PNUosOV9FFJx?authuser=1#scrollTo=WZoQkC8Mdt6I) for more context):
<img width=""1176"" alt=""image"" src=""https://user-images.githubusercontent.com/31798555/192012636-9a68bfa7-2601-43b1-afeb-b4e0877db6a4.png"">

Randomly generated examples (the same ones as in https://github.com/pytorch/pytorch/pull/60191)
<img width=""1176"" alt=""image"" src=""https://user-images.githubusercontent.com/31798555/192012804-1c639595-b3e6-48c9-a385-ad851c13e1c2.png"">

Open below to see old + not super relevant benchmarking results:
<details>
Benchmark results BEFORE this PR (on Linux -- I will update devices so they are consistent later):
<img width=""776"" alt=""image"" src=""https://user-images.githubusercontent.com/31798555/190807274-18f71fce-556e-47f4-b18c-e0f7d0c0d5aa.png"">

Benchmark results with the code on this PR (on my x86 mac):
For the CPU internal use case --
![image](https://user-images.githubusercontent.com/31798555/190801376-6f591b00-cebd-4ca7-bb23-ae8f17f1634e.png)

For the general use case --
It looks like numpy opt still does better in several of these random cases, but torch einsum opt is consistently faster than torch.einsum.
![image](https://user-images.githubusercontent.com/31798555/190811730-fbb6797d-af59-4f5a-92da-ba4103372014.png)
<details>

Pull Request resolved: https://github.com/pytorch/pytorch/pull/84890
Approved by: https://github.com/albanD, https://github.com/soulitzer"
pytorch/pytorch,12ae3bea437e760d4fede3f1c50c2c81af3f687c,"Faster mul(sparse, sparse) with broadcasting in dense dims. (#85336)

This is a combo PR of https://github.com/pytorch/pytorch/pull/84929 and ~https://github.com/pytorch/pytorch/pull/83428~.

Preliminary benchmarks (square matrices of shape (n, n)).

<details>

<summary>Script</summary>

```python
import torch
import math
from IPython import get_ipython
from itertools import product, repeat
import pickle
from torch.utils.benchmark import Timer, Compare

torch.manual_seed(13)

problem_dims = (
    # n > nnz
    (10000, 100),
    (100000, 1000),
    (1000000, 10000),
    # n < nnz
    (10, 100),
    (10, 1000),
    (10, 10000),
    (100, 1000),
    (100, 10000),
    (1000, 10000),
    (1000, 100000),
    (1000, 1000000),
    #(1000000, 1000000000),
)

name = ""PR""
device = ""cuda""
results = []

for n, nnz in problem_dims:
    def gen_tensor(coalesce=False):
        shape = (n, n)
        nrows, ncols = shape
        rowidx = torch.randint(low=0, high=nrows, size=(nnz,), device=device)
        colidx = torch.randint(low=0, high=ncols, size=(nnz,), device=device)
        itemidx = torch.vstack((rowidx, colidx))
        xvalues = torch.randn(nnz, device=device)
        itemidx = torch.hstack((itemidx, itemidx))
        xvalues = torch.hstack((xvalues, xvalues))
        res = torch.sparse_coo_tensor(itemidx, xvalues, size=shape)
        if coalesce:
            return res.coalesce()
        else:
            return res

    for x_coalesce, y_coalesce in product(*repeat((True, False), 2)):
        x = gen_tensor(x_coalesce)
        y = gen_tensor(y_coalesce)
        smtp = ""x * y""
        timer = Timer(smtp,
                      globals=globals(),
                      label=""coo.mul"",
                      description=f""{name}: mul, device: {device}"",
                      sub_label=f""n={n}, nnz={nnz}, coalesce=({x_coalesce, y_coalesce})"",
                      num_threads=torch.get_num_threads())
        results.append(timer.blocked_autorange())

compare = Compare(results)
compare.trim_significant_figures()
compare.print()

with open(f""{name}_{device}_mul.pickle"", 'wb') as f:
    pickle.dump(results, f)

```

</details>

<details>

<summary>Gather results</summary>

```python
import pickle
from torch.utils.benchmark import Timer, Compare

files = [
        ""PR"",
        ""master""
        ]

device = 'cuda'

timers = []
for name in files:
    with open(""{}_{}_mul.pickle"".format(name, device), 'rb') as f:
        timers += pickle.load(f)

compare = Compare(timers)
compare.trim_significant_figures()
compare.print()

```

</details>

<details>

<summary>CUDA</summary>

```
[------------------------------------------------- coo.mul -------------------------------------------------]
                                                       |  PR: mul, device: cuda  |  master: mul, device: cuda
24 threads: -------------------------------------------------------------------------------------------------
      n=10000, nnz=100, coalesce=((True, True))        |             95          |                91
      n=10000, nnz=100, coalesce=((True, False))       |             87          |               242
      n=10000, nnz=100, coalesce=((False, True))       |             87          |               226
      n=10000, nnz=100, coalesce=((False, False))      |            130          |               371
      n=100000, nnz=1000, coalesce=((True, True))      |            100          |               521
      n=100000, nnz=1000, coalesce=((True, False))     |             90          |               649
      n=100000, nnz=1000, coalesce=((False, True))     |            100          |               659
      n=100000, nnz=1000, coalesce=((False, False))    |            200          |               781
      n=1000000, nnz=10000, coalesce=((True, True))    |            100          |              4861
      n=1000000, nnz=10000, coalesce=((True, False))   |            100          |              5012
      n=1000000, nnz=10000, coalesce=((False, True))   |             98          |              5010
      n=1000000, nnz=10000, coalesce=((False, False))  |            384          |              5174
      n=10, nnz=100, coalesce=((True, True))           |            100          |                79
      n=10, nnz=100, coalesce=((True, False))          |            100          |               221
      n=10, nnz=100, coalesce=((False, True))          |            100          |               221
      n=10, nnz=100, coalesce=((False, False))         |            100          |               350
      n=10, nnz=1000, coalesce=((True, True))          |            100          |               100
      n=10, nnz=1000, coalesce=((True, False))         |            100          |               240
      n=10, nnz=1000, coalesce=((False, True))         |            100          |               254
      n=10, nnz=1000, coalesce=((False, False))        |            100          |               392
      n=10, nnz=10000, coalesce=((True, True))         |            100          |               110
      n=10, nnz=10000, coalesce=((True, False))        |            110          |               286
      n=10, nnz=10000, coalesce=((False, True))        |            110          |               286
      n=10, nnz=10000, coalesce=((False, False))       |            271          |               455
      n=100, nnz=1000, coalesce=((True, True))         |            110          |               851
      n=100, nnz=1000, coalesce=((True, False))        |            110          |              1000
      n=100, nnz=1000, coalesce=((False, True))        |            110          |               990
      n=100, nnz=1000, coalesce=((False, False))       |            140          |              1124
      n=100, nnz=10000, coalesce=((True, True))        |            110          |              5137
      n=100, nnz=10000, coalesce=((True, False))       |            110          |              5391
      n=100, nnz=10000, coalesce=((False, True))       |            100          |              5405
      n=100, nnz=10000, coalesce=((False, False))      |            249          |              5539
      n=1000, nnz=10000, coalesce=((True, True))       |            100          |              8598
      n=1000, nnz=10000, coalesce=((True, False))      |            100          |              8800
      n=1000, nnz=10000, coalesce=((False, True))      |            100          |              8782
      n=1000, nnz=10000, coalesce=((False, False))     |            255          |              8956
      n=1000, nnz=100000, coalesce=((True, True))      |            120          |             84500
      n=1000, nnz=100000, coalesce=((True, False))     |            200          |             88560
      n=1000, nnz=100000, coalesce=((False, True))     |            160          |             89000
      n=1000, nnz=100000, coalesce=((False, False))    |            373          |             89000
      n=1000, nnz=1000000, coalesce=((True, True))     |            312          |            606400
      n=1000, nnz=1000000, coalesce=((True, False))    |           1340          |            609200
      n=1000, nnz=1000000, coalesce=((False, True))    |           1340          |            609100
      n=1000, nnz=1000000, coalesce=((False, False))   |           4408          |            611400

Times are in microseconds (us).
```

</details>

<details>

<summary>CPU</summary>

```
[------------------------------------------------ coo.mul ------------------------------------------------]
                                                       |  PR: mul, device: cpu  |  master: mul, device: cpu
24 threads: -----------------------------------------------------------------------------------------------
      n=10000, nnz=100, coalesce=((True, True))        |              8         |                8
      n=10000, nnz=100, coalesce=((True, False))       |             32         |               34
      n=10000, nnz=100, coalesce=((False, True))       |             32         |               34
      n=10000, nnz=100, coalesce=((False, False))      |             41         |               56
      n=100000, nnz=1000, coalesce=((True, True))      |             24         |               24
      n=100000, nnz=1000, coalesce=((True, False))     |             90         |              100
      n=100000, nnz=1000, coalesce=((False, True))     |             87         |              100
      n=100000, nnz=1000, coalesce=((False, False))    |            231         |              255
      n=1000000, nnz=10000, coalesce=((True, True))    |            190         |              200
      n=1000000, nnz=10000, coalesce=((True, False))   |            908         |             2023
      n=1000000, nnz=10000, coalesce=((False, True))   |            800         |             2036
      n=1000000, nnz=10000, coalesce=((False, False))  |           3684         |             3989
      n=10, nnz=100, coalesce=((True, True))           |              8         |                7
      n=10, nnz=100, coalesce=((True, False))          |             34         |               30
      n=10, nnz=100, coalesce=((False, True))          |             33         |               30
      n=10, nnz=100, coalesce=((False, False))         |             44         |               50
      n=10, nnz=1000, coalesce=((True, True))          |              8         |                7
      n=10, nnz=1000, coalesce=((True, False))         |            100         |              100
      n=10, nnz=1000, coalesce=((False, True))         |            130         |              100
      n=10, nnz=1000, coalesce=((False, False))        |            746         |              210
      n=10, nnz=10000, coalesce=((True, True))         |              8         |                7
      n=10, nnz=10000, coalesce=((True, False))        |           1000         |             1500
      n=10, nnz=10000, coalesce=((False, True))        |           1000         |             1510
      n=10, nnz=10000, coalesce=((False, False))       |           3063         |             2457
      n=100, nnz=1000, coalesce=((True, True))         |             25         |               25
      n=100, nnz=1000, coalesce=((True, False))        |            180         |              130
      n=100, nnz=1000, coalesce=((False, True))        |            200         |              130
      n=100, nnz=1000, coalesce=((False, False))       |            271         |              255
      n=100, nnz=10000, coalesce=((True, True))        |            100         |              100
      n=100, nnz=10000, coalesce=((True, False))       |           2444         |             2290
      n=100, nnz=10000, coalesce=((False, True))       |           2455         |             2357
      n=100, nnz=10000, coalesce=((False, False))      |           5316         |             3783
      n=1000, nnz=10000, coalesce=((True, True))       |            204         |              211
      n=1000, nnz=10000, coalesce=((True, False))      |           2457         |             2480
      n=1000, nnz=10000, coalesce=((False, True))      |           2448         |             2539
      n=1000, nnz=10000, coalesce=((False, False))     |           3665         |             4801
      n=1000, nnz=100000, coalesce=((True, True))      |           2293         |             2374
      n=1000, nnz=100000, coalesce=((True, False))     |           9000         |            24620
      n=1000, nnz=100000, coalesce=((False, True))     |           8000         |            25080
      n=1000, nnz=100000, coalesce=((False, False))    |          26500         |            47650
      n=1000, nnz=1000000, coalesce=((True, True))     |          10000         |            13000
      n=1000, nnz=1000000, coalesce=((True, False))    |          80000         |           362200
      n=1000, nnz=1000000, coalesce=((False, True))    |          78050         |           392600
      n=1000, nnz=1000000, coalesce=((False, False))   |         312100         |           766900

Times are in microseconds (us).
```

</details>

Pull Request resolved: https://github.com/pytorch/pytorch/pull/85336
Approved by: https://github.com/cpuhrsch"
pytorch/pytorch,eb570ab7d0fd5df88fccf90cdadc581c722d20ef,"Revert ""add amp tests (#85434)""

This reverts commit c2f4bbe66918d167401ff5804c6b2d24fc6bda40.

Reverted https://github.com/pytorch/pytorch/pull/85434 on behalf of https://github.com/clee2000 due to broke rocm and slow tests on trunk https://hud.pytorch.org/pytorch/pytorch/commit/c2f4bbe66918d167401ff5804c6b2d24fc6bda40"
pytorch/pytorch,f371b5267deb93caa4413482a5c942d9d14a8c2c,"Made max_pool2d_with_indices_backward_cuda contiguify `indices` (#85493)

Currently, max_pool2d_with_indices_backward(grad_output, self, ..., indices)
(on cuda) assumes that indices has the same suggested memory format as
self.

This is indeed always true in regular PyTorch: the max_pool2d_with_indices
forward pass returns indices with the same suggeted memory format as
self.

However, we'd like to make an argument that always contiguifying indices
is good for consistency, has negligible added cost, and is more robust
(for Tensor Subclass authors):

- the max_pool2d_with_indices_backward implementation for CPU always
contiguifies `indices`. Ditto for the max_pool3d_with_indices_backward
implementation.
- Calling .contiguous() has almost no cost (compared to before) because
there is a fast-path that checks the cached memory_format on the
TensorImpl.
- functorch has trouble writing a batching rule for
`max_pool2d_with_indices_backward`. Having it accept `indices` with
arbitrary strides helps make it so that vmap doesn't need to special
case the batching rule for the strides of `indices`.

Test Plan:
- Not sure if it's worth writing a separate test case
- this PR fixes one of functorch's test cases.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/85493
Approved by: https://github.com/ezyang"
pytorch/pytorch,f0869cc8d095c9bdbcaca147ba52857932e7a743,"Make CUDA exceptions unlikely and isolate C10_CUDA_CHECK body (#85256)

This marks CUDA exception checks as unlikely, which might have a positive performance impact.

If further isolates part of `C10_CUDA_CHECK` into a separate function and file so that code can be made more expressive in subsequent diffs without bloating functions using the check or creating readability issues.

Test Plan: Sandcastle

Differential Revision: D39619861

Pull Request resolved: https://github.com/pytorch/pytorch/pull/85256
Approved by: https://github.com/ezyang, https://github.com/ngimel"
pytorch/pytorch,76d60778eb01b4213735be1c6e126fe2da519b8e,"[ONNX] Use decorators for symbolic function registration (#84448)

This is the 4th PR in the series of #83787. It enables the use of `@onnx_symbolic` across `torch.onnx`.

- **Backward breaking**: Removed some symbolic functions from `__all__` because of the use of  `@onnx_symbolic` for registering the same function on multiple aten names.
- Decorate all symbolic functions with `@onnx_symbolic`
- Move Quantized and Prim ops out from classes to functions defined in the modules. Eliminate the need for `isfunction` checking, speeding up the registration process by 60%.
    - Remove the outdated unit test `test_symbolic_opset9.py`
- Symbolic function registration moved from the first call to `_run_symbolic_function` to init time.
- Registration is fast:
  ![image](https://user-images.githubusercontent.com/11205048/189164959-f3fca173-19bc-4682-b150-f13a586387bf.png)

Pull Request resolved: https://github.com/pytorch/pytorch/pull/84448
Approved by: https://github.com/AllenTiTaiWang, https://github.com/BowenBao"
pytorch/pytorch,a4dca9822dfabcdbd1b36a12c013764f2af87613,"[composite compliance] prod (#81969)

Ref: #69991

Also fixes #82644 (fix similar to #81617)

For CompositeCompliance, we can't use `item` to choose a special fast-path when Tensor is a Subclass. Instead we always dispatch to the slower but safer implementation.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/81969
Approved by: https://github.com/zou3519"
pytorch/pytorch,6c48a01cef029075d3787acab18d2a4e32b2cb4c,"[Quant] Improve performance of ONEDNN backend (#84470)

## Description
This PR improves performance of ONEDNN quantization backend by adding fast paths. For qconv, qconv_transpose and qlinear.
It uses a cache to store reusable data on the first run thus reducing runtime overhead afterwards.

Note: Other quantization backends not affected.

## Validation
**Correctness**:
Covered by UT

**Performance**:
(Time to run each op, in microseconds)
Convolution, 1 core per instance, multiple instances on whole socket
shape | onednn (old) | onednn (new) | Improvement
-- | -- | -- | --
mb1_ic128oc128_id2od2kd3sd1dd0pd1 _ih8oh8kh3sh1dh0ph1_iw10ow10kw3sw1dw0pw1 | 767.038 | 415.238 | 45.86%
mb1_ic256oc128_id4od4kd1sd1dd0pd0 _ih16oh16kh1sh1dh0ph0_iw20ow20kw1sw1dw0pw0 | 194.979 | 167.353 | 14.17%
mb1_ic32oc16_ih112oh112kh1sh1dh0ph0 _iw112ow112kw1sw1dw0pw0 | 104.024 | 78.206 | 24.82%
mb1_ic3oc16_ih224oh112kh3sh2dh0ph1 _iw224ow112kw3sw2dw0pw1 | 104.178 | 81.559 | 21.71%
mb30_ic256oc256_ih14oh14kh3sh1dh0ph1 _iw14ow14kw3sw1dw0pw1 | 12249.438 | 12079.125 | 1.39%
mb56_ic3oc28_ih24oh22kh3sh1dh0ph0 _iw24ow22kw3sw1dw0pw0 | 438.046 | 405.21 | 7.50%
mb100_ic128oc128_ih16oh16kh3sh1dh0ph1 _iw16ow16kw3sw1dw0pw1 | 13893.188 | 13797.609 | 0.69%
g2mb1_ic128oc256_ih28oh28kh3sh1dh0ph1 _iw28ow28kw3sw1dw0pw1 | 499.014 | 475.333 | 4.75%
g32mb1_ic1024oc1024_ih14oh14kh3sh1dh0ph1 _iw14ow14kw3sw1dw0pw1 | 294.877 | 270.568 | 8.24%
g64mb1_ic1024oc2048_ih14oh7kh3sh2dh0ph1 _iw14ow7kw3sw2dw0pw1 | 122.664 | 95.503 | 22.14%
g256mb1_ic256oc256_ih10oh5kh3sh2dh0ph1 _iw10ow5kw3sw2dw0pw1 | 31.343 | 13.522 | 56.86%
g512mb1_ic512oc512_ih19oh10kh3sh2dh0ph1 _iw19ow10kw3sw2dw0pw1 | 54.116 | 34.651 | 35.97%
g1024mb1_ic1024oc1024_ih10oh10kh3sh1dh0ph1 _iw10ow10kw3sw1dw0pw1 | 74.989 | 55.566 | 25.90%

Convolution, 4 cores per instance, multiple instances on whole socket
shape | onednn (old) | onednn (new) | Improvement
-- | -- | -- | --
mb1_ic128oc128_id2od2kd3sd1dd0pd1 _ih8oh8kh3sh1dh0ph1_iw10ow10kw3sw1dw0pw1 | 249.978 | 160.429 | 35.82%
mb1_ic256oc128_id4od4kd1sd1dd0pd0 _ih16oh16kh1sh1dh0ph0_iw20ow20kw1sw1dw0pw0 | 102.726 | 89.555 | 12.82%
mb1_ic32oc16_ih112oh112kh1sh1dh0ph0 _iw112ow112kw1sw1dw0pw0 | 72.993 | 57.622 | 21.06%
mb1_ic3oc16_ih224oh112kh3sh2dh0ph1 _iw224ow112kw3sw2dw0pw1 | 76.607 | 61.847 | 19.27%
mb30_ic256oc256_ih14oh14kh3sh1dh0ph1 _iw14ow14kw3sw1dw0pw1 | 3109.625 | 3006.062 | 3.33%
mb56_ic3oc28_ih24oh22kh3sh1dh0ph0 _iw24ow22kw3sw1dw0pw0 | 191.194 | 175.997 | 7.95%
mb100_ic128oc128_ih16oh16kh3sh1dh0ph1 _iw16ow16kw3sw1dw0pw1 | 3435.625 | 3391.438 | 1.29%
g2mb1_ic128oc256_ih28oh28kh3sh1dh0ph1 _iw28ow28kw3sw1dw0pw1 | 205.209 | 191.931 | 6.47%
g32mb1_ic1024oc1024_ih14oh14kh3sh1dh0ph1 _iw14ow14kw3sw1dw0pw1 | 157.004 | 142.82 | 9.03%
g64mb1_ic1024oc2048_ih14oh7kh3sh2dh0ph1 _iw14ow7kw3sw2dw0pw1 | 83.262 | 71.689 | 13.90%
g256mb1_ic256oc256_ih10oh5kh3sh2dh0ph1 _iw10ow5kw3sw2dw0pw1 | 31.848 | 13.378 | 57.99%
g512mb1_ic512oc512_ih19oh10kh3sh2dh0ph1 _iw19ow10kw3sw2dw0pw1 | 50.216 | 32.663 | 34.95%
g1024mb1_ic1024oc1024_ih10oh10kh3sh1dh0ph1 _iw10ow10kw3sw1dw0pw1 | 67.359 | 49.779 | 26.10%

Transposed Convolution, 1 core per instance, multiple instances on whole socket
shape | onednn (old) | onednn (new) | Improvement
-- | -- | -- | --
mb1_ic512oc256_ih4oh8kh4sh2dh0ph1_iw4ow8kw4sw2dw0pw1 | 537.12 | 505.142 | 5.95%
mb1_ic256oc128_ih8oh16kh4sh2dh0ph1_iw8ow16kw4sw2dw0pw1 | 296.95 | 275.724 | 7.15%
mb1_ic128oc64_ih16oh32kh4sh2dh0ph1_iw16ow32kw4sw2dw0pw1 | 266.933 | 251.175 | 5.90%
mb1_ic64oc3_ih32oh64kh4sh2dh0ph1_iw32ow64kw4sw2dw0pw1 | 141.77 | 126.41 | 10.83%
mb1_ic100oc512_ih1oh4kh4sh1dh0ph0_iw1ow4kw4sw1dw0pw0 | 89.511 | 66.719 | 25.46%

Transposed Convolution, 4 cores per instance, multiple instances on whole socket
shape | onednn (old) | onednn (new) | Improvement
-- | -- | -- | --
mb1_ic512oc256_ih4oh8kh4sh2dh0ph1 _iw4ow8kw4sw2dw0pw1 | 181.594 | 163.77 | 9.82%
mb1_ic256oc128_ih8oh16kh4sh2dh0ph1 _iw8ow16kw4sw2dw0pw1 | 163 | 145.104 | 10.98%
mb1_ic128oc64_ih16oh32kh4sh2dh0ph1 _iw16ow32kw4sw2dw0pw1 | 163.158 | 150.71 | 7.63%
mb1_ic64oc3_ih32oh64kh4sh2dh0ph1 _iw32ow64kw4sw2dw0pw1 | 109.955 | 98.603 | 10.32%
mb1_ic100oc512_ih1oh4kh4sh1dh0ph0 _iw1ow4kw4sw1dw0pw0 | 69.502 | 54.523 | 21.55%

Linear, 1 core per instance, multiple instances on whole socket
shape | onednn (old) | onednn (new) | Improvement
-- | -- | -- | --
mb1ic16oc8 | 54.415 | 35.816 | 34.18%
mb1ic32oc16 | 26.764 | 16.041 | 40.07%
mb1ic64oc32 | 26.735 | 16.007 | 40.13%
mb1ic100oc1 | 26.712 | 16.06 | 39.88%
mb1ic512oc1000 | 65.261 | 51.947 | 20.40%
mb1ic1024oc1000 | 112.603 | 98.175 | 12.81%
mb1ic2048oc1000 | 207.294 | 192.014 | 7.37%
mb1ic4096oc4096 | 3761.094 | 3745.609 | 0.41%
mb1ic9216oc4096 | 8918.672 | 8912.547 | 0.07%
mb20ic2048oc91 | 52.487 | 44.623 | 14.98%
mb30ic512oc37 | 29.257 | 19.642 | 32.86%
mb100ic128oc256 | 39.32 | 29.81 | 24.19%
mb100ic256oc512 | 74.499 | 64.322 | 13.66%
mb100ic512oc1024 | 220.029 | 204.745 | 6.95%
mb100ic1024oc784 | 352.311 | 336.309 | 4.54%

Linear, 4 cores per instance, multiple instances on whole socket
shape | onednn (old) | onednn (new) | Improvement
-- | -- | -- | --
mb1ic16oc8 | 58.252 | 40.433 | 30.59%
mb1ic32oc16 | 23.901 | 15.549 | 34.94%
mb1ic64oc32 | 24.594 | 16.214 | 34.07%
mb1ic100oc1 | 24.011 | 15.4 | 35.86%
mb1ic512oc1000 | 49.781 | 41.988 | 15.65%
mb1ic1024oc1000 | 70.304 | 61.88 | 11.98%
mb1ic2048oc1000 | 92.259 | 85.715 | 7.09%
mb1ic4096oc4096 | 794.937 | 781.137 | 1.74%
mb1ic9216oc4096 | 2081.375 | 2067.75 | 0.65%
mb20ic2048oc91 | 66.929 | 58.338 | 12.84%
mb30ic512oc37 | 35.332 | 26.337 | 25.46%
mb100ic128oc256 | 42.21 | 38.908 | 7.82%
mb100ic256oc512 | 66.49 | 63.967 | 3.79%
mb100ic512oc1024 | 130.828 | 122.673 | 6.23%
mb100ic1024oc784 | 160.987 | 154.765 | 3.86%

Environment:

- PyTorch version: 1.13.0a0+gitcdd625b
- Is debug build: False
- CUDA used to build PyTorch: None
- ROCM used to build PyTorch: N/A

- OS: Ubuntu 20.04.3 LTS (x86_64)
- GCC version: (Ubuntu 9.3.0-17ubuntu1~20.04) 9.3.0
- Clang version: Could not collect
- CMake version: version 3.22.5
- Libc version: glibc-2.31

- Python version: 3.9.12 (main, Jun  1 2022, 11:38:51)  [GCC 7.5.0] (64-bit runtime)
- Python platform: Linux-5.11.0-27-generic-x86_64-with-glibc2.31
- Is CUDA available: False
- CUDA runtime version: No CUDA
- GPU models and configuration: No CUDA
- Nvidia driver version: No CUDA
- cuDNN version: No CUDA
- HIP runtime version: N/A
- MIOpen runtime version: N/A
- Is XNNPACK available: True

Versions of relevant libraries:
- [pip3] intel-extension-for-pytorch==1.13.0+cpu
- [pip3] numpy==1.23.3
- [pip3] pytorch-widedeep==0.3.7
- [pip3] torch==1.13.0a0+git48b423b
- [pip3] torchvision==0.14.0a0+ebb68f3
- [conda] blas                      1.0                         mkl
- [conda] intel-extension-for-pytorch 1.13.0+cpu               pypi_0    pypi
- [conda] mkl                       2021.4.0           h06a4308_640
- [conda] mkl-include               2022.1.0                 pypi_0    pypi
- [conda] mkl-service               2.4.0            py39h7f8727e_0
- [conda] mkl-static                2022.1.0                 pypi_0    pypi
- [conda] mkl_fft                   1.3.1            py39hd3c417c_0
- [conda] mkl_random                1.2.2            py39h51133e4_0
- [conda] numpy                     1.23.3                   pypi_0    pypi
- [conda] numpy-base                1.22.3           py39hf524024_0
- [conda] torch                     1.13.0a0+git48b423b          pypi_0    pypi
- [conda] torchvision               0.14.0a0+ebb68f3          pypi_0    pypi

Pull Request resolved: https://github.com/pytorch/pytorch/pull/84470
Approved by: https://github.com/jerryzh168"
pytorch/pytorch,6ed90379a848e1ff1422fb906253e38683c25c90,"Revert ""Legalize BFloat16 in NNC. (#83988)""

This reverts commit b049493ed52292c344c5b17f6db16a0242419865.

Reverted https://github.com/pytorch/pytorch/pull/83988 on behalf of https://github.com/clee2000 due to broke slow tests in trunk, https://github.com/pytorch/pytorch/actions/runs/3084421000/jobs/4986706931"
pytorch/pytorch,7234eb06f73f0e2c0aaa02727aee4afb5300ff1a,"Revert ""Land ""Make ceil,floor,round,trunc handle integers"" (#85144)""

This reverts commit b27eb8d377fc8ac267fdaed7f95a03d609764604.

Reverted https://github.com/pytorch/pytorch/pull/85144 on behalf of https://github.com/clee2000 due to broke slow tests in trunk  ex https://ossci-raw-job-status.s3.amazonaws.com/log/8433956087"
pytorch/pytorch,f38f9dfbfae27f255e83791670890c4383be98da,"When tracing SymInts, peephole optimize multiply by one (#85261)

This shows up a lot in graphs, so it is nice to not bother recording
useless info.  On pytorch_BERT, this optimization doesn't seem to
speed anything up, so it's mostly for cleanliness.

Signed-off-by: Edward Z. Yang <ezyang@fb.com>
Pull Request resolved: https://github.com/pytorch/pytorch/pull/85261
Approved by: https://github.com/wconstab"
pytorch/pytorch,81620c3360d4a15d266b8ad7daf556069db6dfc6,"Revert ""Faster mul(sparse, sparse) with broadcasting in dense dims. (#83428)""

This reverts commit d49943bda8e495bdb358e20b6eb114c442afa6e9.

Reverted https://github.com/pytorch/pytorch/pull/83428 on behalf of https://github.com/osalpekar due to Reverted because __restrict symbol not supported by certain MSVC compilers, leading to undefined symbol error at compilation time"
pytorch/pytorch,e33b464ffc8f08d9fb93b09816708d7f32500e68,"Revert ""Refs and decompositions for index_{add,copy,select,fill} (#85002)""

This reverts commit 2f0b3de443dd8d4477d70c5a56fa14496d1eebe3.

Reverted https://github.com/pytorch/pytorch/pull/85002 on behalf of https://github.com/huydhn due to Broke trunk slow tests"
pytorch/pytorch,cd7e6d4ad1df9fb42bc557b6c8dffaa5535bae74,"[ONNX] New symbolic function registry (#84382)

## Summary

The change brings the new registry for symbolic functions in ONNX. The `SymbolicRegistry` class in `torch.onnx._internal.registration` replaces the dictionary and various functions defined in `torch.onnx.symbolic_registry`.

The new registry

- Has faster lookup by storing only functions in the opset version they are defined in
- Is easier to manage and interact with due to its class design
- Builds the foundation for the more flexible registration process detailed in #83787

Implementation changes

- **Breaking**: Remove `torch.onnx.symbolic_registry`
- `register_custom_op_symbolic` and `unregister_custom_op_symbolic` in utils maintain their api for compatibility
- Update _onnx_supported_ops.py for doc generation to include quantized ops.
- Update code to register python ops in `torch/csrc/jit/passes/onnx.cpp`

## Profiling results

-0.1 seconds in execution time. -34% time spent in `_run_symbolic_function`. Tested on the alexnet example in public doc.

### After
```
   └─ 1.641 export  <@beartype(torch.onnx.utils.export) at 0x7f19be17f790>:1
      └─ 1.641 export  torch/onnx/utils.py:185
         └─ 1.640 _export  torch/onnx/utils.py:1331
            ├─ 0.889 _model_to_graph  torch/onnx/utils.py:1005
            │  ├─ 0.478 _optimize_graph  torch/onnx/utils.py:535
            │  │  ├─ 0.214 PyCapsule._jit_pass_onnx_graph_shape_type_inference  <built-in>:0
            │  │  │     [2 frames hidden]  <built-in>
            │  │  ├─ 0.190 _run_symbolic_function  torch/onnx/utils.py:1670
            │  │  │  └─ 0.145 Constant  torch/onnx/symbolic_opset9.py:5782
            │  │  │     └─ 0.139 _graph_op  torch/onnx/_patch_torch.py:18
            │  │  │        └─ 0.134 PyCapsule._jit_pass_onnx_node_shape_type_inference  <built-in>:0
            │  │  │              [2 frames hidden]  <built-in>
            │  │  └─ 0.033 [self]
```

### Before
![image](https://user-images.githubusercontent.com/11205048/188032302-688d881e-860d-4046-bdba-90da54233576.png)

### Start up time

The startup process takes 0.03 seconds. Calls to `inspect` will be eliminated when we switch to using decorators for registration in #84448

![image](https://user-images.githubusercontent.com/11205048/188208910-250f0434-475d-4872-9abc-781535519305.png)

Pull Request resolved: https://github.com/pytorch/pytorch/pull/84382
Approved by: https://github.com/AllenTiTaiWang, https://github.com/BowenBao"
pytorch/pytorch,c6c3346d5aaa548d82f3dff14ed54e687af50116,"[FSDP] Short-term fix to remove `optim_input` (#84201)

This is a short-term quick fix to accommodate using the existing optimizer state APIs without passing `optim_input`. It preserves the existing `optim_input` code path but if `optim_input` is `None` while `optim` is not, then the APIs will use the new code path that relies on `self.param_groups` to get the information previously provided by `optim_input`.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/84201
Approved by: https://github.com/rohan-varma"
pytorch/pytorch,490727a35f213778d2e709a3b03d899ad502c5f9,"New calling convention for Python dispatcher (#85133)

Instead of calling into the Python dispatcher for EVERY dispatcher
call, we now have a two step process.  First, we
getattr(op: OpOverload, dispatch_key) to ""load"" the handler for the
function.  This can either be a conventional function (in which
case we will call it, in the same way the old Python dispatcher
worked), or it can be a DispatchKey, in which case we will directly
call that DispatchKey in C++, bypassing marshalling between Python
and C++ entirely.  OpOverload.__getattr__ is carefully written so
that it will cache the

A further optimization would be to define __slots__ on OpOverload,
and ensuring that the DispatchKey strings are interned.

The resulting Python dispatcher is less flexible: after the first
lookup, the handler is cached and we won't recompute it.  Furthermore,
by default, dispatches will not go into Python, and so you won't
get stack frames for the Python dispatcher by default.  But we get
a huge performance improvement: on the following microbenchmark
we go from 2.5s to 1.9s.

```
import time
import torch
from functorch import make_fx

def f(x):
    for i in range(1000):
        x = x * x
    return x

begin = time.time()
res = make_fx(f, tracing_mode=""symbolic"")(torch.randn(10, 20))
print(time.time()-begin)
```

Signed-off-by: Edward Z. Yang <ezyang@fb.com>
Pull Request resolved: https://github.com/pytorch/pytorch/pull/85133
Approved by: https://github.com/wconstab"
pytorch/pytorch,e5fac7f5dc4f16070193bb7d06322e0faaa94099,"Optimize torch.ops.ns.opname.overload accessor in torch dispatch (#85132)

This doesn't actually seem to help all that much.

Signed-off-by: Edward Z. Yang <ezyang@fb.com>
Pull Request resolved: https://github.com/pytorch/pytorch/pull/85132
Approved by: https://github.com/wconstab"
pytorch/pytorch,607eccb13ca586f775fb09daeb728a4b4e30ebdd,"[FSDP] Option to keep grads in lower prec (#85134)

Differential Revision: [D39565189](https://our.internmc.facebook.com/intern/diff/D39565189)

Rehash of a similar PR from a month ago that got stale. Adds a config to FSDP MP so that gradients can be kept in lower precision, to support optimizers such as AnyPrecisionOptimizer which would like to keep grads in bf16.

To do this, for sharded cases, we cannot simply omit the cast back to the full precision param dtype, otherwise when setting `p.grad = p._saved_grad_shard` in finalize_params, autograd will throw an error indicating that the grad dtype should match the param dtype when it is being set.

As a workaround, we re-cast after setting this. Although, this means that for cases that use gradient accumulation, p._saved_grad_shard will be of the reduced dtype because it is set to p.grad in `_prep_grad_for_backward`. As a result, add a check + recast here as well.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/85134
Approved by: https://github.com/awgu"
pytorch/pytorch,7b3e177b8772d95e5aaa92415a632d280320c740,"Increase docker build timeout (#85156)

Docker builds used to take around 15 mins to run (more than the 10 min timeout) and have recently started taking even longer due to conda's slow dependency resolver.

We were in this bad state where we _depended_ on the retry to complete the build. That is, the first attempt would try to build docker, timeout, then the second attempt would continue to build on top of the cache the first build had setup, etc.

Increasing the timeout so that docker builds actually have enough time to complete the build within a single attempt

Pull Request resolved: https://github.com/pytorch/pytorch/pull/85156
Approved by: https://github.com/huydhn"
pytorch/pytorch,00ce302c077cf1b26e9190da146b008dd319eed2,"Performance optimizations to proxy tensor (#85049)

- Lazily allocate FX nodes for size/stride accessors on proxy tensor
- Properly track derived computations on strides/numel/etc
- Remove unnecessary tree_map at end of proxy tensor trace checking
  invariants; we will just have to be smart (it's too expensive)
- Avoid tree_map in sym proxy tracing

Signed-off-by: Edward Z. Yang <ezyang@fb.com>
Pull Request resolved: https://github.com/pytorch/pytorch/pull/85049
Approved by: https://github.com/wconstab"
pytorch/pytorch,d49943bda8e495bdb358e20b6eb114c442afa6e9,"Faster mul(sparse, sparse) with broadcasting in dense dims. (#83428)

Preliminary benchmarks (square matrices of shape (n, n)).

<details>

<summary>Script</summary>

```python
import torch
import math
from IPython import get_ipython
from itertools import product, repeat
import pickle
from torch.utils.benchmark import Timer, Compare

torch.manual_seed(13)

# specifies (n, nnz)
problem_dims = (
    # n > nnz
    (10000, 100),
    (100000, 1000),
    (1000000, 10000),
    # n < nnz
    (10, 100),
    (10, 1000),
    (10, 10000),
    (100, 1000),
    (100, 10000),
    (1000, 10000),
    (1000, 100000),
    (1000, 1000000),
    #(1000000, 1000000000),
)

name = ""PR""
device = ""cuda""
results = []

for n, nnz in problem_dims:
    def gen_tensor(coalesce=False):
        shape = (n, n)
        nrows, ncols = shape
        rowidx = torch.randint(low=0, high=nrows, size=(nnz,), device=device)
        colidx = torch.randint(low=0, high=ncols, size=(nnz,), device=device)
        itemidx = torch.vstack((rowidx, colidx))
        xvalues = torch.randn(nnz, device=device)
        itemidx = torch.hstack((itemidx, itemidx))
        xvalues = torch.hstack((xvalues, xvalues))
        res = torch.sparse_coo_tensor(itemidx, xvalues, size=shape)
        if coalesce:
            return res.coalesce()
        else:
            return res

    for x_coalesce, y_coalesce in product(*repeat((True, False), 2)):
        x = gen_tensor(x_coalesce)
        y = gen_tensor(y_coalesce)
        smtp = ""x * y""
        timer = Timer(smtp,
                      globals=globals(),
                      label=""coo.mul"",
                      description=f""{name}: mul, device: {device}"",
                      sub_label=f""n={n}, nnz={nnz}, coalesce=({x_coalesce, y_coalesce})"",
                      num_threads=torch.get_num_threads())
        results.append(timer.blocked_autorange())

compare = Compare(results)
compare.trim_significant_figures()
compare.print()

with open(f""{name}_{device}_mul.pickle"", 'wb') as f:
    pickle.dump(results, f)

```

</details>

<details>

<summary>Gather results</summary>

```python
import pickle
from torch.utils.benchmark import Timer, Compare

files = [
        ""PR"",
        ""master""
        ]

device = 'cuda'

timers = []
for name in files:
    with open(""{}_{}_mul.pickle"".format(name, device), 'rb') as f:
        timers += pickle.load(f)

compare = Compare(timers)
compare.trim_significant_figures()
compare.print()

```

</details>

<details>

<summary>CUDA</summary>

```
[------------------------------------------------- coo.mul -------------------------------------------------]
                                                       |  PR: mul, device: cuda  |  master: mul, device: cuda
24 threads: -------------------------------------------------------------------------------------------------
      n=10000, nnz=100, coalesce=((True, True))        |             95          |                91
      n=10000, nnz=100, coalesce=((True, False))       |             87          |               242
      n=10000, nnz=100, coalesce=((False, True))       |             87          |               226
      n=10000, nnz=100, coalesce=((False, False))      |            130          |               371
      n=100000, nnz=1000, coalesce=((True, True))      |            100          |               521
      n=100000, nnz=1000, coalesce=((True, False))     |             90          |               649
      n=100000, nnz=1000, coalesce=((False, True))     |            100          |               659
      n=100000, nnz=1000, coalesce=((False, False))    |            200          |               781
      n=1000000, nnz=10000, coalesce=((True, True))    |            100          |              4861
      n=1000000, nnz=10000, coalesce=((True, False))   |            100          |              5012
      n=1000000, nnz=10000, coalesce=((False, True))   |             98          |              5010
      n=1000000, nnz=10000, coalesce=((False, False))  |            384          |              5174
      n=10, nnz=100, coalesce=((True, True))           |            100          |                79
      n=10, nnz=100, coalesce=((True, False))          |            100          |               221
      n=10, nnz=100, coalesce=((False, True))          |            100          |               221
      n=10, nnz=100, coalesce=((False, False))         |            100          |               350
      n=10, nnz=1000, coalesce=((True, True))          |            100          |               100
      n=10, nnz=1000, coalesce=((True, False))         |            100          |               240
      n=10, nnz=1000, coalesce=((False, True))         |            100          |               254
      n=10, nnz=1000, coalesce=((False, False))        |            100          |               392
      n=10, nnz=10000, coalesce=((True, True))         |            100          |               110
      n=10, nnz=10000, coalesce=((True, False))        |            110          |               286
      n=10, nnz=10000, coalesce=((False, True))        |            110          |               286
      n=10, nnz=10000, coalesce=((False, False))       |            271          |               455
      n=100, nnz=1000, coalesce=((True, True))         |            110          |               851
      n=100, nnz=1000, coalesce=((True, False))        |            110          |              1000
      n=100, nnz=1000, coalesce=((False, True))        |            110          |               990
      n=100, nnz=1000, coalesce=((False, False))       |            140          |              1124
      n=100, nnz=10000, coalesce=((True, True))        |            110          |              5137
      n=100, nnz=10000, coalesce=((True, False))       |            110          |              5391
      n=100, nnz=10000, coalesce=((False, True))       |            100          |              5405
      n=100, nnz=10000, coalesce=((False, False))      |            249          |              5539
      n=1000, nnz=10000, coalesce=((True, True))       |            100          |              8598
      n=1000, nnz=10000, coalesce=((True, False))      |            100          |              8800
      n=1000, nnz=10000, coalesce=((False, True))      |            100          |              8782
      n=1000, nnz=10000, coalesce=((False, False))     |            255          |              8956
      n=1000, nnz=100000, coalesce=((True, True))      |            120          |             84500
      n=1000, nnz=100000, coalesce=((True, False))     |            200          |             88560
      n=1000, nnz=100000, coalesce=((False, True))     |            160          |             89000
      n=1000, nnz=100000, coalesce=((False, False))    |            373          |             89000
      n=1000, nnz=1000000, coalesce=((True, True))     |            312          |            606400
      n=1000, nnz=1000000, coalesce=((True, False))    |           1340          |            609200
      n=1000, nnz=1000000, coalesce=((False, True))    |           1340          |            609100
      n=1000, nnz=1000000, coalesce=((False, False))   |           4408          |            611400

Times are in microseconds (us).
```

</details>

<details>

<summary>CPU</summary>

```
[------------------------------------------------ coo.mul ------------------------------------------------]
                                                       |  PR: mul, device: cpu  |  master: mul, device: cpu
24 threads: -----------------------------------------------------------------------------------------------
      n=10000, nnz=100, coalesce=((True, True))        |              8         |                8
      n=10000, nnz=100, coalesce=((True, False))       |             32         |               34
      n=10000, nnz=100, coalesce=((False, True))       |             32         |               34
      n=10000, nnz=100, coalesce=((False, False))      |             41         |               56
      n=100000, nnz=1000, coalesce=((True, True))      |             24         |               24
      n=100000, nnz=1000, coalesce=((True, False))     |             90         |              100
      n=100000, nnz=1000, coalesce=((False, True))     |             87         |              100
      n=100000, nnz=1000, coalesce=((False, False))    |            231         |              255
      n=1000000, nnz=10000, coalesce=((True, True))    |            190         |              200
      n=1000000, nnz=10000, coalesce=((True, False))   |            908         |             2023
      n=1000000, nnz=10000, coalesce=((False, True))   |            800         |             2036
      n=1000000, nnz=10000, coalesce=((False, False))  |           3684         |             3989
      n=10, nnz=100, coalesce=((True, True))           |              8         |                7
      n=10, nnz=100, coalesce=((True, False))          |             34         |               30
      n=10, nnz=100, coalesce=((False, True))          |             33         |               30
      n=10, nnz=100, coalesce=((False, False))         |             44         |               50
      n=10, nnz=1000, coalesce=((True, True))          |              8         |                7
      n=10, nnz=1000, coalesce=((True, False))         |            100         |              100
      n=10, nnz=1000, coalesce=((False, True))         |            130         |              100
      n=10, nnz=1000, coalesce=((False, False))        |            746         |              210
      n=10, nnz=10000, coalesce=((True, True))         |              8         |                7
      n=10, nnz=10000, coalesce=((True, False))        |           1000         |             1500
      n=10, nnz=10000, coalesce=((False, True))        |           1000         |             1510
      n=10, nnz=10000, coalesce=((False, False))       |           3063         |             2457
      n=100, nnz=1000, coalesce=((True, True))         |             25         |               25
      n=100, nnz=1000, coalesce=((True, False))        |            180         |              130
      n=100, nnz=1000, coalesce=((False, True))        |            200         |              130
      n=100, nnz=1000, coalesce=((False, False))       |            271         |              255
      n=100, nnz=10000, coalesce=((True, True))        |            100         |              100
      n=100, nnz=10000, coalesce=((True, False))       |           2444         |             2290
      n=100, nnz=10000, coalesce=((False, True))       |           2455         |             2357
      n=100, nnz=10000, coalesce=((False, False))      |           5316         |             3783
      n=1000, nnz=10000, coalesce=((True, True))       |            204         |              211
      n=1000, nnz=10000, coalesce=((True, False))      |           2457         |             2480
      n=1000, nnz=10000, coalesce=((False, True))      |           2448         |             2539
      n=1000, nnz=10000, coalesce=((False, False))     |           3665         |             4801
      n=1000, nnz=100000, coalesce=((True, True))      |           2293         |             2374
      n=1000, nnz=100000, coalesce=((True, False))     |           9000         |            24620
      n=1000, nnz=100000, coalesce=((False, True))     |           8000         |            25080
      n=1000, nnz=100000, coalesce=((False, False))    |          26500         |            47650
      n=1000, nnz=1000000, coalesce=((True, True))     |          10000         |            13000
      n=1000, nnz=1000000, coalesce=((True, False))    |          80000         |           362200
      n=1000, nnz=1000000, coalesce=((False, True))    |          78050         |           392600
      n=1000, nnz=1000000, coalesce=((False, False))   |         312100         |           766900

Times are in microseconds (us).
```

</details>

Pull Request resolved: https://github.com/pytorch/pytorch/pull/83428
Approved by: https://github.com/cpuhrsch"
pytorch/pytorch,52a2b612035e081626ff6f23eec87782bde6643c,"Fix fetch function which breaks user code (#85099)

The [fastNLP](https://github.com/fastnlp/fastNLP/blob/v0.6.0/fastNLP/core/batch.py#L51) model uses DataSetGetter to fetch data from the dataset. The following code breaks because of https://github.com/pytorch/pytorch/pull/84301:

```
from fastNLP.io.pipe.qa import CMRC2018BertPipe
input_dir = os.path.join(os.path.dirname(os.path.abspath(__file__)), "".data"", ""cmrc2018-sim"")
data_bundle = CMRC2018BertPipe().process_from_file(paths=input_dir)
data_bundle.rename_field('chars', 'words')
data_bundle.get_dataset('dev')
dataset = DataSetGetter(dataset, as_numpy)
dataiter = torch.utils.data.DataLoader(dataset=dataset)
for batch in dataiter:
    # data-processing...
```

This is because for the `DataSetGetter` class, the following condition holds:
```
# hasattr(dataset_getter, '__getitems__') == True
# dataset_getter.__getitems__ == None
```

This PR adds an additional check to make sure `__getitems__` is only called when it is not None.

This error was found by the torchbench nightly CI, original error stack trace:
```
ERROR: test_fastNLP_Bert_train_cuda (__main__.TestBenchmark)
----------------------------------------------------------------------
components._impl.workers.subprocess_rpc.ChildTraceException: Traceback (most recent call last):
  File ""/home/circleci/project/components/_impl/workers/subprocess_rpc.py"", line 470, in _run_block
    exec(  # noqa: P204
  File ""<subprocess-worker>"", line 35, in <module>
  File ""<subprocess-worker>"", line 12, in _run_in_worker_f
  File ""/home/circleci/project/torchbenchmark/util/model.py"", line 16, in __call__
    obj = type.__call__(cls, *args, **kwargs)
  File ""/home/circleci/project/torchbenchmark/models/fastNLP_Bert/__init__.py"", line 93, in __init__
    self.example_inputs = self._prefetch(example_inputs)
  File ""/home/circleci/project/torchbenchmark/models/fastNLP_Bert/__init__.py"", line 133, in _prefetch
    for batch_x, batch_y in example_inputs:
  File ""/home/circleci/miniconda3/lib/python3.8/site-packages/fastNLP/core/batch.py"", line 266, in __iter__
    for indices, batch_x, batch_y in self.dataiter:
  File ""/home/circleci/miniconda3/lib/python3.8/site-packages/torch/utils/data/dataloader.py"", line 681, in __next__
    data = self._next_data()
  File ""/home/circleci/miniconda3/lib/python3.8/site-packages/torch/utils/data/dataloader.py"", line 719, in _next_data
    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration
  File ""/home/circleci/miniconda3/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py"", line 56, in fetch
    data = self.dataset.__getitems__(possibly_batched_index)
TypeError: 'NoneType' object is not callable
```

Full error log: https://app.circleci.com/pipelines/github/pytorch/benchmark/5143/workflows/0676f36d-0ab4-42bd-adb4-90e6b0df76d1/jobs/5293
Pull Request resolved: https://github.com/pytorch/pytorch/pull/85099
Approved by: https://github.com/ejguan"
pytorch/pytorch,33352336b443dbfce1394f6b4950e8f33eff2cef,"[FSDP] Add rate limiter (#83917)

**Overview**
This PR adds a `bool` argument `limit_all_gathers` to the FSDP constructor, defaulted to `False`.
- Setting `limit_all_gathers=True` limits the max number of inflight all-gathers to 2 (an empirically chosen constant), preventing a fast CPU thread from over-allocating blocks to the all-gather stream.
- When experiencing a high number of CUDA malloc retries, the limiter can help reduce the number and hence lead to QPS improvement.

**Exploration**
I experimented with both a count-based limiter and size-based limiter (where the size is based on the inflight all-gather size in bytes).
- The size-based limiter did not provide any advantage, only confusing the developer and user alike on what threshold to set.
- For the count-based approach, I decided not to expose the max number of inflight all-gathers to the user since values other than 2 do not show improvements and exposing the knob may confuse users.

**T5-11B**
T5-11B evidences the performance gain from enabling the limiter and that a limit of 2 is a reasonable choice. This is run on an AWS cluster with 8 A100s per node and EFA. For both 2 and 4 nodes, we scale the batch size maximally before hitting OOM, which is a common practice.

<p float=""left"">
  <img src=""https://user-images.githubusercontent.com/31054793/188936036-04427da9-f492-4e50-9b35-ff64665d9815.png"" width=""400"" />
  <img src=""https://user-images.githubusercontent.com/31054793/188936045-f44e659f-1e18-4ea7-8c78-0fce4ff8fb48.png"" width=""400"" />
</p>

For 2 nodes, the limit of 2 yields 3.01x QPS improvement, and for 4 nodes, the limit of 2 yields 2.87x QPS improvement.

We need more data points, but the limiter may simplify the batch size scaling workflow. Normally, a practitioner may scale until hitting OOM and back off until there are few CUDA malloc retries. However, now the practitioner may be able to scale until hitting OOM and simply turn on the limiter to reduce the number of retries instead of backing off.

Differential Revision: [D39331201](https://our.internmc.facebook.com/intern/diff/D39331201)
Pull Request resolved: https://github.com/pytorch/pytorch/pull/83917
Approved by: https://github.com/zhaojuanmao"
pytorch/pytorch,afcc7c7f5c7cef740241ff0abdae8d4f2ad22a03,"[FSDP] Generalize prefetching; lower unshard/reshard to handle (#83665)

### Additional Constructor Changes
- `self.sharding_strategy`
    - If the world size is 1, I clamp the sharding strategy to `NO_SHARD`, regardless of the passed-in sharding strategy, since the behavior is fully equivalent. This absolves the need for `p._is_sharded or self.world_size == 1` checks in the core code. Once we fully shift the paradigm to using handles, this should result in a clear net positive. However, for now, we still have some places where we interface directly with the `FlatParameter`, in which case we have some temporary hacky code.
- `HandleConfig`
    - As a part of the new design abstraction, much logic is lowered to the `FlatParamHandle`. This requires the handle be aware of mixed precision, CPU offloading, sharding strategy, and the process group (for world size > 1). To be less error-prone, I re-defined the `dataclass`s and `enum`s for the handle. These can be removed and coalesced with the existing ones.
    - The drawback is that the `FlattenParamsWrapper` constructor now takes in the `HandleConfig` to forward it to the `FlatParamHandle` constructor. I tolerate this since we plan to retire the FPW. For now, the handle's process group attributes are set later when we call `handle.shard()`.
    - We will dive into this logic lowering later. For now, the idea is we need to pass some extra info to the handle, which must go through the FPW.
- `FullyShardedDataParallel._shard_parameters()` -> `FlatParamHandle.shard()`
- [Important] Generalizing attributes to remove the 1 `FullyShardedDataParallel` : 1 `FlatParameter` assumption
    - **Before:** `_fsdp_graph_order`, `_pre_backward_hook_full_params_prefetched`, `_forward_full_params_prefetched`, `reshard_after_forward` are with respect to 1 `FullyShardedDataParallel`
    - **After:** (1) We use `FlatParamHandle` in place of `FullyShardedDataParallel`. (2) The atomic unit for forward and pre-backward is a _group_ of handles involved in the same module's forward/pre-backward. This is represented as `Tuple[FlatParamHandle, ...]`. For now, this is **always a singleton tuple**, but this shift enables a module having multiple FSDP parameters (which we have use cases for).
- `_reset_lazy_init()` attributes
    - The prefetched flags are merged into `self._handles_prefetched`, which is directly defined in the constructor. `reshard_after_forward` is retired since it can be fully determined by other attributes (`_is_root` and `sharding_strategy`).

## FSDP Runtime: Unshard

The first step is to read the existing `_rebuild_full_params()`. A few notable observations:
- It returns `Tuple[Tensor, bool]`. The first element is the _padded unsharded flattened parameter_, and the second element is whether we can free it upon exiting `summon_full_params()`. This return value is **only used in `summon_full_params()`**.
- If parameter mixed precision is enabled and the `FlatParameter` is already unsharded, then the low precision shard (`_mp_shard`) is still re-allocated on GPU. (It is freed at the end of the method.)
- If CPU offloading is enabled and the `FlatParameter` is already unsharded, then there is a no-op `p.data = p.data.to(self.compute_device, non_blocking=True)`.
- Inside `summon_full_params()`, `mixed_precision_cast_ran` is always `False`. Therefore, the return value for the `not p._is_sharded and mixed_precision_cast_ran` branch is unused.
-`summon_full_params()` can only be called (before forward or after backward) or (between forward and backward). Given this, I cannot think of a case where we call `summon_full_params()`, the `FlatParameter` is already unsharded, but `reshard_after_forward` is `True`. The `FlatParameter` should be sharded (before forward or after backward), and the `FlatParameter` may only be unsharded (between forward and backward) if `reshard_after_forward` is `False`.
- If parameter mixed precision is enabled and the sharding strategy is a sharded one, then inside `summon_full_params()`, the `FlatParameter` is unsharded in full precision. This involves allocating a new padded unsharded flattened parameter on GPU in full precision since `_full_param_padded` is in the low precision.

Some comments:
- Ideally, we reduce the complexity of the core code path: i.e. unshard for forward and pre-backward. If the return value is only used for `summon_full_params()`, we should consider if we can compartmentalize that logic.
- The branching is complex, and some return values are never used, where this fact is not immediately obvious. We should see if we can reduce the branch complexity.

Disclaimer: The difference in attribute semantics between `NO_SHARD` and the sharded strategies makes it challenging to unify the cases. This PR does not attempt to address that since it requires more design thought. However, it does attempt to reduce the complexity for the sharded strategies.

### Unshard: Core Code Path
Let us trace through the new logical unshard.
1. `FullyShardedDataParallel._unshard(self, handles: List[FlatParamHandle], prepare_gradient: bool)`
    - This iterates over the handles and calls `handle.pre_unshard()`, `handle.unshard()`, and `handle.post_unshard(prepare_gradient)` in the all-gather stream.
2. `FlatParamHandle.needs_unshard(self)`
    - We take an aside to look at this key subroutine.
    - For `NO_SHARD`, this returns `False`.
    - For sharded strategies, this checks if the padded unsharded flattened parameter is allocated. The padded unsharded flattened parameter is the base tensor for the unpadded unsharded flattened parameter, which is a view into the padded one. Thus, the padded one's allocation fully determines if the `FlatParameter` is unsharded.
    - For sharded strategies, to accommodate the parameter mixed precision + `summon_full_params()` case, we introduce `_full_prec_full_param_padded`, which is the padded unsharded flattened parameter in full precision. The helper `_get_padded_unsharded_flat_param()` takes care of this casing and returns the padded unsharded flattened parameter. Instead of allocating a new tensor each time, we manually manage `_full_prec_full_param_padded`'s storage just like for `_full_param_padded`.
3. `FlatParamHandle.pre_unshard(self)`
    - For sharded strategies, the postcondition is that the handle's `FlatParameter` points to the tensor to all-gather. This should be on the communication device and in the desired precision. The allocation and usage of the low precision shard for parameter mixed precision and the CPU -> GPU copy for CPU offloading both classify naturally in the pre-unshard.
    - For sharded strategies, if the `FlatParameter` does not need to be unsharded, `pre_unshard()` is a no-op. This avoids unnecessarily allocating and freeing the low precision shard.
    - For `NO_SHARD`, we simply preserve the existing semantics.
4. `FlatParamHandle.unshard(self)`
    - If the handle was resharded without freeing the padded unsharded flattened parameter (e.g. `summon_full_params()` between forward and backward when `reshard_after_forward=False`), then the `FlatParameter` points to the sharded flattened parameter. We need to switch to using the unsharded parameter. This is a design choice. Alternatively, we may not switch to using the sharded flattened parameter in `reshard()` if we do not free the padded unsharded flattened parameter. However, the postcondition that the `FlatParameter` points to the sharded flattened parameter after `reshard()` is helpful logically, so I prefer this approach.
    - Otherwise, this allocates the padded unsharded flattened parameter, all-gathers, and switches to using the unpadded unsharded flattened parameter.
    - In the future, we may add an option to `unshard()` that additionally all-gathers the gradient.
5. `FlatParamHandle.post_unshard(self, prepare_gradient: bool)`
    - For sharded strategies, if using parameter mixed precision, this frees the low precision shard. More generally, this should free any sharded allocations made in `pre_unshard()` since the all-gather has been launched. If using CPU offloading, the GPU copy of the local shard goes out of scope after `unshard()` and is able to be garbage collected. **We should understand if there is any performance difference between manually freeing versus deferring to garbage collection since our usage is inconsistent.** For now, I preserve the existing semantics here.
    - `prepare_gradient` is meant to be set to `True` for the pre-backward unshard and `False` for the forward unshard. This runs the equivalent logic of `_prep_grads_for_backward()`.
    - This post-unshard logic (notably the gradient preparation) now runs in the all-gather stream, which is fine because we always have the current stream wait for the all-gather stream immediately after `FullyShardedDataParallel._unshard()`. IIUC, we do not need to call `_mp_shard.record_stream(current_stream)` (where `current_stream` is the default stream) because `_mp_shard` is allocated and freed in the same (all-gather) stream.
    - A postcondition is that the `FlatParameter` is on the compute device. It should also have the unpadded unsharded size (though I do not have a check for this at the moment).

### Unshard: `summon_full_params()`
Now that we see how the logical unshard has been reorganized for the core code path, let us dive into `summon_full_params()`.

The two constraints are:
1. If using parameter mixed precision, we should unshard in full precision.
2. We must determine if we should free the padded unsharded flattened parameter upon exiting.

The first constraint is addressed as described before in the core unshard code path, so it remains to explore the second constraint.

I propose a simple rule: **We free iff we actually unshard the `FlatParameter` in `summon_full_params()`** (i.e. it was not already unsharded). We perform a case analysis:

**Parameter mixed precision enabled:**
* `NO_SHARD`: `flat_param.data` points to `flat_param._local_shard`, which is the full precision unsharded flattened parameter. This is **not safe to free**.
* `FULL_SHARD` / `SHARD_GRAD_OP`: We force full precision and all-gather to `_full_prec_full_param_padded`. We do not support `nested summon_full_params()`, so `_full_prec_full_param_padded` must be unallocated. We unshard, and it is **safe to free**.

**Parameter mixed precision disabled:**
* `NO_SHARD`: This is the same as with mixed precision enabled. This is **not safe to free**.
* `FULL_SHARD` / `SHARD_GRAD_OP`: We all-gather to `_full_param_padded`. It may already be unsharded.
    * Already unsharded: The unshard is a no-op. This is **not safe to free**.
        * For `FULL_SHARD`, this can happen for the root FSDP instance after `forward()` but before backward.
        * For `SHARD_GRAD_OP`, this can happen for all FSDP instances after `forward()` but before backward.
    * Needs unshard: We unshard. This is **safe to free**.

Therefore, we see that it is not safe to free when using `NO_SHARD` and when using a sharded strategy but the `FlatParameter` is already unsharded. This is precisely the proposed rule.

There were two notable edge cases that the existing code did not address.
1. The existing code tests if the `FlatParameter` is already unsharded by checking the allocation status of `_full_param_padded`. When using parameter mixed precision, this is the incorrect tensor to check. If `_full_param_padded` is allocated (e.g. when `reshard_after_forward=False` and calling `summon_full_params()` between forward and backward), the already-unsharded check is a false positive, and `summon_full_params()` does not correctly force full precision. https://github.com/pytorch/pytorch/issues/83068
    - This PR's `needs_unshard()` check correctly routes to the appropriate padded unsharded flattened parameter depending on the calling context (i.e. if it needs to force full precision or not).
2. The existing code does not free the GPU copy of the padded unsharded flattened parameter when calling `summon_full_params(offload_to_cpu=True)`. It unshards the `FlatParameter`, moves the padded unsharded flattened parameter to CPU, and sets the `FlatParameter` data to be the appropriate unpadded view into the padded unsharded flattened parameter on CPU. However, `_full_param_padded` still points to the all-gathered padded unsharded flattened parameter on GPU, which is kept in memory. https://github.com/pytorch/pytorch/issues/83076
    - This PR frees the GPU copy and reallocates it upon exiting `summon_full_params()`. This is essential for avoiding peak GPU memory usage from increasing as we recurse through the module tree. There may be some cases where we can avoid reallocation altogether, but that can be addressed in a follow-up PR.
    - This PR offloads the *unpadded* unsharded flattened parameter to CPU directly instead of the *padded* one. As far as I can tell, there is no need to include the padding since unflattening the original parameters does not require the padding.
    - The relevant code is in the context manager `FlatParamHandle.to_cpu()`.

### Unshard: Mixed-Precision Stream

This PR removes the mixed precision stream usage. As is, I do not think there is any extra overlap being achieved by the stream usage.

The low precision shard is allocated and copied to in the mixed precision stream ([code](https://github.com/pytorch/pytorch/blob/1f99bdfcc4a3f97d28471a531d2b69def762f6ba/torch/distributed/fsdp/fully_sharded_data_parallel.py#L1401-L1412)), and the current stream (in this case the all-gather stream) waits for the mixed precision stream ([code](https://github.com/pytorch/pytorch/blob/1f99bdfcc4a3f97d28471a531d2b69def762f6ba/torch/distributed/fsdp/fully_sharded_data_parallel.py#L1414)). However, we immediately schedule an all-gather that communicates that exact low precision shard ([code](https://github.com/pytorch/pytorch/blob/1f99bdfcc4a3f97d28471a531d2b69def762f6ba/torch/distributed/fsdp/fully_sharded_data_parallel.py#L3338)) with no other meaningful computation between. If we remove the mixed precision stream, the low precision shard is allocated and copied to in the all-gather stream (including the non-blocking CPU -> GPU copy if using CPU offloading).

Under this PR's design, we may consider a ""pre-unshard"" stream for all logical pre-unshard data transfers if we want to overlap in the future. IIUC, the overlap opportunity exists if there are multiple `FlatParameter`s per module, and we only have the all-gather stream wait for the data transfer corresponding to the local shard it communicates, not the others.

If we agree on removing the mixed-precision stream for now, I will remember to delete it from `_init_streams()`.

## FSDP Runtime: Reshard

Like with unshard, the first step is the look at the existing `_free_full_params()` and `_use_param_local_shard()`. A few notable observations:
- For only `NO_SHARD`, `_free_full_params()` includes a call to `_free_mp_shard()`.
- For `summon_full_params()`, there is a separate `_free_full_params_and_use_local_shard()` that duplicates the main logic of `_free_full_params()` and calls `_use_param_local_shard()`.
- In `forward()`, if `reshard_after_forward=True`, we call `_free_full_params()` and then `_free_mp_shard()`. Hence, for `NO_SHARD`, the `_free_mp_shard()` is a no-op.
- In the post-backward hook, we typically call `_free_full_params()` and `_free_mp_shard()`. The `_free_mp_shard()` is a no-op for `NO_SHARD` and if `reshard_after_forward=True`.

Some comments:
- The code certainly works, but some of the no-ops are subtle. When possible, we should make it clear when calls are no-ops or not. It is good that the existing code documents that `_free_mp_shard()` is a no-op in the post-backward hook when `reshard_after_forward=True`. However, there are still some non-obvious no-ops (around `NO_SHARD`).
- We should see if we can avoid the duplicate `_free_full_params_and_use_local_shard()`.

Let us trace through the logical reshard:
1. `FullyShardedDataParallel._reshard(self, handles: List[FlatParamHandle], free_unsharded_flat_params: List[bool])`
    - The two args should have the same length since they are to be zipped.
    - The goal of having `free_unsharded_flat_params` is that the caller should be explicit about whether the (padded) unsharded flattened parameter should be freed. The low precision shard is always meant to be freed (as early as possible), so there is no corresponding `List[bool]`.
2. `FlatParamHandle.reshard(self, free_unsharded_flat_param: bool)`
    - This frees the (padded) unsharded flattened parameter if `free_unsharded_flat_param` and switches to using the sharded flattened parameter.
    - Echoing back to forcing full precision in `summon_full_params()`, `_free_unsharded_flat_param()` frees the correct tensor by using `_get_padded_unsharded_flat_parameter()`.
3. `FlatParamHandle.post_reshard(self)`
    - I am not fully content with the existence of this method, but this seems to be an unavoidable consequence of `NO_SHARD`. Perhaps, this may be useful in the future for other reasons though.
    - Right now, this method is only meaningful for `NO_SHARD` + parameter mixed precision + outside `summon_full_params()`. `_mp_shard` is not freed in the post-unshard since it is also the low precision _unsharded_ flattened parameter, so we must delay the free until the the post-reshard.

Below the `FlatParamHandle.reshard()` and `post_reshard()` layer, there should not be any no-ops.

One final comment I will mention is that I like the `pre_unshard()`, `unshard()`, `post_unshard()`, and `reshard()`, `post_reshard()` organization because it makes it clear what the boundaries are and their temporal relationship. Through that, we can set pre- and post-conditions. Furthermore, we can eventually convert logic to hooks that may be registered on the `FlatParamHandle` (for `pre_unshard()`, `post_unshard()`, and `post_reshard()`). This may improve the customizability of FSDP.

 ## FSDP Runtime: `forward()`

- This PR reorganizes `forward()` in preparation for non-recursive wrapping, which uses pre-forward and post-forward hooks that expect the signature `hook(module, input)`. For FSDP, the `module` and `input` arguments are not used.
- This PR creates a new method `_fsdp_root_pre_forward()` to handle the logic only the root FSDP should run.

## FSDP Prefetching

Finally, we dive into the prefetching changes. Some highlights:
1. This PR unifies the execution order validation and prefetching implementations.
    - Both involve the execution order and can be unified to share some boilerplate.
2. Execution order validation only runs when the distributed debug level is `INFO`.
    - We have yet to have one success case where we actually catch an unintended source of dynamism. The warning is also too verbose. Hence, we are gating it by the `INFO` level.
3. This PR moves prefetching to be with respect to groups of handles (as mentioned in the constructor comment).
    - This is essential for supporting prefetching with non-recursive wrapping.
4. This PR does not include ""bubbles"", i.e. modules with no handles, in the recorded execution order(s). This deviates from the existing implementation.
    - This makes prefetching possibly more aggressive (when there are such bubbles), but it should not have significant performance implications either way.
5. This PR changes backward prefetching to reset the post-forward order each iteration (as intended).
6. This PR changes forward prefetching to use the first iteration's pre-forward order instead of the first iteration's post-forward order. (We can discuss whether we want this in this PR or not. Otherwise, I can keep it as using the post-forward order to preserve the existing semantics.) This PR also removes the `all_gather_stream.wait_stream(current_stream)` before forward prefetching because it does not help with high GPU reserved memory. We can add that back if desired.

### Appendix
#### Reverse Post-Forward Order Is Not Always the Pre-Backward Order
The existing PT-D FSDP pre-backward prefetching uses the reverse post-forward order.
<details>
  <summary>Model Code</summary>

  ```
  class Model(nn.Module):
    def __init__(self):
        super().__init__()
        self.block1 = nn.Sequential(
            nn.Conv2d(3, 4, kernel_size=3),
            nn.BatchNorm2d(4),
            nn.ReLU(inplace=True),
        )
        self.block2 = nn.Sequential(
            nn.Conv2d(4, 4, kernel_size=3),
            nn.BatchNorm2d(4),
            nn.ReLU(inplace=False),
        )
        self.block3 = nn.Linear(12, 8)
        self.head = nn.Sequential(
            nn.AdaptiveAvgPool2d(output_size=(1, 1)),
            nn.Flatten(),
            nn.Linear(4, 10),
        )

    def forward(self, x):
        x = self.block1(x)
        x = self.block2(x)
        x = self.block3(x)
        return self.head(x)

  model = Model().cuda()
  fsdp_kwargs = {}
  model.block1[1] = FSDP(model.block1[1], **fsdp_kwargs)  # BN2d
  model.block2[1] = FSDP(model.block2[1], **fsdp_kwargs)  # BN2d
  model.block1 = FSDP(model.block1, **fsdp_kwargs)
  model.block2 = FSDP(model.block2, **fsdp_kwargs)
  model.block3 = FSDP(model.block3, **fsdp_kwargs)
  model = FSDP(model, **fsdp_kwargs)
  ```
</details>

<details>
  <summary>Execution Orders </summary>

  ```
  Pre-backward hook for ('head.2.weight', 'head.2.bias') 140339520587136 (model)
  Pre-backward hook for ('weight', 'bias') 140339461194656 (block3)
  Pre-backward hook for ('0.weight', '0.bias') 140339520589776 (block2)
  Pre-backward hook for ('weight', 'bias') 140339520587664 (block2 BN)
  Pre-backward hook for ('weight', 'bias') 140339520586656 (block1 BN)
  Pre-backward hook for ('0.weight', '0.bias') 140339520588768 (block1)

  Pre-forward order:
  ('head.2.weight', 'head.2.bias') 140339520587136 (model)
  ('0.weight', '0.bias') 140339520588768 (block1)
  ('weight', 'bias') 140339520586656 (block1 BN)
  ('0.weight', '0.bias') 140339520589776 (block2)
  ('weight', 'bias') 140339520587664 (block2 BN)
  ('weight', 'bias') 140339461194656 (block3)

  Reverse post-forward order:
  ('head.2.weight', 'head.2.bias') 140339520587136 (model)
  ('weight', 'bias') 140339461194656 (block3)
  ('0.weight', '0.bias') 140339520589776 (block2)
  ('weight', 'bias') 140339520587664 (block2 BN)
  ('0.weight', '0.bias') 140339520588768 (block1)
  ('weight', 'bias') 140339520586656 (block1 BN)
  ```
</details>

Differential Revision: [D39293429](https://our.internmc.facebook.com/intern/diff/D39293429)
Pull Request resolved: https://github.com/pytorch/pytorch/pull/83665
Approved by: https://github.com/zhaojuanmao"
pytorch/pytorch,7e900f204f8494ab52f4ad089608c8cb008a273c,"Avoid throwing an exception when ScriptList doesn't match. (#84921)

This prevents 'catch throw' gdb breakpoint pollution and
should also improve performance.

Signed-off-by: Edward Z. Yang <ezyang@fb.com>
Pull Request resolved: https://github.com/pytorch/pytorch/pull/84921
Approved by: https://github.com/Chillee"
pytorch/pytorch,9d5b3e4da8723bbac6879fc7cae8f27177f0c26d,"[FSDP] Remove `forward_prefetch` (#84600)

We are removing the `forward_prefetch` option. By the nature of async GPU kernel execution, launching the CPU kernel for the next layer's all-gather early does not actually improve performance. Moreover, the existing `forward_prefetch` uses the post-forward order instead of the pre-forward order, which leads to mis-targeted prefetched all-gathers.

Differential Revision: [D39454217](https://our.internmc.facebook.com/intern/diff/D39454217)
Pull Request resolved: https://github.com/pytorch/pytorch/pull/84600
Approved by: https://github.com/zhaojuanmao"
pytorch/pytorch,dc865bff4e9de5e02ea27d0b702a74c2bf63f02f,"[Profiler] set_class util (part 1 of Record Optimizer) (#84779)

Summary:
Part 1 of Record Optimizer param_groups and states (https://github.com/pytorch/pytorch/pull/84063)
- nnModule and Optimizer have duplicated parts
- create a util function to avoid duplication

Test Plan: buck run mode/opt //caffe2/test:profiler

Differential Revision: D39397210

Pull Request resolved: https://github.com/pytorch/pytorch/pull/84779
Approved by: https://github.com/robieta"
pytorch/pytorch,a2cccb2d6b0461b7f9c9922096a74240225ebc7b,"add oneDNN graph fuser context API and unittest (#82491)

### Description
Add oneDNN graph context manager API to be consistent with other fusers.

NNC and nvFuser have two ways to use: 1) a function to enable/disable and 2) a context manager. And the later way is used extensively in libraries like Dynamo. Currently oneDNN Graph fuser only has the former way. To promote the usage of oneDNN graph fuser, this PR creates the context manager for oneDNN graph fuser.

This PR should not affect any performance.

### Testing
A unit-test `test_context_manager` is added under `test/test_jit_llga_fuser.py`

Pull Request resolved: https://github.com/pytorch/pytorch/pull/82491
Approved by: https://github.com/malfet"
pytorch/pytorch,034f2db1fdb253421e79bf36edca5423fd390e3a,"Revert ""Delete SymIntArrayRef wrapper struct (#84837)""

This reverts commit 9c78f599e40eac7fee027d86e03af06e251705b5.

Reverted https://github.com/pytorch/pytorch/pull/84837 on behalf of https://github.com/ZainRizvi due to The test test_post_localSGD_optimizer_step_reload in the X linux-bionic-cuda11.6-py3.10-gcc7 workflow has started consistently failing since this PR was submitted"
pytorch/pytorch,5c0c8f2ce344f74849afaed88df93292cb30ce0b,"[coreml][bug] coreml gpu flag not set (#84725)

Summary:
Delegated CoreML models with cpuAndGPU flag set does not properly run models on CPU

- Fix will allow us to target models on CPU

Test Plan: brymkowski can you test this on your performance benchmarks?

Reviewed By: salilsdesai

Differential Revision: D39361382

Pull Request resolved: https://github.com/pytorch/pytorch/pull/84725
Approved by: https://github.com/jmdetloff"
pytorch/pytorch,28c830ac0725c3689fb7cd9ff293fdf4b0453941,"[FSDP] Optimizer states may be on CPU, copy them to GPU before gathering (#84708)

**Background**:
Optimizer states may not always on GPUs. Some examples include, 1.) CPU offload is enable, 2.) after lightning trainer fit() is called.

**What Does This PR Do?**
If states are not on GPUs, move them to GPUs before gathering the global states.

Differential Revision: [D39332300](https://our.internmc.facebook.com/intern/diff/D39332300/)
Pull Request resolved: https://github.com/pytorch/pytorch/pull/84708
Approved by: https://github.com/awgu"
pytorch/pytorch,591b75bf98b92acd4f3d0a1dc934198afeaa6fc1,"Redo how custom/python_custom methods on TensorImpl work (#84641)

A longstanding confusion in the implementation of fake tensor and proxy tensor is what to do about torch.ops.aten.sym_sizes and related calls. In particular, when you have a tensor that (1) has symbolic shapes and (2) has a `__torch_dispatch__` call, previously, you would always get `__torch_dispatch__` calls for sizes/strides query, *even if you didn't request it* via the dispatch kwargs in `make_wrapper_subclass`.

The reason for this is because we were previously mixing several concepts: ""I want to dispatch to Python"", ""I want to call a virtual method"" and ""I have dynamic shapes"". A single boolean variable controlled all of these things, and so it was not possible to understand inside TensorImpl what the user had actually originally requested.

In this PR, we track each of these concepts individually so that we can preserve user intent. Then, we combine these into a single ""policy"" variable that controls whether or not we can use the fastpath or not. For the policy to trigger, we only need one of the exceptional cases to be true.

Billing of changes:
* Rename `set_sizes_strides_policy` to `set_custom_sizes_strides`; in general, you cannot DIRECTLY set policy; you have to indirectly set it by the public functions.
* Some helpers for sizes and strides, since it's more complicated (as it is an enum, rather than just bools as is the case for device and layout). `matches_python_custom` is used to test the Python dispatch user ask. `matches_policy` does the policy test (only used in the user facing functions.)
* I reorged the accessor methods so that they are more logical. This makes the diff bad, so I recommend reading the final code directly.
* The default custom implementations now more reliably call their default() implementations
* As bonus refactor, I devirtualized some functions that don't need to be virtual
* `set_sym_sizes_and_strides` is renamed to `set_sizes_and_strides` to make it easier to use in template contexts; it optionally takes a storage offset now so you can set all three values at the same time. If you use the SymInt overload but there are no symbolic integers, we give you a normal resize.
* This adds `sym_storage_offset` since we had that in the symbolic shapes branch and there's no reason not to put it in (and it reduces merge conflicts)

Signed-off-by: Edward Z. Yang <ezyang@fb.com>
Pull Request resolved: https://github.com/pytorch/pytorch/pull/84641
Approved by: https://github.com/wconstab"
pytorch/pytorch,1840f24df737046d085691d230ca2fe86cccb0d2,"[FSDP] Ensure that all ranks use the same order to iterate through optimizer states (#84654)

**Background:**
Optimizer states are of the type `Dict[int, Dict[str, torch.Tensor]]` and the order of `dict.items()`  is the creation order of keys. Without checkpoint (state_dict/load_state_dict), the creation order of keys depends on the implementation of the optimizer (e.g., Adam seems to creates `exp_avg` then `exp_avg_sq`). However, when loading states from a checkpoint, since the optimizer state are lazily initialized, the order depends on the user code (reading state_dict from IO). See the following example:

```
optimizer_state_dict = USER_CODE_TO_READ_STATE_FROM_IO()
optimizer.load_state_dict(optimizer_state_dict)
```
The key order of `optimizer_state_dict` depends on `USER_CODE_TO_READ_STATE_FROM_IO` and there is no guarantee that the order is the same across ranks.

**What Can Go Wrong?**
After the first checkpoint load, the key order of optimizer may not be the same on different ranks. When users try to save another checkpoint, user will call `_unflatten_optim_state()` to save the optimizer states. Inside `_unflatten_optim_state()`, `dict.itmes()` will be called to iterate all the local optimizer state and `all_gather()` will be used to gather the local states. Since the order may be different across ranks, the gathered states are not correct.

We have seen some models get NaN loss after the second checkpoint load because of this issue.

**What This PR Does?**
This PR implements a `sorted_items()` to return sorted `(key, value)` pairs. We can do this because the key is either an integer or a string.

Differential Revision: [D39315184](https://our.internmc.facebook.com/intern/diff/D39315184/)
Pull Request resolved: https://github.com/pytorch/pytorch/pull/84654
Approved by: https://github.com/awgu"
pytorch/pytorch,7a5d5a00207e91d5a7d1820781109a989aadc86c,"Disable Transformer/MHA fast path when autocast is enabled (#84722)

Differential Revision: D39362298

Pull Request resolved: https://github.com/pytorch/pytorch/pull/84722
Approved by: https://github.com/cpuhrsch"
pytorch/pytorch,2feb31cb269bd640ff2858ebe8adb3fb0aec8dc0,"Improve torch::jit::as_{module,object} performance (#84399)

This caches the import of `torch.jit.ScriptModule`,
`torch.ScriptObject` and `torch.jit.RecursiveScriptClass`. I measure
a ~0.8 us performance uplift locally when calling a `torch.ops`
function with a `ScriptObject` argument.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/84399
Approved by: https://github.com/ezyang"
pytorch/pytorch,19e27b15562b261e87e3e629cb32cb6876b9caca,"Make dispatcher registrations of SymInt functions backwards compatible (#84557)

Previously, when we SymInt-ify a schema, this is a BC-breaking change
for all people who registered functions for that function; they
must accept c10::SymInt where they previously accepted int64_t.
This is not great.

With this change, I accept old type registrations transparently.  The
idea is in several parts:

- At the registration site, at compile time I have no idea whether or not
  if the function being registered has a SymInt schema or not.  So I
  must defer the exact compatibility check.  What I do instead is
  check if the function pointer registered to me has SymInt in the
  argument or not.  If it does, I assume it is new-style and ensure
  it is also registered to a special sym_ slot on KernelFunction.
  If not, it only goes in the conventional slot.

- At the dispatcher site, I know at compile time whether or not this
  is a SymInt function.  If it is, I check for a sym_ slot on the
  KernelFunction, and preferentially use that.  If no such slot
  exists, I then fall back to the regular slot... but I convert
  all SymInt arguments to int64_t arguments (doing assertions that
  no true symbolic integer was passed.)  I can skip this test entirely
  if the function doesn't have any SymInts in it; in that case I know
  that only the original slot could have been registered. Fortunately,
  both branches of the short circuit typecheck, so I didn't have to
  use SFINAE or if-constexpr to make it work; just a plain if statement
  that I expect the compiler to optimize away.

- Schema validation is now modestly more complicated. There are two parts. First, function schema validation proceeds by checking if the signature in question has any SymInt-like types in it or not. If it does, we do function schema validation against the real types; if it doesn't, we do validation against the fake types (but only for symint; MemoryFormat is always MemoryFormat). Second, cpp signature validation also keeps track of a ""symint"" cpp signature and a ""non-symint"" cpp signature. We only compare symint with symint, and non-symint with non-symint. I did not implement checking a conflict between a symint and non-symint cpp signature, though in principle you could try converting the SymInt types to non-SymInt types and doing the comparison that way.

To show it is working, I remove a bunch of c10::asIntArrayRefSlow shims, as the dispatcher is able to insert them automatically now.

I didn't update the Metal registrations (though they can get similar treatment) as OSS CI coverage is insufficient for this case.

Signed-off-by: Edward Z. Yang <ezyang@fb.com>

Differential Revision: [D39280965](https://our.internmc.facebook.com/intern/diff/D39280965)
Pull Request resolved: https://github.com/pytorch/pytorch/pull/84557
Approved by: https://github.com/wconstab"
pytorch/pytorch,3eb16509c761c41f50163d404428246ea117c7fd,"optimize householder product backward to be more memory-efficient (#84627)

A follow-up on discussions in https://github.com/pytorch/pytorch/pull/84180.
Makes backward more memory efficient with the lesser number of kernel calls.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/84627
Approved by: https://github.com/kshitij12345, https://github.com/zou3519"
pytorch/pytorch,189768ed64561e61ff05c9e42adfa40139388204,"Add mkl implementation for exponential on CPU (#69967)

### Description
Add mkl implementation for exponential on CPU to improve the performance of exponential.

### Testing
data type: float32
single socket (28cores):
```
before: torch.Size([10, 128, 10, 124])  0.065 ms
        torch.Size([10, 128, 20, 124])  0.130 ms

after:  torch.Size([10, 128, 10, 124])  5.9e-05 ms
        torch.Size([10, 128, 20, 124])  0.000113 ms
```
single core:
```
before: torch.Size([10, 128, 10, 124])  0.065 ms
        torch.Size([10, 128, 20, 124])  0.130 ms

after:  torch.Size([10, 128, 10, 124])  0.00117 ms
        torch.Size([10, 128, 20, 124])  0.002347 ms
```
Pull Request resolved: https://github.com/pytorch/pytorch/pull/69967
Approved by: https://github.com/frank-wei"
pytorch/pytorch,d9ceda49c497bc39c2b360b038ee07e145b32f5b,"ONNX: fix default function value in _optimize_graph (#83996)

The default value for params_dict in _optimize_graph, which is None, throw the following error:

>     _C._jit_pass_onnx_unpack_quantized_weights(
> TypeError: _jit_pass_onnx_unpack_quantized_weights(): incompatible function arguments. The following argument types are supported:
>     1. (arg0: torch::jit::Graph, arg1: Dict[str, IValue], arg2: bool) -> Dict[str, IValue]

Replacing it by an empty dict fixes the issue (and makes more sense).

Pull Request resolved: https://github.com/pytorch/pytorch/pull/83996
Approved by: https://github.com/BowenBao"
pytorch/pytorch,02da9437b0ed501c2403e133b8c81eab5802c586,"Store SymInt out of line (#84390)

swolchok reported that non-tracing usage of Tensor we are wasting a lot
of time on is_symbolic() tests, e.g., when destructing SymInts.  This
is a regression for no good reason because we don't actually ever
have SymInts in those cases.  This PR moves the stored SymInts on
Tensor out of line, into a separate ExtraMeta struct, which is only
allocated when we make a Tensor store symbolic sizes/strides.

To avoid adding another word to TensorImpl, I take over the named tensor
metadata field.  This makes named tensor require a double indirection
and use up more space, but it's OK since we're going to delete this
feature anyway soon.

I restore regular int64_t storage on Tensor.  This entailed reverting
https://github.com/pytorch/pytorch/pull/82467 ; there are no other
substantive changes to SizesAndStrides so a close review is not
necessary.

I don't bother optimizes sizes and strides in ExtraMeta in the same
way stock tensor is optimized.  I add a SymDimVector alias.  I make
SymInt UNCHECKED constructor public as it is a useful optimization
in some situations when the int is known to be positive.

I thought about storing the SymInts on the Python object instead.
However, because we can allocate symbolic shape tensors directly
from C++, we cannot guarantee that there is a PyInterpreter for
a Tensor. So we do it this way instead; it's also faster since you
don't have to take out the GIL to do accesses.

Signed-off-by: Edward Z. Yang <ezyang@fb.com>
Pull Request resolved: https://github.com/pytorch/pytorch/pull/84390
Approved by: https://github.com/swolchok, https://github.com/Krovatkin"
pytorch/pytorch,7243264c61a15446bad0fcd412a1fee1bc08ec1e,"fix: Allowed optimizers with more than 2 betas (#84486)

Hello there :wave:

As discussed in #84485, this PR enables more flexibility on the optimizers that are wrapped by LR schedulers in PyTorch. Currently, it is incompatible with optimizers that have a number of betas different than 2. This PR fixes that with minimal modifications.

Fixes #84485

Any feedback is welcome!

Pull Request resolved: https://github.com/pytorch/pytorch/pull/84486
Approved by: https://github.com/Lezcano, https://github.com/soulitzer"
pytorch/pytorch,c771d73461449f89e26bc4130d1641340a03e05d,"[composite compliance] fix max_pool1d (#84127)

max_pool1d has a fast path for CPU tensors that do not require grad that
directly accesses the data_ptr. This PR makes the change that if the
input Tensor is a Tensor Subclass, then we want to walk through the
""slow path"" of calling max_pool1d_with_indices.

Test Plan:
- wait for tests
Pull Request resolved: https://github.com/pytorch/pytorch/pull/84127
Approved by: https://github.com/kshitij12345, https://github.com/samdow, https://github.com/malfet"
pytorch/pytorch,139599ba954e084ed6962dc94c99f5f2ce6ec2e7,"Contiguify bias in slow_conv_transpose3d kernel (#84125)

Users never run into this because PyTorch now comes with cudnn by
default and cudnn has a better conv_transpose implementation. However we
seem to test without cudnn in our CI; and also, ROCM goes down this
path.

The .contiguous() call does not regress anything because previously it
was a runtime error. Because this kernel is the ""slow conv transpose3d
kernel"", we don't care much for its performance.

Test Plan:
- wait for tests
Pull Request resolved: https://github.com/pytorch/pytorch/pull/84125
Approved by: https://github.com/ngimel"
pytorch/pytorch,f125bd2cbb8301c12685957ace573c301e1056e2,"Support torch.ScriptObject in torch::jit::as_object (#84398)

When a torchbind class is returned from an operator, it has the class
`torch.ScriptObject`, yet the `torch.ops` interface checks against
`torch.jit.RecursiveScriptClass` or else falls back to a much slower
path that doesn't return the original c++ object.

On my machine I see a 2 us performance improvement when calling a
`torch.ops` function with a `ScriptObject` argument.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/84398
Approved by: https://github.com/ezyang"
pytorch/pytorch,3dfbf09afebc067f5ddea60f7db5cd2aa0b98f93,"Optimise the decomposition for `adaptive_avg_pool2d` wrt. TorchInductor (#84483)

This fixes some part of the implementation that did not work with
TorchInductor (e.g. the indices in TorchInductor need to be `int64`s,
while in PyTorch we can have `int32`s).

It also brings up the performance of the kernel to similar numbers than
those of the lowering (benchmarks below).
Pull Request resolved: https://github.com/pytorch/pytorch/pull/84483
Approved by: https://github.com/jansel"
pytorch/pytorch,65beff5acb0d7c0c484bd0558bcaf8ddc9c96aab,"Dispatch torch.norm to linalg.vector_norm and linalg.matrix_norm (#81761)

`torch.norm` is very odd. Some notable issues are:

- The default value of `""fro""` in `torch.norm` has an odd behaviour when `dim=None`. This is handled in the new dispatch
- The treatment of the `dtype` argument in `torch.norm` was completely wrong. This should fix it
- Some `out=` variants in the previous implementation were also wrong. This should fix those.
- This new dispatch should make some paths much faster. For example, `torch.norm(x)` where `x` is complex.

I'll try to make the changes in these PRs as incremental as possible as this is a tricky one.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/81761
Approved by: https://github.com/ngimel"
pytorch/pytorch,a5a01e443ce1dd8e31ef7d0b3fd6a2359881a922,"Dense->BSR performance improvment (#83085)

Applies the algorithm for re-batching compressed indices to avoid n-batch kernel launches. This is an optimization for `dim()>= 3` inputs and does not change behavior in any way.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/83085
Approved by: https://github.com/bhosmer, https://github.com/nikitaved"
pytorch/pytorch,5e5c610a587d671044303c4fa56af20f33eee5dd,"Move slow-grad checks to CUDA-11.6 (#84313)

Mitigates #84192 by skipping two tests

Please note: We tried to increase the tolerance for test_fn_gradgrad_linalg_det_singular_cuda_float64 but this did not help.
Ref:
Increase `test_fn_gradgrad_linalg_det_singular_cuda_float64` error tolerance to  1e-4 as suggested in https://github.com/pytorch/pytorch/issues/84192#issuecomment-1230644574

Pull Request resolved: https://github.com/pytorch/pytorch/pull/84313
Approved by: https://github.com/malfet, https://github.com/huydhn, https://github.com/Lezcano"
pytorch/pytorch,90d6112a948644dac77120cfcf1de9ac5566ab79,"Test distributed backends in parallel (#84034)

This allows multiple backends (nccl, gloo) to be tested in parallel and speed up the process. The improvement is mainly in the 1st distributed CUDA shard where the long pole `distributed/test_distributed_spawn` test is executed:

* [linux-bionic-cuda11.6-py3.10-gcc7 / test (distributed, 1, 2, linux.8xlarge.nvidia.gpu)](https://github.com/pytorch/pytorch/runs/8007596825?check_suite_focus=true#logs) takes 1h24m. This is better than the current average expectation of 2h12m

On the other hand, there is no improvement for the following two jobs:

* [linux-focal-py3.7-gcc7 / test (distributed, 1, 1, linux.2xlarge)](https://github.com/pytorch/pytorch/runs/8007417353?check_suite_focus=true#logs) takes 1h47m
* [linux-bionic-cuda11.6-py3.10-gcc7 / test (distributed, 2, 2, linux.8xlarge.nvidia.gpu)](https://github.com/pytorch/pytorch/runs/8007596870?check_suite_focus=true#logs) takes 1h40m

This is still a gain though because it allows us to add more shards for distributed test if needed.

Issue https://github.com/pytorch/pytorch/issues/83694
Pull Request resolved: https://github.com/pytorch/pytorch/pull/84034
Approved by: https://github.com/wanchaol"
pytorch/pytorch,744019ece76aef07c38e64dcb53a9801c5b51d49,"[AIBench] Pass Vulkan Profiling Data to Kineto Profiler in lite_predictor_benchmark (#84185)

Summary: This lets us more easily analyze operator-level performance of models run with Vulkan

Test Plan: Generated chrometrace with vulkan events recorded

Reviewed By: kimishpatel

Differential Revision: D38280587

Pull Request resolved: https://github.com/pytorch/pytorch/pull/84185
Approved by: https://github.com/SS-JIA"
pytorch/pytorch,3ae5be74ac7aa4feed6ec8e7c29b280b148651a7,"Test distributed backends in parallel (#84034)

This allows multiple backends (nccl, gloo) to be tested in parallel and speed up the process. The improvement is mainly in the 1st distributed CUDA shard where the long pole `distributed/test_distributed_spawn` test is executed:

* [linux-bionic-cuda11.6-py3.10-gcc7 / test (distributed, 1, 2, linux.8xlarge.nvidia.gpu)](https://github.com/pytorch/pytorch/runs/8007596825?check_suite_focus=true#logs) takes 1h24m. This is better than the current average expectation of 2h12m

On the other hand, there is no improvement for the following two jobs:

* [linux-focal-py3.7-gcc7 / test (distributed, 1, 1, linux.2xlarge)](https://github.com/pytorch/pytorch/runs/8007417353?check_suite_focus=true#logs) takes 1h47m
* [linux-bionic-cuda11.6-py3.10-gcc7 / test (distributed, 2, 2, linux.8xlarge.nvidia.gpu)](https://github.com/pytorch/pytorch/runs/8007596870?check_suite_focus=true#logs) takes 1h40m

This is still a gain though because it allows us to add more shards for distributed test if needed.

Issue https://github.com/pytorch/pytorch/issues/83694
Pull Request resolved: https://github.com/pytorch/pytorch/pull/84034
Approved by: https://github.com/wanchaol"
pytorch/pytorch,71369051ee99f679cbb026b571e2521e3845a93e,"[Nested Tensor] fix from_padded bug (#84217)

Fixes #84082

Explained in the issue that the problem was arising from grad being not contiguous and the fast kernel not handiling this case gracefully.  The other thing I can do is add a contiguous call to https://github.com/pytorch/pytorch/blob/d144594512e10ab2a9625347816c2dee1fb55667/aten/src/ATen/native/nested/cuda/NestedTensorTransformerFunctions.cpp#L45

Pull Request resolved: https://github.com/pytorch/pytorch/pull/84217
Approved by: https://github.com/albanD"
pytorch/pytorch,7088a98fba3a5031a2afc293cbf25cec09f248a5,"conv2d: require bias to have the same dtype as input and weight on cpu (#83686)

Fixes https://github.com/pytorch/pytorch/issues/83505

BC-breaking message:
- Previously we only required input and weight to have the same dtype on cpu (when input is non-complex). After this change, the dtype of bias is now also expected to have the same dtype. This change was necessary to improve the error message for certain combinations of inputs. This behavior now also matches that of convolution on cuda.

<details>
<summary>
Old plan
</summary>
Previously convolution (at least for slow_conv2d) did not perform type promotion, i.e. the output of `conv(int, int, float)` is an int, and that leads to the autograd assert.

This PR adds type promotion handling at the `at::native::conv2d` (this is a composite) level. We also need to correct or remove many tests that assume that conv errors when input types are mixed

Pros:
- Doing type promotion at this level avoids the complex path from having any special handling for mixed dtypes, and can potentially speed up mixed dtype inputs to now dispatch to faster kernels which are only capable of handling floats.

Cons:
- Doing type promotion at this level has the risk of introducing extra overhead when we would've dispatched to a kernel capable of handle mixed type anyway. I don't know if any of these exist at all though - it is possible that inputs with any non-float arguments are dispatched to the slow path.

If this approach is OK, we can proceed with the other convolutions as well:
</details>
Pull Request resolved: https://github.com/pytorch/pytorch/pull/83686
Approved by: https://github.com/ngimel"
pytorch/pytorch,3aae6ff1e13128412e44a69ca3da5582f17fac02,"Add nvprims.var_mean (#83508)

This PR adds nvfuser-specific primitive - `var_mean`.
Interpretation `torch.var_mean` -> `torch.ops.nvprims.var_mean` is handled by `TorchRefsNvfuserCapabilityMode` context manager.

I moved some helper code from `_prims/__init__.py` to `_prims_common`. Correctness is tested with OpInfo tests (see `PythonRefInfo(""ops.nvprims.var_mean""`).

Layer norm reference now uses `torch.var_mean` instead of `torch._refs.var_mean` to allow interception. Here's a simple comparison of performance with this PR and master (on 3080ti):
```py
import torch
from torch._prims.context import TorchRefsNvfuserCapabilityMode
from torch.fx.experimental.proxy_tensor import make_fx
from torch._prims.executor import execute

def func(a):
    return torch.native_layer_norm(a, (1024,), None, None, 1e-6)

a = torch.randn(10, 512, 1024, dtype=torch.float16, device=""cuda"")

with TorchRefsNvfuserCapabilityMode():
    gm = make_fx(func)(a)

for _ in range(10):
    execute(gm, a, executor=""strictly_nvfuser"");
```
run with `PYTORCH_NVFUSER_DUMP=dump_eff_bandwidth python script.py`
```py
# WITH THIS PR
# kernel1 run in 0.032768 ms, achieved: 641.25 GB/s
# kernel1 run in 0.033792 ms, achieved: 621.818 GB/s
# kernel1 run in 0.032768 ms, achieved: 641.25 GB/s
# kernel1 run in 0.032608 ms, achieved: 644.396 GB/s
# kernel1 run in 0.031744 ms, achieved: 661.935 GB/s
# kernel1 run in 0.031744 ms, achieved: 661.935 GB/s
# kernel1 run in 0.032768 ms, achieved: 641.25 GB/s
# kernel1 run in 0.03072 ms, achieved: 684 GB/s
# kernel1 run in 0.031744 ms, achieved: 661.935 GB/s
# kernel1 run in 0.031744 ms, achieved: 661.935 GB/s

# ON MASTER
# kernel1 run in 0.05632 ms, achieved: 373.091 GB/s
# kernel1 run in 0.044032 ms, achieved: 477.209 GB/s
# kernel1 run in 0.044032 ms, achieved: 477.209 GB/s
# kernel1 run in 0.044032 ms, achieved: 477.209 GB/s
# kernel1 run in 0.043808 ms, achieved: 479.649 GB/s
# kernel1 run in 0.043008 ms, achieved: 488.571 GB/s
# kernel1 run in 0.044032 ms, achieved: 477.209 GB/s
# kernel1 run in 0.043008 ms, achieved: 488.571 GB/s
# kernel1 run in 0.043008 ms, achieved: 488.571 GB/s
# kernel1 run in 0.043008 ms, achieved: 488.571 GB/s
```
So this PR gives about 35% improvement in performance using nvfuser executor with this specific normalized shape.

Also this PR fixes https://github.com/pytorch/pytorch/issues/83506 (see the change in `torch/csrc/jit/python/pybind_utils.cpp`).

Ref. https://github.com/pytorch/pytorch/issues/80187

Pull Request resolved: https://github.com/pytorch/pytorch/pull/83508
Approved by: https://github.com/ngimel"
pytorch/pytorch,7e7694b6615fbf46abfab234615fa891c2819eb7,"Add nvprims.var_mean (#83508)

This PR adds nvfuser-specific primitive - `var_mean`.
Interpretation `torch.var_mean` -> `torch.ops.nvprims.var_mean` is handled by `TorchRefsNvfuserCapabilityMode` context manager.

I moved some helper code from `_prims/__init__.py` to `_prims_common`. Correctness is tested with OpInfo tests (see `PythonRefInfo(""ops.nvprims.var_mean""`).

Layer norm reference now uses `torch.var_mean` instead of `torch._refs.var_mean` to allow interception. Here's a simple comparison of performance with this PR and master (on 3080ti):
```py
import torch
from torch._prims.context import TorchRefsNvfuserCapabilityMode
from torch.fx.experimental.proxy_tensor import make_fx
from torch._prims.executor import execute

def func(a):
    return torch.native_layer_norm(a, (1024,), None, None, 1e-6)

a = torch.randn(10, 512, 1024, dtype=torch.float16, device=""cuda"")

with TorchRefsNvfuserCapabilityMode():
    gm = make_fx(func)(a)

for _ in range(10):
    execute(gm, a, executor=""strictly_nvfuser"");
```
run with `PYTORCH_NVFUSER_DUMP=dump_eff_bandwidth python script.py`
```py
# WITH THIS PR
# kernel1 run in 0.032768 ms, achieved: 641.25 GB/s
# kernel1 run in 0.033792 ms, achieved: 621.818 GB/s
# kernel1 run in 0.032768 ms, achieved: 641.25 GB/s
# kernel1 run in 0.032608 ms, achieved: 644.396 GB/s
# kernel1 run in 0.031744 ms, achieved: 661.935 GB/s
# kernel1 run in 0.031744 ms, achieved: 661.935 GB/s
# kernel1 run in 0.032768 ms, achieved: 641.25 GB/s
# kernel1 run in 0.03072 ms, achieved: 684 GB/s
# kernel1 run in 0.031744 ms, achieved: 661.935 GB/s
# kernel1 run in 0.031744 ms, achieved: 661.935 GB/s

# ON MASTER
# kernel1 run in 0.05632 ms, achieved: 373.091 GB/s
# kernel1 run in 0.044032 ms, achieved: 477.209 GB/s
# kernel1 run in 0.044032 ms, achieved: 477.209 GB/s
# kernel1 run in 0.044032 ms, achieved: 477.209 GB/s
# kernel1 run in 0.043808 ms, achieved: 479.649 GB/s
# kernel1 run in 0.043008 ms, achieved: 488.571 GB/s
# kernel1 run in 0.044032 ms, achieved: 477.209 GB/s
# kernel1 run in 0.043008 ms, achieved: 488.571 GB/s
# kernel1 run in 0.043008 ms, achieved: 488.571 GB/s
# kernel1 run in 0.043008 ms, achieved: 488.571 GB/s
```
So this PR gives about 35% improvement in performance using nvfuser executor with this specific normalized shape.

Also this PR fixes https://github.com/pytorch/pytorch/issues/83506 (see the change in `torch/csrc/jit/python/pybind_utils.cpp`).

Ref. https://github.com/pytorch/pytorch/issues/80187

Pull Request resolved: https://github.com/pytorch/pytorch/pull/83508
Approved by: https://github.com/ngimel"
pytorch/pytorch,3fae89d4a468a02be501357eb123ce2bf7086d2f,"Read via FileAdapter when loading files in torch if not flatbuffer (#84028)

Summary: This will optimize memory usage at the small cost of loading time when loading mobile models restoring the behavior before D36926217 (https://github.com/pytorch/pytorch/commit/fed12ff680813c0fab7dba7232f6b4cd8b33b8d3).

Test Plan: Signals

Reviewed By: qihqi

Differential Revision: D38998858

Pull Request resolved: https://github.com/pytorch/pytorch/pull/84028
Approved by: https://github.com/qihqi, https://github.com/cccclai"
pytorch/pytorch,f5a3515083cce8335913e1985b54d5e6ead95498,"Make linalg.inv composite of linalg.solve (#80074)

The `getri` kernel calls inside `getrs` so we can do so explicitly
ourselves and save ourselves from having to maintain an extra kernel.
This way we just need to optimise `lu_factor` and `lu_solve` and `inv`
will be as efficient as it can be, as it'll be choosing the best backend
to perform the factorisation and the best backend (not necessarily the
same) to perform the solve.

Fixes https://github.com/pytorch/pytorch/issues/77498

The benchmarks: https://github.com/pytorch/pytorch/pull/80074#issuecomment-1164309071
Pull Request resolved: https://github.com/pytorch/pytorch/pull/80074
Approved by: https://github.com/IvanYashchuk, https://github.com/albanD, https://github.com/malfet"
pytorch/pytorch,02c3781332031981988cd0cadfd573a210210b33,"Enable cache action for lint workflow (#84026)

Cache all python dependencies using [GHA cache](https://docs.github.com/en/actions/using-workflows/caching-dependencies-to-speed-up-workflows).  I'm doing this for lint workflow first and will slowly roll it out to other workflows.

### Testing

Before caching, pip cache is not found. Dependencies installation continues as usual:

![Screen Shot 2022-08-24 at 16 36 15](https://user-images.githubusercontent.com/475357/186543554-9d7f5978-2c2d-4362-9535-c3b17e922da1.png)

After caching https://github.com/pytorch/pytorch/runs/8006214772?check_suite_focus=true. The long hash at the end of the cache key is the hash of requirements files

![Screen Shot 2022-08-24 at 16 51 51](https://user-images.githubusercontent.com/475357/186543825-055ea025-3d42-42fc-877d-baec358de0ed.png)

Note that the cache is in the runners themselves. This should be a transparent process.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/84026
Approved by: https://github.com/seemethere, https://github.com/suo, https://github.com/malfet"
pytorch/pytorch,2000eba4547f885dc937c4335bee4ba1a71b4df5,"NCCL: Re-enable parallel builds (#83696)

Since #83173 was merged I have noticed some CI being slowed down by
the nccl building step. e.g. if there are no C++ changes then sccache
compiles everything else very quickly and nccl becomes the limiting
factor.

This re-enables parallel builds with some safeguards to protect
against oversubscription. When `make` is the parent build system, we
can use `$(MAKE)` and the `make` jobserver will coordinate job
allocation with the sub-process. For other build systems, this calls
`make` with the `-l` flag which should prevent it launching jobs when
the system load average is already too high.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/83696
Approved by: https://github.com/malfet"
pytorch/pytorch,6b597595b2fb54dcc63e25169d58a2c4602306c1,"[Quant] Vectorize scalar remainder in quantized kernel for normalization (#79673)

## Description
This PR improves performance of quantized kernel for normalize by vectorizing scalar remainder.
In the current implementation [here](https://github.com/pytorch/pytorch/blob/master/aten/src/ATen/native/quantized/cpu/kernels/QuantizedOpKernels.cpp), the computation is vectorized while the scalar remainder is handled in a `for` loop. The remainder is also vectorized to improve performance in this PR.
This kernel is for contiguous (NCHW) memory layout. For channels-last memory layout, a fast path is added in this PR https://github.com/pytorch/pytorch/pull/70520
The improvement is beneficial for layer norm, group norm and instance norm as this kernel is used for them.

## Changes
1. Add an argument `size` to `Vectorized<T>::loadu()` for vec256_qint and vec512_qint.
2. Load the remainder with the new `loadu` and do computation in the similar way as for vectorized part.

## Validation
### Test method:
Run quantized group norm with group = 2.
Op CPU time measured by `torch.profiler.profile` with warmup = 20, active = 200

### Common environment:
- Intel(R) Xeon(R) Platinum 8260 CPU @ 2.40GHz
- OS: CentOS Linux 7 (Core) (x86_64)
- Python version: 3.7.10
- Use JeMalloc memory allocator
- MALLOC_CONF=oversize_threshold:1,background_thread:true,metadata_thp:auto
- Using Intel OpenMP
- KMP_AFFINITY=granularity=fine,compact,1,0
- KMP_BLOCKTIME=1

### Case 1: AVX2
**Environment**
- GCC version: (GCC) 8.3.1 20190311 (Red Hat 8.3.1-3)
- AVX2 enabled, AVX512 disabled, i.e., vec256 used

**Run a single instance on a single core**

Shape | New impl (us) | Old impl (us) | Fp32 (us) | New/old | New/fp32 | Comments
-- | -- | -- | -- | -- | -- | --
(1, 2, 8, 5) | 3.73 | 3.75 | 4.51 | 99.41% | 82.75% | Remainder size = 8
(1, 2, 8, 6) | 3.76 | 4.00 | 4.53 | 93.93% | 82.95% | Remainder size = 16
(1, 2, 8, 7) | 3.74 | 4.01 | 4.52 | 93.34% | 82.84% | Remainder size = 24
(1, 2, 8, 8) | 3.90 | 3.96 | 4.49 | 98.49% | 87.00% | No remainder
(1, 2, 8, 17) | 4.00 | 4.17 | 4.72 | 95.83% | 84.69% | Remainder size = 8
(1, 2, 8, 18) | 4.00 | 4.23 | 4.72 | 94.54% | 84.89% | Remainder size = 16
(1, 2, 8, 19) | 4.03 | 4.29 | 4.76 | 94.01% | 84.70% | Remainder size = 24
(1, 2, 8, 20) | 3.92 | 3.93 | 4.76 | 99.67% | 82.29% | No remainder
(1, 2, 8, 33) | 4.10 | 4.18 | 5.06 | 97.92% | 81.00% | Remainder size = 8
(1, 2, 8, 34) | 4.07 | 4.23 | 5.06 | 96.40% | 80.53% | Remainder size = 16
(1, 2, 8, 35) | 4.11 | 4.42 | 5.09 | 93.03% | 80.72% | Remainder size = 24
(1, 2, 8, 36) | 4.03 | 4.06 | 5.11 | 99.24% | 78.83% | No remainder

![image](https://user-images.githubusercontent.com/12522207/173979129-e393e13f-71f5-4987-95ea-ac6e0c895bd7.png)

**Run a single instance on two cores**
Shape | New impl (us) | Old impl (us) | Fp32 (us) | New/old | New/fp32 | Comments
-- | -- | -- | -- | -- | -- | --
(1, 4, 8, 5) | 5.09 | 5.24 | 5.52 | 97.17% | 92.29% | Remainder size = 8
(1, 4, 8, 6) | 5.22 | 5.50 | 5.56 | 94.95% | 93.86% | Remainder size = 16
(1, 4, 8, 7) | 5.04 | 5.60 | 5.51 | 89.97% | 91.44% | Remainder size = 24
(1, 4, 8, 8) | 5.30 | 5.29 | 5.56 | 100.23% | 95.27% | No remainder
(1, 4, 8, 17) | 5.36 | 5.56 | 6.05 | 96.53% | 88.69% | Remainder size = 8
(1, 4, 8, 18) | 5.48 | 5.71 | 6.25 | 95.99% | 87.67% | Remainder size = 16
(1, 4, 8, 19) | 5.44 | 5.81 | 6.25 | 93.65% | 87.11% | Remainder size = 24
(1, 4, 8, 20) | 5.43 | 5.34 | 6.07 | 101.76% | 89.43% | No remainder
(1, 4, 8, 33) | 5.52 | 5.58 | 6.51 | 98.89% | 84.75% | Remainder size = 8
(1, 4, 8, 34) | 5.50 | 5.71 | 6.63 | 96.22% | 82.95% | Remainder size = 16
(1, 4, 8, 35) | 5.50 | 6.16 | 6.40 | 89.33% | 85.95% | Remainder size = 24
(1, 4, 8, 36) | 5.37 | 5.48 | 6.54 | 97.94% | 81.98% | No remainder

![image](https://user-images.githubusercontent.com/12522207/173981377-6222e278-0948-4f52-809b-28899399ca65.png)

### Case 2: AVX512
**Environment**
- GCC version: (GCC) 9.3.1 20200408 (Red Hat 9.3.1-2)
- AVX512 enabled, i.e., vec512 used

**Run a single instance on a single core**

Shape | New impl (us) | Old impl (us) | Fp32 (us) | New/old | New/fp32 | Comments
-- | -- | -- | -- | -- | -- | --
(1, 2, 16, 5) | 3.66 | 3.94 | 4.52 | 92.79% | 80.93% | Remainder size = 16
(1, 2, 16, 6) | 3.77 | 4.28 | 4.60 | 88.15% | 81.90% | Remainder size = 32
(1, 2, 16, 7) | 3.85 | 4.41 | 4.57 | 87.36% | 84.20% | Remainder size = 48
(1, 2, 16, 8) | 3.70 | 3.76 | 4.62 | 98.62% | 80.10% | No remainder
(1, 2, 16, 17) | 3.91 | 4.06 | 4.97 | 96.43% | 78.71% | Remainder size = 16
(1, 2, 16, 18) | 3.82 | 4.34 | 5.01 | 88.19% | 76.30% | Remainder size = 32
(1, 2, 16, 19) | 3.86 | 4.56 | 5.05 | 84.63% | 76.28% | Remainder size = 48
(1, 2, 16, 20) | 3.80 | 3.87 | 5.08 | 98.14% | 74.73% | No remainder
(1, 2, 16, 33) | 3.89 | 4.23 | 5.65 | 91.94% | 68.85% | Remainder size = 16
(1, 2, 16, 34) | 3.91 | 4.46 | 5.70 | 87.68% | 68.61% | Remainder size = 32
(1, 2, 16, 35) | 4.04 | 4.68 | 5.72 | 86.44% | 70.64% | Remainder size = 48
(1, 2, 16, 36) | 4.00 | 3.99 | 5.71 | 100.28% | 69.96% | No remainder

![image](https://user-images.githubusercontent.com/12522207/173982490-4687c5bc-50e8-49aa-9fe2-7967c738dbfb.png)

**Run a single instance on two cores**

Shape | New impl (us) | Old impl (us) | Fp32 (us) | New/old | New/fp32 | Comments
-- | -- | -- | -- | -- | -- | --
(1, 4, 16, 5) | 5.43 | 5.53 | 5.92 | 98.12% | 91.60% | Remainder size = 16
(1, 4, 16, 6) | 5.35 | 5.85 | 6.05 | 91.53% | 88.54% | Remainder size = 32
(1, 4, 16, 7) | 5.31 | 6.04 | 6.18 | 87.97% | 85.93% | Remainder size = 48
(1, 4, 16, 8) | 5.30 | 5.27 | 6.30 | 100.66% | 84.16% | No remainder
(1, 4, 16, 17) | 5.47 | 5.67 | 6.48 | 96.51% | 84.45% | Remainder size = 16
(1, 4, 16, 18) | 5.53 | 5.86 | 6.59 | 94.28% | 83.78% | Remainder size = 32
(1, 4, 16, 19) | 5.48 | 6.13 | 6.57 | 89.39% | 83.38% | Remainder size = 48
(1, 4, 16, 20) | 5.35 | 5.31 | 6.95 | 100.79% | 76.91% | No remainder
(1, 4, 16, 33) | 5.62 | 5.77 | 7.31 | 97.28% | 76.80% | Remainder size = 16
(1, 4, 16, 34) | 5.56 | 5.85 | 7.06 | 95.03% | 78.71% | Remainder size = 32
(1, 4, 16, 35) | 5.67 | 6.10 | 7.09 | 93.03% | 79.98% | Remainder size = 48
(1, 4, 16, 36) | 5.50 | 5.39 | 7.20 | 102.15% | 76.42% | No remainder

![image](https://user-images.githubusercontent.com/12522207/173982748-5f003630-18a4-4c3d-a643-b8711892cc39.png)

Pull Request resolved: https://github.com/pytorch/pytorch/pull/79673
Approved by: https://github.com/jerryzh168"
pytorch/pytorch,bf8d5e83289a35e64a0ce98ef36fd45ab2dd0d43,"Pretty print stack trace with gm.print_readable() (#83706)

Precondition: https://github.com/pytorch/torchdynamo/pull/899

Given following function
```
def my_relu(a):
    return a.relu()

def func(a, b):
    d = torch.square(a + b)
    e = my_relu(d)
    f = d.sin()
    s = torch.stack([e, f])
    s = s.sum()
```

Here are the possible result with various tracing frontend: dynamo, symbolic_trace, make_fx
- joint graph with torchdynamo.optimize(""aot_nop"")
Notice that it has a special stack for gradient addition node (for multiple uses of tensor) in backward
Notice that ""No stacktrace found for following nodes"" are shown for nodes with stacktrace
```
def forward(self, primals, tangents):
    primals_1, primals_2, tangents_1, = fx_pytree.tree_flatten_spec([primals, tangents], self._in_spec)

    # File ""/fsx/users/bahuang/repos/pytorch_fsx/test.py"", line 41, in func, d = torch.square(a + b)
    add_tensor = torch.ops.aten.add.Tensor(primals_1, primals_2);  primals_1 = primals_2 = None
    pow_tensor_scalar = torch.ops.aten.pow.Tensor_Scalar(add_tensor, 2)

    # File ""/fsx/users/bahuang/repos/pytorch_fsx/test.py"", line 38, in my_relu, return a.relu()
    relu_default = torch.ops.aten.relu.default(pow_tensor_scalar)
    detach_default = torch.ops.aten.detach.default(relu_default)

    # File ""/fsx/users/bahuang/repos/pytorch_fsx/test.py"", line 43, in func, f = d.sin()
    sin_default = torch.ops.aten.sin.default(pow_tensor_scalar)

    # File ""/fsx/users/bahuang/repos/pytorch_fsx/test.py"", line 44, in func, s = torch.stack([e, f])
    stack_default = torch.ops.aten.stack.default([relu_default, sin_default]);  relu_default = sin_default = None

    # File ""/fsx/users/bahuang/repos/pytorch_fsx/test.py"", line 45, in func, s = s.sum()
    sum_default = torch.ops.aten.sum.default(stack_default);  stack_default = None

    # No stacktrace found for following nodes
    is_same_size_default = torch.ops.aten.is_same_size.default(sum_default, tangents_1)

    # File ""/fsx/users/bahuang/repos/pytorch_fsx/test.py"", line 45, in func, s = s.sum()
    expand_default = torch.ops.aten.expand.default(tangents_1, [2, 10, 10]);  tangents_1 = None

    # File ""/fsx/users/bahuang/repos/pytorch_fsx/test.py"", line 44, in func, s = torch.stack([e, f])
    unbind_int = torch.ops.aten.unbind.int(expand_default);  expand_default = None
    getitem = unbind_int[0]
    getitem_1 = unbind_int[1];  unbind_int = None

    # File ""/fsx/users/bahuang/repos/pytorch_fsx/test.py"", line 43, in func, f = d.sin()
    cos_default = torch.ops.aten.cos.default(pow_tensor_scalar);  pow_tensor_scalar = None
    mul_tensor = torch.ops.aten.mul.Tensor(getitem_1, cos_default);  getitem_1 = cos_default = None

    # File ""/fsx/users/bahuang/repos/pytorch_fsx/test.py"", line 38, in my_relu, return a.relu()
    detach_default_1 = torch.ops.aten.detach.default(detach_default);  detach_default = None
    threshold_backward_default = torch.ops.aten.threshold_backward.default(getitem, detach_default_1, 0);  getitem = detach_default_1 = None

    # Gradient addition node due to mulitple use of tensor around:, File ""/fsx/users/bahuang/repos/pytorch_fsx/test.py"", line 38, in my_relu, return a.relu()
    add_tensor_1 = torch.ops.aten.add.Tensor(mul_tensor, threshold_backward_default);  mul_tensor = threshold_backward_default = None

    # File ""/fsx/users/bahuang/repos/pytorch_fsx/test.py"", line 41, in func, d = torch.square(a + b)
    pow_tensor_scalar_1 = torch.ops.aten.pow.Tensor_Scalar(add_tensor, 1.0);  add_tensor = None
    mul_scalar = torch.ops.aten.mul.Scalar(pow_tensor_scalar_1, 2.0);  pow_tensor_scalar_1 = None
    mul_tensor_1 = torch.ops.aten.mul.Tensor(add_tensor_1, mul_scalar);  add_tensor_1 = mul_scalar = None
    sum_sym_int = torch.ops.aten.sum.SymInt(mul_tensor_1, [0], True)
    view_sym_int = torch.ops.aten.view.SymInt(sum_sym_int, [10]);  sum_sym_int = None
    return pytree.tree_unflatten([sum_default, mul_tensor_1, view_sym_int], self._out_spec)
```
- default symbolic_trace
Notice that nodes without stacktrace are folded under same region
```
def forward(self, a, b):

    # No stacktrace found for following nodes
    add = a + b;  a = b = None
    square = torch.square(add);  add = None
    relu = square.relu()
    sin = square.sin();  square = None
    stack = torch.stack([relu, sin]);  relu = sin = None
    sum_1 = stack.sum();  stack = None
    return sum_1
```
- symbolic_trace with record_stack_traces=True
```
def forward(self, a, b):

    # File ""/fsx/users/bahuang/repos/pytorch_fsx/test.py"", line 41, in func, d = torch.square(a + b)
    add = a + b;  a = b = None
    square = torch.square(add);  add = None

    # File ""/fsx/users/bahuang/repos/pytorch_fsx/test.py"", line 38, in my_relu, return a.relu()
    relu = square.relu()

    # File ""/fsx/users/bahuang/repos/pytorch_fsx/test.py"", line 43, in func, f = d.sin()
    sin = square.sin();  square = None

    # File ""/fsx/users/bahuang/repos/pytorch_fsx/test.py"", line 44, in func, s = torch.stack([e, f])
    stack = torch.stack([relu, sin]);  relu = sin = None

    # File ""/fsx/users/bahuang/repos/pytorch_fsx/test.py"", line 45, in func, s = s.sum()
    sum_1 = stack.sum();  stack = None
    return sum_1
```

- make_fx without decomposition
```
def forward(self, a_1, b_1):

    # File ""/fsx/users/bahuang/repos/pytorch_fsx/test.py"", line 41, in func, d = torch.square(a + b)
    add_tensor = torch.ops.aten.add.Tensor(a_1, b_1);  a_1 = b_1 = None
    pow_tensor_scalar = torch.ops.aten.pow.Tensor_Scalar(add_tensor, 2);  add_tensor = None

    # File ""/fsx/users/bahuang/repos/pytorch_fsx/test.py"", line 38, in my_relu, return a.relu()
    relu_default = torch.ops.aten.relu.default(pow_tensor_scalar)
    detach_default = torch.ops.aten.detach.default(relu_default)

    # File ""/fsx/users/bahuang/repos/pytorch_fsx/test.py"", line 43, in func, f = d.sin()
    sin_default = torch.ops.aten.sin.default(pow_tensor_scalar);  pow_tensor_scalar = None

    # File ""/fsx/users/bahuang/repos/pytorch_fsx/test.py"", line 44, in func, s = torch.stack([e, f])
    stack_default = torch.ops.aten.stack.default([relu_default, sin_default]);  relu_default = sin_default = None

    # File ""/fsx/users/bahuang/repos/pytorch_fsx/test.py"", line 45, in func, s = s.sum()
    sum_default = torch.ops.aten.sum.default(stack_default);  stack_default = None
    return sum_default
```
- make_fx with decomposition to prims
```
def forward(self, a_1, b_1):

    # File ""/fsx/users/bahuang/repos/pytorch_fsx/test.py"", line 41, in func, d = torch.square(a + b)
    broadcast_in_dim_default = torch.ops.prims.broadcast_in_dim.default(b_1, [10, 10], [1]);  b_1 = None
    add_default = torch.ops.prims.add.default(a_1, broadcast_in_dim_default);  a_1 = broadcast_in_dim_default = None
    mul_default = torch.ops.prims.mul.default(add_default, add_default);  add_default = None

    # File ""/fsx/users/bahuang/repos/pytorch_fsx/test.py"", line 38, in my_relu, return a.relu()
    le_default = torch.ops.prims.le.default(mul_default, 0.0)
    where_default = torch.ops.prims.where.default(le_default, 0.0, mul_default);  le_default = None

    # File ""/fsx/users/bahuang/repos/pytorch_fsx/test.py"", line 43, in func, f = d.sin()
    sin_default = torch.ops.prims.sin.default(mul_default);  mul_default = None

    # File ""/fsx/users/bahuang/repos/pytorch_fsx/test.py"", line 44, in func, s = torch.stack([e, f])
    cat_default = torch.ops.prims.cat.default([where_default, sin_default], 0);  where_default = sin_default = None
    split_dim_default = torch.ops.prims.split_dim.default(cat_default, 0, 2);  cat_default = None

    # File ""/fsx/users/bahuang/repos/pytorch_fsx/test.py"", line 45, in func, s = s.sum()
    convert_element_type_default = torch.ops.prims.convert_element_type.default(split_dim_default, torch.float32);  split_dim_default = None
    sum_default = torch.ops.prims.sum.default(convert_element_type_default, [0, 1, 2]);  convert_element_type_default = None
    return sum_default
```
Pull Request resolved: https://github.com/pytorch/pytorch/pull/83706
Approved by: https://github.com/Chillee, https://github.com/ezyang"
pytorch/pytorch,e72256604f80e66b6a380479e4b610be19c82e71,"Enhance add_out_dense_sparse_cpu for hybrid sparse tensor (#23057)

This is to improve the performance for hybrid sparse coo tensor on CPU path. This case is appeared at the DLRM terabyte test.
With this fix, according to the previous performance test data, it got ~10x performance improvement on DLRM execution.
without this, the DLRM will run as
Finished training it 100/1000 of epoch 0, 2969.25 ms/it, loss 0.220505, accuracy 0.000 %
with this, the DLRM will run as
Finished training it 100/1000 of epoch 0, 270.71 ms/it, loss 0.220505, accuracy 0.000 %
Pull Request resolved: https://github.com/pytorch/pytorch/pull/23057
Approved by: https://github.com/VitalyFedyunin, https://github.com/malfet"
pytorch/pytorch,ef782e730dff7bef078fc95d7fb5b78bafcc5284,"Support BF16 for fast layernorm (#83971)

Fixes #83970

Pull Request resolved: https://github.com/pytorch/pytorch/pull/83971
Approved by: https://github.com/ngimel"
pytorch/pytorch,4737b3361479f4104efaa3bfa2ea517eaacb60fb,"Make linalg.inv composite of linalg.solve (#80074)

The `getri` kernel calls inside `getrs` so we can do so explicitly
ourselves and save ourselves from having to maintain an extra kernel.
This way we just need to optimise `lu_factor` and `lu_solve` and `inv`
will be as efficient as it can be, as it'll be choosing the best backend
to perform the factorisation and the best backend (not necessarily the
same) to perform the solve.

Fixes https://github.com/pytorch/pytorch/issues/77498

The benchmarks: https://github.com/pytorch/pytorch/pull/80074#issuecomment-1164309071
Pull Request resolved: https://github.com/pytorch/pytorch/pull/80074
Approved by: https://github.com/IvanYashchuk, https://github.com/albanD, https://github.com/malfet"
pytorch/pytorch,84e45e7e907484f300cade2ce23e5272da660e4f,"Revert ""Optimize transpose copy on CPU using fbgemm transpose (#83327)""

This reverts commit 04d8da88a6a1abf0da2b11096c85244bf38d3b2a.

Reverted https://github.com/pytorch/pytorch/pull/83327 on behalf of https://github.com/weiwangmeta due to breaking internal builds/causing out-of-bounds errors/training accuracy"
pytorch/pytorch,8db04c111363bde5c90885fee3debbd288605336,"reinplace pass: special handling for view_scatter ops (#83846)

There is already special handling in the reinplacing pass for removing `{view}_scatter` ops, but there is another case that needs special handling. In this code:
```
         def f():
             a = torch.zeros(4, 4, 4)
             a[:, 2:] = torch.ones(4, 2, 4)
             return a
```

Tracing normally with `make_fx()` gives you:
```

def forward(self):
    zeros = torch.ops.aten.zeros.default([4, 4, 4], device = device(type='cpu'), pin_memory = False)
    ones = torch.ops.aten.ones.default([4, 2, 4], device = device(type='cpu'), pin_memory = False)
    slice_tensor = torch.ops.aten.slice.Tensor(zeros, 0, 0, 9223372036854775807)
    slice_tensor_1 = torch.ops.aten.slice.Tensor(slice_tensor, 1, 2, 9223372036854775807);  slice_tensor = None
    copy__default = torch.ops.aten.copy_.default(slice_tensor_1, ones);  slice_tensor_1 = ones = None
    return zeros
```
Functionalizing it gives you:

```
def forward(self):
    zeros = torch.ops.aten.zeros.default([4, 4, 4], device = device(type='cpu'), pin_memory = False)
    ones = torch.ops.aten.ones.default([4, 2, 4], device = device(type='cpu'), pin_memory = False)
    slice_tensor = torch.ops.aten.slice.Tensor(zeros, 0, 0, 9223372036854775807)
    slice_tensor_1 = torch.ops.aten.slice.Tensor(slice_tensor, 1, 2, 9223372036854775807);  slice_tensor = None
    slice_tensor_2 = torch.ops.aten.slice.Tensor(zeros, 0, 0, 9223372036854775807)
    slice_scatter_default = torch.ops.aten.slice_scatter.default(slice_tensor_2, ones, 1, 2, 9223372036854775807);  slice_tensor_2 = ones = None
    slice_scatter_default_1 = torch.ops.aten.slice_scatter.default(zeros, slice_scatter_default, 0, 0, 9223372036854775807);  zeros = slice_scatter_default = None
    return slice_scatter_default_1
```

Notice that there are not any functional ops to directly re-inplace! What actually happened is that functionalization turned the `copy_()` into a `copy()`, but the out-of-place `copy()` operator gets optimized away because it's a no-op (when the input and output metadata are the same, `out = copy(a, b)` just returns `b`).

What we actually want is to replace this line:
```
slice_scatter_default = torch.ops.aten.slice_scatter.default(slice_tensor_2, ones, 1, 2, ...);
```
with this:
```
new_slice = torch.ops.aten.slice.Tensor(slice_tensor_2, 1, 2, ...);
_ = torch.ops.aten.copy_.default(new_slice, ones)
```

In the above, we're taking a fresh slice of the ""base"" tensor, and performing a `copy_()` on the slice, adding back what functionalization removed.

We actually need to create a fresh ""slice"" node, because we're not guaranteed that one already exists in the graph (technically there should be one, but it might have been DCE'd by the time we hit re-inplacing)

I also updated the docs for re-inplacing to more closely match the order of the logic.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/83846
Approved by: https://github.com/ezyang"
pytorch/pytorch,a315a2c79bbd25dfb022e294df8df67568b14dea,"[ROCm] restore MIOpen benchmark flag default to true (#82656)

### Description
PR https://github.com/pytorch/pytorch/pull/77438 allowed MIOpen to support the benchmark flag. Previously, the benchmark flag was ignored by MIOpen such that benchmarking was always turned on. This commit restores the behavior that MIOpen benchmarking is by default turned on.

### Testing
CI unit tests cover this capability.  Torchvision models demonstrate the performance delta.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/82656
Approved by: https://github.com/ngimel"
pytorch/pytorch,04d8da88a6a1abf0da2b11096c85244bf38d3b2a,"Optimize transpose copy on CPU using fbgemm transpose (#83327)

### Description
Optimize transpose copy on CPU using fbgemm transpose

### Testing
single socket (28cores):
```
before: torch.Size([10, 128, 10, 124]) -> torch.Size([10, 128, 124, 10]) fp32: 4.819e-05 ms; bf16: 4.846e-05 ms
        torch.Size([10, 128, 30, 124]) -> torch.Size([10, 128, 124, 30]) fp32: 0.000171 ms; bf16: 0.000129 ms

after: torch.Size([10, 128, 10, 124]) -> torch.Size([10, 128, 124, 10])  fp32: 2.439e-05 ms; bf16: 2.152e-05 ms
        torch.Size([10, 128, 30, 124]) -> torch.Size([10, 128, 124, 30]) fp32: 0.000132 ms; bf16: 3.916e-05 ms
```
single core:
```
before: torch.Size([10, 128, 10, 124]) -> torch.Size([10, 128, 124, 10]) fp32: 0.00109 ms;  bf16: 0.00103 ms
        torch.Size([10, 128, 30, 124]) -> torch.Size([10, 128, 124, 30]) fp32: 0.00339 ms; bf16: 0.00295 ms

after: torch.Size([10, 128, 10, 124]) -> torch.Size([10, 128, 124, 10]) fp32: 0.000566  ms; bf16: 0.000382 ms
        torch.Size([10, 128, 30, 124]) -> torch.Size([10, 128, 124, 30]) fp32: 0.00282 ms; bf16: 0.000999 ms
```
Pull Request resolved: https://github.com/pytorch/pytorch/pull/83327
Approved by: https://github.com/frank-wei"
pytorch/pytorch,53cda905be74e03161e6732d679be3b9cb2c65b0,"Revert ""Optimize transpose copy on CPU using fbgemm transpose (#83327)""

This reverts commit f56720ea7c7ad0bcb4c5af669e28bf7de8122cb6.

Reverted https://github.com/pytorch/pytorch/pull/83327 on behalf of https://github.com/janeyx99 due to Sorry, reverting as this breaks mac functorch tests on trunk https://hud.pytorch.org/pytorch/pytorch/commit/f56720ea7c7ad0bcb4c5af669e28bf7de8122cb6"
pytorch/pytorch,f56720ea7c7ad0bcb4c5af669e28bf7de8122cb6,"Optimize transpose copy on CPU using fbgemm transpose (#83327)

### Description
Optimize transpose copy on CPU using fbgemm transpose

### Testing
single socket (28cores):
```
before: torch.Size([10, 128, 10, 124]) -> torch.Size([10, 128, 124, 10]) fp32: 4.819e-05 ms; bf16: 4.846e-05 ms
        torch.Size([10, 128, 30, 124]) -> torch.Size([10, 128, 124, 30]) fp32: 0.000171 ms; bf16: 0.000129 ms

after: torch.Size([10, 128, 10, 124]) -> torch.Size([10, 128, 124, 10])  fp32: 2.439e-05 ms; bf16: 2.152e-05 ms
        torch.Size([10, 128, 30, 124]) -> torch.Size([10, 128, 124, 30]) fp32: 0.000132 ms; bf16: 3.916e-05 ms
```
single core:
```
before: torch.Size([10, 128, 10, 124]) -> torch.Size([10, 128, 124, 10]) fp32: 0.00109 ms;  bf16: 0.00103 ms
        torch.Size([10, 128, 30, 124]) -> torch.Size([10, 128, 124, 30]) fp32: 0.00339 ms; bf16: 0.00295 ms

after: torch.Size([10, 128, 10, 124]) -> torch.Size([10, 128, 124, 10]) fp32: 0.000566  ms; bf16: 0.000382 ms
        torch.Size([10, 128, 30, 124]) -> torch.Size([10, 128, 124, 30]) fp32: 0.00282 ms; bf16: 0.000999 ms
```
Pull Request resolved: https://github.com/pytorch/pytorch/pull/83327
Approved by: https://github.com/frank-wei"
pytorch/pytorch,e0f2eba93d2804d22cd53ea8c09a479ae546dc7f,"Move odd num_head in TransformerEncoder to slow_path (#83483)

Summary: odd nhead is not supported for masked softmax, therefore we just move it to use old slow_path

Test Plan: CI

Differential Revision: D38720086

Pull Request resolved: https://github.com/pytorch/pytorch/pull/83483
Approved by: https://github.com/erichan1"
pytorch/pytorch,155343ef2d9e75701ddf772e942a7b76b1f80570,"Pin sphinxcontrib.katex to 0.8.6 (#83774)

sphinxcontrib.katex 0.9.0 adds a local KaTeX server to speed up pre-rendering but it doesn't seem to work and hangs around idly. The initial thought is probably something related to Docker setup. We can investigate this later.

Here is the release change log from the [sphinxcontrib-katex](https://github.com/hagenw/sphinxcontrib-katex/commit/e27a051532dee33fbe329636b042426bf3ad6e26)
Pull Request resolved: https://github.com/pytorch/pytorch/pull/83774
Approved by: https://github.com/janeyx99, https://github.com/malfet"
pytorch/pytorch,da520a43f228d4b2f5fda6ec0412080504822fc7,"[Vulkan] Fix issues in GRU and LSTM (#83722)

Summary:
This diffs fixes several issues in GRU and LSTM vulkan ops:
- Add create_gru_context and create_lstm_context to vulkanFoldPrePackingOps
- Add filter to insertPrePackedGruOp and insertPrePackedLstmOp to avoid matching gru.data and lstm.data usages
- Fixed output dimension of GRU and LSTM
- Allowed batch_first to be false when batch=1 and seq=1

Test Plan:
Check that optimize_for_mobile runs and correctly folds the create context ops
```
buck run :export_for_mobile ~/ferraris/ferraris.ptl ~/ferraris
```

Check that vulkan api tests are still passing
```
buck run //xplat/caffe2:pt_vulkan_api_test_binAppleMac\#macosx-arm64
```

Reviewed By: SS-JIA

Differential Revision: D38811967

Pull Request resolved: https://github.com/pytorch/pytorch/pull/83722
Approved by: https://github.com/SS-JIA"
pytorch/pytorch,b8d647e1d56114514b98a6fdc8f5141784b8a016,"Revert ""Manually shard slow-gradcheck CI job to prevent timeout #83354"" (#83704)

Now that https://github.com/pytorch/test-infra/pull/529 exists, we can undo the custom sharding from #83354 for slow grad check

test plan: look at logs to see if it sharded + look at time to see that its evenly distributed
Pull Request resolved: https://github.com/pytorch/pytorch/pull/83704
Approved by: https://github.com/huydhn"
pytorch/pytorch,a7baad04f6f29a97743e98d25c369b21aed18faf,"Preserve stack trace for backward nodes over AOTAutograd (#83558)

For the following program.
```
def my_relu(a):
    return a.relu()

def func(a, b):
    a = torch.nn.Linear(10, 10)(a)
    d = torch.square(b)
    d = my_relu(d)
    loss = d.sum()

    return loss

with torchdynamo.optimize(""aot_nop""):
    x = torch.rand(10, 10, requires_grad=True)
    y = torch.rand(10, 10, requires_grad=True)
    out = func(x, y)
```

It would generate the following fx graph with stack_trace populated in both forward and backward nodes.
```
def forward(self, primals, tangents):
    primals_1, primals_2, primals_3, primals_4, tangents_1, = fx_pytree.tree_flatten_spec([primals, tangents], self._in_spec)
    t_default = torch.ops.aten.t.default(primals_3);  primals_3 = None
    addmm_default = torch.ops.aten.addmm.default(primals_4, primals_1, t_default);  primals_4 = primals_1 = t_default = None
    pow_tensor_scalar = torch.ops.aten.pow.Tensor_Scalar(primals_2, 2)
    relu_default = torch.ops.aten.relu.default(pow_tensor_scalar);  pow_tensor_scalar = None
    detach_default = torch.ops.aten.detach.default(relu_default)
    sum_default = torch.ops.aten.sum.default(relu_default);  relu_default = None
    is_same_size_default = torch.ops.aten.is_same_size.default(sum_default, tangents_1)
    expand_default = torch.ops.aten.expand.default(tangents_1, [10, 10]);  tangents_1 = None
    detach_default_1 = torch.ops.aten.detach.default(detach_default);  detach_default = None
    threshold_backward_default = torch.ops.aten.threshold_backward.default(expand_default, detach_default_1, 0);  expand_default = detach_default_1 = None
    pow_tensor_scalar_1 = torch.ops.aten.pow.Tensor_Scalar(primals_2, 1.0);  primals_2 = None
    mul_scalar = torch.ops.aten.mul.Scalar(pow_tensor_scalar_1, 2.0);  pow_tensor_scalar_1 = None
    mul_tensor = torch.ops.aten.mul.Tensor(threshold_backward_default, mul_scalar);  threshold_backward_default = mul_scalar = None
    return pytree.tree_unflatten([sum_default, None, mul_tensor, None, None], self._out_spec)

====== joint graph =======
primals_1 None
primals_2 None
primals_3 None
primals_4 None
tangents_1 None
t_default   File ""/fsx/users/bahuang/repos/pytorch_fsx/test.py"", line 12, in func
    def func(a, b):
  File ""/fsx/users/bahuang/repos/pytorch_fsx/torch/nn/modules/linear.py"", line 114, in forward
    return F.linear(input, self.weight, self.bias)

addmm_default   File ""/fsx/users/bahuang/repos/pytorch_fsx/test.py"", line 12, in func
    def func(a, b):
  File ""/fsx/users/bahuang/repos/pytorch_fsx/torch/nn/modules/linear.py"", line 114, in forward
    return F.linear(input, self.weight, self.bias)

pow_tensor_scalar   File ""/fsx/users/bahuang/repos/pytorch_fsx/test.py"", line 14, in func
    d = torch.square(b)

relu_default   File ""/fsx/users/bahuang/repos/pytorch_fsx/test.py"", line 15, in func
    d = my_relu(d)
  File ""/fsx/users/bahuang/repos/pytorch_fsx/test.py"", line 10, in my_relu
    return a.relu()

detach_default   File ""/fsx/users/bahuang/repos/pytorch_fsx/test.py"", line 15, in func
    d = my_relu(d)
  File ""/fsx/users/bahuang/repos/pytorch_fsx/test.py"", line 10, in my_relu
    return a.relu()

sum_default
is_same_size_default
expand_default
detach_default_1   File ""/fsx/users/bahuang/repos/pytorch_fsx/test.py"", line 15, in func
    d = my_relu(d)
  File ""/fsx/users/bahuang/repos/pytorch_fsx/test.py"", line 10, in my_relu
    return a.relu()

threshold_backward_default   File ""/fsx/users/bahuang/repos/pytorch_fsx/test.py"", line 15, in func
    d = my_relu(d)
  File ""/fsx/users/bahuang/repos/pytorch_fsx/test.py"", line 10, in my_relu
    return a.relu()

pow_tensor_scalar_1   File ""/fsx/users/bahuang/repos/pytorch_fsx/test.py"", line 14, in func
    d = torch.square(b)

mul_scalar   File ""/fsx/users/bahuang/repos/pytorch_fsx/test.py"", line 14, in func
    d = torch.square(b)

mul_tensor   File ""/fsx/users/bahuang/repos/pytorch_fsx/test.py"", line 14, in func
    d = torch.square(b)

output None
```

Pull Request resolved: https://github.com/pytorch/pytorch/pull/83558
Approved by: https://github.com/albanD"
pytorch/pytorch,abcf01196cd27805349aa892db847f9a61f52c0e,"Release the GIL when munmap'ing tensors - fixes #77139 (#83623)

Fixes #77139, where deallocating large tensors with munmap takes a significant amount of time while holding the GIL. This causes the pin_memory thread to interfere with the main thread = performance sadness.

Thanks @igozali @zhengwy888 @colesbury as well.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/83623
Approved by: https://github.com/albanD"
pytorch/pytorch,aad89bb77176a755cf7f916b4cb16bc4a021d1bb,"Make the derivative of masked_fill more efficient (#83515)

There's no need to add all the zeros if we extract all the non-zero
elements.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/83515
Approved by: https://github.com/albanD, https://github.com/soulitzer"
pytorch/pytorch,6dc8673b1bb7a0f22a2453049751089943cc1f3b,"Update ideep for NNC post-op (#82705)

### Description
This PR is to add NNC post-op fusion support in ideep for further NNC development. It includes:

- element wise post op fusion
- conv/matmal/linear + binary post op fusion

### Performance
**Common configuration:**
- Jemalloc and iomp enabled
- BS=1
- num_warmup = 300
- num_run = 500
- Average time of 1 iteration in ms is used
- time_before: no fusion
- time_after: with fusion
- Eltwise OPs selected: hardswish and abs
- Using oneDNN v2.6

**On ICX (32 cores per socket):
Conv2d FP32 (in channels Last format)**

  | shape | time_(ms)_before | time_(ms)_after | Gain
-- | -- | -- | -- | --
1socket | Conv+abs_kernel=3_N=1_iC=64_H=56_W=56_oC=64_stride=1_pad=1_dilates=1_groups=1 | 0.112174 | 0.071106 | 36.61%
1socket | Conv+hardswish_kernel=3_N=1_iC=64_H=56_W=56_oC=64_stride=1_pad=1_dilates=1_groups=1 | 0.11269 | 0.070586 | 37.36%
1socket | Conv+abs_kernel=3_N=1_iC=512_H=56_W=56_oC=512_stride=2_pad=1_dilates=1_groups=32 | 0.164219 | 0.129498 | 21.14%
1socket | Conv+hardswish_kernel=3_N=1_iC=512_H=56_W=56_oC=512_stride=2_pad=1_dilates=1_groups=32 | 0.169371 | 0.1277 | 24.60%
  |   |   |   |  
  | shape | time_(ms)_before | time_(ms)_after | Gain
1thread | Conv+abs_kernel=3_N=1_iC=64_H=56_W=56_oC=64_stride=1_pad=1_dilates=1_groups=1 | 1.994555 | 1.429813 | 28.31%
1thread | Conv+hardswish_kernel=3_N=1_iC=64_H=56_W=56_oC=64_stride=1_pad=1_dilates=1_groups=1 | 1.715168 | 1.459937 | 14.88%
1thread | Conv+abs_kernel=3_N=1_iC=512_H=56_W=56_oC=512_stride=2_pad=1_dilates=1_groups=32 | 2.997382 | 2.47915 | 17.29%
1thread | Conv+hardswish_kernel=3_N=1_iC=512_H=56_W=56_oC=512_stride=2_pad=1_dilates=1_groups=32 | 3.044476 | 2.499366 | 17.90%
  |   |   |   |  
  | shape | time_(ms)_before | time_(ms)_after | Gain
4thread | Conv+abs_kernel=3_N=1_iC=64_H=56_W=56_oC=64_stride=1_pad=1_dilates=1_groups=1 | 0.405204 | 0.38117 | 5.93%
4thread | Conv+hardswish_kernel=3_N=1_iC=64_H=56_W=56_oC=64_stride=1_pad=1_dilates=1_groups=1 | 0.410145 | 0.389279 | 5.09%
4thread | Conv+abs_kernel=3_N=1_iC=512_H=56_W=56_oC=512_stride=2_pad=1_dilates=1_groups=32 | 0.67917 | 0.662792 | 2.41%
4thread | Conv+hardswish_kernel=3_N=1_iC=512_H=56_W=56_oC=512_stride=2_pad=1_dilates=1_groups=32 | 0.682302 | 0.671226 | 1.62%

**On CPX (28 cores per socket):
Conv2d BF16 (in channels Last format)**

  | shape | time_(ms)_before | time_(ms)_after | Gain
-- | -- | -- | -- | --
1socket | Conv+abs_kernel=3_N=1_iC=64_H=56_W=56_oC=64_stride=1_pad=1_dilates=1_groups=1 | 0.119289 | 0.091015 | 23.70%
1socket | Conv+hardswish_kernel=3_N=1_iC=64_H=56_W=56_oC=64_stride=1_pad=1_dilates=1_groups=1 | 0.144116 | 0.09339 | 35.20%
1socket | Conv+abs_kernel=3_N=1_iC=512_H=56_W=56_oC=512_stride=2_pad=1_dilates=1_groups=32 | 0.209975 | 0.177111 | 15.65%
1socket | Conv+hardswish_kernel=3_N=1_iC=512_H=56_W=56_oC=512_stride=2_pad=1_dilates=1_groups=32 | 0.234777 | 0.179945 | 23.36%
  |   |   |   |  
  | shape | time_(ms)_before | time_(ms)_after | Gain
1thread | Conv+abs_kernel=3_N=1_iC=64_H=56_W=56_oC=64_stride=1_pad=1_dilates=1_groups=1 | 1.296252 | 1.086423 | 16.19%
1thread | Conv+hardswish_kernel=3_N=1_iC=64_H=56_W=56_oC=64_stride=1_pad=1_dilates=1_groups=1 | 1.364738 | 1.131289 | 17.11%
1thread | Conv+abs_kernel=3_N=1_iC=512_H=56_W=56_oC=512_stride=2_pad=1_dilates=1_groups=32 | 3.99519 | 3.736147 | 6.48%
1thread | Conv+hardswish_kernel=3_N=1_iC=512_H=56_W=56_oC=512_stride=2_pad=1_dilates=1_groups=32 | 4.03415 | 3.77981 | 6.30%
  |   |   |   |  
  | shape | time_(ms)_before | time_(ms)_after | Gain
4thread | Conv+abs_kernel=3_N=1_iC=64_H=56_W=56_oC=64_stride=1_pad=1_dilates=1_groups=1 | 0.27474 | 0.245281 | 10.72%
4thread | Conv+hardswish_kernel=3_N=1_iC=64_H=56_W=56_oC=64_stride=1_pad=1_dilates=1_groups=1 | 0.28595 | 0.254748 | 10.91%
4thread | Conv+abs_kernel=3_N=1_iC=512_H=56_W=56_oC=512_stride=2_pad=1_dilates=1_groups=32 | 0.847318 | 0.791453 | 6.59%
4thread | Conv+hardswish_kernel=3_N=1_iC=512_H=56_W=56_oC=512_stride=2_pad=1_dilates=1_groups=32 | 0.870212 | 0.801594 | 7.89%

**On CPX (28 cores per socket):
Linear BF16**

  | shape | time_(ms)_before | time_(ms)_after | Gain
-- | -- | -- | -- | --
1socket | Linear+abs_N=1_iC=1024_oC=4096 | 0.043199 | 0.037603 | 12.95%
1socket | Linear+hardswish_N=1_iC=1024_oC=4096 | 0.041845 | 0.038332 | 8.40%
1socket | Linear+abs_N=1_iC=4096_oC=1024 | 0.048282 | 0.044281 | 8.29%
1socket | Linear+hardswish_N=1_iC=4096_oC=1024 | 0.048362 | 0.044106 | 8.80%
1socket | Linear+abs_N=1_iC=2048_oC=1000 | 0.036302 | 0.0344 | 5.24%
1socket | Linear+hardswish_N=1_iC=2048_oC=1000 | 0.035734 | 0.035593 | 0.39%
  |   |   |   |  
  | shape | time_(ms)_before | time_(ms)_after | Gain
1thread | Linear+abs_N=1_iC=1024_oC=4096 | 0.365143 | 0.36279 | 0.64%
1thread | Linear+hardswish_N=1_iC=1024_oC=4096 | 0.364464 | 0.363392 | 0.29%
1thread | Linear+abs_N=1_iC=4096_oC=1024 | 0.384498 | 0.379902 | 1.20%
1thread | Linear+hardswish_N=1_iC=4096_oC=1024 | 0.382545 | 0.381252 | 0.34%
1thread | Linear+abs_N=1_iC=2048_oC=1000 | 0.213244 | 0.209999 | 1.52%
1thread | Linear+hardswish_N=1_iC=2048_oC=1000 | 0.212003 | 0.208567 | 1.62%
  |   |   |   |  
  | shape | time_(ms)_before | time_(ms)_after | Gain
4thread | Linear+abs_N=1_iC=1024_oC=4096 | 0.126096 | 0.12157 | 3.59%
4thread | Linear+hardswish_N=1_iC=1024_oC=4096 | 0.126627 | 0.121662 | 3.92%
4thread | Linear+abs_N=1_iC=4096_oC=1024 | 0.132845 | 0.128921 | 2.95%
4thread | Linear+hardswish_N=1_iC=4096_oC=1024 | 0.132642 | 0.12783 | 3.63%
4thread | Linear+abs_N=1_iC=2048_oC=1000 | 0.079582 | 0.072584 | 8.79%
4thread | Linear+hardswish_N=1_iC=2048_oC=1000 | 0.077761 | 0.071981 | 7.43%

Pull Request resolved: https://github.com/pytorch/pytorch/pull/82705
Approved by: https://github.com/frank-wei, https://github.com/eellison"
pytorch/pytorch,817a82704ff140cec001fab942437b96d901da42,"Delete ProxyTensor wrapper subclass (#83330)

I was working on https://github.com/pytorch/torchdynamo/issues/80 and my
working hypothesis for what was causing the error was that proxy tensor
was not advertising correct dispatch keys, causing AMP to operate
differently when you traced.  I could have fixed this directly by
replicating fake tensor's fix for setting dispatch keys to also apply to
proxy tensor, but I was like, ""Why must I repeat myself.""

This PR is the result.  It completely deletes the ProxyTensor wrapper
subclass, so that when we are tracing, the tensors flowing through the
program are the *original* real or fake tensors, depending on what the
user requested in the top-level API.  There is no more wrapping.  To
store the Proxy objects necessary for actually doing tracing, I store
the property directly on the tensors.  (Note: I never
clean up old entries from the map at the moment, this is easily fixed
by using a weak map)

Benefits of doing this:

* No more tip-toeing around no_dispatch() creation of new ProxyTensors;
  we never create new tensors (except when we call the underlying func),
  so you don't have to worry about accidentally tracing them.

* No more syncing up metadata from in place operators.  In particular
  https://github.com/pytorch/pytorch/issues/81526 is mooted

* This fixes https://github.com/pytorch/torchdynamo/issues/519 as we no longer need to teach proxy tensor to support sparse tensor.

* No more schlepping symbolic integers from the inner fake tensor to the
  outer proxy tensor.  If you can make a fake tensor with symbolic ints,
  you're done, nothing else to do.

To avoid having to rewrite all of the guts, when I get to the actual
proxy tensor handler, I first ""fetch"" the stored ProxyTensor data from
the weakmap via a tree_map, and then operate on the consequent data as
before.  A more optimized implementation is possible.

Signed-off-by: Edward Z. Yang <ezyang@fb.com>
Pull Request resolved: https://github.com/pytorch/pytorch/pull/83330
Approved by: https://github.com/Chillee"
pytorch/pytorch,cf5330977d10a3585358fb02c049939bf1401074,"[CI] Move torch-deploy to cuda-11.6 (#83572)

As we are slowly deprecating 11.3

Pull Request resolved: https://github.com/pytorch/pytorch/pull/83572
Approved by: https://github.com/huydhn, https://github.com/ZainRizvi"
pytorch/pytorch,84c4b079328c2a97d78d95c47b841f8dca6036bb,"Make sure that we can load old optimizer checkpoint (#83588)

We want to make sure that we can load checkpoints that were saved with older version of the code (which doesn't contain the differentiable attribute).
Pull Request resolved: https://github.com/pytorch/pytorch/pull/83588
Approved by: https://github.com/mikaylagawarecki"
pytorch/pytorch,5aab57e112d244f0cf3bbab30db640e52a0c2c44,"Make Adam optimizer differentiable (#82205)

Continues [80938](https://github.com/pytorch/pytorch/pull/80938)
Pull Request resolved: https://github.com/pytorch/pytorch/pull/82205
Approved by: https://github.com/albanD"
pytorch/pytorch,a09c3fcb8d0e290b8d398c110ddbfd845e6c4058,"Add loss operators to fp32 cast policy of AutocastCPU (#81689)

### Description
Add loss operators to fp32 cast policy of AutocastCPU to improve accuracy of BFloat16 training. There will be no performance impact on fp32, only a slight impact on bf16 training.

#### Remove _convolution
This is because conv transpose does not fully support bf16 before, and it will be replaced to _convolution in graph mode. If _convolution is in lower precision cast policy it will throw dtype related errors.
conv transpose does not fully support bf16 yet, so _convolution still needs to be removed.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/81689
Approved by: https://github.com/malfet"
pytorch/pytorch,d3a176a156819d57ed442579b806ff027402f4dc,"[PT-D][BE][TP perf 1/N] Get rid of unnecessary collectives in Embedding/EmbeddingBag and use autograd-enabled collectives (#81853)

These two ops (Embedding and EmbeddingBag for ShardedTensor) especially for row-wise sharding is very inefficient and hard to fit in the concept of future design. So this PR is trying to:
1. Remove all unnecessary collective communications. Only one gather and one reduce(or reduce scatter) is needed.
2. Use auto-grad enabled collectives so that we can use these ops in real model training.
3. Some minor code cleaning
4. Treat input differently when it's replicated tensor. (Will add more for this for the next few PRs).

Differential Revision: [D37965687](https://our.internmc.facebook.com/intern/diff/D37965687/)
Pull Request resolved: https://github.com/pytorch/pytorch/pull/81853
Approved by: https://github.com/wanchaol"
pytorch/pytorch,0e2efaf9cca53890004718eba76dfefa74838aa3,"use global var for disabled and slow test dicts (#83487)

as in title

Additional changes:
* run isort for imports
* rename some variables
* warning instead of print

Test plan
* checked logs to see that tests were still being disabled
* checked pytest xmls to check that pytest still disables things
Pull Request resolved: https://github.com/pytorch/pytorch/pull/83487
Approved by: https://github.com/malfet, https://github.com/huydhn"
pytorch/pytorch,0faf10b0f4da0bcaaaba8834fa6984bbd7f793f9,"Split ScanKernels.cu (#83422)

On my machine `ScanKernels.cu` takes 10 minutes for just a single
architecture which is by far the highest compile time of any single
file. So this splits it into multiple files, the slowest being
`LogcumsumexpKernel.cu` which takes 2m 30s
Pull Request resolved: https://github.com/pytorch/pytorch/pull/83422
Approved by: https://github.com/ngimel"
pytorch/pytorch,8473e6968487d736c75470eeae4d63b11156b622,"[ROCm] Fixes the kernel asserts API declaration mismatch error (#81790)

This problem updates the the PR [#73040](https://github.com/pytorch/pytorch/pull/73040)

The compilation error in pyTorch with ROCm is successful with these changes when `NDEBUG` is enabled.

Solution:
For HIP we keep `__device__ __assert_fail()`
and for host side compilation we want to use the `__assert_fail()` from the glibc library.

Tested the code by compiling with below steps
```
python3 tools/amd_build/build_amd.py
python3 setup.py develop --cmake-only
cmake -DHIP_HIPCC_FLAGS_RELEASE=""-DNDEBUG"" build
cmake --build build
```

The UT test_fixed_cuda_assert_async is still skipped due performance overhead.

cc @jithunnair-amd

Pull Request resolved: https://github.com/pytorch/pytorch/pull/81790
Approved by: https://github.com/shintaro-iwasaki, https://github.com/jeffdaily, https://github.com/malfet"
pytorch/pytorch,4c8cfb57aa3ac58112efb693635198b07edf008f,"Convert SymInt tracing to mode based tracing (#83380)

We're on our way to deleting ProxyTensor entirely (see https://github.com/pytorch/pytorch/pull/83330 ), but before we can do that, we have to delete ProxySymInt first. Here's the plan.

Changes in torch.fx.experimental.symbolic_shapes

* The general idea is to do mode based tracing. This means we need a mode that can interpose on all SymInt operations. There are a few ways to do this, but I've done it the easy way: (1) I have a separate mode for SymInt operations specifically called SymDispatchMode, and (2) this mode operates on PySymInt (and not the basic SymInt which is user visible). I elided Int from the name because if we add SymFloats I want to use the same mode to handle those as well, and I used Dispatch rather than Function because this is the ""inner"" dispatch operating PySymInt and not SymInt (this is not a perfect analogy, but SymFunctionMode definitely seemed wrong as you still must go through the C++ binding.) The mode is entirely implemented in Python for ease of implementation. We could have implemented this more symmetrically to TorchFunctionMode in C++, but I leave that as later work; this API is unlikely to get used by others (unlike TorchFunctionMode). One downside to not doing the mode in C++ is that we still have to do the hop via a preexisting PySymInt to wrap; this is currently not a big deal as conversion to SymInts only really happens when there is already another SymInt floating around. SymDispatchMode is pared down from TorchDispatchMode; there is no ancestor tracking since I don't expect people to be mixing up SymDispatchModes.
*  I made some improvements for tracing. When I invoke the SymDispatchMode handler, I would like constants to show up as constants, so they can be directly inlined into the FX graph (rather than going through a wrapping process first, and then the wrapped SymInt being used in the operation). To do this, I directly track if a PySymInt is a constant at construction time. Only wrapped PySymInts are constants.
* For convenience, PySymInts now support all magic methods that regular SymInts do. This is so that redispatch inside the SymDispatchMode can be written the idiomatic way `func(*args, **kwargs)` where func is an operator. The original names are retained for direct C++ calls.

Changes in torch.fx.experimental.proxy_tensor

* OK, so we got a new SymDispatchMode, so we define a ProxySymDispatchMode and activate it when we start tracing. This mode is currently unconditionally activated although technically we only need to activate it when doing symbolic tracing (it doesn't matter either way as there are no SymInts if you are not doing symbolic tracing).
* We delete ProxySymInt. To do this, we must now record the proxy for the SymInt some other way. Based on discussion with Chillee, it is more intuitive to him if the proxies are still recorded on the SymInt in some way. So we store them in the `__dict__` of the PySymInt, indexed by Tracer. An improvement is to make this a weak map, so that we remove all of these entries when the tracer dies. In an original version of this PR, I keyed on the mode itself, but tracer is better as it is accessible from both modes (and as you will see, we will need to fetch the map from both the ProxySymDispatchMode as well as the ProxyTorchDispatchMode.) The implementation of SymDispatchMode now simply retrieves the proxies, performs the underlying operation as well as the FX graph recording, and then records the output proxy to the PySymInt. Note that FX tracing does not work with proxies and SymInts, so we manually call `call_function` to ensure that the correct operations get recorded to the graph. This means conventional FX retracing with proxies only will not work with these graphs, but there wasn't really any reason to do this (as opposed to `make_fx` retracing) anyway. Constants are detected and converted directly into Python integers.
* SymInts can show up as arguments to tensor operations, so they must be accounted for in ProxyTorchDispatchMode as well. This is done by searching for SymInt arguments and converting them into proxies before the proxy call. This can be done more efficiently in a single `tree_map` but I'm lazy. The helper `unwrap_symint_proxy` conveniently implements the unwrapping in one place given a tracer; unfortunately it cannot be shared with SymDispatchMode as SymDispatchMode gets PySymInts, but ProxyTensorMode gets SymInts. Similarly, tensors that are returned from tensor operations can have SymInts in their shapes, which need fresh proxies allocated. To avoid leaking internal details of SymInt shape computation to the tensor operation graph, these SymInts are always given proxies derived from `x.size(dim)` call on their return tensor. We also need to do this for strides and numel but have not done so yet. Furthermore, we must avoid tracing internal SymInt calls while we run meta operations on the true operation; this is achieved by also disabling SymInt tracing on the inside of tensor tracing. This is analogous to how tensor tracing is disabled inside the implementation of tracing mode, but unfortunately we are unable to use the same mechanism (this would have been easier if the two modes could be combined somehow, and I am amenable to suggestions to try harder to achieve this.)
* Because there are no more ProxySymInts, we no longer need to do anything to unwrap SymInt. Furthermore, we do not need to reallocate ProxySymInts on class creation.
* If a bare SymInt without a Proxy is encountered, it is assumed that this must be a constant. `create_arg` handles this case. Non-constant free SymInts result in an assert error.
* The initial input handling in `dispatch_trace` involves traversing all of the input tensors, traversing over their shapes, and assigning proxies for the SymInts in shapes in the same way we handle proxies for the output tensors.

The preexisting testing is inadequate but will be better after I rebase past https://github.com/pytorch/pytorch/pull/82209

Signed-off-by: Edward Z. Yang <ezyang@fb.com>
Pull Request resolved: https://github.com/pytorch/pytorch/pull/83380
Approved by: https://github.com/samdow"
pytorch/pytorch,ff75562cffb54d7500a94a1091e06dc9b5c284fc,"Adding maximize to rprop (#81864)

Added the maximize flag #68052 to rprop optimizer and updates the respective tests.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/81864
Approved by: https://github.com/albanD"
pytorch/pytorch,43f950af201f8a39e5728a65e03cfcafec04585d,"Manually shard slow-gradcheck CI job to prevent timeout (#83354)

Fixes https://github.com/pytorch/pytorch/issues/83335
Pull Request resolved: https://github.com/pytorch/pytorch/pull/83354
Approved by: https://github.com/malfet, https://github.com/albanD"
pytorch/pytorch,dfc97df64d3f74ca436732a53fe6fdcabe1c9de1,"Add fastpath test for mask check flag (#82999)

Summary: Check that fastpath is taken, which type (sparsity fastpath or normal) for mask that is aligned and one that is not.

Test Plan: buck test caffe2/test:test_transformers

Differential Revision: D38259928

Pull Request resolved: https://github.com/pytorch/pytorch/pull/82999
Approved by: https://github.com/jbschlosser"
pytorch/pytorch,382ef1fda75dfce07c37a920e908ce96d99bf970,"Autograd graphtask trim unnecessary edges (#82544)

### Introduction
<!-- What did you change and why was it needed? -->

Removing unnecessary weight gradient calculation is very important for applications that need high-order derivatives during training. However, this is not supported by the current Autograd engine.

For more detail: The backward function of a `matmul` operator (e.g., `linear` `addmm` `mm`), has two matmuls, one for `input gradient` and another for `weight gradient`. For a typical neural network (nn) with a few linear layers and activation functions, if the user calls `torch.autograd.grad()` to calculate the derivative of the nn output `y` w.r.t the nn input `x`,  only the `input gradient` of the `matmul` operator is needed, and the `weight gradient` is discarded. However, the current PyTorch autograd engine will always calculate the `weight gradient` if `weight` requires gradient (the calculation of the high-order derivative is performed during training).

The figure attached shows the autograd graph of the following code snippet:
```py
y = torch.nn.functional.linear(x, weight, bias)
y = y.pow(2)
# first order derivative
y__x, = torch.autograd.grad(y, x, grad_outputs=grad_outputs, create_graph=True)
# first order derivative
y__x__x, = torch.autograd.grad(y__x, x, grad_outputs=grad_outputs, create_graph=True)
```
The path with :x: is not needed when calculating derivatives.

<img width=""50%"" alt=""image"" src=""https://user-images.githubusercontent.com/9999318/182018117-719c5a23-bcc6-4a63-8e8d-1bca3ebda2e3.png"">

### Issue
<!-- Link to Issue ticket or RFP -->
Related issue: https://github.com/pytorch/pytorch/issues/56500

### Method
When calling `torch.autograd.grad`, `exec_info_` is created for each GraphTask, which allows filtering paths on the graph that are not needed. However, when the GraphTask calls into the node, the node still does not know whether the edges are needed or not. In the case of matmul, `weight.requires_grad is True` so the weight gradient is always calculated.

Following https://github.com/pytorch/pytorch/issues/56500#issuecomment-825694656, this PR passes the graph task's thread_local `exec_info_` into the node, so it could trim unnecessary edges during `torch.autograd.grad` calls.

### Benchmark
Benchmark script: https://gist.github.com/yueyericardo/24158433a2021c51eeef9c3e2722df99

Benchmark result:
6 hidden layers, batch size 10000, on A100

FP32 result
| hessian benchmark             | FP32 (before) | FP32 (After)      | FP32 (Functorch v0.1.1) |
| ----------------------------- | ------------- | ----------------- | ----------------------- |
| Linear + ReLU (no backward)   | 55.658 ms     | 29.392 ms (1.90X) | 29.547 ms (1.90X)       |
| Linear + ReLU (with backward) | 81.173 ms     | 54.917 ms (1.47X) | 68.988 ms (1.18X)       |

TF32 result
| hessian benchmark             | TF32 (before) | TF32 (after)      | TF32 (Functorch v0.1.1) |
| ----------------------------- | ------------- | ----------------- | ----------------------- |
| Linear + ReLU (no backward)   | 19.801 ms     | 11.259 ms (1.76X) | 10.754 ms (1.84X)       |
| Linear + ReLU (with backward) | 29.167 ms     | 20.466 ms (1.42X) | 22.784 ms (1.28X)       |

For FP32 result, we could get 1.9X speed up for hessian calculation, and 1.47X speed up during training, which is even faster than functorch `vmap(jacfwd(jacrev` implementation. (functorch has performance regression on v0.2.0, https://github.com/pytorch/functorch/issues/989, so we are using v0.1.1 for benchmark)

@zou3519 does functorch also includes similar optimizations during hessian calculation? If not, what do we need to do so the functorch could also benefit from this PR?

### Testing
<!-- How did you test your change? -->

- [x] we need to figure out a way for unittest

### Thanks
Thanks for the great blog: [How Computational Graphs are Executed in PyTorch | PyTorch](https://pytorch.org/blog/how-computational-graphs-are-executed-in-pytorch/)

cc @zasdfgbnm @albanD
Pull Request resolved: https://github.com/pytorch/pytorch/pull/82544
Approved by: https://github.com/soulitzer"
pytorch/pytorch,abb2204f6a9c5af4e14e11cc69de8bcb5cceaea0,"Fix TORCH_CHECK macros when glog is used (#83216)

Makes TORCH_CHECK_* run unconditionally, leaving only TORCH_DCHECK_*
special-cased to be optimized out in release builds.

Fixes a bug in #82032, relating to this comment
https://github.com/pytorch/pytorch/pull/82032#issuecomment-1203726409

Pull Request resolved: https://github.com/pytorch/pytorch/pull/83216
Approved by: https://github.com/ezyang, https://github.com/datumbox"
pytorch/pytorch,9213751970e4bb4eecd0b5dc6317740f017460a0,"Add exception handler for stoull in caffe2 (#77557)

Hi!

I was playing with libfuzzer and found bug when loading a model from file via `torch::jit::load` function.
There is an unhandled exception in caffe2/serialize when calling a `stoull` function on unsanitized version string.

The bug can be reproduced with `aot_model_compiler` binary:
```
aot_model_compiler --model=crash-stoull --model_name=name --model_version=1 --input_dims='1,3,224,224;2,2' --input_types='float;float'
```

Crash file is provided in [crash.zip](https://github.com/pytorch/pytorch/files/8701504/crash.zip).

gdb output:
```
Temporary breakpoint 1, main (argc=6, argv=0x7ffcd160f9f8) at /pytorch_master/binaries/aot_model_compiler.cc:87
87	      ""Run NNC AOT compiler for pytorch model. Example usage:\n""
(gdb) c
Continuing.
terminate called after throwing an instance of 'std::invalid_argument'
  what():  stoull

Program received signal SIGABRT, Aborted.
__GI_raise (sig=sig@entry=6) at ../sysdeps/unix/sysv/linux/raise.c:50
50	../sysdeps/unix/sysv/linux/raise.c: No such file or directory.
(gdb) bt
#0  __GI_raise (sig=sig@entry=6) at ../sysdeps/unix/sysv/linux/raise.c:50
#1  0x00007fa637f16859 in __GI_abort () at abort.c:79
#2  0x00007fa6381c1911 in ?? () from /lib/x86_64-linux-gnu/libstdc++.so.6
#3  0x00007fa6381cd38c in ?? () from /lib/x86_64-linux-gnu/libstdc++.so.6
#4  0x00007fa6381cd3f7 in std::terminate() () from /lib/x86_64-linux-gnu/libstdc++.so.6
#5  0x00007fa6381cd6a9 in __cxa_throw () from /lib/x86_64-linux-gnu/libstdc++.so.6
#6  0x00007fa6381c42ce in std::__throw_invalid_argument(char const*) () from /lib/x86_64-linux-gnu/libstdc++.so.6
#7  0x000000000247d567 in __gnu_cxx::__stoa<unsigned long long, unsigned long long, char, int> (__str=0x7ffcd160f228 ""ZZ"", __idx=0x0, __base=10, __convf=<optimized out>, __name=<optimized out>)
    at /usr/bin/../lib/gcc/x86_64-linux-gnu/10/../../../../include/c++/10/ext/string_conversions.h:83
#8  std::__cxx11::stoull (__str=""ZZ"", __idx=0x0, __base=10) at /usr/bin/../lib/gcc/x86_64-linux-gnu/10/../../../../include/c++/10/bits/basic_string.h:6577
#9  caffe2::serialize::PyTorchStreamReader::init (this=this@entry=0x8c11ce0) at /pytorch_master/caffe2/serialize/inline_container.cc:145
#10 0x000000000247d9c7 in caffe2::serialize::PyTorchStreamReader::PyTorchStreamReader (this=0x8c11ce0, in=std::shared_ptr<class caffe2::serialize::ReadAdapterInterface> (empty) = {...})
    at /pytorch_master/caffe2/serialize/inline_container.cc:88
#11 0x00000000035b7ba4 in __gnu_cxx::new_allocator<caffe2::serialize::PyTorchStreamReader>::construct<caffe2::serialize::PyTorchStreamReader, std::shared_ptr<caffe2::serialize::ReadAdapterInterface> > (
    __p=0x2, __args=..., this=<optimized out>) at /usr/bin/../lib/gcc/x86_64-linux-gnu/10/../../../../include/c++/10/ext/new_allocator.h:150
#12 std::allocator_traits<std::allocator<caffe2::serialize::PyTorchStreamReader> >::construct<caffe2::serialize::PyTorchStreamReader, std::shared_ptr<caffe2::serialize::ReadAdapterInterface> > (__a=...,
    __p=0x2, __p@entry=0x8c11ce0, __args=...) at /usr/bin/../lib/gcc/x86_64-linux-gnu/10/../../../../include/c++/10/bits/alloc_traits.h:512
#13 0x00000000035b1988 in std::_Sp_counted_ptr_inplace<caffe2::serialize::PyTorchStreamReader, std::allocator<caffe2::serialize::PyTorchStreamReader>, (__gnu_cxx::_Lock_policy)2>::_Sp_counted_ptr_inplace<std::shared_ptr<caffe2::serialize::ReadAdapterInterface> > (this=0x8c11cd0, __a=..., __args=...) at /usr/bin/../lib/gcc/x86_64-linux-gnu/10/../../../../include/c++/10/bits/shared_ptr_base.h:551
#14 std::__shared_count<(__gnu_cxx::_Lock_policy)2>::__shared_count<caffe2::serialize::PyTorchStreamReader, std::allocator<caffe2::serialize::PyTorchStreamReader>, std::shared_ptr<caffe2::serialize::ReadAdapterInterface> > (this=0x7ffcd160f3a8, __p=@0x7ffcd160f3a0: 0x10, __args=..., __a=...) at /usr/bin/../lib/gcc/x86_64-linux-gnu/10/../../../../include/c++/10/bits/shared_ptr_base.h:683
#15 std::__shared_ptr<caffe2::serialize::PyTorchStreamReader, (__gnu_cxx::_Lock_policy)2>::__shared_ptr<std::allocator<caffe2::serialize::PyTorchStreamReader>, std::shared_ptr<caffe2::serialize::ReadAdapterInterface> > (this=0x7ffcd160f3a0, __args=..., __tag=...) at /usr/bin/../lib/gcc/x86_64-linux-gnu/10/../../../../include/c++/10/bits/shared_ptr_base.h:1371
#16 std::shared_ptr<caffe2::serialize::PyTorchStreamReader>::shared_ptr<std::allocator<caffe2::serialize::PyTorchStreamReader>, std::shared_ptr<caffe2::serialize::ReadAdapterInterface> > (this=0x7ffcd160f3a0,
    __args=..., __tag=...) at /usr/bin/../lib/gcc/x86_64-linux-gnu/10/../../../../include/c++/10/bits/shared_ptr.h:408
#17 std::allocate_shared<caffe2::serialize::PyTorchStreamReader, std::allocator<caffe2::serialize::PyTorchStreamReader>, std::shared_ptr<caffe2::serialize::ReadAdapterInterface> > (__args=..., __a=...)
    at /usr/bin/../lib/gcc/x86_64-linux-gnu/10/../../../../include/c++/10/bits/shared_ptr.h:859
#18 std::make_shared<caffe2::serialize::PyTorchStreamReader, std::shared_ptr<caffe2::serialize::ReadAdapterInterface> > (__args=...)
    at /usr/bin/../lib/gcc/x86_64-linux-gnu/10/../../../../include/c++/10/bits/shared_ptr.h:875
#19 torch::jit::load (rai=std::shared_ptr<class caffe2::serialize::ReadAdapterInterface> (empty) = {...}, device=device@entry=..., Python Exception <class 'gdb.error'> No type named std::__detail::_Hash_node<struct std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, true>.:
extra_files=std::unordered_map with 0 elements)
    at /pytorch_master/torch/csrc/jit/serialization/import.cpp:474
#20 0x00000000035b1ef6 in torch::jit::load (filename=""crash-stoull"", device=device@entry=..., Python Exception <class 'gdb.error'> No type named std::__detail::_Hash_node<struct std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, true>.:
extra_files=std::unordered_map with 0 elements) at /pytorch_master/torch/csrc/jit/serialization/import.cpp:444
#21 0x00000000035b1d22 in torch::jit::load (filename="""", device=device@entry=...) at /pytorch_master/torch/csrc/jit/serialization/import.cpp:424
#22 0x00000000008f9be3 in main (argc=1, argv=0x7ffcd160f9f8) at /pytorch_master/binaries/aot_model_compiler.cc:128
```

Pull Request resolved: https://github.com/pytorch/pytorch/pull/77557
Approved by: https://github.com/Gamrix"
pytorch/pytorch,2fe3ea65c2b9147077ea3a3dc4757f1768483ba4,"RowwiseMoments: use float as acc type for bfloat16 inputs (#81850)

To fix https://github.com/pytorch/pytorch/issues/77507

Originally `utils::RowwiseMoments<BFloat16>` will still accululate on BFloat16,
which is not only slow but also introducing additional rounding errors.

This patch will do accumulation on float for the bfloat16 inputs:
each of bfloat16 vec (size 16) will be converted to two float vec (size 8),
and accumulated on m1(mean) and m2(rstd) vecs which are all float vecs.

No effect on float performance, will improve bfloat16 performance:
* avx512 single socket:
```
before: LayerNorm((1024,), eps=1e-05, elementwise_affine=True) : 32x128x1024: fp32: 0.210 ms; bf16: 0.770 ms
after:  LayerNorm((1024,), eps=1e-05, elementwise_affine=True) : 32x128x1024: fp32: 0.215 ms; bf16: 0.178 ms
```
* avx512 single core:
```
before: LayerNorm((1024,), eps=1e-05, elementwise_affine=True) : 32x128x1024: fp32: 2.661 ms; bf16: 12.267 ms
after:  LayerNorm((1024,), eps=1e-05, elementwise_affine=True) : 32x128x1024: fp32: 2.618 ms; bf16: 2.309 ms
```
* avx2 single socket:
```
before: LayerNorm((1024,), eps=1e-05, elementwise_affine=True) : 32x128x1024: fp32: 0.540 ms; bf16: 2.030 ms
after:  LayerNorm((1024,), eps=1e-05, elementwise_affine=True) : 32x128x1024: fp32: 0.527 ms; bf16: 0.458 ms
```
* avx2 single core:
```
before: LayerNorm((1024,), eps=1e-05, elementwise_affine=True) : 32x128x1024: fp32: 4.349 ms; bf16: 19.252 ms
after:  LayerNorm((1024,), eps=1e-05, elementwise_affine=True) : 32x128x1024: fp32: 4.416 ms; bf16: 3.524 ms
```
Pull Request resolved: https://github.com/pytorch/pytorch/pull/81850
Approved by: https://github.com/ezyang, https://github.com/malfet"
pytorch/pytorch,693a8dd04cda39737fef27276dfe4b8cecf12c46,"[NNC] enable fusion of conv with elementwise OP (#77157)

## Pitch
Enable Conv-Eltwise fusion in NNC.

## Description
This PR adds a `FuseConvWithEltwise` pass to fuse convolution with elementwise OP for TE subgraph. This pass will insert prepack and packed run ops for conv2d and enable fusion of conv2d with elementwise OPs. The fused packed run ops is implemented via external call in NNC.

## Code structure
Graph rewrite pass related code is placed in:
```
torch/csrc/jit/passes/mkldnn_rewrite.h
torch/csrc/jit/passes/mkldnn_rewrite.cpp
```

NNC integration of fused conv-eltwise OP via external call is located in:
```
torch/csrc/jit/tensorexpr/kernel.cpp

torch/csrc/jit/tensorexpr/operators/conv2d.h
torch/csrc/jit/tensorexpr/operators/conv2d.cpp

torch/csrc/jit/tensorexpr/lowerings.cpp
torch/csrc/jit/tensorexpr/external_functions.cpp
```

Fused prepack OP context is in:
```
aten/src/ATen/native/mkldnn/Common.h
aten/src/ATen/native/mkldnn/RegisterMkldnnOpContextClass.cpp
aten/src/ATen/native/mkldnn/OpContext.h
aten/src/ATen/native/mkldnn/OpContext.cpp
```

Fused OP implementation is done in:
```
aten/src/ATen/native/mkldnn/ConvPrepack.h
aten/src/ATen/native/mkldnn/ConvPrepack.cpp
```

## OP benchmark for conv-relu
The below performance is measured on top of these two PRs to support NHWC: https://github.com/pytorch/pytorch/pull/76948 and https://github.com/pytorch/pytorch/pull/78238.

- Measured on Cascade Lake 8280
- Jemalloc enabled
- batch_size = 1
- Channels Last format

### Single thread:
<html xmlns:v=""urn:schemas-microsoft-com:vml""
xmlns:o=""urn:schemas-microsoft-com:office:office""
xmlns:x=""urn:schemas-microsoft-com:office:excel""
xmlns=""http://www.w3.org/TR/REC-html40"">

<head>

<meta name=ProgId content=Excel.Sheet>
<meta name=Generator content=""Microsoft Excel 15"">
<link id=Main-File rel=Main-File
href=""file:///C:/Users/chunyuan/AppData/Local/Temp/msohtmlclip1/01/clip.htm"">
<link rel=File-List
href=""file:///C:/Users/chunyuan/AppData/Local/Temp/msohtmlclip1/01/clip_filelist.xml"">

</head>

<body link=""#0563C1"" vlink=""#954F72"">

shape | time (us)_no_fusion | time (us)_fusion | Gain
-- | -- | -- | --
kernel=3, N=1, iC=64, H=56, W=56, oC=64,   stride=1, pad=1, dilates=1, g=1 | 1706.22 | 1371.97 | 19.59%
kernel=1, N=1, iC=256, H=56, W=56,   oC=512, stride=2, pad=0, dilates=1, g=1 | 2499.28 | 1571.52 | 37.12%
kernel=3, N=1, iC=256, H=56, W=56,   oC=256, stride=1, pad=1, dilates=1, g=32 | 4169.52 | 2738.53 | 34.32%
kernel=3, N=1, iC=512, H=56, W=56,   oC=512, stride=2, pad=1, dilates=1, g=32 | 3998.77 | 3085.85 | 22.83%
kernel=1, N=1, iC=64, H=56, W=56, oC=64,   stride=1, pad=0, dilates=1, g=1 | 673.73 | 430.81 | 36.06%
kernel=1, N=1, iC=256, H=56, W=56, oC=64,   stride=1, pad=0, dilates=1, g=1 | 1101.87 | 801.07 | 27.30%
kernel=1, N=1, iC=256, H=56, W=56,   oC=256, stride=1, pad=0, dilates=1, g=1 | 4692.91 | 3116.13 | 33.60%
kernel=1, N=1, iC=512, H=28, W=28,   oC=512, stride=1, pad=0, dilates=1, g=1 | 3310.64 | 2503.39 | 24.38%

</body>

</html>

### 4 threads:
<html xmlns:v=""urn:schemas-microsoft-com:vml""
xmlns:o=""urn:schemas-microsoft-com:office:office""
xmlns:x=""urn:schemas-microsoft-com:office:excel""
xmlns=""http://www.w3.org/TR/REC-html40"">

<head>

<meta name=ProgId content=Excel.Sheet>
<meta name=Generator content=""Microsoft Excel 15"">
<link id=Main-File rel=Main-File
href=""file:///C:/Users/chunyuan/AppData/Local/Temp/msohtmlclip1/01/clip.htm"">
<link rel=File-List
href=""file:///C:/Users/chunyuan/AppData/Local/Temp/msohtmlclip1/01/clip_filelist.xml"">

</head>

<body link=""#0563C1"" vlink=""#954F72"">

shape | time (us)_no_fusion | time (us)_fusion | Gain
-- | -- | -- | --
kernel=3, N=1, iC=64, H=56, W=56, oC=64,   stride=1, pad=1, dilates=1, g=1 | 360.07 | 321.21 | 10.79%
kernel=1, N=1, iC=256, H=56, W=56,   oC=512, stride=2, pad=0, dilates=1, g=1 | 391.49 | 323.17 | 17.45%
kernel=3, N=1, iC=256, H=56, W=56,   oC=256, stride=1, pad=1, dilates=1, g=32 | 536.4 | 465.97 | 13.13%
kernel=3, N=1, iC=512, H=56, W=56,   oC=512, stride=2, pad=1, dilates=1, g=32 | 674.98 | 616.32 | 8.69%
kernel=1, N=1, iC=64, H=56, W=56, oC=64,   stride=1, pad=0, dilates=1, g=1 | 160.97 | 70.05 | 56.48%
kernel=1, N=1, iC=256, H=56, W=56, oC=64,   stride=1, pad=0, dilates=1, g=1 | 215.81 | 182.6 | 15.39%
kernel=1, N=1, iC=256, H=56, W=56,   oC=256, stride=1, pad=0, dilates=1, g=1 | 658.45 | 576.97 | 12.37%
kernel=1, N=1, iC=512, H=28, W=28,   oC=512, stride=1, pad=0, dilates=1, g=1 | 702.18 | 566.39 | 19.34%

</body>

</html>

### 1 socket (28 cores):
<html xmlns:v=""urn:schemas-microsoft-com:vml""
xmlns:o=""urn:schemas-microsoft-com:office:office""
xmlns:x=""urn:schemas-microsoft-com:office:excel""
xmlns=""http://www.w3.org/TR/REC-html40"">

<head>

<meta name=ProgId content=Excel.Sheet>
<meta name=Generator content=""Microsoft Excel 15"">
<link id=Main-File rel=Main-File
href=""file:///C:/Users/chunyuan/AppData/Local/Temp/msohtmlclip1/01/clip.htm"">
<link rel=File-List
href=""file:///C:/Users/chunyuan/AppData/Local/Temp/msohtmlclip1/01/clip_filelist.xml"">

</head>

<body link=""#0563C1"" vlink=""#954F72"">

shape | time (us)_no_fusion | time (us)_fusion | Gain
-- | -- | -- | --
kernel=3, N=1, iC=64, H=56, W=56, oC=64,   stride=1, pad=1, dilates=1, g=1 | 149.92 | 103.78 | 30.78%
kernel=1, N=1, iC=256, H=56, W=56,   oC=512, stride=2, pad=0, dilates=1, g=1 | 192.76 | 110.87 | 42.48%
kernel=3, N=1, iC=256, H=56, W=56,   oC=256, stride=1, pad=1, dilates=1, g=32 | 160.67 | 127.24 | 20.81%
kernel=3, N=1, iC=512, H=56, W=56,   oC=512, stride=2, pad=1, dilates=1, g=32 | 212.45 | 180.55 | 15.02%
kernel=1, N=1, iC=64, H=56, W=56, oC=64,   stride=1, pad=0, dilates=1, g=1 | 114.57 | 50.58 | 55.85%
kernel=1, N=1, iC=256, H=56, W=56, oC=64,   stride=1, pad=0, dilates=1, g=1 | 198.64 | 70.6 | 64.46%
kernel=1, N=1, iC=256, H=56, W=56,   oC=256, stride=1, pad=0, dilates=1, g=1 | 281.35 | 155.8 | 44.62%
kernel=1, N=1, iC=512, H=28, W=28,   oC=512, stride=1, pad=0, dilates=1, g=1 | 262.15 | 162.94 | 37.84%

</body>

</html>

## UT
```
test/test_mkldnn_fusion.py
```
Pull Request resolved: https://github.com/pytorch/pytorch/pull/77157
Approved by: https://github.com/ZolotukhinM"
pytorch/pytorch,b2363520363d8c85496f6954fde005834c188b5d,"Add mask identifier for multiplexed src_mask/src_key_padding_mask in BT (#81947)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/81947

Transformer fastpath multiplexes two arguments, src_mask [seq_len x seq_len] and src_key_padding_mask [batch_size x seq_len], and later deduces the type based on mask shape.

In the event that batch_size == seq_len, any src_mask is wrongly interpreted as a src_key padding_mask. This is fixed by requiring a mask_type identifier be supplied whenever batch_size == seq_len.

Additionally, added support for src_mask in masked_softmax CPU path.

Test Plan: existing unit tests + new unit tests (batch_size == seq_len)

Differential Revision: D37932240

Pull Request resolved: https://github.com/pytorch/pytorch/pull/81947
Approved by: https://github.com/zrphercule"
pytorch/pytorch,726d04069285277f36cf5ccea6d83d6f8ff9ec75,"annotated allocator snapshots (#82146)

Record stack trace information for each allocated segment in the allocator.
It takes around 1.5us to record 50 stack frames of context.
Since invoking a Pytorch operator is around 8us, this adds minimal overhead but we still leave it disabled by default so that we can test it more on real workloads first.

Stack information is kept both for allocated blocks and the last allocation used inactive blocks. We could potential keep around the _first_ allocation that caused the block to get allocated from cuda as well.

Potential Followups:
* stack frame entries are small (16 bytes), but the list of Frames is not compressed eventhough most frames will share some entries. So far this doesn't produce huge dumps (7MB for one real workload that uses all memory on the GPU), but it can be much smaller through compression.
* Code to format the information is slow (a few seconds) because it uses python and FlameGraph.pl
* Things allocated during the backward pass have no stack frames because they are run on another C++ thread.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/82146
Approved by: https://github.com/albanD"
pytorch/pytorch,4d3f7df33fa49a80b20ae328d5a5d71c655e3634,"Properly compare floats in gemm-block-sparse-microkernel-tester (#82947)

Similar to https://github.com/pytorch/pytorch/pull/82688

Example of current failure:

```
xplat/caffe2/aten/src/ATen/native/quantized/cpu/qnnpack/test/gemm-block-sparse-microkernel-tester.h:480
Expected equality of these values:
  c[mIndex * cStride() + nIndex]
    Which is: 34730.7
  acc[mIndex * n() + nIndex]
    Which is: 34730.7
at 7, 2: reference = 34730.65625, optimized = 34730.65234375, Mr x Nr = 8 x 4, M x N x K = 8 x 4 x 5
```
Pull Request resolved: https://github.com/pytorch/pytorch/pull/82947
Approved by: https://github.com/huydhn"
pytorch/pytorch,5b51849b48a7dbccd297286cc0110def4706f9e7,"Increase size limit on calling CublasLt in addmm by 32x (#82922)

Summary:
Increase the limit by 32 times to go to cublasLt fastpath for linear/addmm.

Why?
Discovered this when looking at linear performance for a linear with input/output  [512, 512] and an input of size [1024, 82, 512]. It was slow.

Did a sweep on inputs, and discovered there was a perf cliff between input sizes [799, 82, 512] and [800, 82, 512]. There is a check to call into CublasLt when input dim 1 is < 65535. So 799 * 82 = 65518 (fastpath), 800 * 82 = 65600 (slowpath).

With CublasLt we just get one nice sgemm. Without CublasLt, there is a copy then an sgemm, which can be almost 2x slower. This change roughly 1.6-1.9x the speed of linear/addmm. However, we only increased the limit. For matrices with even bigger sizes, it will go to the slow path again.

Test Plan: CI, manual testing with linear.

Differential Revision: D38478430

Pull Request resolved: https://github.com/pytorch/pytorch/pull/82922
Approved by: https://github.com/ngimel, https://github.com/malfet"
pytorch/pytorch,8f38f6773aa0654cde55df5025125158f4da75ee,"[Quant][fx] Remove dequant-quant around getitem (#82675)

Summary: https://github.com/pytorch/pytorch/issues/82480 saw
unnecessary dequant-quant pairs around the getitem op, which led
to significant slowdowns. This commit simply removes this pair in
the lowering step, since getitem already handles quantized inputs.

Test Plan:
python test/test_quantization.py TestQuantizeFxOps.test_getitem_no_dequant_quant

Reviewers: jerryzh168

Subscribers: jerryzh168, supriyar

Tasks: https://github.com/pytorch/pytorch/issues/82480

Differential Revision: [D38427508](https://our.internmc.facebook.com/intern/diff/D38427508)
Pull Request resolved: https://github.com/pytorch/pytorch/pull/82675
Approved by: https://github.com/jerryzh168"
pytorch/pytorch,2f9d046d6717d9423e012c5249624fbeeb506cbb,"[NestedTensor]Remove tensor buffer replace with Storage type (#82757)

### Description
In order to enable NestedTensor views NestedTensorImpl no longer stores its data in a at::Tensor buffer_ instead it conforms to the practice of most TensorImpls and uses a Storage class. This change will enable NestedTensor to use the view constructor defined on the base TensorImpl.

### Issue
#82671

### Testing
The existing nested_tensor tests are utilized since this is core functionality and would break these tests if not successful.

### Performance
One change that has potentially large performance impact is that most nested_tensor kernels call `get_buffer` to get the buffer in Tensor form and perform ops on this buffer. Previously this was free since we stored the data as a Tensor but now each kernel must construct a Tensor from the storage. The most performance critical/heavy user of nested tensors is BetterTransformer. I would be curious to see if this change significantly impacts performance for this and other workloads.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/82757
Approved by: https://github.com/albanD, https://github.com/jbschlosser"
pytorch/pytorch,b1922e03ab27db482432334e41745554753aad3d,"Test that multi_tensor optimizer state buffers match with single_tensor state buffers (#81894)

Add testing for state of multitensor optimizers suggested in #78807 (previously only the equality of model parameters after a few optimizer steps was being tested)

Pull Request resolved: https://github.com/pytorch/pytorch/pull/81894
Approved by: https://github.com/albanD"
pytorch/pytorch,8e33396cf4d27af62315fc4469884e7949a85e80,"[vulkan] Add buffer to texture and texture to buffer copies (#82799)

This diff adds functions to transfer data to GPU textures not through the `nchw_to_image` and `image_to_nchw` shaders but with host side functions that convert CPU tensors between NCHW to NC4HW formats, allowing data to be copied directly into and out from image textures via `vkCmdCopy*` API calls. These functions can be used when loading tensors from memory when loading a model, as it has the following benefits:

1. No need to construct a compute pipeline to transfer data to/from the GPU. This saves time when loading Vulkan models/performing first inference.
2. `vkCmdCopy*` is faster than compute shaders. However, due to the need to rearrange data between NCHW and NC4HW formats, it is still faster to use compute shaders to transfer input and output tensors. However, for weight tensors that can be serialized directly in NC4HW format, it is more beneficial to copy data directly.
3. It is much clearer from the code how data is represented on the GPU

Note that this change necessitated changes to how the `StagingBuffer` class worked (now called `StorageBuffer`. This is because the data size and alignment of data copied to and from an image texture is dependent on the image format of the texture.

If an image texture is `VK_FORMAT_R16G16B16A16_SFLOAT` the buffer copied from the texture can be interpreted as a contiguous array of 16 bit values. If the image texture is `VK_FORMAT_R32G32B32A32_SFLOAT` then the data buffer is an array of 32 bit values. Previously, `StagingBuffer` was constructed under the assumption that an array of 32 bit floats would be used to receive the data from the texture. This causes isses when `USE_VULKAN_FP16_INFERENCE` is turned on, which forces the `VK_FORMAT_R16G16B16A16_SFLOAT` image format to be used. Therefore, the construction of `StorageBuffer` now requires a data type and number of elements to be specified in its constructor to ensure proper sizing of the data buffer.

Differential Revision: [D38264300](https://our.internmc.facebook.com/intern/diff/D38264300/)
Pull Request resolved: https://github.com/pytorch/pytorch/pull/82799
Approved by: https://github.com/manuelcandales"
pytorch/pytorch,82f558feee1ef9909fda26df85139d3c92909e38,"Allow user to assert no mask contiguous check is necessary (#82533)

Summary:
Allow user to assert no mask contiguous check is necessary:
(1) Prevents sync event which will disrupt CUDA Graph collection, and
(2) offers slightly better performance by avoid a sync

This needs to be a separate opt-in option because we change behavior of malformed masks.  It's the only way to get BT into CUDA Graph based on what I understood about CUDA Graph collection from ngimel.

Test Plan: sandcastle unit tests

Differential Revision: D38040418

Pull Request resolved: https://github.com/pytorch/pytorch/pull/82533
Approved by: https://github.com/jbschlosser, https://github.com/zrphercule"
pytorch/pytorch,9887d51c8e752b73cafff36f1ed075aed66cf35b,"Properly compare floats in fully-connected-sparse-operator-tester (#82768)

Similar to https://github.com/pytorch/pytorch/pull/82688

Example of current failure:

```
xplat/caffe2/aten/src/ATen/native/quantized/cpu/qnnpack/test/fully-connected-sparse-operator-tester.h:582
Expected equality of these values:
  output_dynamic[i * outputChannels() + c]
    Which is: -3064.26
  accumulators_float[i * outputChannels() + c]
    Which is: -3064.26
at 0, 1: reference = -3064.2568359375, optimized = -3064.256591796875
```
Pull Request resolved: https://github.com/pytorch/pytorch/pull/82768
Approved by: https://github.com/seemethere"
pytorch/pytorch,a3909d958a716590418c4bb19c07d7d8b6307746,"Golf the function contiguous_strides (#82709)

This provides a small optimisation as the code generated is different
https://godbolt.org/z/M95qcnYsf, and runs faster
https://quick-bench.com/q/HEdCSw5CBvumdLlOvX-OfvdjH0M
Pull Request resolved: https://github.com/pytorch/pytorch/pull/82709
Approved by: https://github.com/ezyang"
pytorch/pytorch,39ffad392c49aafcfeba05e2704bb1b666247471,"Fix faulty, vectorized `pow` function on VSX (#82646)

This fixes the remaining bug introduced by the VSX optimized code in https://github.com/pytorch/pytorch/pull/41541

Followup to https://github.com/pytorch/pytorch/pull/59382

### Description

The code currently returns wrong results on POWER9LE making e.g. the `test_binary_ufuncs` fail.

### Testing

Build and ran tests on PPC
Pull Request resolved: https://github.com/pytorch/pytorch/pull/82646
Approved by: https://github.com/ezyang"
pytorch/pytorch,9647bec0ec41176adfd0cd5b80ab72c5bf6b2141,"Properly compare floats in fully-connected-operator-tester (#82688)

Need to use `ASSERT_FLOAT_EQ` for floats.

Right now the test often fails internally like this:

```
xplat/caffe2/aten/src/ATen/native/quantized/cpu/qnnpack/test/fully-connected-operator-tester.h:362
Expected equality of these values:
  output_dynamic[i * outputChannels() + c]
    Which is: -601.09
  ((float)accumulators[i * outputChannels() + c] * requantization_scales[c]) + float(bias[c])
    Which is: -601.09
at 0, 18: reference = -601.0899658203125, optimized = -601.09002685546875
```

```
xplat/caffe2/aten/src/ATen/native/quantized/cpu/qnnpack/test/fully-connected-operator-tester.h:362
Expected equality of these values:
  output_dynamic[i * outputChannels() + c]
    Which is: -65.6251
  ((float)accumulators[i * outputChannels() + c] * requantization_scales[c]) + float(bias[c])
    Which is: -65.6251
at 0, 7: reference = -65.625106811523438, optimized = -65.625099182128906
```
Pull Request resolved: https://github.com/pytorch/pytorch/pull/82688
Approved by: https://github.com/mehtanirav"
pytorch/pytorch,15a284b09e3912d4299275120d41669c75fe37da,"optimize softmax backward and logsoftmax backward (#80114)

Currently, if we run softmax_backward/logsoftmax_backward which are not along the last dim, the calculation will fall to a [scalar version](https://github.com/pytorch/pytorch/blob/32593ef2dd26e32ed44d3c03d3f5de4a42eb149a/aten/src/ATen/native/SoftMax.cpp#L220-L287). And we find actually we have the chance to vectorize the calculation along the inner_size dim.

Changes we made:

Use vectorized softmax_backward_kernel/log_softmax_backward_kernel instead of host_softmax_backward when not along the last dim.

We collected the benchmark data of softmax_backward and logsoftmax_backward for BFloat16 and Float32 data type by using the operator_benchmark tool of PyTorch on the platform of Intel(R) Xeon(R) Platinum 8260L CPU @ 2.40GHz.
Number of cores: 24 cores(1 socket)
[softmax_benchmark_32593ef.log](https://github.com/pytorch/pytorch/files/8962956/softmax_benchmark_32593ef.log)
[softmax_benchmark_the_pr.log](https://github.com/pytorch/pytorch/files/8962958/softmax_benchmark_the_pr.log)

Pull Request resolved: https://github.com/pytorch/pytorch/pull/80114
Approved by: https://github.com/frank-wei"
pytorch/pytorch,6830573c5a77757eacb82a407afb684bfff2ebbe,"[JIT] Revert SchemaInfo usage in AliasDb (#82475)

Temporary revert until we investigate performance issues in #82343
Pull Request resolved: https://github.com/pytorch/pytorch/pull/82475
Approved by: https://github.com/goldenxuett"
pytorch/pytorch,38b4114278bcab35f4221945b2777c8c1fff37a0,"[MPS] Add MPS implementation for constant_pad_nd() (#75) (#82366)

MPS has a native implementation of the constant pad nd. Adding that instead of going through the view ops helps improve performance in several benchmarks in torchbench.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/82366
Approved by: https://github.com/malfet, https://github.com/razarmehr"
pytorch/pytorch,fb54ddfe66c5bea7c2220ed59dbe5c01b1c673f3,"map_nested_tensor (#82240)

### Description
Example implementation of a slow utility function to make it easier to gain coverage.

### Issue
There's been various user requests for coverage (e.g. conv2d).

### Testing
This is a refactor. Existing tests apply.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/82240
Approved by: https://github.com/jbschlosser"
pytorch/pytorch,1ebe98220c7268fa8f3d3706be86fe4a77ab8636,"Optimize the copy of BFloat16 to Float and Float to BFloat16 (#79685)

Optimize the copy of BFloat16 to Float and Float to BFloat16.
* Vectorize the copy of BFLoat16 <-> Float
* Use `at::internal::serial_for_each` instead of directly using `cpu_kernel_vec` as  `cpu_kernel_vec` can't handle that input and output has different data types.

single socket (28cores):
```
before: torch.Size([10, 128, 10, 124])  bf16 -> fp32: 4.18e-05 ms;   fp32 -> bf16: 5.04e-05 ms
        torch.Size([10, 128, 30, 124])  bf16 -> fp32: 0.00011868 ms; fp32 -> bf16: 0.0001476 ms

after:  torch.Size([10, 128, 10, 124])  bf16 -> fp32: 1.35e-05 ms;   fp32 -> bf16: 1.97e-05 ms
        torch.Size([10, 128, 30, 124])  bf16 -> fp32: 7.32e-05 ms;   fp32 -> bf16: 5.70e-05 ms
```
single core:
```
before: torch.Size([10, 128, 10, 124])  bf16 -> fp32: 0.000848 ms;   fp32 -> bf16: 0.00105 ms
        torch.Size([10, 128, 30, 124])  bf16 -> fp32: 0.00269 ms;    fp32 -> bf16: 0.00321 ms

after:  torch.Size([10, 128, 10, 124])  bf16 -> fp32: 0.000370 ms;   fp32 -> bf16: 0.000382 ms
        torch.Size([10, 128, 30, 124])  bf16 -> fp32: 0.00153 ms;    fp32 -> bf16: 0.00113 ms
```
Pull Request resolved: https://github.com/pytorch/pytorch/pull/79685
Approved by: https://github.com/malfet"
pytorch/pytorch,b472852778eef777ac182e2efcc008429f5fc4fe,"ci: Use SCCACHE_S3_KEY_PREFIX in CI builds (#82103)

Was noticing some concerning stats when fixing some macOS builds where we were experiencing a ton of cache misses. This makes it so that there is a separate cache for each particular workflow which should hopefully reduce the amount of cache misses we experience and speed up CI by a lot.

Relates to https://github.com/mozilla/sccache/pull/811
Pull Request resolved: https://github.com/pytorch/pytorch/pull/82103
Approved by: https://github.com/kit1980"
pytorch/pytorch,0e957465802204fb30e2a94cd330c16ba71955a6,"[RFC] enable oneMKL&oneDNN on-demands verbose functinality (#63212)

**RFC:
Problem statement** 
Intel oneMKL and oneDNN are used to accelerate performance on Intel platforms. Both these 2 libraries provide verbose functionality to dump detailed operator execution information as well as execution time. These verbose messages are very helpful to performance profiling. However, the verbose functionality works for the entire execution. In many scenarios, though, we only would like to profile partial of the execution process. This feature is to expose PyTorch API functions to control oneDNN and oneMKL verbose functionality in runtime.

**Additional context**  
The most used performance profiling steps are shown as the following code snippet:

```
def inference(model, inputs):
    # step0 (optional): jit
    model = torch.jit.trace(model, inputs)

    # step1: warmup
    for _ in range(100):
        model(inputs)

    # step2: performance profiling. We only care the profiling result, as well as oneDNN and oneMKL verbose messages, of this step
    model(inputs)

    # step3 (optional): benchmarking
    t0 = time.time()
    for _ in range(100):
        model(inputs)
    t1 = time.time()
    print(‘dur: {}’.format((t1-t0)/100))
    return model(inputs)
```

Since environment variables MKL_VERBOSE and DNNL_VERBOSE will be effect to the entire progress, we will get a great number of verbose messages for all of 101 iterations (if step3 is not involved). However, we only care about the verbose messages dumped in step2. It is very difficult to filter unnecessary verbose messages out if we are running into a complicated usages scenario. Also, jit trace will also bring more undesired verbose messages.

Furthermore, there are more complicated topologies or usages like cascaded topologies as below:

```
model1 = Model1()
model2 = Model2()
model3 = Model3()
x1 = inference(model1, x)
x2 = inference(model2, x1)
y = inference(model3, x2)
```

There are many cases that it is very hard to split these child topologies out. In this scenario, it is not possible to investigate performance of each individual topology with `DNNL_VERBOSE` and `MKL_VERBOSE`.

To solve this issue, oneDNN and oneMKL provide API functions to make it possible to control verbose functionality in runtime.
```
int mkl_verbose (int enable)
status dnnl::set_verbose(int level)
```

oneDNN and oneMKL print verbose messages to stdout when oneMKL or oneDNN ops are executed.
Sample verbose messages:
```
MKL_VERBOSE SGEMM(t,n,768,2048,3072,0x7fff64115800,0x7fa1aca58040,3072,0x1041f5c0,3072,0x7fff64115820,0x981f0c0,768) 8.52ms CNR:OFF Dyn:1 FastMM:1 TID:0  NThr:44
dnnl_verbose,exec,cpu,inner_product,brgemm:avx512_core,forward_training,src_f32::blocked:ab:f0 wei_f32::blocked:AB16b64a:f0 bia_f32::blocked:a:f0 dst_f32::blocked:ab:f0,,,mb16ic768oc768,0.0839844
```

**Design and implementation** 
The design is to make python-interfaced wrap functions to invoke mkl_verbose and dnnl::set_verbose functions.

**Design concern**  

- Need to add wrapper C++ functions for mkl_verbose and dnnl::set_verbose functions in torch/csrc and aten/csrc.
- Python API functions will be added to device-specific backends
  - with torch.backends.mkl.verbose(1):
  - with torch.backends.mkldnn.verbose(1):

**Use cases**  
```
def inference(model, inputs):
    # step0 (optional): jit
    model = torch.jit.trace(model, inputs)

    # step1: warmup
    for _ in range(100):
        model(inputs)

    # step2: performance profiling
    with torch.backends.mkl.verbose(1), torch.backends.mkldnn.verbose(1):
        model(inputs)

    # step3 (optional): benchmarking
    t0 = time.time()
    for _ in range(100):
        model(inputs)
    t1 = time.time()
    print(‘dur: {}’.format((t1-t0)/100))
    return model(inputs)
```
Pull Request resolved: https://github.com/pytorch/pytorch/pull/63212
Approved by: https://github.com/VitalyFedyunin, https://github.com/malfet"
pytorch/pytorch,d537f868f37a0f773d66a33b94e5e0479dcf45a6,"[TorchTidy] Add Pattern to detect Synchronous Data Loader (#81740)

Summary: By setting num_workers > 0 in DataLoader, we can achieve async data loading, which is non blocking to the computation. This helps speed up the training process. By matching the call structure, we can detect if we are using Synchronous Data Loader.

Test Plan:
Added test in test.profiler.py

Differential Revision: [D38082644](https://our.internmc.facebook.com/intern/diff/D38082644)
Pull Request resolved: https://github.com/pytorch/pytorch/pull/81740
Approved by: https://github.com/robieta"
pytorch/pytorch,52aae5aa1991cab99b5cca39976a16648c533757,"[Sparse Adam] Fix error in loading serialized models due to introduction of new parameter (#82273)

### Description
PR #80336  introduced a new parameter to the Sparse Adam optimizer. The new parameter is accessed inside the `step` method of the optimizer. If we try to deserialize and run an older version of the optimizer before this change was introduced, it fails in the step that tries to access the missing parameter.

I have added a workaround to set a default value in case the parameter is unavailable in the optimizer.

### Issue
<!-- Link to Issue ticket or RFP -->

### Testing
* Testing on PyTorch CI
* Manual validation against existing serialized models to make sure they continue to work
Pull Request resolved: https://github.com/pytorch/pytorch/pull/82273
Approved by: https://github.com/mehtanirav, https://github.com/albanD"
pytorch/pytorch,c1b564607dc7aed6fcf56eef2895b3411e81bf93,"add graph dumping utilities (#82184)

### Description
Add compiler function to dump the forward, backward, and joint graphs. The partitioner is default partition.
The input meta to each dumped graphs will also be dumped as a pickle file.

Example usage:

```
    save_fx_func = graph_dumper_aot(current_name, folder_name, dump_example_input = False)
    optimize_ctx = torchdynamo.optimize(
        save_fx_func
    )
    with torch.enable_grad():
        with optimize_ctx:
            result = forward_and_backward_pass(model, example_inputs)
```

Pull Request resolved: https://github.com/pytorch/pytorch/pull/82184
Approved by: https://github.com/Chillee"
pytorch/pytorch,58c330fcbb09f6a1d6b422a26bd631e0ececf830,"[ao][sparsity] Data Sparsifier Benchmarking: Forward time evaluation of the sparse dlrm model with torch.sparse (#81780)

The objective is to check if introducing torch sparse coo in the sparse dlrm model improves the inference time
over different sparsity levels.
The ```evaluate_forward_time.py``` makes use of the ```sparse_model_metadata.csv``` file dumped by the
```evaluate_disk_savings.py```. Records forward time for the sparse dlrm model using sparse coo
tensors and without using sparse coo tensors and dumps it into a csv file ```dlrm_forward_time_info.csv```

**Results**: The dlrm model with sparse coo tensor is slower (roughly 2x).

After running, `evaluate_memory_savings.py`, run: `python evaluate_forward_time.py --raw_data_file=<path_to_raw_data_txt_file> --processed_data_file=<path_to_kaggleAdDisplayChallenge_processed.npz> --sparse_model_metadata=<path_to_sparse_model_metadata_csv>`

Dependencies: DLRM Repository (https://github.com/facebookresearch/dlrm)

Test Plan: None
Pull Request resolved: https://github.com/pytorch/pytorch/pull/81780
Approved by: https://github.com/z-a-f"
pytorch/pytorch,dc5762462277e752e1295c2f1df82a0eb594750c,"Revert ""Add prim, ref, and OpInfo for arange (#81734)""

This reverts commit 67dc9abbd8bb0456ae5d96d45d7edd219ca9e421.

Reverted https://github.com/pytorch/pytorch/pull/81734 on behalf of https://github.com/kit1980 due to Broke trunk slow tests https://hud.pytorch.org/pytorch/pytorch/commit/67dc9abbd8bb0456ae5d96d45d7edd219ca9e421"
pytorch/pytorch,0fcdf936e703db544f5285dfa5fa2cb2e267f8a4,"Skip tests that don't call gradcheck in slow gradcheck CI (#82117)

Pull Request resolved: https://github.com/pytorch/pytorch/pull/82117
Approved by: https://github.com/kit1980, https://github.com/albanD"
pytorch/pytorch,eca21fbd17ccbaf2a51dcde27902558b0261a457,"[ao][sparsity] Data Sparsifier Benchmarking: Model quality evaluation of the sparsified DLRM model (#81779)

The objective is to perform evaluation of the model quality after sparsifying the embeddings of the dlrm model.
The ```evaluation_model_metrics.py``` makes use of the ```sparse_model_metadata.csv``` file dumped by the
```evaluate_disk_savings.py```. The model metrics such as accuracy, auc, f1 etc are calculated on the test-dataset
for various sparsity levels, block shapes and norms available on the metadata csv file.

**Results**: The model accuracy decreases slowly with sparsity levels. Even at 90% sparsity levels, the model accuracy decreases only by 2%.

After running `evaluate_memory_savings.py`, run: `python evaluate_model_metrics.py --raw_data_file=<path_to_raw_data_txt_file> --processed_data_file=<path_to_kaggleAdDisplayChallenge_processed.npz> --sparse_model_metadata=<path_to_sparse_model_metadata_csv>`

Dependencies: DLRM Repository (https://github.com/facebookresearch/dlrm)

Test Plan: None
Pull Request resolved: https://github.com/pytorch/pytorch/pull/81779
Approved by: https://github.com/z-a-f"
pytorch/pytorch,49b4f45781af8dc63af4120f60a41bde1ef89d4d,"Add initial support for differentiable optimizers (#80938)

Adds the `differentiable` argument, a method for updating parameters in an existing optimizer, and a template for testing the differentiability of multiple optimizers.

This is all based in discussions with @albanD & @jbschlosser
Pull Request resolved: https://github.com/pytorch/pytorch/pull/80938
Approved by: https://github.com/albanD"
pytorch/pytorch,0888a4844c2879eb1581c7990f467fdc01bd763f,"Revert ""Added dynamic shape POC (#81093)""

This reverts commit 8169a85dc64710bfe1d8425e32636f0d66957d38.

Reverted https://github.com/pytorch/pytorch/pull/81093 on behalf of https://github.com/janeyx99 due to Broke slow tests on trunk https://hud.pytorch.org/pytorch/pytorch/commit/8169a85dc64710bfe1d8425e32636f0d66957d38."
pytorch/pytorch,35d97e21c8e2e28fae4a4744ce42f1544972ba1f,"[DataPipe] Simple graph snapshotting (#79479)

This mostly completes the ""poor man's snapshotting"" implementation (named ""simple snapshotting""). This is the most basic version of snapshotting but it should work for all DataPipes. I will be adding more efficient implementation for different types of DataPipes in future PRs.

### Implementation

The general idea of the simple snapshot is that we will:
1. Create a new iterator
2. Move that iterator forward by `n_iterations`
3. Save that as the `_fast_forward_iterator` of the DataPipe
4. The next time `iter` is called on the DataPipe, use the `_fast_forward_iterator`

### Usage
As of this implementation, the usage will something like:
```python
rng = torch.Generator()
initial_rng_state = rng.get_state()
datapipe: IterDataPipe = ...
# Some usage of the DataPipe, here maybe yielding the first 5 values
n_iter = 5
it = iter(datapipe)
for _ in range(n_iter):
    next(it)
serialized_graph = pickle.dumps(datapipe)

# The serialized object has most of the sufficient information for simple snapshot (except for initial RNG state)
# It can be deserialized at a later point in time or by a different process
deserialized_graph = pickle.loads(serialized_graph)
# I think `DataLoader2` or `ReadingService` should store `initial_rng_state` that can be saved by the API that we later use
rng_for_deserialized = torch.Generator()
rng_for_deserialized.set_state(initial_rng_state)
n_iterations = deserialized_graph._number_of_samples_yielded

_simple_snapshot_graph(deserialized_graph, n_iterations, rng=rng_for_deserialized)
# The while DataPipe graph should have the same state as before serialization, such that:
self.assertEqual(list(it), list(deserialized_graph))  # True
```

### Next Steps
If this looks acceptable, the next step is I will modify `DataLoader2`'s prototype ReadingService (the one with queues) to remember things like `initial_rng_state` and to have methods `save_snapshot` that will return the `(serialized graph, initial_rng)` and `restore_snapshot`. This should work for single worker data loading.

Note that, in the long term, `initial_rng_state` may not be necessary if we are able to directly save/restore the buffer and RNG state of `Shuffler` (that is work in progress). However, `initial_rng_state` and simple snapshot is still a good fall-back option for some edge cases where the buffer can't be stored.

Differential Revision: [D37943406](https://our.internmc.facebook.com/intern/diff/D37943406)
Pull Request resolved: https://github.com/pytorch/pytorch/pull/79479
Approved by: https://github.com/ejguan"
pytorch/pytorch,aeb97d9559ac0088e48db907a567177b5df438ea,"[TorchTidy] Add pattern to detect if single-tensor implementation optimizer is used (#81733)

Summary: For Adam, SGD, AdamW, enabling multi-tensor implementation is faster. So for these optimizers, we detect if single-tensor implementation is used.

Test Plan:
Added test in test_profiler.py

Differential Revision: [D38082641](https://our.internmc.facebook.com/intern/diff/D38082641)
Pull Request resolved: https://github.com/pytorch/pytorch/pull/81733
Approved by: https://github.com/robieta"
pytorch/pytorch,64c6387c0ff82d49a5bfdcae579b522ae830c2c8,"[Profiler] Add speedup estimate for FP32 pattern and Extra CUDA Copy Pattern (#81501)

Summary: The main idea is that we can run some baseline benchmarks after we are done matching the events. This gives us ability to accurate measure speed gain because system performance varies from machine to machine.

Test Plan: I did some manually testing on all the models in torchbench, as well as added a simple test in test_profiler.py

Differential Revision: [D37894566](https://our.internmc.facebook.com/intern/diff/D37894566)
Pull Request resolved: https://github.com/pytorch/pytorch/pull/81501
Approved by: https://github.com/robieta"
pytorch/pytorch,f114468fd2d23aa21e7369d55b668b3656605884,"Allow user to disable built-in fuser when using TorchDynamo (#81731)

Pytorch's built-in fuser seems have higher priority than my fuser registered via
```cpp
  torch::jit::RegisterPass pass([accelerator_symbol](std::shared_ptr<torch::jit::Graph>& g) {
    OrtFuseGraph(g, Accelerator::Supported, accelerator_symbol);
  });
```
With this PR, I can reuse `aot_autograd` backend in TorchDynamo with my own JIT fuser. My custom context is
```python
class AOTAutogradOrtFusionWithContext:
    """"""Pass nvfuser context to TorchDynamo""""""

    def __init__(self):
        self.backend_ctx_ctor = lambda: torch.jit.fuser(""none"")

    def __call__(self, gm: torch.fx.GraphModule, example_inputs):
        return AOTAutogradMemoryEfficientFusion.compile_fn(gm, example_inputs)

aot_autograd_ort_strategy = AOTAutogradOrtFusionWithContext()
```
Pull Request resolved: https://github.com/pytorch/pytorch/pull/81731
Approved by: https://github.com/davidberard98"
pytorch/pytorch,50c655d5e33eb2ec4d501fe3b03647720a3e67c8,"Adding maximize to ASGD (#81875)

Added the maximize flag #68052 to ASGD optimizer and updates the respective tests.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/81875
Approved by: https://github.com/albanD"
pytorch/pytorch,f595467e5c6569d4a457033d7ee46c7b9b1d28b1,"Reenable slow gradcheck and make it pass (#80514)

Context: For a while slow gradcheck CI was skipping nearly all tests and this hid the fact that it should've been failing and timing out (10+h runtime for TestGradients). The CI configuration has since been fixed to correct this, revealing the test failures. This PR reenables slow gradcheck CI and makes it pass again.

This PR:
- makes slow and failing tests run in fast gradcheck mode only
- reduce the input size for slow gradcheck only for unary/binary ufuncs (alternatively, skip the test entirely)
- skip entire test files on slow gradcheck runner if they don't use gradcheck (test_ops, test_meta, test_decomp, test_ops_jit)
- reduces the input size for some ops

Follow ups:
1. Investigate slow mode failures https://github.com/pytorch/pytorch/issues/80411
2. See if we can re-enable slow gradcheck tests for some of the slow tests by reducing the sizes of their inputs

The following are failing in slow mode, they are now running in fast mode only.
```
test_fn_fwgrad_bwgrad___rmod___cuda_float64
test_fn_fwgrad_bwgrad_linalg_householder_product_cuda_complex128
test_fn_fwgrad_bwgrad__masked_prod_cuda_complex128
test_fn_fwgrad_bwgrad__masked_prod_cuda_float64
test_fn_fwgrad_bwgrad_linalg_matrix_power_cuda_complex128
test_fn_fwgrad_bwgrad_cat_cuda_complex128
test_fn_fwgrad_bwgrad_linalg_lu_factor_ex_cuda_float64
test_fn_fwgrad_bwgrad_copysign_cuda_float64
test_fn_fwgrad_bwgrad_cholesky_inverse_cuda_complex128
test_fn_fwgrad_bwgrad_float_power_cuda_complex128
test_fn_fwgrad_bwgrad_fmod_cuda_float64
test_fn_fwgrad_bwgrad_float_power_cuda_float64
test_fn_fwgrad_bwgrad_linalg_lu_cuda_float64
test_fn_fwgrad_bwgrad_remainder_cuda_float64
test_fn_fwgrad_bwgrad_repeat_cuda_complex128
test_fn_fwgrad_bwgrad_prod_cuda_complex128
test_fn_fwgrad_bwgrad_slice_scatter_cuda_float64
test_fn_fwgrad_bwgrad_tile_cuda_complex128
test_fn_fwgrad_bwgrad_pow_cuda_float64
test_fn_fwgrad_bwgrad_pow_cuda_complex128
test_fn_fwgrad_bwgrad_fft_*
test_fn_fwgrad_bwgrad_zero__cuda_complex128
test_fn_gradgrad_linalg_lu_factor_cuda_float64
test_fn_grad_div_trunc_rounding_cuda_float64
test_fn_grad_div_floor_rounding_cuda_float64
```

Marks the OpInfos for the following ops that run slowly in slow gradcheck as `fast_gradcheck` only (the left column represents runtime in seconds):
```
0  918.722  test_fn_fwgrad_bwgrad_nn_functional_conv_transpose3d_cuda_float64
1  795.042  test_fn_fwgrad_bwgrad_nn_functional_unfold_cuda_complex128
2  583.63  test_fn_fwgrad_bwgrad_nn_functional_max_pool3d_cuda_float64
3  516.946  test_fn_fwgrad_bwgrad_svd_cuda_complex128
4  503.179  test_fn_fwgrad_bwgrad_linalg_svd_cuda_complex128
5  460.985  test_fn_fwgrad_bwgrad_linalg_lu_cuda_complex128
6  401.04  test_fn_fwgrad_bwgrad_linalg_lstsq_grad_oriented_cuda_complex128
7  353.671  test_fn_fwgrad_bwgrad_nn_functional_max_pool2d_cuda_float64
8  321.903  test_fn_fwgrad_bwgrad_nn_functional_gaussian_nll_loss_cuda_float64
9  307.951  test_fn_fwgrad_bwgrad_stft_cuda_complex128
10  266.104  test_fn_fwgrad_bwgrad_svd_lowrank_cuda_float64
11  221.032  test_fn_fwgrad_bwgrad_istft_cuda_complex128
12  183.741  test_fn_fwgrad_bwgrad_lu_unpack_cuda_complex128
13  132.019  test_fn_fwgrad_bwgrad_nn_functional_unfold_cuda_float64
14  125.343  test_fn_fwgrad_bwgrad_nn_functional_pad_constant_cuda_complex128
15  124.2  test_fn_fwgrad_bwgrad_kron_cuda_complex128
16  123.721  test_fn_fwgrad_bwgrad_pca_lowrank_cuda_float64
17  121.074  test_fn_fwgrad_bwgrad_nn_functional_max_unpool3d_cuda_float64
18  119.387  test_fn_fwgrad_bwgrad_rot90_cuda_complex128
19  112.889  test_fn_fwgrad_bwgrad__masked_normalize_cuda_complex128
20  107.541  test_fn_fwgrad_bwgrad_dist_cuda_complex128
21  106.727  test_fn_fwgrad_bwgrad_diff_cuda_complex128
22  104.588  test_fn_fwgrad_bwgrad__masked_cumprod_cuda_complex128
23  100.135  test_fn_fwgrad_bwgrad_nn_functional_feature_alpha_dropout_with_train_cuda_float64
24  88.359  test_fn_fwgrad_bwgrad_mH_cuda_complex128
25  86.214  test_fn_fwgrad_bwgrad_nn_functional_max_unpool2d_cuda_float64
26  83.037  test_fn_fwgrad_bwgrad_nn_functional_bilinear_cuda_float64
27  79.987  test_fn_fwgrad_bwgrad__masked_cumsum_cuda_complex128
28  77.822  test_fn_fwgrad_bwgrad_diag_embed_cuda_complex128
29  76.256  test_fn_fwgrad_bwgrad_mT_cuda_complex128
30  74.039  test_fn_fwgrad_bwgrad_linalg_lu_solve_cuda_complex128
```
```
0  334.142  test_fn_fwgrad_bwgrad_unfold_cuda_complex128
1  312.791  test_fn_fwgrad_bwgrad_linalg_lu_factor_cuda_complex128
2  121.963  test_fn_fwgrad_bwgrad_nn_functional_max_unpool3d_cuda_float64
3  108.085  test_fn_fwgrad_bwgrad_diff_cuda_complex128
4  89.418  test_fn_fwgrad_bwgrad_nn_functional_max_unpool2d_cuda_float64
5  72.231  test_fn_fwgrad_bwgrad___rdiv___cuda_complex128
6  69.433  test_fn_fwgrad_bwgrad___getitem___cuda_complex128
7  68.582  test_fn_fwgrad_bwgrad_ldexp_cuda_complex128
8  68.572  test_fn_fwgrad_bwgrad_linalg_pinv_cuda_complex128
9  67.585  test_fn_fwgrad_bwgrad_nn_functional_glu_cuda_float64
10  66.567  test_fn_fwgrad_bwgrad_lu_cuda_float64
```
```
0  630.13  test_fn_gradgrad_nn_functional_conv2d_cuda_complex128
1  81.086  test_fn_gradgrad_linalg_solve_triangular_cuda_complex128
2  71.332  test_fn_gradgrad_norm_cuda_complex128
3  64.308  test_fn_gradgrad__masked_std_cuda_complex128
4  59.519  test_fn_gradgrad_div_no_rounding_mode_cuda_complex128
5  58.836  test_fn_gradgrad_nn_functional_adaptive_avg_pool3
```

Reduces the sizes of the inputs for:
- diff
- diag_embed

Pull Request resolved: https://github.com/pytorch/pytorch/pull/80514
Approved by: https://github.com/albanD"
pytorch/pytorch,1afb604c22829c2548c81c1d05361bff2211d613,"[functorch] Generate 2^n tests, not 3^n tests for vmap (pytorch/functorch#937)

Previously, our vmap tests were generating 3^n tests per OpInfo sample.
For each tensor argument, we would generate all permutations of bdim =
(0, -1, None).

This is pretty redundant and also performance intensive. The original
purpose of this was to make sure functorch's batching rules work with
bdim other than 0 (it's really easy to forget that the bdim is not
always at the front of the tensor).

The new strategy is to generate all permutations of bdim = (-1, None)
and also include the case where all bdims are 0 as a sanity check.
This leads to 2^n tests.

On my machine test_vmap goes from 3m25s to 2m45s, which is promising.
However the biggest wins are going to be in test_ops where n can be as
high as 10."
pytorch/pytorch,cded2e29c1da33cb95cf513978cc8e14c7f2a844,"[functorch] Add ninja to circleci builds (pytorch/functorch#938)

Builds should be faster"
pytorch/pytorch,cdd7693650c0a01063e6a3608574f2432622f97c,"[functorch] Util dump chrome trace (pytorch/functorch#869)

* Output the chrome trace of running f(input, **kwargs_for_f) with [optimize_ctx]
    [num_runs] times to [trace_filename]."
pytorch/pytorch,fcd66defc5c7f9c6341053bd5be6f413ebd56514,"[functorch] Implement a Common Subexpression Elimination (CSE) pass in AOTAutograd (pytorch/functorch#852)

Implement a Common Subexpression Elimination (CSE) pass in AOTAutograd

The test is in test/test_memory_efficient_fusion.py."
pytorch/pytorch,07865422666473033b369751aad88ee9d3c19f55,"[functorch] jvp x vjp testing (pytorch/functorch#343)

Failures are either:
- lack of PyTorch forward-mode AD support (mostly)
- efficient zero tensors errors
- CUDA asserts (really need to be investigated).

All of the problems should be reproducible on the pytorch/pytorch side."
pytorch/pytorch,3c8b626b9521d7089eb60c4ac0f1fb8b5529e1a4,"[functorch] Use empty tensor instead of efficient zeros tensor (pytorch/functorch#406)

Why?
- efficient zeros tensor doesn't have efficient implementation for
convolution_backward yet
- allocating an empty tensor on CUDA doesn't actually launch any
kernels (as compared to a zero-filled tensor)"
pytorch/pytorch,183e88cdf234f9d18b630792f3c49233805421bd,"[functorch] Cleanup for memory efficient fusion (pytorch/functorch#388)

* Cleanup for memory efficient fusion

* Linters"
pytorch/pytorch,e3c1ec684f6830ec964fd3e937d014084fd0f4ba,"[functorch] convolution_backward batch rule (pytorch/functorch#359)

This is the most ridiculous batching rule we have.
Featuring a guest appearance of efficient zeros tensors.

We should really consider upstreaming einops.rearrange
(https://einops.rocks/api/rearrange/). Many batching rules are straight
up dimension manipulation and if we could specify that with strings we'd
be done 10x faster.

Test Plan:
- run tests"
pytorch/pytorch,30c7c337a2b3f1bc50d62cf54d05883993cd68e4,"[functorch] Single cache (pytorch/functorch#319)

* [CompileCache] Adding compilation cache

* [Single cache]

* Cleaning up

* Remove the templating for args

* Make things faster

* sped it up even more

* remove benchmarking coe

* make stupid fix

Co-authored-by: Horace He <horacehe2007@yahoo.com>"
pytorch/pytorch,1aa9098362b47c0ae9a1103e4312cc50b86c7d97,"[functorch] [Operator Authoring] Memory efficient pointwise fusion (pytorch/functorch#233)

* [Operator Authoring] Memory efficient pointwise fusion

* Added NVFuser and some tests"
pytorch/pytorch,cbf900b369edad2321f09737e9383c152143761e,"[functorch] [Partitioning] Recompute forward in the backward pass (pytorch/functorch#213)

Summary: Recomputation fwd in the bwd pass can improve the performance
of pointwise operators, where it helps us in reduce memory bandwidth
pressure at the expense of more computation. This PR adds a new
partitioning function to enable this type of recomputation."
pytorch/pytorch,768be7a30e11e5cf0c9c82796296e4146121414c,"[functorch] Make one of the boxed fallbacks faster

We should do this for the others"
pytorch/pytorch,ab71bc7b8135be2c71555a1c01c12461a521ee95,[functorch] made compile_module faster
pytorch/pytorch,5de1f0c0a1464bb804d131056df590523d438038,[functorch] Mention that users should install ninja to speed up the build
pytorch/pytorch,6b59f1ad78ba62fb043b4b3da34afb8ce6ec3416,"[functorch] Add `functorch_additional_op_db`

PyTorch OpInfo coverage doesn't include nn.functional.* ops. We can use
functorch_additional_op_db as a staging ground where we implement
nn.functional.* OpInfos and then later upstream them.

Why not just upstream them immediately? I can write OpInfos a few times
faster if I don't have to worry about figuring out what the correct
flags are to pass PyTorch tests..."
pytorch/pytorch,bea0df36c2a7bab50b986b99214cd0660c820ad1,"[functorch] Clarify the vmap fallback warning message

I got feedback from two users that they were unsure of if the warning
meant anything. The warning now clearly states that there is a
performance degredation and to file us an issue about it."
pytorch/pytorch,eb788ae9865c21947c1c6df009819aa3417311d6,"[functorch] maxpool_2d_with_indices_backward batch rule for specific case

This gives us coverage on all the batch rules for the cifar10 dp
example. We're slightly slower than opacus though, the numbers on my
machine are:
- 4 it/s for functorch vmap+grad
- 4.2 it/s for opacus

The differential should be investigated and I am also not sure if the
benchmarks are comparing the right things."
pytorch/pytorch,d65bb48b46adca5dd60bfe15a8d450dd2c2bd238,[functorch] Added a way to call the slow fallback from plumbing
pytorch/pytorch,86bd888bc6566763073dc5402d68f5aec09f0a4c,"ci: Fix mac names, bump builds to bigger runner (#81874)

Mac names were inconsistent so I changed them to be more reflective of
what they're actually testing. As well wanted to update the builders for
the build portions to run on more powerful runners to reduce tts

Some data:
* Cuts build time by 2/3 for `arm64` cross builds:
  * [Trunk](https://github.com/pytorch/pytorch/runs/7447691552?check_suite_focus=true): 1h 22min
  * [This PR](https://github.com/pytorch/pytorch/runs/7449648493?check_suite_focus=true): 30 min
* Similar speed ups for `x86`:
  * [Trunk](https://github.com/pytorch/pytorch/runs/7447691171?check_suite_focus=true): 1h 47min
  * [This PR](https://github.com/pytorch/pytorch/runs/7449647307?check_suite_focus=true): 41min

Signed-off-by: Eli Uriegas <eliuriegas@fb.com>
Pull Request resolved: https://github.com/pytorch/pytorch/pull/81874
Approved by: https://github.com/albanD, https://github.com/atalman"
pytorch/pytorch,c179597753f20089056115542d3b9c74ab8b864c,"Add native impl for group norm on quantized CPU for channels-last inputs (#70520)

**Description**
This PR adds a native implementation for group norm on quantized CPU for channels-last inputs. It will also be used by instance norm since the latter would call group norm kernel eventually.

For channels last, group norm has an input shape of `{N, H, W, GD}`, mean and rstd are collected per each n and g, which involves reduction on non-adjacent dimensions. We can parallel in the following 2 impls:
- impl-1: parallel on `N * G`. Only need one omp session but memory access per thread is non-contiguous.
- impl-2: parallel on `N * HxW`. Memory access per thread is contiguous but requires help of extra temp buffer of size `{T, N, 2C}`.

Generally, impl-2 has better performance when `HxW` is large enough, so that data per thread `{NHWC / T}` is much larger than temp buffer per thread `{2NC}`
A threshold is defined to switch between the two implementations, which is found by tests.

The unit test for quantized group norm is modified to cover more cases.

**Performance test results**
Test Env:
- Intel® Xeon® CLX-8260
- 1 instance, 4 cores
- Using Jemalloc

Test method:
Create channels-last tensors as inputs, do group norm by
- Converting to contiguous then using NCHW kernel
- Using NHWC impl-1
- Using NHWC impl-2
- Using fp32 kernel (no quantization)

C=64
![image](https://user-images.githubusercontent.com/12522207/147727406-baeb5c03-12a3-4925-8816-2ce84f2742a8.png)

C=256
![image](https://user-images.githubusercontent.com/12522207/147727617-6f631448-a1d3-483f-9f7a-60129bd36804.png)

C=1024
![image](https://user-images.githubusercontent.com/12522207/147727927-14ad766c-0ee3-4528-99b4-4434fc4a0a75.png)

We selected 512 as the threshold as mentioned above according to the test results.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/70520
Approved by: https://github.com/vkuzo"
pytorch/pytorch,0aedda25bce24f66b7205c0254808edffeb3ff05,"[PyTorch] Reporting OOM events to the Pytorch Profiler. (#80050)

Summary: Similar to reporting alloc and dealloc events in the PyTorch profiler, we are now reporting Out of Memory events as well. This is useful for performance troubleshooting

Test Plan: Added test_oom_tracing to test/test_profiler.py

Differential Revision: D36268132

Pull Request resolved: https://github.com/pytorch/pytorch/pull/80050
Approved by: https://github.com/robieta"
pytorch/pytorch,a60907ec1132a1a73523bc615dc23cfbbc48517b,"Adding fsdp fp16 and bf16 hooks (#81711)

Recently, `register_comm_hook` was introduced to `FSDP`, which at the moment supports only `NO_SHARD` strategy and has a default `all_reduce` hook implemented. This PR adds two lower precision hooks to an existing default hook.

I've also made slight adjustments to existing implementation of an `all_reduce` hook including:

`AllReduceState` ->` DefaultState `, motivation: `AllReduceState` is not specific to all_reduce. Gradients' pre- and post-division factors are also useful for other hooks, that require pre- and post-division, e.g. `fp16_hook` and `bf16_hook`.
I've put all 3 hooks into `default_hooks.py`
Additionally, `FSDP` supports `MixedPrecision` and, theoretically, it is possible to specify MixedPrecision for gradients and attach a lower precision hook to the model. To avoid double-casting, I've added a couple of checks to `fully_sharded_data_parallel`, i.e. casting to precision and back is performed by a lower precision hook only. I think, as a next step, it would be nice to ensure that user can't have both lower precision hook and MixedPrecision(reduce_dtype=<precision>) specified, but I am happy to discuss this and adjust current implementation.

As a test, I create two models: one with a lower precision hook and one with a `MixedPrecision(reduce_dtype=<precision>)` specified, perform one forward/backward and optimizer step and compare gradients.

PS. first version of this PR was reverted, because added unittests didn't include NCCL version checks for `bf16_hook` (thus failed on trunk). In this version, I've added appropriate checks for tests.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/81711
Approved by: https://github.com/rohan-varma"
pytorch/pytorch,081eedca1e502cc21ce26351cb9e701d0f8dd3ee,"Revert ""[complex] conv_tranpose1d (#79694)""

This reverts commit 0f61414579f5b2e4df93d60009fc225f770fa11b.

Reverted https://github.com/pytorch/pytorch/pull/79694 on behalf of https://github.com/anjali411 due to broke slow test https://github.com/pytorch/pytorch/runs/7414560957?check_suite_focus=true#step:9:31516"
pytorch/pytorch,06a0cfc0ea0cc703a1ebc8148181ac3e3cb80ab5,"pytest to run test_ops, test_ops_gradients, test_ops_jit in non linux cuda environments (#79898)

This PR uses pytest to run test_ops, test_ops_gradients, and test_ops_jit in parallel in non linux cuda environments to decrease TTS.  I am excluding linux cuda because running in parallel results in errors due to running out of memory

Notes:
* update hypothesis version for compatability with pytest
* use rerun-failures to rerun tests (similar to flaky tests, although these test files generally don't have flaky tests)
  * reruns are denoted by a rerun tag in the xml.  Failed reruns also have the failure tag.  Successes (meaning that the test is flaky) do not have the failure tag.
* see https://docs.google.com/spreadsheets/d/1aO0Rbg3y3ch7ghipt63PG2KNEUppl9a5b18Hmv2CZ4E/edit#gid=602543594 for info on speedup (or slowdown in the case of slow tests)
  * expecting windows tests to decrease by 60 minutes total
* slow test infra is expected to stay the same - verified by running pytest and unittest on the same job and check the number of skipped/run tests
* test reports to s3 changed - add entirely new table to keep track of invoking_file times
Pull Request resolved: https://github.com/pytorch/pytorch/pull/79898
Approved by: https://github.com/malfet, https://github.com/janeyx99"
pytorch/pytorch,aa1466d542c6addd5719f268f57ccdcbc0dbf84f,"Raise proper timeout when sharing the distributed shared seed (#81666)

Fixes https://github.com/pytorch/data/issues/659

- This would fix the problem that a slow DataLoader on rank 0 would cause TimeoutError as I have removed the `wait` operation on other Ranks.
- This PR also adds a [default timeout](https://github.com/pytorch/pytorch/blob/f6a45f79841fb7cdc4dfa294dbdd66d7e4b75c18/torch/csrc/distributed/c10d/ProcessGroup.hpp#L26-L27) as 30 * 60 seconds (taking reference from the distributed team's implementation). When the distributed seed is stuck on any rank, a proper timeout with detailed message will be raised.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/81666
Approved by: https://github.com/NivekT"
pytorch/pytorch,ed0091f8db1265449f13e2bdd1647bf873bd1fea,"fix view_copy kernel striding check logic (#81553)

The composite kernel for `view_copy` that we generate is special-cased a bit for efficiency to avoid having to do extra clones in some cases.

That logic was slightly wrong though, and is fixed here (it needs to mirror the logic in `reshape()`).

It manifested as a debug assert firing for Lazy Tensor, which I confirmed no longer fires when running this script:
```
# ran with ""python test_ltc_only_torch.py --device=lazy --sync=1 --nvtx=1""
import torch

import torch._lazy
from torch._lazy.ts_backend import init as init_ts_backend
init_ts_backend()
torch.manual_seed(42)
from transformers import BertForSequenceClassification

def parse_args():
  import argparse
  parser = argparse.ArgumentParser(description='')
  parser.add_argument('--device', type=str, default='cuda')
  parser.add_argument('--sync', type=bool, default=False)
  parser.add_argument('--nvtx', type=bool, default=False)
  return parser.parse_args()

args = parse_args()

device = args.device
model = BertForSequenceClassification.from_pretrained('bert-base-uncased', return_dict=True)

from transformers import AdamW
from transformers import BertTokenizer
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
text_batch = [""I love Pixar."", ""I don't care for Pixar.""]
encoding = tokenizer(text_batch, return_tensors='pt', padding=True, truncation=True)
input_ids = encoding['input_ids'].to(device)
attention_mask = encoding['attention_mask'].to(device)

model = model.to(device)
model.train()

no_decay = ['bias', 'LayerNorm.weight']
optimizer_grouped_parameters = [
    {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},
    {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}
]
optimizer = AdamW(optimizer_grouped_parameters, lr=1e-5)
labels = torch.tensor([1,0]).unsqueeze(0).to(device)
for _ in range(6):
  torch.cuda.nvtx.range_push(f'Iter{_}')

  torch.cuda.nvtx.range_push('F')
  outputs = model(input_ids, attention_mask=attention_mask, labels=labels)
  if args.sync:
    torch._lazy.mark_step()
    torch._lazy.wait_device_ops()
  torch.cuda.nvtx.range_pop()

  loss = outputs.loss

  torch.cuda.nvtx.range_push('B')
  optimizer.zero_grad()
  loss.backward()
  if args.sync:
    torch._lazy.mark_step()
    torch._lazy.wait_device_ops()
  torch.cuda.nvtx.range_pop()

  torch.cuda.nvtx.range_push('O')
  optimizer.step()
  if args.sync:
    torch._lazy.mark_step()
    torch._lazy.wait_device_ops()
  torch.cuda.nvtx.range_pop()

  torch.cuda.nvtx.range_pop()
torch._lazy.mark_step()
torch._lazy.wait_device_ops()
```

Pull Request resolved: https://github.com/pytorch/pytorch/pull/81553
Approved by: https://github.com/ezyang"
pytorch/pytorch,d80bce7afc144ae19eea5e4ba16ae890f8934ba1,"shard `trunk / linux-bionic-cuda10.2-py3.9-gcc7 / test (slow` to 2 (#81569)

`trunk / linux-bionic-cuda10.2-py3.9-gcc7 / test (slow` currently has an average TTS of 4 hrs and job duration of 3.3 hrs, so I would like to shard it in order to reduce the time.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/81569
Approved by: https://github.com/huydhn"
pytorch/pytorch,7230b67cdef51c2c04b395cd4b79e5bc5c7c6c48,"[PyTorch] Support norm_first in nn.TransformerEncoderLayer fast path (#78269)

Straightforward after previous diffs in stack cleaning up code and adding test coverage.

Differential Revision: [D36564008](https://our.internmc.facebook.com/intern/diff/D36564008/)
Pull Request resolved: https://github.com/pytorch/pytorch/pull/78269
Approved by: https://github.com/jbschlosser"
pytorch/pytorch,f7d68284679e9fdcc997ec951ef219bdaecf171a,"Adding fsdp fp16 and bf16 hooks (#80557)

Recently, `register_comm_hook` was introduced to `FSDP`, which at the moment supports only `NO_SHARD` strategy and has a default `all_reduce` hook implemented. This PR adds two lower precision hooks to an existing default hook.

I've also made slight adjustments to existing implementation of an `all_reduce` hook including:

- `AllReduceState` ->  `DefaultState` , motivation: `AllReduceState` is not specific to `all_reduce`. Gradients' pre- and post-division factors are also useful for other hooks, that require pre- and post-division, e.g. fp16_hook and bf16_hook.
- I've put all 3 hooks into `default_hooks.py`

Additionally, `FSDP` supports `MixedPrecision` and, theoretically, it is possible to specify `MixedPrecision` for gradients and attach a lower precision hook to the model. To avoid double-casting, I've added a couple of checks to `fully_sharded_data_parallel`, i.e. casting to precision and back is performed by a lower precision hook only. I think, as a next step, it would be nice to ensure that user can't have both lower precision hook and `MixedPrecision(reduce_dtype=<precision>)` specified, but I am happy to discuss this and adjust current implementation.

As a test, I create two models: one with a lower precision hook and one with a `MixedPrecision(reduce_dtype=<precision>)` specified, perform one forward/backward and optimizer step and compare gradients.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/80557
Approved by: https://github.com/rohan-varma"
pytorch/pytorch,84c8a9f88e15b6d94407e8d1839db1ea8eb6ac6a,"Use slow but safe formula for prod_backward (#81617)

prod performs a sync to test for zeros as the formula is substantially
simpler if there are no zeros, but this doesn't work for meta tensors.
The double backwards formula works great in all cases though!

Signed-off-by: Edward Z. Yang <ezyang@fb.com>
Pull Request resolved: https://github.com/pytorch/pytorch/pull/81617
Approved by: https://github.com/soulitzer"
pytorch/pytorch,547e4997317d36f86cfea48c21d4350578597cb7,"Enable Zero1's ddp_with_overlap for hpu backend (#80438)

Enable zero with ddp overlap feature along with a simple interface to insert functional optimizer to the map

Signed-off-by: Jerome <janand@habana.ai>

Fixes #ISSUE_NUMBER

Pull Request resolved: https://github.com/pytorch/pytorch/pull/80438
Approved by: https://github.com/rohan-varma, https://github.com/awgu"
pytorch/pytorch,b22166fd62b345ff0ac51ca76d27f6c800c7ec62,"Add a small fastpath test for native mha (#81432)

Summary: We dont have a small fast path passing test for mha before, this diff added one for better testing

Test Plan: buck build mode/dev-nosan -c fbcode.platform=platform009 -c fbcode.enable_gpu_sections=true caffe2/test:nn && buck-out/dev/gen/caffe2/test/nn\#binary.par -r test_multihead_attn_fast_path_small_test

Differential Revision: D37834319

Pull Request resolved: https://github.com/pytorch/pytorch/pull/81432
Approved by: https://github.com/erichan1"
pytorch/pytorch,23088fcfdf77632d4e6db4d35ce62735ca6622d2,"disable src mask for transformer and multiheadattention fastpath (#81277)

Disable fastpath if src_mask passed to TransformerEncoderLayer and MultiheadAttention.
- Refactored test_transformerencoder from test_nn.py to test_transformers.py. Added a src_mask test there.
- Added a specific src_mask test in test_transformers.py

Fixes https://github.com/pytorch/pytorch/issues/81129

Pull Request resolved: https://github.com/pytorch/pytorch/pull/81277
Approved by: https://github.com/zrphercule"
pytorch/pytorch,cc62603938a6a62ea5c35ce94a1255fc21805386,"CPUBlas: Use mkldnn optimized BFloat16 matmul for gemm (#65840)

Pull Request resolved: https://github.com/pytorch/pytorch/pull/65840
Approved by: https://github.com/malfet"
pytorch/pytorch,bebe74001a81ede05d3f649909b9f89149f0b2b4,"[Bootcamp T124004534] Better Transformer fastpath diagnostics (#81013)

Summary:
update the boolean logic for TransformerEncoder => https://www.internalfb.com/code/fbsource/[0da9d46c97ca76432303e55492b1e8b4131c781a]/fbcode/caffe2/torch/nn/modules/transformer.py?lines=206-212%2C228-230
and TransformerEncoderLayer => https://www.internalfb.com/code/fbsource/[0da9d46c97ca76432303e55492b1e8b4131c781a]/fbcode/caffe2/torch/nn/modules/transformer.py?lines=410-417%2C432-436

according to the decision logic => https://www.internalfb.com/code/fbsource/[0da9d46c97ca76432303e55492b1e8b4131c781a]/fbcode/caffe2/torch/nn/modules/activation.py?lines=1059-1091%2C1103-1109

Differential Revision: D37666451

Pull Request resolved: https://github.com/pytorch/pytorch/pull/81013
Approved by: https://github.com/jbschlosser"
pytorch/pytorch,15845f98fb1ed2a5b821dc63fe96a8455aaa5038,"[vulkan] Use busy polling when waiting for VkFence (#81470)

When waiting for a `VkFence`, busy poll instead of calling `vkWaitForFences`.

It appears that `vkWaitForFences` is implemented in such a way that the calling thread is put to sleep until it receives a signal from the fence. This causes the reduction in CPU frequency and causes subsequent CPU operations (such as the memcpy back to a CPU tensor) to take much longer than it otherwise would have. Busy waiting keeps the CPU hot and therefore avoids this problem.

Making this change drastically reduces benchmark latency. Before this change, the xirp20a model on Pixel 3 ran at an average latency of 16 ms per iteration. After switching to busy polling, benchmark latency sits at 10 ms per iteration. This is presumably caused by having the CPU stay at a clock frequency rather than constantly switching between high and low frequencies.

Differential Revision: [D37800456](https://our.internmc.facebook.com/intern/diff/D37800456/)
Pull Request resolved: https://github.com/pytorch/pytorch/pull/81470
Approved by: https://github.com/kimishpatel"
pytorch/pytorch,880b9728411502308ac2efb6d4fa8a41201e3c9a,"More efficient indices validations for compressed sparse formats. (#81108)

As per title.

Some of the features:
- native kernels both for the CPU and CUDA without device syncs.
- If needed, invariant checks 5.1 - 5.5 could be improved to utilize vectorization. This will require implementing a conversion `Vectorized -> bool`. That's a follow-up.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/81108
Approved by: https://github.com/amjames, https://github.com/pearu, https://github.com/cpuhrsch"
pytorch/pytorch,6997ac79d64c4e61b28f9a6610a80deab9c5538d,"Revert ""[primTorch] Implement constant_pad_nd (#80182)""

This reverts commit 77cfa9f7a15bef84004b1d49a65afea60571bba9.

Reverted https://github.com/pytorch/pytorch/pull/80182 on behalf of https://github.com/clee2000 due to causes failures on trunk / linux-bionic-py3.7-clang9-slow / test (slow https://github.com/pytorch/pytorch/runs/7343337014?check_suite_focus=true"
pytorch/pytorch,667607b3abf7854127b74b8c8b2cb2237dd67dcb,"[MPS] Reduce the number of command_buf created and improve performance (#81338)

The PR improves performance and reduces the CPU overhead by reducing the number of command buffers created. It uses commit and continue feature in MPS.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/81338
Approved by: https://github.com/razarmehr"
pytorch/pytorch,1810c876df592ebaf4adc6ee109cc45d37974ec6,"Remove noexcept from dtype (#80991)

Assuming we virtualize it soon the noexcept would cause bugs.
Layout's noexcept is being removed by the layout slow path PR.

Signed-off-by: Edward Z. Yang <ezyang@fb.com>
Pull Request resolved: https://github.com/pytorch/pytorch/pull/80991
Approved by: https://github.com/cpuhrsch"
pytorch/pytorch,fb93c3988a167e2fb8b636b6b5855705aa80a9d8,"[build] Split `.cu` to improve compile times (#81193)

The goal is to speed up CUDA builds. I was looking at bulid times and found that we have large CUDA compilation units that take forever to compile and make parallelism less effective. This PR splits them up into different `.cu` files so we can parallelize compilation better. We've done this sort of thing in the past with some success.

With a cold build, timing before: 5m42.019s, timing after: 4m30.275s. That's a speedup of 18.1% for me.

Behaviorally this should be a no-op, I'm just moving code around. There is still more we can do here but I did most of the ones that are copypasta. The full list of remaining chonky compilation units is [here](https://gist.github.com/suo/0dc217733f40f59898a8cc4f60529d60).

## Details
Here's a screenshot from a ninja trace, with the following command:
```
MAX_JOBS=64 CCACHE_DISABLE=1 TORCH_CUDA_ARCH_LIST=Ampere BUILD_CAFFE2_OPS=0 USE_FBGEMM=0 USE_DISTRIBUTED=0 USE_MKLDNN=0 BUILD_TEST=0 USE_GOLD_LINKER=1 USE_OPENMP=1 USE_NCCL=0 DEBUG=0 python setup.py develop
```
<img width=""1475"" alt=""image"" src=""https://user-images.githubusercontent.com/1617424/178170276-ee0e5eb0-4c16-4b86-b4af-2a9e615b7f5f.png"">

([source trace](https://gist.github.com/suo/5f5458f2630f9ab6dcbea6989e892195), which you can visualize in [perfetto](https://ui.perfetto.dev/))

After this PR, we get somewhat better utilization (although there is plenty still left to do):
<img width=""1466"" alt=""image"" src=""https://user-images.githubusercontent.com/1617424/178178944-63ca9ff0-9cd3-4008-9a6d-d8623b5148c5.png"">

([source trace](https://gist.github.com/suo/5607335bcd4bd412d42b0c9334259184))
Pull Request resolved: https://github.com/pytorch/pytorch/pull/81193
Approved by: https://github.com/cpuhrsch, https://github.com/malfet"
pytorch/pytorch,b946e7a7f2d56d510952bb6099cf3e7e2f46f1a2,"[TorchArrow][efficiency][2/n] added inference_wrapper_run_flat_out fused version registration call (#81131)

Summary:
Added registration call to make pytorch runtime aware of `inference_wrapper_run_flat_out` functionality. Added fused version of op which will enable out variants when [OptimizeGraph pass](https://fburl.com/code/lsagnmge) is made.

Next: need to add variadic version of fused and unfused op.

Reviewed By: tgalkovskyi

Differential Revision: D37139204

Pull Request resolved: https://github.com/pytorch/pytorch/pull/81131
Approved by: https://github.com/tenpercent, https://github.com/qxy11"
pytorch/pytorch,f3008be9004418c3651bf89de764355580b994a2,"[FSDP] Getting the parameter execution information using torch.fx (#80294)

This support allows one to get the parameter execution order at the FSDP module construction time rather than at runtime for some models, under which case the preparation step can be removed. This will be used in the non-recursive wrapping policy later on.

Note that this support is based on the assumption that the tracer provided by the user will be able to successfully trace the forward pass.

### Advantage of using `torch.fx` to get the parameter order rather than using backward hook:
When using backward hook, the parameter execution order will be the reversed ordering of the parameter gradient ready order. One problem is that we are not able to get the number of times each parameter is used inside the forward function. For example, consider the following forward function,
```python
def forward(self, x):
    z = self.relu(self.layer0(x))
    z = self.relu(self.layer2(z))
    z = self.relu(self.layer1(z))
    z = self.relu(self.layer0(x))
    return z
```
Based on the parameter gradient ready order, the current parameter execution order for the example is `[layer0.weight, layer2.weight, layer1.weight]`. However, we don't get the information that layer0 is called twice.
Using `torch.fx`, we can get a more detailed parameter execution order: [layer0.weight, layer2.weight, layer1.weight, layer0.weight]. This allows us to implement more scheduling algorithms that could be useful in multiple regimes. For example, since we know that `layer0` will be called twice, we can delay the resharding of `layer0.weight` to the end, which would costs more memory but faster.

### Example of API usage
The execution information is recorded via calling `tracer.trace` in the `_patch_tracer` context manager:
```python
tracer = torch.fx.Tracer() # or an instance of Tracer's children class
execution_info = _init_execution_info(model)
with _patch_tracer(
    tracer=tracer, root_module=model, execution_info=execution_info
):
    tracer.trace(model, concrete_args=...)
```
The execution information will be recorded in `execution_info.module_forward_order` and `execution_info.module_to_execution_infos`.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/80294
Approved by: https://github.com/mrshenli, https://github.com/zhaojuanmao"
pytorch/pytorch,f24c94d7ae00a7acc1fc3d543e6217509cfeb885,"Adding maximize to SparseAdam (#80336)

Added the maximize flag #68052 to SparseAdam optimizer and updates the respective tests.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/80336
Approved by: https://github.com/albanD"
pytorch/pytorch,495aa9bc3adf88e1499a042323871dc4650c7c6f,"Adding maximize to rprop (#80335)

Added the maximize flag #68052 to rprop optimizer and updates the respective tests.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/80335
Approved by: https://github.com/albanD"
pytorch/pytorch,a1fd5b42730924c5e5a2beb5dabc779725768830,"Adding maximize to RMSprop (#80326)

Added the maximize flag #68052 to RMSprop optimizer and updates the respective tests.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/80326
Approved by: https://github.com/albanD"
pytorch/pytorch,14bd5bd6eee8e9a33c557aa7625a082e96a70242,"Adding maximize to ASGD (#80323)

Added the maximize flag #68052 to ASGD optimizer and updates the respective tests.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/80323
Approved by: https://github.com/albanD"
pytorch/pytorch,3b78c5682b483086f66d875749f94b7551072a05,"Don't implicitly convert to channels-first in MaxPool3D on CUDA (#80748)

MaxPool3D currently converts inputs implicitly to channels-first (via `.contiguous()`) which may yield unexpected regressions in workloads that expect a full channels-last path. This PR preserves the channels-last format in MaxPool3D while attempting to avoid seriously regressing performance.

Currently, typical case (kernel size == 2 == stride) looks good, but larger kernel sizes (>4) or the unusual case of stride 1 can sometimes be slower than converting to channels-first before doing MaxPool3D.

Additionally, this PR adds a test for 64bit-indexing backwards as testing of these changes uncovered an IMA for large tensors when doing the backwards pass with MaxPool3D.

Performance comparison on A6000:

```
[------------------------------------- max_pool3d ---------------------------------------------------------]
                                          |  channels_last=False  |   curr ch_last=True |  new ch_last=True
1 threads: ---------------------------------------------------------------------------- ---------------------
      [64, 256, 32, 32, 32] 4x4 stride 4  |        20093.5        |       34823.4       |       20640.0
      [64, 256, 32, 32, 32] 4x4 stride 2  |        28623.7        |       42625.6       |       27935.5
      [64, 256, 32, 32, 32] 4x4 stride 1  |        68177.5        |       79147.2       |       85604.8
      [64, 256, 32, 32, 32] 2x2 stride 4  |        17237.7        |       32071.3       |       16641.6
      [64, 256, 32, 32, 32] 2x2 stride 2  |        25252.5        |       39993.2       |       25054.8
      [64, 256, 32, 32, 32] 2x2 stride 1  |        43185.2        |       58164.6       |       48416.9
      [64, 256, 16, 16, 16] 4x4 stride 4  |         3017.7        |        3952.4       |        2593.8
      [64, 256, 16, 16, 16] 4x4 stride 2  |         4581.5        |        5384.3       |        3294.3
      [64, 256, 16, 16, 16] 4x4 stride 1  |        11334.1        |       11534.7       |        8651.1
      [64, 256, 16, 16, 16] 2x2 stride 4  |         2346.9        |        3304.6       |        2098.8
      [64, 256, 16, 16, 16] 2x2 stride 2  |         3550.8        |        4526.5       |        3143.6
      [64, 256, 16, 16, 16] 2x2 stride 1  |         6898.1        |        7816.0       |        5820.8
      [64, 256, 4, 4, 4] 4x4 stride 4     |          191.5        |         176.3       |          77.5
      [64, 256, 4, 4, 4] 4x4 stride 2     |          191.8        |         176.8       |          94.1
      [64, 256, 4, 4, 4] 4x4 stride 1     |          191.3        |         176.4       |          97.3
      [64, 256, 4, 4, 4] 2x2 stride 4     |           96.4        |         114.4       |          93.6
      [64, 256, 4, 4, 4] 2x2 stride 2     |          172.1        |         178.6       |          93.7
      [64, 256, 4, 4, 4] 2x2 stride 1     |          263.0        |         279.4       |          92.4
      [64, 64, 32, 32, 32] 4x4 stride 4   |         5033.2        |        7208.3       |        5167.5
      [64, 64, 32, 32, 32] 4x4 stride 2   |         7216.1        |        9218.7       |        6637.1
      [64, 64, 32, 32, 32] 4x4 stride 1   |        17192.1        |       18392.9       |       20489.0
      [64, 64, 32, 32, 32] 2x2 stride 4   |         4318.0        |        6511.2       |        4193.1
      [64, 64, 32, 32, 32] 2x2 stride 2   |         6324.4        |        8657.7       |        6263.6
      [64, 64, 32, 32, 32] 2x2 stride 1   |        10855.0        |       13040.2       |       12055.9
      [64, 64, 16, 16, 16] 4x4 stride 4   |          764.1        |         975.6       |         671.3
      [64, 64, 16, 16, 16] 4x4 stride 2   |         1163.1        |        1333.4       |         833.6
      [64, 64, 16, 16, 16] 4x4 stride 1   |         2890.0        |        2898.5       |        2209.8
      [64, 64, 16, 16, 16] 2x2 stride 4   |          593.5        |         811.2       |         536.3
      [64, 64, 16, 16, 16] 2x2 stride 2   |          895.9        |        1112.3       |         794.5
      [64, 64, 16, 16, 16] 2x2 stride 1   |         1742.5        |        1968.0       |        1475.2
      [64, 64, 4, 4, 4] 4x4 stride 4      |          101.1        |         112.2       |          93.4
      [64, 64, 4, 4, 4] 4x4 stride 2      |           96.7        |         114.6       |          92.5
      [64, 64, 4, 4, 4] 4x4 stride 1      |           98.9        |         111.9       |          96.5
      [64, 64, 4, 4, 4] 2x2 stride 4      |          100.1        |         107.1       |          94.2
      [64, 64, 4, 4, 4] 2x2 stride 2      |           96.6        |         108.0       |          94.5
      [64, 64, 4, 4, 4] 2x2 stride 1      |           96.7        |         107.9       |          95.2
      [64, 3, 32, 32, 32] 4x4 stride 4    |          250.1        |         326.6       |         278.0
      [64, 3, 32, 32, 32] 4x4 stride 2    |          350.4        |         414.0       |         323.2
      [64, 3, 32, 32, 32] 4x4 stride 1    |          825.6        |         846.9       |         982.5
      [64, 3, 32, 32, 32] 2x2 stride 4    |          213.3        |         289.8       |         219.9
      [64, 3, 32, 32, 32] 2x2 stride 2    |          308.2        |         384.9       |         305.9
      [64, 3, 32, 32, 32] 2x2 stride 1    |          523.5        |         594.7       |         589.9
      [64, 3, 16, 16, 16] 4x4 stride 4    |          103.8        |         116.7       |          93.0
      [64, 3, 16, 16, 16] 4x4 stride 2    |          100.9        |         108.3       |          93.3
      [64, 3, 16, 16, 16] 4x4 stride 1    |          139.4        |         140.7       |         104.8
      [64, 3, 16, 16, 16] 2x2 stride 4    |           97.5        |         114.7       |          92.7
      [64, 3, 16, 16, 16] 2x2 stride 2    |           97.4        |         108.8       |          91.7
      [64, 3, 16, 16, 16] 2x2 stride 1    |           99.9        |         108.0       |          94.1
      [64, 3, 4, 4, 4] 4x4 stride 4       |           97.2        |         110.2       |          94.7
      [64, 3, 4, 4, 4] 4x4 stride 2       |          105.7        |         107.4       |          92.8
      [64, 3, 4, 4, 4] 4x4 stride 1       |           98.0        |         110.0       |          93.7
      [64, 3, 4, 4, 4] 2x2 stride 4       |           98.3        |         116.7       |          93.0
      [64, 3, 4, 4, 4] 2x2 stride 2       |           98.6        |         107.5       |          92.8
      [64, 3, 4, 4, 4] 2x2 stride 1       |          100.6        |         110.3       |          94.0
      [16, 256, 32, 32, 32] 4x4 stride 4  |         5034.2        |        8838.0       |        5165.9
      [16, 256, 32, 32, 32] 4x4 stride 2  |         7236.3        |       10869.9       |        7038.2
      [16, 256, 32, 32, 32] 4x4 stride 1  |        17385.4        |       21401.6       |       21900.7
      [16, 256, 32, 32, 32] 2x2 stride 4  |         4318.7        |        8101.2       |        4172.9
      [16, 256, 32, 32, 32] 2x2 stride 2  |         6324.0        |       10147.5       |        6279.7
      [16, 256, 32, 32, 32] 2x2 stride 1  |        10899.7        |       14826.0       |       12256.3
      [16, 256, 16, 16, 16] 4x4 stride 4  |          765.4        |        1012.7       |         675.6
      [16, 256, 16, 16, 16] 4x4 stride 2  |         1162.8        |        1376.9       |         843.4
      [16, 256, 16, 16, 16] 4x4 stride 1  |         2928.9        |        2969.8       |        2222.5
      [16, 256, 16, 16, 16] 2x2 stride 4  |          593.5        |         845.8       |         534.2
      [16, 256, 16, 16, 16] 2x2 stride 2  |          896.9        |        1152.2       |         796.9
      [16, 256, 16, 16, 16] 2x2 stride 1  |         1750.2        |        2009.4       |        1481.8
      [16, 256, 4, 4, 4] 4x4 stride 4     |           96.6        |         107.1       |          92.7
      [16, 256, 4, 4, 4] 4x4 stride 2     |           97.9        |         114.9       |          93.8
      [16, 256, 4, 4, 4] 4x4 stride 1     |           98.2        |         115.6       |          94.0
      [16, 256, 4, 4, 4] 2x2 stride 4     |           97.0        |         106.7       |          93.8
      [16, 256, 4, 4, 4] 2x2 stride 2     |           96.8        |         108.1       |          93.3
      [16, 256, 4, 4, 4] 2x2 stride 1     |           95.8        |         120.9       |          95.7
      [16, 64, 32, 32, 32] 4x4 stride 4   |         1266.4        |        1815.4       |        1312.3
      [16, 64, 32, 32, 32] 4x4 stride 2   |         1818.5        |        2328.0       |        1678.9
      [16, 64, 32, 32, 32] 4x4 stride 1   |         4352.9        |        4649.3       |        5204.6
      [16, 64, 32, 32, 32] 2x2 stride 4   |         1090.0        |        1631.2       |        1060.8
      [16, 64, 32, 32, 32] 2x2 stride 2   |         1589.4        |        2141.1       |        1576.4
      [16, 64, 32, 32, 32] 2x2 stride 1   |         2733.5        |        3286.0       |        3041.6
      [16, 64, 16, 16, 16] 4x4 stride 4   |          201.7        |         259.6       |         175.0
      [16, 64, 16, 16, 16] 4x4 stride 2   |          301.0        |         350.1       |         226.3
      [16, 64, 16, 16, 16] 4x4 stride 1   |          740.1        |         748.7       |         570.6
      [16, 64, 16, 16, 16] 2x2 stride 4   |          156.0        |         214.8       |         140.8
      [16, 64, 16, 16, 16] 2x2 stride 2   |          232.3        |         292.3       |         208.7
      [16, 64, 16, 16, 16] 2x2 stride 1   |          449.1        |         504.0       |         382.1
      [16, 64, 4, 4, 4] 4x4 stride 4      |           97.5        |         111.4       |          94.5
      [16, 64, 4, 4, 4] 4x4 stride 2      |           98.8        |         111.9       |          94.4
      [16, 64, 4, 4, 4] 4x4 stride 1      |           98.2        |         112.0       |          95.2
      [16, 64, 4, 4, 4] 2x2 stride 4      |           99.7        |         111.0       |          94.0
      [16, 64, 4, 4, 4] 2x2 stride 2      |          100.3        |         110.0       |          93.2
      [16, 64, 4, 4, 4] 2x2 stride 1      |           97.5        |         107.6       |          93.5
      [16, 3, 32, 32, 32] 4x4 stride 4    |          100.5        |         117.1       |          95.7
      [16, 3, 32, 32, 32] 4x4 stride 2    |           97.5        |         121.3       |          92.5
      [16, 3, 32, 32, 32] 4x4 stride 1    |          216.0        |         227.4       |         258.4
      [16, 3, 32, 32, 32] 2x2 stride 4    |           97.1        |         109.0       |          91.9
      [16, 3, 32, 32, 32] 2x2 stride 2    |           95.8        |         108.5       |          92.9
      [16, 3, 32, 32, 32] 2x2 stride 1    |          139.4        |         161.2       |         157.8
      [16, 3, 16, 16, 16] 4x4 stride 4    |           96.4        |         113.6       |          91.9
      [16, 3, 16, 16, 16] 4x4 stride 2    |           97.4        |         108.1       |          93.5
      [16, 3, 16, 16, 16] 4x4 stride 1    |           99.0        |         107.5       |          92.1
      [16, 3, 16, 16, 16] 2x2 stride 4    |           96.9        |         118.1       |          93.4
      [16, 3, 16, 16, 16] 2x2 stride 2    |           97.3        |         106.7       |          95.8
      [16, 3, 16, 16, 16] 2x2 stride 1    |           98.8        |         109.2       |          93.8
      [16, 3, 4, 4, 4] 4x4 stride 4       |           97.8        |         108.0       |          94.2
      [16, 3, 4, 4, 4] 4x4 stride 2       |           92.7        |         108.0       |          93.9
      [16, 3, 4, 4, 4] 4x4 stride 1       |           97.8        |         107.6       |          93.5
      [16, 3, 4, 4, 4] 2x2 stride 4       |          100.3        |         107.7       |          94.3
      [16, 3, 4, 4, 4] 2x2 stride 2       |           97.2        |         107.5       |          96.1
      [16, 3, 4, 4, 4] 2x2 stride 1       |           98.1        |         111.1       |          93.8

Times are in microseconds (us).
```

Performance comparison on V100:
(these times have been updated after working around some noisy measurements in my setup)
```
[------------------------------------- max_pool3d ---------------------------------------------------------]
                                          |  channels_last=False  |  curr ch_last=True |  new ch_last=True
1 threads: -------------------------------------------------------------------------------------------------
      [64, 256, 32, 32, 32] 4x4 stride 4  |        15810.7        |       33807.7      |        16452.9
      [64, 256, 32, 32, 32] 4x4 stride 2  |        24422.7        |       42515.3      |        27700.3
      [64, 256, 32, 32, 32] 4x4 stride 1  |        71756.0        |       89916.5      |       106464.0
      [64, 256, 32, 32, 32] 2x2 stride 4  |        12102.9        |       30210.4      |        11319.8
      [64, 256, 32, 32, 32] 2x2 stride 2  |        19101.7        |       37210.8      |        20373.3
      [64, 256, 32, 32, 32] 2x2 stride 1  |        41418.0        |       59650.5      |        53009.2
      [64, 256, 16, 16, 16] 4x4 stride 4  |         2362.0        |        4210.3      |         2114.0
      [64, 256, 16, 16, 16] 4x4 stride 2  |         4102.4        |        5897.4      |         3179.7
      [64, 256, 16, 16, 16] 4x4 stride 1  |        11339.3        |       13116.6      |        10032.6
      [64, 256, 16, 16, 16] 2x2 stride 4  |         1709.7        |        3506.7      |         1423.6
      [64, 256, 16, 16, 16] 2x2 stride 2  |         2966.6        |        4760.8      |         2499.3
      [64, 256, 16, 16, 16] 2x2 stride 1  |         6998.4        |        8797.3      |         6152.0
      [64, 256, 4, 4, 4] 4x4 stride 4     |          173.0        |         176.3      |          127.9
      [64, 256, 4, 4, 4] 4x4 stride 2     |          149.1        |         176.3      |          125.5
      [64, 256, 4, 4, 4] 4x4 stride 1     |          150.0        |         177.2      |          125.6
      [64, 256, 4, 4, 4] 2x2 stride 4     |          158.0        |         192.7      |          127.9
      [64, 256, 4, 4, 4] 2x2 stride 2     |          169.7        |         199.2      |          125.3
      [64, 256, 4, 4, 4] 2x2 stride 1     |          289.6        |         318.2      |          116.5
      [64, 64, 32, 32, 32] 4x4 stride 4   |         3914.4        |        6993.3      |         4141.4
      [64, 64, 32, 32, 32] 4x4 stride 2   |         6107.4        |        9186.4      |         6378.5
      [64, 64, 32, 32, 32] 4x4 stride 1   |        17920.0        |       20993.5      |        23891.1
      [64, 64, 32, 32, 32] 2x2 stride 4   |         3029.7        |        6112.6      |         2895.6
      [64, 64, 32, 32, 32] 2x2 stride 2   |         4787.8        |        7870.6      |         4724.8
      [64, 64, 32, 32, 32] 2x2 stride 1   |        10366.4        |       13446.4      |        12603.8
      [64, 64, 16, 16, 16] 4x4 stride 4   |          605.8        |         962.9      |          499.7
      [64, 64, 16, 16, 16] 4x4 stride 2   |         1037.0        |        1394.8      |          791.6
      [64, 64, 16, 16, 16] 4x4 stride 1   |         2835.4        |        3191.8      |         2484.3
      [64, 64, 16, 16, 16] 2x2 stride 4   |          438.6        |         795.7      |          368.6
      [64, 64, 16, 16, 16] 2x2 stride 2   |          749.1        |        1108.0      |          612.0
      [64, 64, 16, 16, 16] 2x2 stride 1   |         1756.4        |        2112.2      |         1538.5
      [64, 64, 4, 4, 4] 4x4 stride 4      |          132.6        |         163.9      |          115.4
      [64, 64, 4, 4, 4] 4x4 stride 2      |          129.3        |         153.7      |          117.8
      [64, 64, 4, 4, 4] 4x4 stride 1      |          128.0        |         153.8      |          117.6
      [64, 64, 4, 4, 4] 2x2 stride 4      |          128.2        |         154.1      |          117.5
      [64, 64, 4, 4, 4] 2x2 stride 2      |          130.5        |         157.3      |          117.6
      [64, 64, 4, 4, 4] 2x2 stride 1      |          128.8        |         156.4      |          120.6
      [64, 3, 32, 32, 32] 4x4 stride 4    |          200.4        |         261.0      |          228.8
      [64, 3, 32, 32, 32] 4x4 stride 2    |          305.3        |         366.5      |          344.4
      [64, 3, 32, 32, 32] 4x4 stride 1    |          860.9        |         922.1      |         1136.0
      [64, 3, 32, 32, 32] 2x2 stride 4    |          157.0        |         216.9      |          158.1
      [64, 3, 32, 32, 32] 2x2 stride 2    |          240.5        |         300.9      |          247.7
      [64, 3, 32, 32, 32] 2x2 stride 1    |          503.5        |         565.1      |          609.8
      [64, 3, 16, 16, 16] 4x4 stride 4    |          136.0        |         159.0      |          120.3
      [64, 3, 16, 16, 16] 4x4 stride 2    |          131.2        |         156.9      |          120.0
      [64, 3, 16, 16, 16] 4x4 stride 1    |          146.6        |         158.5      |          123.8
      [64, 3, 16, 16, 16] 2x2 stride 4    |          133.8        |         158.4      |          117.1
      [64, 3, 16, 16, 16] 2x2 stride 2    |          132.1        |         160.8      |          117.9
      [64, 3, 16, 16, 16] 2x2 stride 1    |          133.7        |         174.4      |          118.0
      [64, 3, 4, 4, 4] 4x4 stride 4       |          156.8        |         166.2      |          119.4
      [64, 3, 4, 4, 4] 4x4 stride 2       |          126.8        |         150.4      |          118.2
      [64, 3, 4, 4, 4] 4x4 stride 1       |          125.2        |         151.7      |          117.8
      [64, 3, 4, 4, 4] 2x2 stride 4       |          127.3        |         152.7      |          116.2
      [64, 3, 4, 4, 4] 2x2 stride 2       |          128.6        |         153.3      |          114.6
      [64, 3, 4, 4, 4] 2x2 stride 1       |          128.6        |         153.5      |          114.7
      [16, 256, 32, 32, 32] 4x4 stride 4  |         3921.7        |        8445.7      |         4064.7
      [16, 256, 32, 32, 32] 4x4 stride 2  |         6111.7        |       10630.0      |         6944.4
      [16, 256, 32, 32, 32] 4x4 stride 1  |        17938.9        |       22896.8      |        26648.7
      [16, 256, 32, 32, 32] 2x2 stride 4  |         3029.6        |        7552.7      |         2840.9
      [16, 256, 32, 32, 32] 2x2 stride 2  |         4788.0        |        9322.1      |         5110.5
      [16, 256, 32, 32, 32] 2x2 stride 1  |        10363.7        |       14885.9      |        13213.6
      [16, 256, 16, 16, 16] 4x4 stride 4  |          606.0        |        1059.1      |          535.9
      [16, 256, 16, 16, 16] 4x4 stride 2  |         1037.5        |        1491.5      |          822.3
      [16, 256, 16, 16, 16] 4x4 stride 1  |         2835.4        |        3306.8      |         2522.8
      [16, 256, 16, 16, 16] 2x2 stride 4  |          438.6        |         892.3      |          369.0
      [16, 256, 16, 16, 16] 2x2 stride 2  |          749.2        |        1203.7      |          638.7
      [16, 256, 16, 16, 16] 2x2 stride 1  |         1756.1        |        2212.5      |         1547.0
      [16, 256, 4, 4, 4] 4x4 stride 4     |          159.6        |         187.6      |          117.6
      [16, 256, 4, 4, 4] 4x4 stride 2     |          161.1        |         185.5      |          117.3
      [16, 256, 4, 4, 4] 4x4 stride 1     |          160.0        |         148.1      |          117.8
      [16, 256, 4, 4, 4] 2x2 stride 4     |          123.9        |         148.3      |          117.6
      [16, 256, 4, 4, 4] 2x2 stride 2     |          126.0        |         151.7      |          117.4
      [16, 256, 4, 4, 4] 2x2 stride 1     |          127.1        |         152.3      |          117.9
      [16, 64, 32, 32, 32] 4x4 stride 4   |          983.5        |        1756.7      |         1067.8
      [16, 64, 32, 32, 32] 4x4 stride 2   |         1542.4        |        2315.2      |         1621.5
      [16, 64, 32, 32, 32] 4x4 stride 1   |         4498.7        |        5273.4      |         6006.7
      [16, 64, 32, 32, 32] 2x2 stride 4   |          767.2        |        1543.4      |          736.7
      [16, 64, 32, 32, 32] 2x2 stride 2   |         1207.8        |        1981.5      |         1197.0
      [16, 64, 32, 32, 32] 2x2 stride 1   |         2603.3        |        3367.5      |         3161.9
      [16, 64, 16, 16, 16] 4x4 stride 4   |          169.5        |         264.6      |          142.8
      [16, 64, 16, 16, 16] 4x4 stride 2   |          274.6        |         368.9      |          216.8
      [16, 64, 16, 16, 16] 4x4 stride 1   |          723.3        |         820.4      |          643.2
      [16, 64, 16, 16, 16] 2x2 stride 4   |          131.4        |         216.0      |          116.1
      [16, 64, 16, 16, 16] 2x2 stride 2   |          199.9        |         295.0      |          166.8
```
CC @ptrblck

Pull Request resolved: https://github.com/pytorch/pytorch/pull/80748
Approved by: https://github.com/ngimel"
pytorch/pytorch,6ee54a878081f4ebde074dfc028ae181be70f290,"fix weight norm backward bug on CPU when OMP_NUM_THREADS <= 2 (#80930)

fix https://github.com/pytorch/pytorch/issues/80569
root cause: `weight_norm_backward_last_dim_kernel` will create a temp buffer to
do vertical reduction, size of [num_threads, N] (N is the size of last dimension of v)

to save additional memory allocation, the original kernel reuse the buffer after
the vertical sum:
  1st row stores the final result of sum
  2nd row stores coefficient a
  3rd row stores coefficient b

when OMP_NUM_THREADS <=2, this will cause illegal memory access since the buffer size
will be only 1*N or 2*N;

the fix is use a separate buffer (`a_b`) to store the coefficients of a and b.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/80930
Approved by: https://github.com/frank-wei, https://github.com/malfet"
pytorch/pytorch,665fd30a5aaa7585128a28a50ef7f4d374a885b9,"Load CPU MLModel first, and configured MLModel async (#80941)

Summary:
MLModel loads much faster when compute units are set to CPU only. It seems when loading with compute units set to all a large amount of preprocessing work is done during init.

So, in order to speed up our effect load time, load a cpu MLModel synchronously, and a configured MLModel asyncronously. When the second model finishes loading about 600 ms later, swap the models out.

So, for about half a second inference will occur on the cpu, but after that will kick over to gpu or npu.

On iPhone 12 I'm seeing a > 10x improvement in load time as recorded by RenderTimeLogger.cpp

Test Plan:
- Add an override to https://www.internalfb.com/intern/qe2/ig_ios_person_segmentation_universe to opt into the coreml segmentation model
- Launch IG camera and apply an effect that uses segmentation, such as green screen
- Confirm that segmentation works.

https://pxl.cl/277JL

Reviewed By: kimishpatel

Differential Revision: D37597965

Pull Request resolved: https://github.com/pytorch/pytorch/pull/80941
Approved by: https://github.com/mcr229, https://github.com/kimishpatel"
pytorch/pytorch,1d4dbdcefe55c8cbee1aaca960f9b5568dd66b6c,"[mergebot] Fix Land Checks validation (#80907)

Before, it was landing it because at the beginning there's no checks queued and it was checking the length of the object.

Test Plan:
Tested it with a land check commit (Both Success and Fail):
```
(base) kerryz@kerryz-mbp pytorch % /Users/kerryz/miniconda3/bin/python /Users/kerryz/pytorch/.github/scripts/testtrymerge.py
(base) kerryz@kerryz-mbp pytorch % /Users/kerryz/miniconda3/bin/python /Users/kerryz/pytorch/.github/scripts/testtrymerge.py
Traceback (most recent call last):
  File ""/Users/kerryz/pytorch/.github/scripts/testtrymerge.py"", line 14, in <module>
    validate_land_time_checks(repo, ""fc636aa0c359ebee9a70527534007f8ac37ba501"")
  File ""/Users/kerryz/pytorch/.github/scripts/trymerge.py"", line 1007, in validate_land_time_checks
    raise MandatoryChecksMissingError(f""Refusing to merge as land check(s) {checks_to_str(pending_checks)} are not yet run"")
trymerge.MandatoryChecksMissingError: Refusing to merge as land check(s) [macos-11-py3-x86-64 / test (default, 1, 2, macos-12)](https://github.com/pytorch/pytorch/runs/7202200731?check_suite_focus=true), [linux-bionic-cuda10.2-py3.9-gcc7 / test (slow, 1, 1, linux.4xlarge.nvidia.gpu)](https://github.com/pytorch/pytorch/runs/7201784661?check_suite_focus=true), [linux-bionic-cuda10.2-py3.9-gcc7 / test (default, 1, 2, linux.4xlarge.nvidia.gpu)](https://github.com/pytorch/pytorch/runs/7201784432?check_suite_focus=true) are not yet run
```

Tested it with a normal commit that doesn't have any checks associated with it:
```
(base) kerryz@kerryz-mbp pytorch % /Users/kerryz/miniconda3/bin/python /Users/kerryz/pytorch/.github/scripts/testtrymerge.py
Traceback (most recent call last):
  File ""/Users/kerryz/pytorch/.github/scripts/testtrymerge.py"", line 14, in <module>
    validate_land_time_checks(repo, ""d46f36a3ea227b756bd82901948d8ac203435e46"")
  File ""/Users/kerryz/pytorch/.github/scripts/trymerge.py"", line 1001, in validate_land_time_checks
    checks = fetch_check_run_conclusions(repo, commit)
  File ""/Users/kerryz/pytorch/.github/scripts/trymerge.py"", line 994, in fetch_check_run_conclusions
    raise MandatoryChecksMissingError(""Refusing to merge as land check(s) are not yet run"")
trymerge.MandatoryChecksMissingError: Refusing to merge as land check(s) are not yet run
```

Pull Request resolved: https://github.com/pytorch/pytorch/pull/80907
Approved by: https://github.com/janeyx99"
pytorch/pytorch,393f7f6ad7b10adc44500eaead13044efd8c698b,"add layout to slow path (#80429)

Pull Request resolved: https://github.com/pytorch/pytorch/pull/80429
Approved by: https://github.com/ezyang"
pytorch/pytorch,04c50fec1ceec5b2d78934921f1cb366ff3fac81,"[FSDP Optim State] Remove checkpoint prefix (#80480)

Remove `_checkpoint_wrapped_module` prefixes when creating keys for optimizer state_dict.

Having these does not actually create an issue for optim_state_dict save / load, but we'd like to strip these keys out for downstream code that consumes these APIs typically expecting checkpointing prefixes to not exist (as checkpointing should be a transparent operation which should not change module / parameter names).
Pull Request resolved: https://github.com/pytorch/pytorch/pull/80480
Approved by: https://github.com/awgu, https://github.com/fegin"
pytorch/pytorch,9ca561cbd4ab2bee3279b998e3ea4d1f302ced7e,"Add prims::{real, imag}, refs::{real, imag}, remove prims::is_infinite (#80148)

This is a subset of https://github.com/pytorch/pytorch/pull/78655. I want to land this separately because the world is changing so fast, and https://github.com/pytorch/pytorch/pull/78655 is having lots of conflicts with other parts of the world.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/80148
Approved by: https://github.com/mruberry"
pytorch/pytorch,54217e695d71430cff79f04933d4b1384ba99610,"[Quant] Add fast path of qmean/qstd for quantized CPU (reopen #70172) (#80579)

> Note: This is a reopen of https://github.com/pytorch/pytorch/pull/70172 which was merged then reverted.

Add fast path of qmean and qstd when computation is done in innermost dimensions for quantized CPU. The fast path supports inputs in contiguous memory format.
For example:
```python
X = torch.randn((2,3,4,5), dtype=torch.float)
qX = torch.quantize_per_tensor(X, scale, zero_point, torch_type)

# dim can be: -1, (-1, -2), (-1, -2, -3), (-1, -2, -3, -4), 3, (3, 2), (3, 2, 1), (3, 2, 1, 0) or None
dim = -1
qY = torch.mean(qX, dim) # qY = torch.std(qX, dim)
```

**Performance test results**
Test Env:
- Intel® Xeon® CLX-8260
- 1 instance, 4 cores
- Using Jemalloc

Test method:
Create 4d contiguous tensors as inputs, set `dim` to the innermost two dimensions `(-1, -2)`, then do the following tests
- Quantize inputs and use the fast path
- Quantize inputs and use the reference path
- Use fp32 kernel (no quantization)

Mean: exec time (us) vs. shape
![image](https://user-images.githubusercontent.com/12522207/148152617-604f2841-cfcd-495c-ae88-c27d9165b46a.png)

Std: exec time (us) vs. shape
![image](https://user-images.githubusercontent.com/12522207/148152632-3a8dceb1-0057-42c9-af65-1e26d697ff0c.png)

Pull Request resolved: https://github.com/pytorch/pytorch/pull/80579
Approved by: https://github.com/malfet"
pytorch/pytorch,f76bb88205b631392f593e453ea1362e1ebe57a2,"fix docstring of PostLocalSGDOptimizer (#80855)

As title.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/80855
Approved by: https://github.com/awgu, https://github.com/rohan-varma"
pytorch/pytorch,bdee679d3accb3a23f5ac6e7aa3e1928bb281dbe,"[JIT] Refactor SchemaCheckMode to check for mutated metadata (#79991)

- Refactored SchemaCheckMode so that arguments and outputs are only unflattened/unwrapped a single time
- Added cloned_metadata variable inside torch_dispatch which accounts for ops that mutate stride or storage()._cdata
- Fixed aliasing check to compare with any iterable outputs (ie list of tensor output)
- Fixed aliasing check to check ops with an (a -> *) notation argument correctly
- Changed order of mutation and aliasing checks to optimize error message usefulness (generally something incorrectly aliasing implies that something is also incorrectly mutating so an aliasing error gives more information)
- Added tests for resize_, as_strided to ensure that mutated metadata checking works as intended
Pull Request resolved: https://github.com/pytorch/pytorch/pull/79991
Approved by: https://github.com/davidberard98"
pytorch/pytorch,ae0c521b3b7a5b97fdbe58788e288547c3ee76df,"Fix remaining CPU operators for non-standard bools (#79390)

In addition to using `c10::load` in more kernels, this specializes
`Vectorized<bool>::loadu` to first load as `int8_t` then convert to
bool, in the same way as `c10::load` does.

For `unique_cpu`, the values are loaded inside of the
`std::unordered_set` constuctor, so I've added a separate function
specifically for bools. This fixes the loading issue and is faster
because it uses the fact that bools can only have one of two values.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/79390
Approved by: https://github.com/mruberry"
pytorch/pytorch,4e12300e4ee9ee4dadebd42aa2dc8ed28f836291,"[ci] upload test stats to s3 instead of rockset directly (#80593)

Previously we were writing documents to Rockset directly using the write
API. This turned out to be a source of issues, occupying Rockset leaf
CPU and making other queries timeout. Also, we are starting to get rate
limited by Rockset, leading to data loss.

Instead, write test stats to s3 and let Rocket's managed integration
sync it. This appears to be significantly more efficient, and should
solve our throughput issues fundamentally.

Hopefully we can re-enable per-PR stats after this change, but let's see
how it does first.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/80593
Approved by: https://github.com/janeyx99"
pytorch/pytorch,c8d64ba5ec7ee4fb2594b00e022de611bc8f2ac4,"Allow register float16 weight_norm on cpu and speed up test (#80600)

Fixes https://github.com/pytorch/pytorch/issues/80599

Pull Request resolved: https://github.com/pytorch/pytorch/pull/80600
Approved by: https://github.com/malfet"
pytorch/pytorch,b4e491798c0679ab2e61f36a511484d7b8ecf8d3,"Avoid temporary buffers for tensors with torch.save. (#80404)

Fix torch.save _open_zipfile_writer optimization that uses a c++ stream when `f` is a os.PathLike.
This fastpath requires that we don't `open()` in python if possible, so don't do it unconditionally.

Fix PyTorchStreamWriter construction binding that takes a buffer object.
Use py::memoryview instead of py::bytes as the former doesn't copy the data.

Validated with a trivial benchmark that calls torch.save in a loop 20x with a 10M elements float32 tensor
either on cpu or cuda. Saved to /dev/null.

Tried two variants 'str' and 'open'
    In 'str' we pass the string ""/dev/null"" to torch.save.
    In 'open' we pass `open(""/dev/null"", ""wb"")` to torch.save.

Timing in seconds.

Before this patch:
str-cpu :: 0.757
open-cpu :: 0.757
str-cuda :: 1.367
open-cuda :: 1.366

After this patch:
str-cpu :: 0.256
open-cpu :: 0.251
str-cuda :: 0.896
open-cuda :: 0.834

Fixes #ISSUE_NUMBER

Pull Request resolved: https://github.com/pytorch/pytorch/pull/80404
Approved by: https://github.com/jamesr66a"
pytorch/pytorch,560a8cc530242c0d3aa31a9a0f85e847de782711,"Revert ""Add fast path of qmean/qstd for quantized CPU (#70172)""

This reverts commit c1fa9fdff9dc79557767bee23c66dcaa4095300c.

Reverted https://github.com/pytorch/pytorch/pull/70172 on behalf of https://github.com/suo due to broke windows builds, see: https://hud.pytorch.org/pytorch/pytorch/commit/c1fa9fdff9dc79557767bee23c66dcaa4095300c"
pytorch/pytorch,cfe8dce8146056a6de98e161aab6863033afce96,"[Bootcamp] Use Apple's Accelerate framework for blas acceleration (#80449)

Summary: Add Apple Accelerate specific method calls for BLAS acceleration

Differential Revision: D37438092

Pull Request resolved: https://github.com/pytorch/pytorch/pull/80449
Approved by: https://github.com/kimishpatel"
pytorch/pytorch,c1fa9fdff9dc79557767bee23c66dcaa4095300c,"Add fast path of qmean/qstd for quantized CPU (#70172)

Add fast path of qmean and qstd when computation is done in innermost dimensions for quantized CPU. The fast path supports inputs in contiguous memory format.
For example:
```python
X = torch.randn((2,3,4,5), dtype=torch.float)
qX = torch.quantize_per_tensor(X, scale, zero_point, torch_type)

# dim can be: -1, (-1, -2), (-1, -2, -3), (-1, -2, -3, -4), 3, (3, 2), (3, 2, 1), (3, 2, 1, 0) or None
dim = -1
qY = torch.mean(qX, dim) # qY = torch.std(qX, dim)
```

**Performance test results**
Test Env:
- Intel® Xeon® CLX-8260
- 1 instance, 4 cores
- Using Jemalloc

Test method:
Create 4d contiguous tensors as inputs, set `dim` to the innermost two dimensions `(-1, -2)`, then do the following tests
- Quantize inputs and use the fast path
- Quantize inputs and use the reference path
- Use fp32 kernel (no quantization)

Mean: exec time (us) vs. shape
![image](https://user-images.githubusercontent.com/12522207/148152617-604f2841-cfcd-495c-ae88-c27d9165b46a.png)

Std: exec time (us) vs. shape
![image](https://user-images.githubusercontent.com/12522207/148152632-3a8dceb1-0057-42c9-af65-1e26d697ff0c.png)

Pull Request resolved: https://github.com/pytorch/pytorch/pull/70172
Approved by: https://github.com/malfet"
pytorch/pytorch,da61ec2a4a3e7149a0cea6538d57f73412cc433b,"[CI] imporve ios simulator test (#80459)

update fastlane version and parameters to improve test reliability

Pull Request resolved: https://github.com/pytorch/pytorch/pull/80459
Approved by: https://github.com/malfet, https://github.com/seemethere"
pytorch/pytorch,57f001f35a22952d4f39a33c739b447c10136a7f,"Don't error if _warned_capturable_if_run_uncaptured not set (#80345)

This can happen if an optimizer was pickled.

Signed-off-by: Edward Z. Yang <ezyang@fb.com>
Pull Request resolved: https://github.com/pytorch/pytorch/pull/80345
Approved by: https://github.com/malfet, https://github.com/albanD"
pytorch/pytorch,5fc2d45a3a11354ae9815ac1bd65a0dbc2fcd85b,"Remove unneeded TODO (#80453)

This TODO is no longer needed, as we use `_register_fused_optim` to register the overlapped optimizer in DDP.  Also, remove comment about API being experimental, as this API is no longer going to be used by end user.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/80453
Approved by: https://github.com/awgu"
pytorch/pytorch,b9d516138b01adf1be6e7eb24e089df4065de5d2,"[PyTorch] Add test_modules test for TransformerEncoderLayer fast path (#78268)

Extend the existing TransformerEncoderLayer test to cover the fast path.

Differential Revision: [D36564009](https://our.internmc.facebook.com/intern/diff/D36564009/)
Pull Request resolved: https://github.com/pytorch/pytorch/pull/78268
Approved by: https://github.com/zrphercule"
pytorch/pytorch,b8e50f512f735fb79b8b525cdb552e095a3873cb,"[DataPipe] Count number of successful yields for IterDataPipe (#79657)

This PR adds an attribute and logic to count the number of successful yields from `IterDataPipe`. This information can be useful to fast-forward a DataPipe (or the entire graph) back to a certain state.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/79657
Approved by: https://github.com/VitalyFedyunin"
pytorch/pytorch,9e86796fe3e1d2ef815ca0e5e78f8f5dbd2b72fc,"simple c10 implementation for std::call_once (#78051)

A long standing bug on std::call_once: https://gcc.gnu.org/bugzilla/show_bug.cgi?id=66146
It could hang during re-entry after an exception handling.

Added a c10 implementation yielding a bulky mutex. Not the most efficient thing but at least it shouldn't hang.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/78051
Approved by: https://github.com/albanD"
pytorch/pytorch,ea987086fce93d544db72706928c425e339b8bce,"Fix test_gradcheck_forward_ad_respects_requires_grad for slow gradcheck (#80401)

Tested locally
Pull Request resolved: https://github.com/pytorch/pytorch/pull/80401
Approved by: https://github.com/albanD"
pytorch/pytorch,590d3e5774110e4657dcaa6acdb387ef69e41b47,"[ci] skip slow gradcheck jobs (#80311)

Skip to restore the CI while we investigate
Pull Request resolved: https://github.com/pytorch/pytorch/pull/80311
Approved by: https://github.com/janeyx99"
pytorch/pytorch,14813536a7120f1104be2270be341b7a383415c5,"Disable AVX512 CPU dispatch by default (#80253)

As it can be slower, see https://github.com/pytorch/pytorch/issues/80252
Update trunk test matrix to test AVX512 config in `nogpu_AVX512` flavor.
Kill `nogpu_noAVX` as AVX support were replaced with AVX512 when https://github.com/pytorch/pytorch/pull/61903 were landed
Pull Request resolved: https://github.com/pytorch/pytorch/pull/80253
Approved by: https://github.com/ngimel"
pytorch/pytorch,2c43876f64603d6ccaedcf6a95f3a10319c754e6,"AT_DISPATCH: Expose switch-case like macro syntax (#79978)

This expands the `AT_DISPATCH` macros to enable writing your own
`AT_DISPATCH_SWITCH` statements with multiple `AT_DISPATCH_CASE`
labels. So, where previously you may have written:

```cpp
if (iter.common_dtype() == kBool) {
  my_bool_kernel(iter);
} else {
  AT_DISPATCH_INTEGRAL_TYPES(iter.common_dtype(), ""my_kernel"", [&] {
    ...
  });
}
```

You can now instead write

```cpp
AT_DISPATCH_SWITCH(iter.common_dtype(), ""my_kernel"",
  AT_DISPATCH_CASE(kBool, [&] { my_kernel_bool(iter); })
  AT_DISPATCH_CASE_INTEGRAL_TYPES([&] { ... })
);
```

The macro syntax is a bit ugly, however the benefits are:
- Greater flexibility, as the kernel code doesn't have to be shared
  for all dtypes.
- Selective build and RECORD_KERNEL_FUNCTION work even for single
  dtype specializations such as the bool case in the example.
- The compiler sees a single switch for all types, which should be
  easier to optimize into a jump table.
- We also now get errors if the same scalar type is handled twice.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/79978
Approved by: https://github.com/ezyang"
pytorch/pytorch,375668cd96bc3c22d59a4da6fc9214c208acaf30,"Remove overly restrictive assert in adam (#80222)

This is causing issues if the user has the step on cuda for a good reason.

These assert prevents code that used to run just fine to fail.
Note that this is a pretty bad thing to do for performance though so it is ok to try and push users away from doing it.

For the 1.12.1 milestone: this is not asking for a dot release to fix this (as this is bad practice anyways). But it would be a great thing to add if we do one: it is very low risk and will prevent breakage for users.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/80222
Approved by: https://github.com/jbschlosser, https://github.com/ngimel"
pytorch/pytorch,42a2359612743f791fd2c0684b75f2ff450bc0b3,"Add forward AD for linalg.det and simplify its backward (#79487)

This PR is in preparation for implementing `logdet` and `slogdet` as
structured kernels + implementing them with more efficient derivatives

We implement forward AD for det. We also simplify the implementation of
the backward, and leave a note on how to implement it properly for
singular matrices. We leave thad for future work.

Note (by looking at the OpInfo) that the current implementation passes
the same tests as the one before. We skip the forward-over-backward in
the singular case, as that one was not working in the gradgrad case
either.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/79487
Approved by: https://github.com/nikitaved, https://github.com/albanD"
pytorch/pytorch,bab1ea8592f7308cded5d146e2d921ed2a6702dc,"[Vulkan] Optimize LSTM operator with pre-packing (#79702)

Summary:
Optimized LSTM operator by using pre-packing for weights and biases in the Vulkan GPU backend
- The weights and biases are always on the CPU side by design.
- The packed and unpacked data are stored in a VulkanOpContext
- Ops:
  - `at::native::vulkan::ops::create_lstm_context`: Creates a VulkanOpContext object with the packed and unpacked data, and returns a pointer to it.
  - `at::native::vulkan::ops::run_lstm_context`: Takes in the three input vulkan tensors (input sequence, initial hidden state and initial cell state) and a pointer to the context, and runs the LSTM operation.
- Registered the ops in [Register.cpp](https://github.com/pytorch/pytorch/blob/master/aten/src/ATen/native/vulkan/ops/Register.cpp).
- Rewrote the subgraph function of LSTM in [vulkan_rewrite.cpp](https://github.com/pytorch/pytorch/blob/master/torch/csrc/jit/passes/vulkan_rewrite.cpp) so that `create_lstm_context` and `run_lstm_context` can be executed instead in the Vulkan GPU backend.
- Added new test for the LSTM pre-packing and run ops: `lstm_prepack_success`

Test Plan: buck run //xplat/caffe2:pt_vulkan_api_test_binAppleMac

Reviewed By: SS-JIA

Differential Revision: D37052597

Pull Request resolved: https://github.com/pytorch/pytorch/pull/79702
Approved by: https://github.com/SS-JIA"
pytorch/pytorch,8c0796e57fa7ad2ad588874168698c0ff1f76e67,"Use cub::BlockRadixSort to improve medium length sort performance (#79628)

In my testing, replacing the custom bitonic sort with cub's block
level radix sort primitives improves overall sort performance by up to
3x, depending on input length. This also benefits from being a stable
sort, and so we get up to 25x speedup for small stable sorts and
around 2x speedup on the largest supported size.

In testing, the radix sort benefits a lot from having more items per
thread meaning it breaks down a bit at very small sizes. So, for the
32-item sort I've left the bitonic sorting algorithm in place.

Binary size is also reduced in this change, because I have moved the
`descending` branch into the kernel itself which I found not to effect
performance.  The result is a 1.9 MB decrease in `torch_cuda.so` on
my build for one cuda architecture.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/79628
Approved by: https://github.com/ngimel"
pytorch/pytorch,817eb94ff494e07e94a9effb626ce2121f226a54,"Speed up module constructor by avoiding module.__setattr__ (#77098)

Module's overridden `__setattr__` has special handling for parameters, submodules, and buffers, resulting in reduces performance for its default attributes (which are not of those types). Setting them directly results in a significant improvement for module construction speed (~10x).
Pull Request resolved: https://github.com/pytorch/pytorch/pull/77098
Approved by: https://github.com/jbschlosser"
pytorch/pytorch,0ee31ce8c8312af1a61c161d21efb21d900a0c13,"Revert ""Use cub::BlockRadixSort to improve medium length sort performance""

This reverts commit 67a5d0bf40b10d8ebfb6b10b86f73583b9a8c461.

Reverted https://github.com/pytorch/pytorch/pull/79628 on behalf of https://github.com/janeyx99 due to Sorry, reverting as it breaks ROCm build on trunk https://hud.pytorch.org/pytorch/pytorch/commit/67a5d0bf40b10d8ebfb6b10b86f73583b9a8c461"
pytorch/pytorch,67a5d0bf40b10d8ebfb6b10b86f73583b9a8c461,"Use cub::BlockRadixSort to improve medium length sort performance

In my testing, replacing the custom bitonic sort with cub's block
level radix sort primitives improves overall sort performance by up to
3x, depending on input length. This also benefits from being a stable
sort, and so we get up to 25x speedup for small stable sorts and
around 2x speedup on the largest supported size.

In testing, the radix sort benefits a lot from having more items per
thread and so it does break down at very small sizes. So, for the
32-item sort I've left the bitonic sorting algorithm in place.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/79628

Approved by: https://github.com/ngimel"
pytorch/pytorch,44ff6be35ae380951e48613977b94dc282b74d04,"Fix backward of binary_cross_entropy_with_logits

The previous PR in this stack uncovered an error in the forward over
backward for this function.

In this PR, we fix this error and we also fix the gradgrad
implementation (and make it more stable and faster using `logsigmoid`).
We also move the double backward for this function to `FunctoinsManual`
as there's no reason for it to be in `native_functions`

Pull Request resolved: https://github.com/pytorch/pytorch/pull/80083

Approved by: https://github.com/zou3519"
pytorch/pytorch,e3d0a3ca8850dc1ffc9fd0f740206523125c58b5,"Revert ""More forward AD formulas""

This reverts commit 6b20ef6b917aaaf332d10d9e65e2854ff79965f0.

Reverted https://github.com/pytorch/pytorch/pull/77975 on behalf of https://github.com/janeyx99 due to I think this is the real culprit of the broken tests in https://hud.pytorch.org/pytorch/pytorch/commit/28a7ee8cec122d6f6c9a9b891b7cc6bb8224c9a7 for the trunk-only slow test job"
pytorch/pytorch,942c371bbcd5713ff24a71d95748f889106f929f,"Revert ""Fix backward of binary_cross_entropy_with_logits""

This reverts commit 28a7ee8cec122d6f6c9a9b891b7cc6bb8224c9a7.

Reverted https://github.com/pytorch/pytorch/pull/79381 on behalf of https://github.com/janeyx99 due to Sorry, https://hud.pytorch.org/pytorch/pytorch/commit/28a7ee8cec122d6f6c9a9b891b7cc6bb8224c9a7 this PR breaks trunk-only slow test job"
pytorch/pytorch,28a7ee8cec122d6f6c9a9b891b7cc6bb8224c9a7,"Fix backward of binary_cross_entropy_with_logits

The previous PR in this stack uncovered an error in the forward over
backward for this function.

In this PR, we fix this error and we also fix the gradgrad
implementation (and make it more stable and faster using `logsigmoid`).
We also move the double backward for this function to `FunctoinsManual`
as there's no reason for it to be in `native_functions`

Pull Request resolved: https://github.com/pytorch/pytorch/pull/79381

Approved by: https://github.com/soulitzer"
pytorch/pytorch,18e19c6b0bc52cfd39b43d3f94ad19b75dc4846e,"Move CI to cuda-11.6 (#79921)

Delete 11.6 from periodic builds and instead move them to pull/trunk

Update debug build from 11.3 to 11.6

CUDA-11.3 have number of know bugs in complex cublas/sparse support as well as performance issue with NVRTC
Pull Request resolved: https://github.com/pytorch/pytorch/pull/79921
Approved by: https://github.com/ngimel, https://github.com/atalman, https://github.com/seemethere"
pytorch/pytorch,61305cd638b6fcd73a0b66b4cde7014fecb9e8ce,"Improve small sort performance on CUDA

Currently, `bitonicSortKVInPlace` is written to sort one array per
block of threads. If that dimension happens to be very small
(<128 elements), this results in low thread occupancy.

Instead, this changes `bitonicSortKVInPlace` to operate with a 2d
block. Sorting happens along the x dimension, and the y dimension
is a fixed size batch.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/79627

Approved by: https://github.com/ngimel"
pytorch/pytorch,765b6a8fab02a55cf85058037f9ab89fb00a5b23,"Fix SequentialLR initialization (#72856)

What was happening is that when we have multiple learning rate schedulers, the order in which they are being initialized is not being taken into account. This is a problem if they were being initialized in sequential order (as one might intuitively do).

Each scheduler calls `step()` on initialization and sets the `lr` in its optimizer's `params_groups`. However, this means that step 0 will be using the `lr` that was set by the very last scheduler (in the case of initializing schedulers sequentially) instead of the first scheduler.

The fix in this PR, addresses the above bug by performing a call to the appropriate scheduler on initialization after decrementing the `last_epoch` values in order to keep them the same post-step. This will ensure that the correct scheduler is the one setting the `lr` values for the optimizer's `param_groups`
Pull Request resolved: https://github.com/pytorch/pytorch/pull/72856
Approved by: https://github.com/jbschlosser"
pytorch/pytorch,9ad91cc6e0661ef88631f4053fdf3ca18fe8065e,"optimize `to_dense` for CSC (#79635)

As per title. Previously it was done via converting to COO.
A better approach could be using `dense.out_`, but `sparse_csc` is yet forbidden.
And are we fine with implementing very critical operations like `add` via transpositions?

Pull Request resolved: https://github.com/pytorch/pytorch/pull/79635
Approved by: https://github.com/cpuhrsch"
pytorch/pytorch,71c24a6a2e54ad4e5809519d6db292cd93d2f20a,"Reduce string formatting overhead in PyWarningHandler

Closes #76952

This does `processErrorMsg` inplace on the warning string, so that in
the fast-path of no type translation it doesn't need to allocate a new
string just to copy the contents over. I also replaced `ostringstream`
with `fmt::format_to` which has noticably better performance.

Overall in a benchmark of `torch.floor_divide`, this drops the
callgrind instruction count from 703,168 to 571,774 and the bechmark
improves by 300 ns from 2.26 us to 1.94 us.

This brings the callgrind count for `~PyWarningHandler` up to ~80%
from `PyErr_WarnEx` so this is probably about as fast as our warning
handling can reasonably get.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/76977

Approved by: https://github.com/swolchok"
pytorch/pytorch,1cf3b24d42d8e3ed1b31e508232d4ba907f29bfb,"Remove unnecessary allocations in `processErrorMsg`

`processErrorMsg` uses a table of string constants to be replaced in
the error message. However, this table is non-static so gets
re-constructed from scratch every time. So, I've made it `constexpr`
by using `std::array` instead of `std::vector` and `c10::string_view`
instead of `std::string`.

To support `c10::string_view` I've also updated `c10::ReplaceAll` to
accept string_view arguments, and added a fast-path that also avoids
repeated string searches when no translation is needed.

Using `torch.floor_divide` to benchmark a python warning, I see the
callgrind instruction count fall from 3,806,446 to 703,168 and a 6.5
us time improvement using `%timeit`.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/76976

Approved by: https://github.com/swolchok"
pytorch/pytorch,e10cbe388042e2dc9b5919c37d91aee7a06cc858,"Revert ""Fix BytesWarning in torch.load() (#74813)""

This reverts commit 6c2e8119ddcafaf9cef31675d6c49559921d8bbc.

Reverted https://github.com/pytorch/pytorch/pull/74813 on behalf of https://github.com/janeyx99 due to Broke slow tests in cuda 10.2 https://github.com/pytorch/pytorch/runs/6944238177?check_suite_focus=true"
pytorch/pytorch,dee43798d762d5282f373598a8f97aaf0f19ae07,"Revert ""Create a new Ubunutu 22.04 (jammy) build for platform010 (#77591)""

This reverts commit 71d82917f45920d096da421f33ce0057cace611f.

Reverted https://github.com/pytorch/pytorch/pull/77591 on behalf of https://github.com/zengk95 due to this is breaking linux slow test on trunk"
pytorch/pytorch,399b3dcac64917b6d49f1f7e7ba47fa29165b4b0,"Do not bind `c10::irange` output by reference (#79815)

First of all, binding integers by reference is slower than by value
Also, it makes no sense as `c10::irange` output does not have permanent storage and results in following compiler warning:
```
aten/src/ATen/core/function_schema.h:521:26: error: loop variable 'i' binds to a temporary value produced by a range of type 'integer_range<unsigned long>' [-Werror,-Wrange-loop-bind-reference]
        for (const auto& i : c10::irange(1, default_val.size())) {
```

Pull Request resolved: https://github.com/pytorch/pytorch/pull/79815
Approved by: https://github.com/kit1980, https://github.com/mehtanirav, https://github.com/bdhirsh"
pytorch/pytorch,6c2e8119ddcafaf9cef31675d6c49559921d8bbc,"Fix BytesWarning in torch.load() (#74813)

Fixes #74812.

I have enabled the `-bb` option when tests are run to prevent regressions. I don't think it will make CI run more slowly, but I'm not entirely sure.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/74813
Approved by: https://github.com/kit1980"
pytorch/pytorch,06274d7a487bf7995da77b9df9b5c1f7dc13f35b,"Add test for torchscripting nn.TransformerEncoder, including fast path (#79796)

Summary:
Add test just to check if TransformerEncoder will crash when enumerating over params [with_no_grad, use_torchscript, training].

Motivation for this was that TransformerEncoder fast path (so with_no_grad=True) and use_torchscript=True would crash with the issue that NestedTensor doesn't have size. This was caused because the TransformerEncoder fast path generates a NestedTensor automatically as a perf optimization and torchscript attempts to find intermediate tensor sizes while it optimizes. But NestedTensor has not implemented a size method, so things fail.

This test goes together with this fix https://github.com/pytorch/pytorch/pull/79480

Test Plan:
```
buck build --show-output mode/opt -c fbcode.enable_gpu_sections=true -c fbcode.nvcc_arch=a100 mode/inplace  //caffe2/test:transformers

./fbcode/buck-out/gen/caffe2/test/transformers#binary.par
```
Test runs and passes together with the changes from the PR above (I made another diff on top of this with those changes). Does not pass without the fix.

Reviewed By: mikekgfb

Differential Revision: D37222923

Pull Request resolved: https://github.com/pytorch/pytorch/pull/79796
Approved by: https://github.com/zrphercule"
pytorch/pytorch,38952d9350e7cced8901df4cae50efddb69470a3,"[ao] Added function to inform dynamic vs static appropriate

Summary: The _detect_dynamic_vs_static function was added to take in a
prepared fx graph model that already had ModelReportObservers built into
it and uses the collected information to determine whether input and
output are stationary or non-stationary and provides feedback on whether
to make linear modules static or dynamic based on this information.

This PR will be followed up soon with another PR that will more
rigoursly test the whole end to end performance of this system, which is
primarily how the function in this PR will be tested for functionality,
which is why this one only has 1 test.

Test Plan: python test/quantization/fx/test_model_report_fx.py TestModelReportDetectDynamicStatic

Reviewers:

Subscribers:

Tasks:

Tags:

Pull Request resolved: https://github.com/pytorch/pytorch/pull/79326

Approved by: https://github.com/HDCharles"
pytorch/pytorch,20675977bc0d574c863a2177ba612c02da731668,"[Static Runtime] Performance optimization for fork operation (#79482)

Summary:
- StaticModule was being created at runtime which was adding overhead to the forked operation
- Move staticModule creation to outside of runtime so that StaticRuntime instance can be created on top of same staticModule that is created once

Differential Revision: D37126923

Pull Request resolved: https://github.com/pytorch/pytorch/pull/79482
Approved by: https://github.com/tenpercent"
pytorch/pytorch,577f87bbff2ecc9f5c5930ac04cf3c5bcada47c5,"Make flatbuffer loads faster if loading as mobile module. (#78998)

BCFC check: verified that flatbuffer file created in this commit can
be loaded in HEAD and file created in HEAD can be loaded in this commit

Fixes #ISSUE_NUMBER

Pull Request resolved: https://github.com/pytorch/pytorch/pull/78998
Approved by: https://github.com/zhxchen17"
pytorch/pytorch,f31f4a3ac2488f8306859d09960538c1b4bc6ac8,"Remove the construction of unused tensors (#79183)

Hi there, this statement, [`auto columns = at::empty({nInputPlane * kW * kH, outputHeight * outputWidth}, input.options());`](https://github.com/pytorch/pytorch/blob/95b15c266baaf989ef7b6bbd7c23a2d90bacf687/aten/src/ATen/native/cuda/ConvolutionMM2d.cu#L154), will construct a new tensor and allocate device memory for it. But I found this tensor will be only used([line185](https://github.com/pytorch/pytorch/blob/95b15c266baaf989ef7b6bbd7c23a2d90bacf687/aten/src/ATen/native/cuda/ConvolutionMM2d.cu#L185) and [line197](https://github.com/pytorch/pytorch/blob/95b15c266baaf989ef7b6bbd7c23a2d90bacf687/aten/src/ATen/native/cuda/ConvolutionMM2d.cu#L197)) when [`requires_columns`](https://github.com/pytorch/pytorch/blob/95b15c266baaf989ef7b6bbd7c23a2d90bacf687/aten/src/ATen/native/cuda/ConvolutionMM2d.cu#L156) is true.

So we can declare an `at::Tensor columns;` variable(This will not allocate device memory for `columns`), and invoke `at::empty` to construct the tensor when `requires_columns` is true. As for the statement [`int64_t n = columns.size(1);`](https://github.com/pytorch/pytorch/blob/95b15c266baaf989ef7b6bbd7c23a2d90bacf687/aten/src/ATen/native/cuda/ConvolutionMM2d.cu#L192), the size can be calculated by the arguments of function `slow_conv2d_forward`.

I profiled the resnet50 in [`pytorch/benchmark`](https://github.com/pytorch/benchmark). I found there are lots of unused tensors in the device memory and they are gone after my optimization. It also works well on vgg16, yolov3, alexnet, etc.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/79183
Approved by: https://github.com/ngimel"
pytorch/pytorch,05624bcf7b831ed18b857701757da2a4e076c493,"add sizes to slowpath

Pull Request resolved: https://github.com/pytorch/pytorch/pull/79295

Approved by: https://github.com/ezyang"
pytorch/pytorch,964d5059587c017b02f1d7d62587722a38683c63,"[ci] remove remaining RDS dependency

With https://github.com/pytorch/pytorch/pull/79366, the only remaining
use case for RDS is import latency stats. Afaik we do not look at these,
so removing, which eliminates our last dependency on RDS.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/79370

Approved by: https://github.com/janeyx99"
pytorch/pytorch,ba27ee9e8fc57f7509d2bf0c0be73510802806c0,"[CUDA graphs] Allows Adam and AdamW to be capture-safe (#77862)

Near term fix for https://github.com/pytorch/pytorch/issues/76368.

Q. Why does the user need to request `capturable=True` in the optimizer constructor? Why can't capture safety be completely automatic?
A. We need to set up capture-safe (device-side) state variables before capture. If we don't, and step() internally detects capture is underway, it's too late: the best we could do is create a device state variable and copy the current CPU value into it, which is not something we want baked into the graph.

Q. Ok, why not just do the capture-safe approach with device-side state variables all the time?
A. It incurs several more kernel launches per parameter, which could really add up and regress cpu overhead for ungraphed step()s. If the optimizer won't be captured, we should allow step() to stick with its current cpu-side state handling.

Q. But cuda RNG is a stateful thing that maintains its state on the cpu outside of capture and replay, and we capture it automatically. Why can't we do the same thing here?
A. The graph object can handle RNG generator increments because its capture_begin, capture_end, and replay() methods can see and access generator object. But the graph object has no explicit knowledge of or access to optimizer steps in its capture scope. We could let the user tell the graph object what optimizers will be stepped in its scope, ie something like
```python
graph.will_use_optimizer(opt)
graph.capture_begin()
...
```
but that seems clunkier than an optimizer constructor arg.

I'm open to other ideas, but right now I think constructor arg is necessary and the least bad approach.

Long term, https://github.com/pytorch/pytorch/issues/71274 is a better fix.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/77862
Approved by: https://github.com/ezyang"
pytorch/pytorch,54949a5abc9890143de4b5dd2f13ff98446376a3,"Simplify and optimize linalg.solve

This PR heavily simplifies the code of `linalg.solve`. At the same time,
this implementation saves quite a few copies of the input data in some
cases (e.g. A is contiguous)

We also implement it in such a way that the derivative goes from
computing two LU decompositions and two LU solves to no LU
decompositions and one LU solves. It also avoids a number of unnecessary
copies the derivative was unnecessarily performing (at least the copy of
two matrices).

On top of this, we add a `left` kw-only arg that allows the user to
solve `XA = B` rather concisely.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/74046

Approved by: https://github.com/nikitaved, https://github.com/IvanYashchuk, https://github.com/mruberry"
pytorch/pytorch,a90f006fe54d6e83e0d3bcdde7ceb48e42e62a8f,"add strides to slow path

Pull Request resolved: https://github.com/pytorch/pytorch/pull/78610

Approved by: https://github.com/ezyang"
pytorch/pytorch,eaaa34daef00c11a2b0a8f79cc1c948336b81b5f,"[ci] write test suites to rockset

Currently we upload all `testcase` elements as individual test runs to
Rockset. It would be nice to also have `testsuite`s as well, which
aggregate high level information.

These aggregations could technically be performed in the backend, but it's
faster to just log the data since we already have it in the XML test
report.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/79265

Approved by: https://github.com/seemethere"
pytorch/pytorch,79f18c1aeec85d1eea751a534a6bf63e23008b3f,"Minor FX test fix for TorchDynamo (#79206)

`torch.nn.Identity` gets optimized away with TorchDynamo. Replacing with `torch.nn.tanh`.

@jansel
Pull Request resolved: https://github.com/pytorch/pytorch/pull/79206
Approved by: https://github.com/jansel"
pytorch/pytorch,af6321f3d81d935744c67620551550447e6729b2,"Port linalg_qr to structured

This PR simplifies the logic of `linalg.qr` using structured kernels. I
also took this chance and merged a few `copy_` operations with other
ops.

This PR removes a the previous magma implementation as is never faster
than that of cusolver and it's rather buggy. This has the side-effect
that now `qr` is not supported in Rocm. Ivan confirmed that this is
fine, given how incredibly slow was QR on Rocm anyway (we were marking
some tests as slow because of this...).

This PR also corrects the dispatch in geqrf. Before, if we called it
with a matrix for which `input.size(-2) <= 256 && batchCount(input) >= std::max<int64_t>(2, input.size(-2) / 16)` is false, and we have cublas but not cusolver, we would end up calling magma rather than cublas. This is not what the heuristic suggested.
Probaly we should benchmark these heuristics again, but that's beyond the scope of this PR.

Note. It looks like `torch.geqrf` maybe broken in MAGMA as per the
previous comment in `linalg_qr_helper_magma`. IvanYashchuk wdyt?

Pull Request resolved: https://github.com/pytorch/pytorch/pull/79054

Approved by: https://github.com/IvanYashchuk, https://github.com/ezyang"
pytorch/pytorch,fe2fbb947d7ffc8bb09661d3983e99a654d478b5,"Run torchdynamo tests on PyTorch Linux CI

These tests are fast to run so it is mostly for more platform coverage
than torchdynamo's own CI.

Signed-off-by: Edward Z. Yang <ezyangfb.com>

Pull Request resolved: https://github.com/pytorch/pytorch/pull/79099

Approved by: https://github.com/jansel"
pytorch/pytorch,1b75a2e4584869b2caf639bc45ea21858a85faa8,"Add PReLU to MKLDNN convertible Ops (#79011)

Although an MKLDNN variant of PReLU is now available, it isn't used from the CPU path in `optimize_for_inference` as it is left off the allowable ops.

It leads to graphs that look like this:

```Python
        %1770 : Tensor = aten::to_dense(%1804, %29)
        %1769 : Tensor = aten::to_dense(%shortcut.15, %29)
        %250 : Tensor = aten::prelu(%1770, %self.body.5.res_layer.2.weight) # /home/sacha/.local/lib/python3.10/site-packages/torch/nn/modules/activation.py:1226:0
        %1806 : Tensor = aten::to_mkldnn(%250, %29)
        %1807 : Tensor = aten::to_mkldnn(%1769, %29)
```
Note: the odd to_dense and to_mkldnn of %shortcut.15 appears to be an artifact that occurs because of the prelu needing conversion. %shortcut.15 could have been left as mkldnn (both lines can actually be deleted).
Pull Request resolved: https://github.com/pytorch/pytorch/pull/79011
Approved by: https://github.com/eellison"
pytorch/pytorch,ff39e3493a6a4f8844d88024d9363fdfc63a1e76,"Test torch._refs with aten and nvfuser executors (#78926)

This PR adds testing of references with ""aten"" and ""nvfuser"" executors using `torch._prims.executor.make_traced`.

Many tests are skipped even for ""aten"" executor because of https://github.com/pytorch/pytorch/issues/78923.

I limited the dtypes for the nvfuser executor tests because it's slow due to compilation overhead (it took about 30 mins in total). With `float32` and `int32` types nvfuser tests take 5 minutes.
```
58 passed, 2507 skipped, 28162 deselected, 79 xfailed, 5 warnings in 297.58s (0:04:57)
```
58 tests passed means that 29 references work correctly with nvfuser executor now.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/78926
Approved by: https://github.com/mruberry"
pytorch/pytorch,c2a3c8186c3f3798684cecd60d62a991c223eeef,"[Profiler] Move python tracing to unified event type (Part 2)

Pull Request resolved: https://github.com/pytorch/pytorch/pull/78164

This PR finishes moving over the python tracer to use the unified event type. Things that changed:

1) The hacky after-the-fact splicing of python events in profiler_kineto.cpp is gone and python events now simply fold into the rest. (Yay!!!) This is a major BE win.
2) Added `ExtraFields<EventType::PyCall>` and `ExtraFields<EventType::PyCCall>`
3) The enter events (time + TraceKey) are now handled by RecordQueue for performance.
4) Python tracing now uses TSC for lower overhead.

Simplifications in profiler_python WRT part 1:
1) Rather than ValueCache emitting an intermediate value_t that gets further converted, load methods can now directly emit ExtraFields<...>
2) The complicated replay in profiler_python.cpp is replaced with a much simpler (and safer) pass to just pair start and end times.
3) During post processing we can now use `CallTypeHelper::map` to automatically pull in all events instead of having to loop over each the entries for each type manually. This will make it simpler to add new types of Python event later.

Differential Revision: [D36515869](https://our.internmc.facebook.com/intern/diff/D36515869/)

Approved by: https://github.com/aaronenyeshi"
pytorch/pytorch,a173613f6dfa01ab54986e3269db4b5035a45590,"[Profiler] Move python tracing to unified event type (Part 1)

Pull Request resolved: https://github.com/pytorch/pytorch/pull/78163

The python function tracer is complicated and separate from the other profile types, so I've chosen to break the change into two diff. The first (this one) reworks the cache structure to make it amenable to integration (as well as some other nice tweaks) and the next one actually moves it over.

The old cache scheme worked very hard to pack all the information about an event into a small struct via bit packing, with a couple secondary caches for things like names. Because of the space constraints on that struct (and the fact that it had to represent all call and return types) there were a lot of subtle invariants swirling around that made it hard to offload anything to a different component. The new cache system is more modular and also, as it turns out, a bit faster. (Benchmarks in part 2)

There is a more detailed description of the cache hierarchy in the PR, but the gist is that I use various specializations to handle the different event types (python call, nn module, c function) and lean on the type system to keep everything safe and organized. (One nice thing about using unique IDs is that they also implicitly encode the event type. They implicitly encode everything!) Given that we are going to want to expand the semantics (e.g. torch ops, DataLoader, etc) this will give a nice way to capture richer semantics without significantly increasing the complexity of the profiler.

Differential Revision: [D36379147](https://our.internmc.facebook.com/intern/diff/D36379147/)

Approved by: https://github.com/aaronenyeshi"
pytorch/pytorch,d4eebca7bc14b28688914577f690b68313cb846f,"Test torch._refs with aten and nvfuser executors (#78926)

This PR adds testing of references with ""aten"" and ""nvfuser"" executors using `torch._prims.executor.make_traced`.

Many tests are skipped even for ""aten"" executor because of https://github.com/pytorch/pytorch/issues/78923.

I limited the dtypes for the nvfuser executor tests because it's slow due to compilation overhead (it took about 30 mins in total). With `float32` and `int32` types nvfuser tests take 5 minutes.
```
58 passed, 2507 skipped, 28162 deselected, 79 xfailed, 5 warnings in 297.58s (0:04:57)
```
58 tests passed means that 29 references work correctly with nvfuser executor now.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/78926
Approved by: https://github.com/mruberry"
pytorch/pytorch,32461ed319866f06c95c1bedd61a6acbceca9529,"Pool cudaEvents in CUDACachingAllocator (#78279)

Summary:
cudaEventCreate/Destroy can be expensive especially when the process is calling lots of other CUDA APIs.

Pool the `cudaEvent_t` objects so that we create them once and reuse as much as possible.

Test Plan:
Unit tests to check the functionality.

Manual performance testing shows that this diff is perf positive.

|  | create_event_internal (us) | free_event_internal/destructor (us) | insert_events (us) | process_events (us) |
| baseline | 2.411 | 2.647 | 3.968 | 0.321 |
| this diff | 0.115 | 0.147 | 2.846 | 0.262 |
| speed up | 20.9x | 18.0x | 1.4x | 1.2x |

Differential Revision: D35729059

Pull Request resolved: https://github.com/pytorch/pytorch/pull/78279
Approved by: https://github.com/jianyuh"
pytorch/pytorch,f7da97101cdd337f83fb21e64977e3863e92931d,"[PyTorch][easy] Remove double hash table lookup from Registry::Create

This is inefficient.

Differential Revision: [D36958180](https://our.internmc.facebook.com/intern/diff/D36958180/)

Pull Request resolved: https://github.com/pytorch/pytorch/pull/78980

Approved by: https://github.com/ezyang"
pytorch/pytorch,bb3e1f30a8a4d04c501ce01d15df8a84b5feb91c,"[Pytorch NNAPI] Add compilation_preference & relax_f32_to_f16 APIs (#78758)

Summary:
compilation_preference is one of:

ANEURALNETWORKS_PREFER_LOW_POWER = 0
ANEURALNETWORKS_PREFER_FAST_SINGLE_ANSWER = 1
ANEURALNETWORKS_PREFER_SUSTAINED_SPEED = 2

relax_f32_to_f16 calls Model_relaxComputationFloat32toFloat16

Test Plan:
Tested on device with nnapi models

* Works with existing exported models
* Works with new exported models with options

Differential Revision: D36433236

Pull Request resolved: https://github.com/pytorch/pytorch/pull/78758
Approved by: https://github.com/kimishpatel"
pytorch/pytorch,a3468b7d4ae58bbcc68d457932687f0818d715bf,"[ao][sparsity] Added the Nearly Diagonal Sparsifier

This sparsifier creates a nearly diagonal mask to be applied to the weight matrix.
    Nearly Diagonal Matrix is a matrix that contains non-zero elements near the diagonal and the rest are zero.
    An example of a nearly diagonal matrix with degree (or nearliness) 3 and 5 are follows respectively.
    1 1 0 0       1 1 1 0
    1 1 1 0       1 1 1 1
    0 1 1 1       1 1 1 1
    0 0 1 1       0 1 1 1
    Note that a nearly diagonal matrix with degree 1 is just a matrix with main diagonal populated

    This sparsifier is controlled by one variable:
    1. `nearliness` defines the number of non-zero diagonal lines that are closest to the main diagonal.
        Currently - supports only odd number

    Note:
        This can be accelerated (vectorized) once the Spdiagonal feature (PR: #78439) is landed or the banded matrix
        feature is landed: https://stackoverflow.com/questions/52463972/generating-banded-matrices-using-numpy

Test Plan:

```
python test/test_ao_sparsity.py TestNearlyDiagonalSparsifier
```

Pull Request resolved: https://github.com/pytorch/pytorch/pull/78448

Approved by: https://github.com/z-a-f, https://github.com/HDCharles"
pytorch/pytorch,c6ca4a4038b84a960c4af66aea4272e0ba603334,"Fuse matmul in row-wise sharded linear to have a single matmul.

Performing a single large matmul is more efficient than having to
perform multiple matmuls in a loop.

Similar improvement to https://github.com/pytorch/pytorch/pull/78449

Differential Revision: [D36828505](https://our.internmc.facebook.com/intern/diff/D36828505/)

Pull Request resolved: https://github.com/pytorch/pytorch/pull/78672

Approved by: https://github.com/fduwjj, https://github.com/wanchaol"
pytorch/pytorch,575c42028727db4428dc6bcd03c8631f7798b9d3,"[DataPipe] Lazily generate exception message for performance

Pull Request resolved: https://github.com/pytorch/pytorch/pull/78673

Approved by: https://github.com/ejguan"
pytorch/pytorch,03cf01bdc03a631a1ab521e27b6523bca1a57f0d,"`index_select` for COO CUDA tensors. (#77551)

Brings a native CUDA implementation for `index_select`. Master silently converts CUDA tensors to CPU for CUDA support.

Case `nnz >> size` could be optimized similar to how https://github.com/pytorch/pytorch/pull/72710 is doing that.

Some benchmarks:
<details>

<summary>PR/torch_sparse/master</summary>

```
[------------------------------- cuda coo.index_select -------------------------------]
                                                    |   PR   |  torch_sparse  |  master
32 threads: ---------------------------------------------------------------------------
      n=10000, nnz=100, index_len=100, dim=0        |    96  |       327      |     70
      n=10000, nnz=100, index_len=100, dim=1        |   120  |       505      |     74
      n=10000, nnz=100, index_len=1000, dim=0       |    90  |       333      |     93
      n=10000, nnz=100, index_len=1000, dim=1       |   120  |       499      |     98
      n=10000, nnz=100, index_len=10000, dim=0      |    92  |       331      |    350
      n=10000, nnz=100, index_len=10000, dim=1      |   100  |       506      |    352
      n=100000, nnz=1000, index_len=100, dim=0      |    53  |       274      |     60
      n=100000, nnz=1000, index_len=100, dim=1      |    90  |       368      |     71
      n=100000, nnz=1000, index_len=1000, dim=0     |    93  |       332      |    100
      n=100000, nnz=1000, index_len=1000, dim=1     |   130  |       501      |    140
      n=100000, nnz=1000, index_len=10000, dim=0    |   100  |       341      |    522
      n=100000, nnz=1000, index_len=10000, dim=1    |   130  |       530      |    549
      n=1000000, nnz=10000, index_len=100, dim=0    |    90  |       429      |    110
      n=1000000, nnz=10000, index_len=100, dim=1    |   296  |       810      |    355
      n=1000000, nnz=10000, index_len=1000, dim=0   |   100  |       435      |    170
      n=1000000, nnz=10000, index_len=1000, dim=1   |   309  |       830      |    548
      n=1000000, nnz=10000, index_len=10000, dim=0  |   110  |       446      |    750
      n=1000000, nnz=10000, index_len=10000, dim=1  |   310  |       830      |   1000
      n=10, nnz=100, index_len=100, dim=0           |    90  |       333      |     74
      n=10, nnz=100, index_len=100, dim=1           |   100  |       497      |     78
      n=10, nnz=100, index_len=1000, dim=0          |    90  |       329      |    140
      n=10, nnz=100, index_len=1000, dim=1          |   100  |       800      |    100
      n=10, nnz=100, index_len=10000, dim=0         |    93  |       340      |    900
      n=10, nnz=100, index_len=10000, dim=1         |   120  |       800      |    489
      n=10, nnz=1000, index_len=100, dim=0          |    90  |       321      |    140
      n=10, nnz=1000, index_len=100, dim=1          |   100  |       680      |    140
      n=10, nnz=1000, index_len=1000, dim=0         |   110  |       349      |    670
      n=10, nnz=1000, index_len=1000, dim=1         |   130  |       740      |    800
      n=10, nnz=1000, index_len=10000, dim=0        |   302  |       503      |   4882
      n=10, nnz=1000, index_len=10000, dim=1        |   325  |      2257      |   5262
      n=10, nnz=10000, index_len=100, dim=0         |   229  |       349      |    810
      n=10, nnz=10000, index_len=100, dim=1         |   433  |       870      |    700
      n=10, nnz=10000, index_len=1000, dim=0        |   666  |       502      |   5581
      n=10, nnz=10000, index_len=1000, dim=1        |   826  |      2379      |   4820
      n=10, nnz=10000, index_len=10000, dim=0       |  2534  |      2700      |  80000
      n=10, nnz=10000, index_len=10000, dim=1       |  2723  |     18540      |  80000
      n=100, nnz=1000, index_len=100, dim=0         |    94  |       324      |    110
      n=100, nnz=1000, index_len=100, dim=1         |   100  |       499      |    110
      n=100, nnz=1000, index_len=1000, dim=0        |    96  |       337      |    150
      n=100, nnz=1000, index_len=1000, dim=1        |   130  |       800      |    140
      n=100, nnz=1000, index_len=10000, dim=0       |   100  |       346      |    900
      n=100, nnz=1000, index_len=10000, dim=1       |   130  |       760      |    900
      n=100, nnz=10000, index_len=100, dim=0        |    90  |       323      |    190
      n=100, nnz=10000, index_len=100, dim=1        |   279  |       800      |    180
      n=100, nnz=10000, index_len=1000, dim=0       |   110  |       339      |    781
      n=100, nnz=10000, index_len=1000, dim=1       |   294  |       870      |    800
      n=100, nnz=10000, index_len=10000, dim=0      |   315  |       505      |   6264
      n=100, nnz=10000, index_len=10000, dim=1      |   497  |      2398      |   5404
      n=1000, nnz=10000, index_len=100, dim=0       |    90  |       333      |    160
      n=1000, nnz=10000, index_len=100, dim=1       |   279  |       635      |    150
      n=1000, nnz=10000, index_len=1000, dim=0      |   100  |       328      |    215
      n=1000, nnz=10000, index_len=1000, dim=1      |   287  |       810      |    207
      n=1000, nnz=10000, index_len=10000, dim=0     |   100  |       339      |    900
      n=1000, nnz=10000, index_len=10000, dim=1     |   291  |       880      |   1000
      n=1000, nnz=100000, index_len=100, dim=0      |    92  |       358      |    435
      n=1000, nnz=100000, index_len=100, dim=1      |   302  |       900      |    530
      n=1000, nnz=100000, index_len=1000, dim=0     |   130  |       360      |   1000
      n=1000, nnz=100000, index_len=1000, dim=1     |   329  |       930      |   1200
      n=1000, nnz=100000, index_len=10000, dim=0    |   343  |       530      |   7000
      n=1000, nnz=100000, index_len=10000, dim=1    |   545  |      2446      |   6100
      n=1000, nnz=1000000, index_len=100, dim=0     |   355  |       394      |   2210
      n=1000, nnz=1000000, index_len=100, dim=1     |  1660  |      2276      |   2674
      n=1000, nnz=1000000, index_len=1000, dim=0    |   877  |       574      |   6700
      n=1000, nnz=1000000, index_len=1000, dim=1    |  2449  |      3782      |   9000
      n=1000, nnz=1000000, index_len=10000, dim=0   |  3112  |      2931      |  57000
      n=1000, nnz=1000000, index_len=10000, dim=1   |  7340  |     20220      |  65700

Times are in microseconds (us).

```

</details>

Pull Request resolved: https://github.com/pytorch/pytorch/pull/77551
Approved by: https://github.com/cpuhrsch"
pytorch/pytorch,44aa4ad8949a9906a88498eda9088d3c1ee31d50,"Use `_all_gather_base` and fuse matmul for sharded linear.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/78477

Use `_all_gather_base` instead of all_gather for col-wise sharding
since `_all_gather_base` returns a single fused tensor that can be used to
perform a single matmul instead of looping through and performing multiple
matmuls.

This improves performance for col-wise sharding.

Differential Revision: [D36754385](https://our.internmc.facebook.com/intern/diff/D36754385/)

Approved by: https://github.com/aazzolini, https://github.com/wanchaol"
pytorch/pytorch,93d5a722b14e406db4dcd85e2cbbcb32d7600560,"[coreml] Introducing Quantization (#78108)

Summary: Adding Quantization mode to preprocess, which allows us to run through quantization for coreml models

Test Plan:
https://fburl.com/anp/r0ntsbq0

Notebook runnining through quantization workflow:

created a custom bentos kernel to run it through coreml

```bento_kernel(
    name = ""coreml"",
    deps = [
        ""fbsource//third-party/pypi/coremltools:coremltools"",
        ""//caffe2:coreml_backend"",
        ""//caffe2:coreml_backend_cpp"",
        ""//caffe2:torch"",
        ""//caffe2/torch/fb/mobile/model_exporter:model_exporter"",
    ],
)
```

Initial benchmarks on iPhone 11:

FP32 Core ML Model:
https://our.intern.facebook.com/intern/aibench/details/203998485252700

Quantized Core ML Model:
https://our.intern.facebook.com/intern/aibench/details/927584023592505

High End Quantized Model:
https://our.intern.facebook.com/intern/aibench/details/396271714697929

Summarized Results
| Backend | Quantization | p50 net latency | Model Size |
|---------|--------------|-----------------|------------|
| Core ML | No           | 1.2200          | 1.2mb      |
| Core ML | Yes          | 1.2135          | 385kb      |
| CPU     | Yes          | 3.1720          | 426kb      |

Reviewed By: SS-JIA

Differential Revision: D36559966

Pull Request resolved: https://github.com/pytorch/pytorch/pull/78108
Approved by: https://github.com/jmdetloff"
pytorch/pytorch,f42b42d3eb9af4ea1d09f00a13e9b6dc9efcc0f8,"MPS: Implement aten::count_nonzero.dim_IntList (#78169)

- See: #77764

Implements the `aten::count_nonzero.dim_IntList` operator (as used by [torch.count_nonzero](https://pytorch.org/docs/stable/generated/torch.count_nonzero.html)) for [MPS](https://pytorch.org/blog/introducing-accelerated-pytorch-training-on-mac/).

Pull Request resolved: https://github.com/pytorch/pytorch/pull/78169
Approved by: https://github.com/malfet, https://github.com/kulinseth, https://github.com/albanD"
pytorch/pytorch,11b9a81e02551261a172e674e873de89a62610ef,"[NNC] channels last propagation within NNC fusion group (#76948)

Decide the memory layout propagation policy and propagate it within the NNC fusion group. The memory layout propagation policy could be `Contiguous` and `Channels-last contiguous`.
 - `Contiguous`: Convert the non-contiguous including channels-last contiguous input tensors to contiguous and generate the contiguous output `Buf` for lowering function.
 - `Channels-last contiguous`: Convert the input tensors to channels-last contiguous and generate the channels-last contiguous output `Buf` for lowering function.

Currently, the rule is simple. If all the input and out tensors of the NNC fusion group are channels-last contiguous, then the propagated memory layout is `Channels-last contiguous`. Otherwise, it is always `Contiguous` which is as same as current situation. It means that this PR provides a fast path to channels-last and the optimization is conservative since its trigger conditions are strict.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/76948
Approved by: https://github.com/ZolotukhinM"
pytorch/pytorch,bde246fcc60372c0ce7ee16dd5e3dc7652a36867,"Speed up test_mps from 9min to 25s

Pull Request resolved: https://github.com/pytorch/pytorch/pull/78488

Approved by: https://github.com/kulinseth"
pytorch/pytorch,50930604cf3b41637eb341c67e6da6a20e92ec9e,"Hackily use addSkip to track flakiness in common_utils.py (#78292)

### Problem:
The current way we detect flakiness is by aggregating results at the end of a job, which has worked so far but is inefficient and potentially inaccurate. We have also been delegating a workflow step towards doing this analysis at the end of every job.

### Solution:
This PR uses unittest.TestResult's addSkip method, which adds a skipped test every time we detect something is flaky. This way, we no longer need to aggregate anything and we can easily scan through the test reports and filter for skipped tests with flaky = True. Not only is this much faster to query for, it rids us of needing to figure out janky aggregation logic.

### Test plan:
I simulated a flaky test locally (test_async_python) and observed that:
With overriding signal ON (so flaky test = green):
- Successes pass are reported just as they normally are with no skips. [override_signal_normal_success.txt](https://github.com/pytorch/pytorch/files/8774012/override_signal_normal_success.txt)
- Failures fail and are reported as they are with no skips. [override_signal_all_fails.txt](https://github.com/pytorch/pytorch/files/8774010/override_signal_all_fails.txt)
- Flaky tests have expected failures + a success + a skip denoting the correct information. [override_signal_1_1.txt](https://github.com/pytorch/pytorch/files/8774005/override_signal_1_1.txt)
 and [override_signal_2_1.txt](https://github.com/pytorch/pytorch/files/8774007/override_signal_2_1.txt)

With overriding signal OFF:
- Successes pass are reported just as they normally are with no skips. [report_only_one_success.txt](https://github.com/pytorch/pytorch/files/8774019/report_only_one_success.txt)
- Failures fail and are reported as they are with no skips. [report_only_all_fails.txt](https://github.com/pytorch/pytorch/files/8774018/report_only_all_fails.txt)
- Flaky tests have failures + unexpected successes + a skip denoting the correct
 information. [report_only_3_1.txt](https://github.com/pytorch/pytorch/files/8774015/report_only_3_1.txt)

Pull Request resolved: https://github.com/pytorch/pytorch/pull/78292
Approved by: https://github.com/suo"
pytorch/pytorch,d6db5ea50d3d05694e43932a82a975d1255da114,"Back out ""add mixed data type mode for LayerNorm forward path""

Pull Request resolved: https://github.com/pytorch/pytorch/pull/78298

Also back out ""improve LayerNorm bfloat16 performance on CPU"".

These layer norm changes seem fine, but they are causing `LayerNorm` to not use AVX2 instructions, which is causing performance on internal models to degrade. More investigation is needed to find the true root cause, but we should unland to mitigate the issue ASAP.

I left `mixed_data_type.h` around since there are some other files depending on it.

Differential Revision: [D36675352](https://our.internmc.facebook.com/intern/diff/D36675352/)

Approved by: https://github.com/tenpercent"
pytorch/pytorch,e9d0f5fb17ad30939b7217f1a1d0552a6d005ba0,"Eliminate Named tensor warnings in XNNPACK and QNNPACK

XNNPACK and QNNPACK propagate dimension names during allocation, but
they never create named tensors from scratch.  However, they were
generating warnings (""Warning: Named tensors and all their associated
APIs are an experimental feature..."") from the call
`namedinference::propagate_names_if_present_and_nonempty(t1, t2.names())`
(because `t2.names()` returns a non-empty DimnameList with wildcards.)

Introduce propagate_names_if_present_and_nonempty, which takes an
`optional<DimnameList>` from `t2.opt_names()`, and use it from QNNPACK
and XNNPACK to eliminate this warning.  Another option would be to make
`propagate_names_if_nonempty` take an optional, which would be
more-or-less backward compatible since
`propagate_names_if_nonempty(t1, t2.names())` would still be valid due
to implicit conversion to optional.  I chose not to do this just because
it seemed riskier.

Test Plan:
For QNNPACK, just ran the NNAPI test, which was previously throwing a
warning.  For XNNPACK, its uses are gated on C10_MOBILE, so I needed to
export a model.

```python
import torch
import torch.utils.bundled_inputs
import torch.utils.mobile_optimizer
import torch.nn.functional as F

class MyModel(torch.nn.Module):
    def __init__(self):
        super().__init__()
        self.conv = torch.nn.Conv2d(1, 1, 1)
        self.convt = torch.nn.ConvTranspose2d(1, 1, 1)
        self.lin = torch.nn.Linear(1, 1)

    def forward(self, t):
        t = self.conv(t)
        t = self.convt(t)
        t = F.hardswish(t, False)
        t = F.hardswish(t[...,::2], True)
        t = F.adaptive_avg_pool2d(t, (1, 1))
        t = torch.broadcast_to(t, (1, 4, 1, 1))
        t = t.contiguous(memory_format=torch.channels_last)
        t = F.channel_shuffle(t, 2)
        t = F.max_pool2d(t.reshape(1, 1, 2, 2), (2, 2))
        t = self.lin(t.reshape(1, 1).contiguous())
        return t

arg = torch.zeros(1,1,1,3)
mtr = torch.jit.trace(MyModel().eval(), arg)
mom = torch.utils.mobile_optimizer.optimize_for_mobile(mtr)
mbi = torch.utils.bundled_inputs.bundle_inputs(mom, [((arg,))])

torch.jit.save(mbi, ""/tmp/model.ptj"")
```

```bash
nice env CMAKE_CXX_COMPILER_LAUNCHER=ccache CMAKE_C_COMPILER_LAUNCHER=ccache ./scripts/build_mobile.sh -DBUILD_BINARY=1

/build_mobile/bin/speed_benchmark_torch --model=/tmp/model.ptj --use_bundled_input=0
```

Pull Request resolved: https://github.com/pytorch/pytorch/pull/77762

Approved by: https://github.com/zou3519"
pytorch/pytorch,b7bb34d7625d95e5088638721dcc07c2bc5e2ade,"[MPS] Add version check (#78192)

Use `instancesRespondToSelector:` to test the presence of
`optimizationLevel` in `MPSGraphCompilationDescriptor`, which according
to
https://developer.apple.com/documentation/metalperformanceshadersgraph/mpsgraphcompilationdescriptor/3922624-optimizationlevel
is only available on 12.3 or newer

This works around a limitations of `@available(macOS 12.3, *)` macro in
shared libraries dynamically loaded by apps targeting older runtime.
And deployment target for macos Python conda binaries is 10.14:
```
% otool -l `which python3`
...
Load command 9
      cmd LC_BUILD_VERSION
  cmdsize 32
 platform 1
    minos 10.14
      sdk 10.14
...
```

Pull Request resolved: https://github.com/pytorch/pytorch/pull/78192
Approved by: https://github.com/atalman, https://github.com/seemethere"
pytorch/pytorch,2d93e1fadaa1ff90fc49d84b1f47b09ccc4093b3,"Add slow path for device

Pull Request resolved: https://github.com/pytorch/pytorch/pull/77684

Approved by: https://github.com/ezyang"
pytorch/pytorch,da16450360ab565af029cc08b3afbd209ff6fbbb,"Beta function (#78031)

Euler beta function:

```Python
torch.special.beta(input, other, *, out=None) → Tensor
```

`reentrant_gamma` and `reentrant_ln_gamma` implementations (using Stirling’s approximation) are provided. I started working on this before I realized we were missing a gamma implementation (despite providing incomplete gamma implementations). Uses the coefficients computed by Steve Moshier to replicate SciPy’s implementation. Likewise, it mimics SciPy’s behavior (instead of the behavior in Cephes).
Pull Request resolved: https://github.com/pytorch/pytorch/pull/78031
Approved by: https://github.com/mruberry"
pytorch/pytorch,c083489f46e3885fbfc8888fd308e238bfcabfd6,"[kineto] Optimize getStepCallbacks for common case of no active callbacks

Pull Request resolved: https://github.com/pytorch/pytorch/pull/77804

IIUC, the result of this function will be empty and unused if there are no sampled callbacks, which is the common case. We can accelerate this case by wrapping the result in an optional to save initializing an empty SmallVector.

Differential Revision: [D36497279](https://our.internmc.facebook.com/intern/diff/D36497279/)

**NOTE FOR REVIEWERS**: This PR has internal Facebook specific changes or comments, please review them on [Phabricator](https://our.internmc.facebook.com/intern/diff/D36497279/)!

Approved by: https://github.com/robieta"
pytorch/pytorch,80c4919bec29a8234e7289c058b905352d12d18e,"[PyTorch] Stack-allocate boxed args for RecordFunction

Pull Request resolved: https://github.com/pytorch/pytorch/pull/76266

Saving a heap allocation in this path improves performance.

Differential Revision: [D34090699](https://our.internmc.facebook.com/intern/diff/D34090699/)

Approved by: https://github.com/ezyang"
pytorch/pytorch,4ea176ea57d28dd8b265e3a77e34b5984958344b,"expose fast get_current_stream (#78165)

Expose fast no-frills version of getting raw `cudaStream_t` in python (200 ns instead of 4 us)

Pull Request resolved: https://github.com/pytorch/pytorch/pull/78165
Approved by: https://github.com/SherlockNoMad, https://github.com/soumith, https://github.com/gchanan"
pytorch/pytorch,c274f2ad52504e0d20724b05171da33c340e60f8,"[cuDNN V8 API] (reopen) Allow the number of kernels profiled under torch.backends.cudnn.benchmark = True to be limitedCudnnv8 benchmark limit (#77002)

(reopening due to botched merge)
The cuDNN V8 API (main support merged in https://github.com/pytorch/pytorch/pull/60755) potentially exposes many more kernels with benchmark=True. While these additional kernels can improve performance, it is often unnecessary to run every kernel returned by the heuristic and doing so may degrade the user experience by causing the first model iteration to be very slow. To alleviate this issue, this PR introduces torch.backends.cudnn.benchmark_limit. benchmark_limit specifies the maximum number of working cuDNN kernels to try for a given workload, with the default being 10 (similar to what TensorFlow does). benchmark_limit = 0 yields the current behavior of trying every kernel returned by the heuristic.

CC @ptrblck @ngimel @xwang233
Pull Request resolved: https://github.com/pytorch/pytorch/pull/77002
Approved by: https://github.com/ngimel"
pytorch/pytorch,cac16e2ee293964033dffa6616f78b68603cd565,"Minor typo in contributing doc fixed (#70284)

Hi. While looking at the contributing doc for pytorch in master. I saw a typo about installing `ccache` via `conda`. This tiny PR fixed it.

Hope this could help someone copy-paste the command faster!
Pull Request resolved: https://github.com/pytorch/pytorch/pull/70284
Approved by: https://github.com/janeyx99"
pytorch/pytorch,294fff16ec7ad5382c551b00fdc7a3c3e15d1bfd,"add slow path for is_contiguous (#77906)

Test Plan: CI

Reviewed By: malfet, b0noI

Differential Revision: D36493890

Pull Request resolved: https://github.com/pytorch/pytorch/pull/77906
Approved by: https://github.com/malfet"
pytorch/pytorch,00a187c373c91bc59b6e5acedc52e2682d579242,"Revert ""add slow path for is_contiguous""

This reverts commit f6beda89c6acbb92ff7f82699b9ea4c5c7428a19.

Reverted https://github.com/pytorch/pytorch/pull/77396 on behalf of https://github.com/malfet"
pytorch/pytorch,eb0ff991f7508f0f9248994a3bd6566696aa9fc2,"[FSDP] Dont move if on CPU (#77720)

After offline discussion, decided that by default moving CPU module to GPU is a bit too risky due to possible OOM during init issue.

Theoretically, we should not OOM because it is required for module that is being wrapped by FSDP to fit into GPU, i.e. during forward. But possibly can be temporary GPU tensors etc allocated during __init___ that break this assumption, it is better for now to allow users a way to init on CPU if needed.

We still warn to use `device_id` for faster init if model is on CPU.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/77720
Approved by: https://github.com/zhaojuanmao"
pytorch/pytorch,c9570e4b88a86d317e7d046d2e63fe75d67b4a4d,"[checkpoint] Synchronize error handling across all ranks (#77091)

Introduce error handling across all ranks when loading and saving checkpoints.

This makes it a lot simpler for users to handle failures and, as a positive side-effect, coordination of when it successfully finished.

This change requires 3 collectives when saving and 1 when loading.
All those collectives carry a small payload so they will be latency bound and write time should dominate it.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/77091
Approved by: https://github.com/pritamdamania87, https://github.com/wanchaol"
pytorch/pytorch,ddb2eb7aee3b6812dde7bfc32fc0f26f5e916e6a,"Micro-optimisations for matmul 2.0: Electric boogaloo

This PR implements the bulk of
https://github.com/pytorch/pytorch/pull/64387

Part of the optimisations were already merged in
https://github.com/pytorch/pytorch/pull/72230

A number of these optimisations include:
- Make the code `const` correct.
- Create `DimVector`'s more efficiently (e.g. prefer `append` over
`insert`).
- Access sizes of the tensors via `sizes().front()` / `sizes().back()`
  / `sizes().end()[-2]`
- Do not create intermediary tensors / vectors when it can be avoided.
- Call `reshape` rather than `expect_contiguous`  + `view`

On top of these, it fixes a correctness issue of `matmul_out`, where the
out parameter was not resized correctly when passed to the backends.
This involves removing the use of `set_` from the calling code, as
requested by ezyang, and it incurs on most of the complexity of the
code that this PR adds.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/75197

Approved by: https://github.com/mruberry"
pytorch/pytorch,ff7b6d6b5fb343c7e5e72c8a9ee4df3302396867,"Update linalg.*norm

This PR does a number of things:
- Move linalg.vector_norm to structured kernels and simplify the logic
- Fixes a number of prexisting issues with the dtype kwarg of these ops
- Heavily simplifies and corrects the logic of `linalg.matrix_norm` and `linalg.norm` to be consistent with the docs
  - Before the `_out` versions of these functions were incorrect
  - Their implementation is now as efficient as expected, as it avoids reimplementing these operations whenever possible.
- Deprecates `torch.frobenius_norm` and `torch.nuclear_norm`, as they were exposed in the API and they are apparently being used in mobile (??!!) even though they were not documented and their implementation was slow.
  - I'd love to get rid of these functions already, but I guess we have to go through their deprecation.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/76547

Approved by: https://github.com/mruberry"
pytorch/pytorch,f6beda89c6acbb92ff7f82699b9ea4c5c7428a19,"add slow path for is_contiguous

Pull Request resolved: https://github.com/pytorch/pytorch/pull/77396

Approved by: https://github.com/ezyang, https://github.com/cpuhrsch"
pytorch/pytorch,fe1232a494fc2c755740dd7a867f2b29176cefc5,"Use FastAtomicAdd for index_add/_index_reduce

Pull Request resolved: https://github.com/pytorch/pytorch/pull/76996

Approved by: https://github.com/cpuhrsch, https://github.com/ngimel"
pytorch/pytorch,2a496e2f80a9937c336aaabf13f99cbc3983998c,"Adding maximize to Adamax (#77409)

Added the maximize flag #68052 to Adamax optimizer and updates the respective tests.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/77409
Approved by: https://github.com/albanD"
pytorch/pytorch,dca416b578eee9ba0cd272e6626f85a77354a855,"Pretty-print dataclasses (#76810)

Unfortunately the built-in pprint module support pretty-print of dataclasses only from python 3.10. The code that I wrote in method `__str__` of OpInfo should do the same job and should also work for any dataclass. For now I've put it there but we can create a function and put it somewhere where is accessible also for other dataclasses. Also the max width (80) is now hardcode but it would ideally be the parameter of the function.

when you call print on an OpInfo you get:
```
OpInfo(name = '__getitem__',
       ref = None,
       aliases = (),
       variant_test_name = '',
       op = <slot wrapper '__getitem__' of 'torch._C._TensorBase' objects>,
       method_variant = <slot wrapper '__getitem__' of 'torch._C._TensorBase' objects>,
       inplace_variant = None,
       skips = (<torch.testing._internal.common_methods_invocations.DecorateInfo object at 0x7f463acbca90>,
                <torch.testing._internal.common_methods_invocations.DecorateInfo object at 0x7f463acbcae0>),
       decorators = (<torch.testing._internal.common_methods_invocations.DecorateInfo object at 0x7f463acbca90>,
                     <torch.testing._internal.common_methods_invocations.DecorateInfo object at 0x7f463acbcae0>),
       sample_inputs_func = <function sample_inputs_getitem at 0x7f463acc6af0>,
       reference_inputs_func = None,
       error_inputs_func = None,
       sample_inputs_sparse_coo_func = <function _DecoratorContextManager.__call__.<locals>.decorate_context at 0x7f463acc6b80>,
       sample_inputs_sparse_csr_func = <function _DecoratorContextManager.__call__.<locals>.decorate_context at 0x7f463acc6c10>,
       dtypes = {torch.int16,
                 torch.float64,
                 torch.int32,
                 torch.int64,
                 torch.complex64,
                 torch.float16,
                 torch.bfloat16,
                 torch.uint8,
                 torch.complex128,
                 torch.bool,
                 torch.float32,
                 torch.int8},
       dtypesIfCUDA = {torch.int16,
                       torch.float64,
                       torch.int32,
                       torch.int64,
                       torch.complex64,
                       torch.float16,
                       torch.bfloat16,
                       torch.uint8,
                       torch.complex128,
                       torch.bool,
                       torch.float32,
                       torch.int8},
       dtypesIfROCM = {torch.int16,
                       torch.float64,
                       torch.int32,
                       torch.int64,
                       torch.complex64,
                       torch.float16,
                       torch.bfloat16,
                       torch.uint8,
                       torch.complex128,
                       torch.bool,
                       torch.float32,
                       torch.int8},
       backward_dtypes = {torch.int16,
                          torch.float64,
                          torch.int32,
                          torch.int64,
                          torch.complex64,
                          torch.float16,
                          torch.bfloat16,
                          torch.uint8,
                          torch.complex128,
                          torch.bool,
                          torch.float32,
                          torch.int8},
       backward_dtypesIfCUDA = {torch.int16,
                                torch.float64,
                                torch.int32,
                                torch.int64,
                                torch.complex64,
                                torch.float16,
                                torch.bfloat16,
                                torch.uint8,
                                torch.complex128,
                                torch.bool,
                                torch.float32,
                                torch.int8},
       backward_dtypesIfROCM = {torch.int16,
                                torch.float64,
                                torch.int32,
                                torch.int64,
                                torch.complex64,
                                torch.float16,
                                torch.bfloat16,
                                torch.uint8,
                                torch.complex128,
                                torch.bool,
                                torch.float32,
                                torch.int8},
       supports_out = False,
       supports_autograd = True,
       supports_gradgrad = True,
       supports_fwgrad_bwgrad = True,
       supports_inplace_autograd = False,
       supports_forward_ad = True,
       gradcheck_wrapper = <function OpInfo.<lambda> at 0x7f463a7a40d0>,
       check_batched_grad = True,
       check_batched_gradgrad = True,
       check_batched_forward_grad = True,
       check_inplace_batched_forward_grad = True,
       gradcheck_nondet_tol = 0.0,
       gradcheck_fast_mode = None,
       aten_name = '__getitem__',
       decomp_aten_name = None,
       aten_backward_name = None,
       assert_autodiffed = False,
       autodiff_nonfusible_nodes = ['aten::__getitem__'],
       autodiff_fusible_nodes = [],
       supports_sparse = False,
       supports_scripting = False,
       supports_sparse_csr = False,
       test_conjugated_samples = True,
       test_neg_view = True,
       assert_jit_shape_analysis = False,
       supports_expanded_weight = False)
```

cc @ezyang

Pull Request resolved: https://github.com/pytorch/pytorch/pull/76810
Approved by: https://github.com/ezyang"
pytorch/pytorch,2166fc55fc009a6f66c96cdeac9a0a308e77bc55,"improve softmax lastdim performance on bfloat16 by adding more fusion

Pull Request resolved: https://github.com/pytorch/pytorch/pull/76278

Approved by: https://github.com/frank-wei"
pytorch/pytorch,59b56ba78549fb3645f060a16923ae025490cd78,"improve group_norm channels last performance on CPU

add channels_last_3d memory format support

add BFloat16 support on CPU

Pull Request resolved: https://github.com/pytorch/pytorch/pull/69067

Approved by: https://github.com/VitalyFedyunin"
pytorch/pytorch,dcc255d10b429e0881a380c051b8e15882a0c648,"add simd horizantal reduce to improve log_softmax and softmax performance on CPU

Pull Request resolved: https://github.com/pytorch/pytorch/pull/73953

Approved by: https://github.com/frank-wei"
pytorch/pytorch,2b7943c47c8561a46103488b0fe9a592b87dc5bb,"fix torchvhsion failed case test_classification_model on slow_conv2d

Pull Request resolved: https://github.com/pytorch/pytorch/pull/77347

Approved by: https://github.com/datumbox, https://github.com/frank-wei"
pytorch/pytorch,b2507592424f4a14ab6613cd05b20f18f49f2663,"mul(dense, csr), mul(csr, dense) via sparse_mask_csr (#77177)

This adds basic coverage, but can be easily made more efficient by providing a native implementation.

Follow up work includes supporting CSR gradients for strided Tensors.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/77177
Approved by: https://github.com/nikitaved, https://github.com/mikaylagawarecki"
pytorch/pytorch,b825e1d472320711a6352eff7c4ba92f1186af21,"Revert autoformat of tools/fast_nvcc/fast_nvcc.py

This was an Meta-internal change that seems to have deleted a bunch of
types and is thus causing us to fail mypy type checking. Reverting that
portion of the change.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/77327

Approved by: https://github.com/qihqi"
pytorch/pytorch,166a466e7f794c38da5179a206fac42d3df258d6,"improve LayerNorm bfloat16 performance on CPU

Pull Request resolved: https://github.com/pytorch/pytorch/pull/71376

Approved by: https://github.com/frank-wei"
pytorch/pytorch,3d0e6f169c6745a548ca3e71af9a719ba67dff20,"add channels last support for slow_conv_dilated2d

Pull Request resolved: https://github.com/pytorch/pytorch/pull/70665

Approved by: https://github.com/VitalyFedyunin"
pytorch/pytorch,a5c9e8863254acaf98cbfd7b6829883d5d485156,"[lint] don't run differently on PR vs. master

I originally thought it might be nice for the CI workflow to only run
lint on files your PR changed, which is marginally faster. But this
doesn't work that well in cases where your change may have affected a
file you *didn't* change, like in the case of
https://github.com/pytorch/pytorch/commit/3a68155ce0973c005457593375801a2cc19de54f.

Since we block/revert on lint, it's better to be safe and just run the
exact same thing on PR and master.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/77211

Approved by: https://github.com/albanD, https://github.com/malfet, https://github.com/seemethere"
pytorch/pytorch,2896f81dd4f51062f4bf5338e31d15cc3c599eb7,"Consolidate customization contiguous/sizes policy into unified policy

Prior to this PR, we had a mish-mash of ways of getting unconventional
sizes/strides behavior:

- In OSS (but not in fbcode), some methods are virtual and you
  can override them directly

- There is a is_contiguous policy which is a bitfield tag that lets
  you toggle is_contiguous to error or hit a virtual method
  is_contiguous_custom if it is set.  Ordinarily is_contiguous()
  is virtual and you can just override it, but this works EVEN IF
  is_contiguous() is non-virtual (e.g., in fbcode)

- There is also a sizes policy which is the same idea but for sizes

This PR unifies these mechanisms, and in doing so, eliminates the
maybe virtual/not-virtualness of the methods in question.  The primary
downside of this change is that it is BC-breaking (but the BC break is
very easy to fix!)

The new scheme works like this: we have three levels of policy for
sizes/strides (order matters).

- The Default policy is a conventional dense tensor, where we use
  all of the built-in fields to directly represent the
  sizes/strides/numel/contiguity of the tensor, and it is possible
  to bypass virtual call entirely.

- The CustomStrides policy represent tensors which have a custom
  notion of strides (most typically, that they don't support them),
  shunting strides() and is_contiguous() to virtual methods
  strides_custom() and is_contiguous_custom().  This INCLUDES handling
  for contiguity, since they typically go hand-in-hand (although
  the situation is murky with batched tensors).  The default
  implementations of these functions raise errors saying the tensor
  doesn't support them.

- The CustomSizes policy represent tensors which have a custom
  notion of sizes (the two notable examples are nested tensor, which
  doesn't have a representation of sizes in the conventional form, and
  XLA/LTC tensor, which synchronizes its sizes with an underlying
  compiler backend).  This shunts sizes(), numel() and dim() (along
  with everything from strides) to _custom() variants.

There is no special policy for erroring; instead, we just do a vcall
and expect the virtual method to raise an exception (the performance
hit from the vcall doesn't matter because you're about to raise a C++
exception anyway).  The default implementations of all overridable
functions are available at _default() which is helpful in some
situations when you just want to do a ""sync"" and then run the
conventional semantics.

This PR could be extended further in two ways but I did not do them
due to time constraints:

- Ideally, all TENSORIMPL_MAYBE_VIRTUAL would be eliminated from
  TensorImpl, by using the same policy trick.

- set_size and set_stride are still virtual; it's not entirely clear
  the same trick should be used here though as these methods are
  deprecated.

Signed-off-by: Edward Z. Yang <ezyangfb.com>

Pull Request resolved: https://github.com/pytorch/pytorch/pull/77036

Approved by: https://github.com/bdhirsh"
pytorch/pytorch,57b54dfec5c07c00aa45b4ec605077e88a156230,"Fix Optimizer.zero_grad type annotation (#76998)

`Optimizer.zero_grad()` defines the `set_to_none` argument as `bool`, not `Optional[bool]`

Fixes #ISSUE_NUMBER

Pull Request resolved: https://github.com/pytorch/pytorch/pull/76998
Approved by: https://github.com/albanD"
pytorch/pytorch,02713221e3bd201dc9f5d7b8ca1ea1caae310017,"[SR] Fuse clamp/nan_to_num

Pull Request resolved: https://github.com/pytorch/pytorch/pull/77094

Fuse `clamp` and `nan_to_num` in an NNC kernel. This leads to a big speed up on many models. We can avoid comparisons since clamp potentially gets rid of all of the `inf`s in the input tensor.

Differential Revision: [D36220967](https://our.internmc.facebook.com/intern/diff/D36220967/)

**NOTE FOR REVIEWERS**: This PR has internal Facebook specific changes or comments, please review them on [Phabricator](https://our.internmc.facebook.com/intern/diff/D36220967/)!

Approved by: https://github.com/navahgar"
pytorch/pytorch,61dcde88a6c2ce60000b59e8e472a9a6c2456f2c,"Jiterator with Python Registration (#77121)

You can now do a lot of crazy things about redefining the behavior of an operator, and still be fast in cuda !!!

Example 1: swapping where's branches
```
code_string = ""template <typename T> T inverted_where(bool cond, T a, T b){ return !cond ? a : b; }""
jitted_fn = torch.cuda.jiterator._create_jit_fn(code_string)
my_lib = torch.library.Library(""aten"", ""IMPL"")
my_lib.impl('aten::where.self', jitted_fn, ""CUDA"")

# torch.where is now overridden
```
Example 2: approximate gelu with relu
```
code_string = ""template <typename T> T fast_gelu(T a){ return a > 0 ? a : 0;}""
jitted_fn = torch.cuda.jiterator._create_jit_fn(code_string)
my_lib = torch.library.Library(""aten"", ""IMPL"")
my_lib.impl('aten::gelu', jitted_fn, ""CUDA"")

# torch.nn.GELU and torch.nn.function.gelu are now overridden
```
Example 3: clipping output for numerical unstable kernels
```
code_string = ""template <typename T> T clipped_exp(T a){ return a > T(10.0) ? T(22026.4657948) : exp(a); }""
jitted_fn = torch.cuda.jiterator._create_jit_fn(code_string)
my_lib = torch.library.Library(""aten"", ""IMPL"")
my_lib.impl('aten::exp', jitted_fn, ""CUDA"")

# torch.exp(x) and x.exp() are now overridden
```
Example 4: Simulate buggy hardware behaviors
```
code_string = ""template <typename T> T buggy_add(T a, T b){ return a + b + T(1); }""
jitted_fn = torch.cuda.jiterator._create_jit_fn(code_string)
my_lib = torch.library.Library(""aten"", ""IMPL"")
my_lib.impl('aten::add.Tensor', jitted_fn, ""CUDA"")

torch.add(x, y), ""x + y"" and x.add(y) are now overridden
```

Pull Request resolved: https://github.com/pytorch/pytorch/pull/77121
Approved by: https://github.com/anjali411"
pytorch/pytorch,54dbfc1e4b10c8c5adcd765fb489a5d2fad5e7e7,"Revert ""speed up 1d sort (#77100)""

This reverts commit ae25d14f23c1dd92060069454619fc0562a52c80.

Reverted https://github.com/pytorch/pytorch/pull/77100 on behalf of https://github.com/ngimel"
pytorch/pytorch,d22d749a0e8436829e564796bf6cd67847199453,"faster batch sampler (#76951)

Fixes #76950

Improve the performance of iteration on `BatchSampler` , especially when `batch_size` is big.

Python 3.6.8:
```
  batch_size  drop_last     speedup
------------  -----------   -------
           4  True          -18.07%
           4  False         15.92%
           8  True          9.43%
           8  False         30.90%
          64  True          54.99%
          64  False         49.64%
         640  True          66.26%
         640  False         48.32%
        6400  True          69.06%
        6400  False         45.17%
```

Python 3.8.12:
```
  batch_size  drop_last    speedup
------------  -----------  --------
           4  True         -10.50%
           4  False        -0.78%
           8  True         24.40%
           8  False        10.20%
          64  True         90.96%
          64  False        26.09%
         640  True         112.88%
         640  False        20.09%
        6400  True         111.80%
        6400  False        18.37%

```

Check the issue page for more details of the tests.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/76951
Approved by: https://github.com/ejguan"
pytorch/pytorch,fc4f8b5edec69c0ddc63e5210d3f34724d4bd672,"Add a ZT fastpath for linalg_cross (#76940)

See #69687.

upto 89000x speed up. see https://github.com/pytorch/pytorch/pull/76940#issuecomment-1120522858
Pull Request resolved: https://github.com/pytorch/pytorch/pull/76940
Approved by: https://github.com/anjali411"
pytorch/pytorch,00a1fb64bb93cb9a9acb941efa7f3f93d9354ced,"Faster `index_select` for sparse COO tensors on CPU. (#72710)

Fixes https://github.com/pytorch/pytorch/issues/72212.

This PR improves the previous algorithm in complexity. It also utilizes the structure of the problem and parallelizes computations when possible.

Benchmark results.

<details>

<summary>Testing script</summary>

```python
import torch
import math
from IPython import get_ipython
from itertools import product
import pickle
from torch.utils.benchmark import Timer, Compare

torch.manual_seed(13)
#torch.set_num_threads(1)
ipython = get_ipython()

index_sizes = (100, 1000, 10000)
# specifies (n, nnz)
problem_dims = (
    # n > nnz
    (10000, 100),
    (100000, 1000),
    (1000000, 10000),
    # n < nnz
    (10, 100),
    (10, 1000),
    (10, 10000),
    (100, 1000),
    (100, 10000),
    (1000, 10000),
    (1000, 100000),
    (1000, 1000000),
    #(1000000, 1000000000),
)

def f(t, d, index):
    s = torch_sparse.SparseTensor.from_torch_sparse_coo_tensor(t)
    ss = s.index_select(d, index)
    return ss.coo()

name = ""PR""
results = []

for (n, nnz), m in product(problem_dims, index_sizes):
    for d in (0, 1):
        if nnz < n:
            shape = (n, n)
        else:
            shape = (n, nnz // n) if d == 0 else (nnz // n, n)
        nrows, ncols = shape
        rowidx = torch.randint(low=0, high=nrows, size=(nnz,))
        colidx = torch.randint(low=0, high=ncols, size=(nnz,))
        itemidx = torch.vstack((rowidx, colidx))
        xvalues = torch.randn(nnz)
        index = torch.randint(low=0, high=n, size=(m,))

        SparseX = torch.sparse_coo_tensor(itemidx, xvalues, size=shape).coalesce()
        smtp = ""SparseX.index_select(d, index)""
        timer = Timer(smtp,
                      globals=globals(),
                      label=""coo.index_select"",
                      description=f""{name}: coo.index_select"",
                      sub_label=f""n={n}, nnz={nnz}, index_len={m}, dim={d}"",
                      num_threads=torch.get_num_threads())
        results.append(timer.blocked_autorange())

compare = Compare(results)
compare.trim_significant_figures()
compare.print()

with open(f""{name}_index_select.pickle"", 'wb') as f:
    pickle.dump(results, f)

```

</details>

<details>

<summary>Gather results</summary>

```python
import pickle
from torch.utils.benchmark import Timer, Compare

files = [
        ""PR"",
        ""torch_sparse"",
        ""master""
        ]

timers = []
for name in files:
    with open(""{}_index_select.pickle"".format(name), 'rb') as f:
        timers += pickle.load(f)

compare = Compare(timers)
compare.trim_significant_figures()
compare.print()

```

</details>

<details>

<summary>PR/torch_sparse/master runtime comparison</summary>

```
[----------------------------------- coo.index_select ----------------------------------]
                                                    |    PR   |  torch_sparse  |   master
32 threads: -----------------------------------------------------------------------------
      n=10000, nnz=100, index_len=100, dim=0        |     14  |        140     |       10
      n=10000, nnz=100, index_len=100, dim=1        |     14  |        200     |       10
      n=10000, nnz=100, index_len=1000, dim=0       |     30  |        180     |       38
      n=10000, nnz=100, index_len=1000, dim=1       |     34  |        240     |       38
      n=10000, nnz=100, index_len=10000, dim=0      |    278  |        460     |      330
      n=10000, nnz=100, index_len=10000, dim=1      |    275  |        516     |      330
      n=100000, nnz=1000, index_len=100, dim=0      |     16  |        290     |       31
      n=100000, nnz=1000, index_len=100, dim=1      |     26  |        390     |       31
      n=100000, nnz=1000, index_len=1000, dim=0     |     45  |        405     |      263
      n=100000, nnz=1000, index_len=1000, dim=1     |     73  |        500     |      261
      n=100000, nnz=1000, index_len=10000, dim=0    |    444  |        783     |     2570
      n=100000, nnz=1000, index_len=10000, dim=1    |    470  |        890     |     2590
      n=1000000, nnz=10000, index_len=100, dim=0    |     25  |       2400     |      270
      n=1000000, nnz=10000, index_len=100, dim=1    |    270  |       4000     |      269
      n=1000000, nnz=10000, index_len=1000, dim=0   |     74  |       2600     |     2620
      n=1000000, nnz=10000, index_len=1000, dim=1   |    464  |       3600     |     2640
      n=1000000, nnz=10000, index_len=10000, dim=0  |    635  |       3300     |    26400
      n=1000000, nnz=10000, index_len=10000, dim=1  |   1000  |       3960     |    26400
      n=10, nnz=100, index_len=100, dim=0           |     16  |        137     |       16
      n=10, nnz=100, index_len=100, dim=1           |     16  |        220     |       16
      n=10, nnz=100, index_len=1000, dim=0          |     63  |        238     |       81
      n=10, nnz=100, index_len=1000, dim=1          |     60  |        698     |       78
      n=10, nnz=100, index_len=10000, dim=0         |    480  |        940     |      862
      n=10, nnz=100, index_len=10000, dim=1         |    330  |       4930     |     1070
      n=10, nnz=1000, index_len=100, dim=0          |     60  |        200     |       73
      n=10, nnz=1000, index_len=100, dim=1          |     56  |        683     |       70
      n=10, nnz=1000, index_len=1000, dim=0         |    480  |        530     |     1050
      n=10, nnz=1000, index_len=1000, dim=1         |    330  |       4550     |     1368
      n=10, nnz=1000, index_len=10000, dim=0        |   3100  |       2900     |     9300
      n=10, nnz=1000, index_len=10000, dim=1        |   3400  |      46000     |     9100
      n=10, nnz=10000, index_len=100, dim=0         |    400  |        453     |      857
      n=10, nnz=10000, index_len=100, dim=1         |    400  |       4070     |     1730
      n=10, nnz=10000, index_len=1000, dim=0        |   2840  |       2600     |    13900
      n=10, nnz=10000, index_len=1000, dim=1        |   3700  |      40600     |    16000
      n=10, nnz=10000, index_len=10000, dim=0       |  83200  |      67400     |   160000
      n=10, nnz=10000, index_len=10000, dim=1       |  68000  |     528000     |   190000
      n=100, nnz=1000, index_len=100, dim=0         |     46  |        148     |       31
      n=100, nnz=1000, index_len=100, dim=1         |     45  |        242     |       37
      n=100, nnz=1000, index_len=1000, dim=0        |     68  |        248     |      240
      n=100, nnz=1000, index_len=1000, dim=1        |     66  |        755     |      290
      n=100, nnz=1000, index_len=10000, dim=0       |    370  |        802     |     2250
      n=100, nnz=1000, index_len=10000, dim=1       |    372  |       5430     |     2770
      n=100, nnz=10000, index_len=100, dim=0        |     82  |        210     |      224
      n=100, nnz=10000, index_len=100, dim=1        |     74  |        986     |      270
      n=100, nnz=10000, index_len=1000, dim=0       |    350  |        618     |     2600
      n=100, nnz=10000, index_len=1000, dim=1       |    370  |       4660     |     4560
      n=100, nnz=10000, index_len=10000, dim=0      |   3000  |       3400     |    41680
      n=100, nnz=10000, index_len=10000, dim=1      |   5000  |      47500     |    30400
      n=1000, nnz=10000, index_len=100, dim=0       |     71  |        160     |      185
      n=1000, nnz=10000, index_len=100, dim=1       |     64  |        516     |      190
      n=1000, nnz=10000, index_len=1000, dim=0      |    100  |        249     |     1740
      n=1000, nnz=10000, index_len=1000, dim=1      |     98  |       1030     |     1770
      n=1000, nnz=10000, index_len=10000, dim=0     |    600  |        808     |    18300
      n=1000, nnz=10000, index_len=10000, dim=1     |    663  |       5300     |    18500
      n=1000, nnz=100000, index_len=100, dim=0      |    160  |        258     |     1890
      n=1000, nnz=100000, index_len=100, dim=1      |    200  |       3620     |     2050
      n=1000, nnz=100000, index_len=1000, dim=0     |    500  |        580     |    18700
      n=1000, nnz=100000, index_len=1000, dim=1     |    640  |       7550     |    30000
      n=1000, nnz=100000, index_len=10000, dim=0    |   3400  |       3260     |   186000
      n=1000, nnz=100000, index_len=10000, dim=1    |   3600  |      49600     |   194000
      n=1000, nnz=1000000, index_len=100, dim=0     |    517  |        957     |    18700
      n=1000, nnz=1000000, index_len=100, dim=1     |    680  |      39600     |    37600
      n=1000, nnz=1000000, index_len=1000, dim=0    |   3600  |       4500     |   186000
      n=1000, nnz=1000000, index_len=1000, dim=1    |   5800  |      76400     |   190000
      n=1000, nnz=1000000, index_len=10000, dim=0   |  50000  |      67900     |  1800000
      n=1000, nnz=1000000, index_len=10000, dim=1   |  45000  |     570000     |  1900000

Times are in microseconds (us).

```

</details>
Pull Request resolved: https://github.com/pytorch/pytorch/pull/72710
Approved by: https://github.com/pearu, https://github.com/cpuhrsch"
pytorch/pytorch,ae25d14f23c1dd92060069454619fc0562a52c80,"speed up 1d sort (#77100)

This speeds up sort for 1d case (by approx 2x for large sizes) where segment sorting is not required and slightly reduces memory usage. I'm not sure if memory usage can be meaningfully improved.
Slightly helps #77049
I'll update PR with doing the same for multiple segments, provided that each segment size is large.

cc @peterbell10, I had to comment out TORCH_ASSERT_NO_OPERATORS because otherwise I was getting compilation errors, do you know what's up?
Pull Request resolved: https://github.com/pytorch/pytorch/pull/77100
Approved by: https://github.com/zasdfgbnm, https://github.com/mruberry"
pytorch/pytorch,8d67972b140f1dd7e8453e3e7aef3bf173cd5e3d,"Revert ""Faster `index_select` for sparse COO tensors on CPU. (#72710)""

This reverts commit ce3857e73ccbfc1970e90ee886f22e9d26cc97fe.

Reverted https://github.com/pytorch/pytorch/pull/72710 on behalf of https://github.com/malfet"
pytorch/pytorch,6ae047b0a961f082fe2c4091bc6f02e1adbf69a3,"Remove misleading statement in optim.Optimizer docs (#76967)

Fixes #76752
Pull Request resolved: https://github.com/pytorch/pytorch/pull/76967
Approved by: https://github.com/jbschlosser"
pytorch/pytorch,614e0459215c677bf686680454520dfaa2867359,"[ROCm] rccl performance improvement via env var (#76985)

The env var HSA_FORCE_FINE_GRAIN_PCIE=1 enables P2P communication in RCCL without intermediate buffers.  This is necessary on hosts with only PCIe and no P2P high-speed interconnect.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/76985
Approved by: https://github.com/ezyang"
pytorch/pytorch,949cbf1d654a616fd9a18e8e64429509ddb07552,"[NVFuser] Opinfos for extremal values in binary ufuncs

Added slow tests for comparing the eager & fused outputs for given extremal inputs.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/75917

Approved by: https://github.com/jjsjann123, https://github.com/eellison"
pytorch/pytorch,eb0bc5b1c9a71548b9fbe7b5f1fcc553a6e14c91,"[sr][pyper] add fusion broadcast_concat_batch_matmul_batch_gather (#76839)

Summary:
Fuse broadcast_stack -> transpose -> matmul -> flatten (sometimes duplicated with a second extraneous no-op call) -> index_select

I added broadcast support to the fused op, although I haven't seen any examples where the inputs actually need broadcasting, so I didn't focus on improving perf on that part.

Test Plan:
Saves ~0.15ms (~22%) from adfinder_story_post_ad_session_exit_model's remote_other net, or ~10% of the models' overall execution time. With this change, this model now overall runs ~4% faster in static runtime than the C2 baseline.

Before:
I0503 09:19:27.760602 2477739 PyTorchPredictorBenchLib.cpp:305] PyTorch run finished. Milliseconds per iter: 0.799885. Iters per second: 1250.18

       0.201173 ms.    27.4953%. static_runtime::flatten_copy (2 nodes, out variant)
       0.108989 ms.    14.8962%. aten::index_select (1 nodes, out variant)
       0.105632 ms.    14.4372%. aten::matmul (1 nodes, out variant)
      0.0637207 ms.    8.70904%. fb::broadcast_stack (1 nodes)
    0.000658989 ms.  0.0900675%. aten::transpose (1 nodes, native)

After:
I0504 10:59:36.388628 1042000 PyTorchPredictorBenchLib.cpp:305] PyTorch run finished. Milliseconds per iter: 0.512515. Iters per second: 1951.16
      0.231542 ms.    46.6407%. fb::broadcast_concat_batch_matmul_batch_gather (1 nodes, out variant)

Differential Revision: D36139538

Pull Request resolved: https://github.com/pytorch/pytorch/pull/76839
Approved by: https://github.com/mikeiovine"
pytorch/pytorch,ce3857e73ccbfc1970e90ee886f22e9d26cc97fe,"Faster `index_select` for sparse COO tensors on CPU. (#72710)

Fixes https://github.com/pytorch/pytorch/issues/72212.

This PR improves the previous algorithm in complexity. It also utilizes the structure of the problem and parallelizes computations when possible.

Benchmark results.

<details>

<summary>Testing script</summary>

```python
import torch
import math
from IPython import get_ipython
from itertools import product
import pickle
from torch.utils.benchmark import Timer, Compare

torch.manual_seed(13)
#torch.set_num_threads(1)
ipython = get_ipython()

index_sizes = (100, 1000, 10000)
# specifies (n, nnz)
problem_dims = (
    # n > nnz
    (10000, 100),
    (100000, 1000),
    (1000000, 10000),
    # n < nnz
    (10, 100),
    (10, 1000),
    (10, 10000),
    (100, 1000),
    (100, 10000),
    (1000, 10000),
    (1000, 100000),
    (1000, 1000000),
    #(1000000, 1000000000),
)

def f(t, d, index):
    s = torch_sparse.SparseTensor.from_torch_sparse_coo_tensor(t)
    ss = s.index_select(d, index)
    return ss.coo()

name = ""PR""
results = []

for (n, nnz), m in product(problem_dims, index_sizes):
    for d in (0, 1):
        if nnz < n:
            shape = (n, n)
        else:
            shape = (n, nnz // n) if d == 0 else (nnz // n, n)
        nrows, ncols = shape
        rowidx = torch.randint(low=0, high=nrows, size=(nnz,))
        colidx = torch.randint(low=0, high=ncols, size=(nnz,))
        itemidx = torch.vstack((rowidx, colidx))
        xvalues = torch.randn(nnz)
        index = torch.randint(low=0, high=n, size=(m,))

        SparseX = torch.sparse_coo_tensor(itemidx, xvalues, size=shape).coalesce()
        smtp = ""SparseX.index_select(d, index)""
        timer = Timer(smtp,
                      globals=globals(),
                      label=""coo.index_select"",
                      description=f""{name}: coo.index_select"",
                      sub_label=f""n={n}, nnz={nnz}, index_len={m}, dim={d}"",
                      num_threads=torch.get_num_threads())
        results.append(timer.blocked_autorange())

compare = Compare(results)
compare.trim_significant_figures()
compare.print()

with open(f""{name}_index_select.pickle"", 'wb') as f:
    pickle.dump(results, f)

```

</details>

<details>

<summary>Gather results</summary>

```python
import pickle
from torch.utils.benchmark import Timer, Compare

files = [
        ""PR"",
        ""torch_sparse"",
        ""master""
        ]

timers = []
for name in files:
    with open(""{}_index_select.pickle"".format(name), 'rb') as f:
        timers += pickle.load(f)

compare = Compare(timers)
compare.trim_significant_figures()
compare.print()

```

</details>

<details>

<summary>PR/torch_sparse/master runtime comparison</summary>

```
[----------------------------------- coo.index_select ----------------------------------]
                                                    |    PR   |  torch_sparse  |   master
32 threads: -----------------------------------------------------------------------------
      n=10000, nnz=100, index_len=100, dim=0        |     14  |        140     |       10
      n=10000, nnz=100, index_len=100, dim=1        |     14  |        200     |       10
      n=10000, nnz=100, index_len=1000, dim=0       |     30  |        180     |       38
      n=10000, nnz=100, index_len=1000, dim=1       |     34  |        240     |       38
      n=10000, nnz=100, index_len=10000, dim=0      |    278  |        460     |      330
      n=10000, nnz=100, index_len=10000, dim=1      |    275  |        516     |      330
      n=100000, nnz=1000, index_len=100, dim=0      |     16  |        290     |       31
      n=100000, nnz=1000, index_len=100, dim=1      |     26  |        390     |       31
      n=100000, nnz=1000, index_len=1000, dim=0     |     45  |        405     |      263
      n=100000, nnz=1000, index_len=1000, dim=1     |     73  |        500     |      261
      n=100000, nnz=1000, index_len=10000, dim=0    |    444  |        783     |     2570
      n=100000, nnz=1000, index_len=10000, dim=1    |    470  |        890     |     2590
      n=1000000, nnz=10000, index_len=100, dim=0    |     25  |       2400     |      270
      n=1000000, nnz=10000, index_len=100, dim=1    |    270  |       4000     |      269
      n=1000000, nnz=10000, index_len=1000, dim=0   |     74  |       2600     |     2620
      n=1000000, nnz=10000, index_len=1000, dim=1   |    464  |       3600     |     2640
      n=1000000, nnz=10000, index_len=10000, dim=0  |    635  |       3300     |    26400
      n=1000000, nnz=10000, index_len=10000, dim=1  |   1000  |       3960     |    26400
      n=10, nnz=100, index_len=100, dim=0           |     16  |        137     |       16
      n=10, nnz=100, index_len=100, dim=1           |     16  |        220     |       16
      n=10, nnz=100, index_len=1000, dim=0          |     63  |        238     |       81
      n=10, nnz=100, index_len=1000, dim=1          |     60  |        698     |       78
      n=10, nnz=100, index_len=10000, dim=0         |    480  |        940     |      862
      n=10, nnz=100, index_len=10000, dim=1         |    330  |       4930     |     1070
      n=10, nnz=1000, index_len=100, dim=0          |     60  |        200     |       73
      n=10, nnz=1000, index_len=100, dim=1          |     56  |        683     |       70
      n=10, nnz=1000, index_len=1000, dim=0         |    480  |        530     |     1050
      n=10, nnz=1000, index_len=1000, dim=1         |    330  |       4550     |     1368
      n=10, nnz=1000, index_len=10000, dim=0        |   3100  |       2900     |     9300
      n=10, nnz=1000, index_len=10000, dim=1        |   3400  |      46000     |     9100
      n=10, nnz=10000, index_len=100, dim=0         |    400  |        453     |      857
      n=10, nnz=10000, index_len=100, dim=1         |    400  |       4070     |     1730
      n=10, nnz=10000, index_len=1000, dim=0        |   2840  |       2600     |    13900
      n=10, nnz=10000, index_len=1000, dim=1        |   3700  |      40600     |    16000
      n=10, nnz=10000, index_len=10000, dim=0       |  83200  |      67400     |   160000
      n=10, nnz=10000, index_len=10000, dim=1       |  68000  |     528000     |   190000
      n=100, nnz=1000, index_len=100, dim=0         |     46  |        148     |       31
      n=100, nnz=1000, index_len=100, dim=1         |     45  |        242     |       37
      n=100, nnz=1000, index_len=1000, dim=0        |     68  |        248     |      240
      n=100, nnz=1000, index_len=1000, dim=1        |     66  |        755     |      290
      n=100, nnz=1000, index_len=10000, dim=0       |    370  |        802     |     2250
      n=100, nnz=1000, index_len=10000, dim=1       |    372  |       5430     |     2770
      n=100, nnz=10000, index_len=100, dim=0        |     82  |        210     |      224
      n=100, nnz=10000, index_len=100, dim=1        |     74  |        986     |      270
      n=100, nnz=10000, index_len=1000, dim=0       |    350  |        618     |     2600
      n=100, nnz=10000, index_len=1000, dim=1       |    370  |       4660     |     4560
      n=100, nnz=10000, index_len=10000, dim=0      |   3000  |       3400     |    41680
      n=100, nnz=10000, index_len=10000, dim=1      |   5000  |      47500     |    30400
      n=1000, nnz=10000, index_len=100, dim=0       |     71  |        160     |      185
      n=1000, nnz=10000, index_len=100, dim=1       |     64  |        516     |      190
      n=1000, nnz=10000, index_len=1000, dim=0      |    100  |        249     |     1740
      n=1000, nnz=10000, index_len=1000, dim=1      |     98  |       1030     |     1770
      n=1000, nnz=10000, index_len=10000, dim=0     |    600  |        808     |    18300
      n=1000, nnz=10000, index_len=10000, dim=1     |    663  |       5300     |    18500
      n=1000, nnz=100000, index_len=100, dim=0      |    160  |        258     |     1890
      n=1000, nnz=100000, index_len=100, dim=1      |    200  |       3620     |     2050
      n=1000, nnz=100000, index_len=1000, dim=0     |    500  |        580     |    18700
      n=1000, nnz=100000, index_len=1000, dim=1     |    640  |       7550     |    30000
      n=1000, nnz=100000, index_len=10000, dim=0    |   3400  |       3260     |   186000
      n=1000, nnz=100000, index_len=10000, dim=1    |   3600  |      49600     |   194000
      n=1000, nnz=1000000, index_len=100, dim=0     |    517  |        957     |    18700
      n=1000, nnz=1000000, index_len=100, dim=1     |    680  |      39600     |    37600
      n=1000, nnz=1000000, index_len=1000, dim=0    |   3600  |       4500     |   186000
      n=1000, nnz=1000000, index_len=1000, dim=1    |   5800  |      76400     |   190000
      n=1000, nnz=1000000, index_len=10000, dim=0   |  50000  |      67900     |  1800000
      n=1000, nnz=1000000, index_len=10000, dim=1   |  45000  |     570000     |  1900000

Times are in microseconds (us).

```

</details>
Pull Request resolved: https://github.com/pytorch/pytorch/pull/72710
Approved by: https://github.com/pearu, https://github.com/cpuhrsch"
pytorch/pytorch,e5915a2216972592ab640dcc0635f0bc7ea8c061,"[PyTorch] Don't enter MHA fast path when bias & query dtypes don't match

Pull Request resolved: https://github.com/pytorch/pytorch/pull/76879

The fast path does not support this: transform_bias_rescale_qkv will try to grab bias.data_ptr() assuming the dtypes are the same. (Also, I have no idea how this happens.)

Differential Revision: [D36156872](https://our.internmc.facebook.com/intern/diff/D36156872/)

Approved by: https://github.com/cpuhrsch"
pytorch/pytorch,621ff0f9735cd8c4c5d6becb291ad050c35e01c0,"Add linalg.vander

This PR adds `linalg.vander`, the linalg version of `torch.vander`.

We add autograd support and support for batched inputs.

We also take this chance to improve the docs (TODO: Check that they
render correctly!) and add an OpInfo.

**Discussion**: The current default for the `increasing` kwargs is extremely
odd as it is the opposite of the classical definition (see
[wiki](https://en.wikipedia.org/wiki/Vandermonde_matrix)). This is
reflected in the docs, where I explicit both the odd defaults that we
use and the classical definition. See also [this stackoverflow
post](https://stackoverflow.com/a/71758047/5280578), which shows how
people are confused by this defaults.

My take on this would be to correct the default to be `increasing=True`
and document the divergence with NumPy (as we do for other `linalg`
functions) as:

- It is what people expect
- It gives the correct determinant called ""the Vandermonde determinant"" rather than (-1)^{n-1} times the Vandermonde det (ugh).
- [Minor] It is more efficient (no `flip` needed)
- Since it's under `linalg.vander`, it's strictly not a drop-in replacement for `np.vander`.

We will deprecate `torch.vander` in a PR after this one in this stack
(once we settle on what's the correct default).

Thoughts? mruberry

cc kgryte rgommers as they might have some context for the defaults of
NumPy.

Fixes https://github.com/pytorch/pytorch/issues/60197

Pull Request resolved: https://github.com/pytorch/pytorch/pull/76303

Approved by: https://github.com/albanD, https://github.com/mruberry"
pytorch/pytorch,849984a2cd174ae23a346b3d48b4308596a56873,"[SR] Sigmoid out variant calls fast_sigmoid (#75661)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/75661

`fast_sigmoid` is a variant of sigmoid in NNC that is implemented in terms of `fast_tanh` (which is a fast rational function approximation).
ghstack-source-id: 155604086

Reviewed By: navahgar, hlu1

Differential Revision: D35481390

fbshipit-source-id: 1d64b5c375539f3b2461a1f3d9b86cd696eae7a1
(cherry picked from commit 8106c2512b8d7b373cb6545a43c3e8fc04805c4b)"
pytorch/pytorch,52af4fc5baee363bb9c3170017d46d4fe0069c56,"[PyTorch] Make RecordFunction store inputs as ArrayRef (#72484)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/72484

Stepping stone toward stack-allocating array of inputs.

Funnily enough, this seems to improve performance too.
ghstack-source-id: 155492056

Test Plan:
1) CI
2) framework overhead benchmark with --stressTestRecordFunction --captureRecordFunctionInputs goes from 0.76 usec/iter to 0.72.

Reviewed By: chaekit, robieta

Differential Revision: D34061169

fbshipit-source-id: 073fedf1d3d162f927c4e9867cfda7dbfabba215
(cherry picked from commit dae77cf1cd8813d902d73999ad97133a3ef8e291)"
pytorch/pytorch,4ee29d6033f6ff4e1b75caa265d2646648b6636c,"[Reland take-2] Add JIT graph fuser for oneDNN Graph API (v0.5)

Re-landing #68111/#74596

## Description
v0.5 PR of this [RFC](https://github.com/pytorch/pytorch/issues/49444).

On the basis of #50256, the below improvements are included:

 * The [v0.5 release branch](https://github.com/oneapi-src/oneDNN/releases/tag/graph-v0.5) of the oneDNN Graph API is used
 * The fuser now works with the profiling graph executor. We have inserted type check nodes to guard the profiled tensor properties.

 ### User API:
The optimization pass is disabled by default. Users could enable it by:

```
 torch.jit.enable_onednn_fusion(True)
```
`torch.jit.freeze` should be used after tracing (recommended) or scripting a model.

 ### Performance:
 [pytorch/benchmark](https://github.com/pytorch/benchmark) tool is used to compare the performance:

 * SkyLake 8180 (1 socket of 28 cores):
   ![image](https://user-images.githubusercontent.com/65992142/151162305-05e44425-a24e-4d5e-94e1-743b40b87a8c.png)
* SkyLake 8180 (single thread):
   ![image](https://user-images.githubusercontent.com/65992142/151162528-69f90b79-d08d-46b8-8775-d80a6ccbce8a.png)
   * By mapping hardswish to oneDNN Graph, it’s 8% faster than PyTorch JIT (NNC + OFI)
   ** We expect performance gain after mapping transpose, contiguous & view to oneDNN graph ops

 ### Directory structure of the integration code
 Fuser-related code is placed under:

 ```
 torch/csrc/jit/codegen/onednn/
 ```

 Optimization pass registration is done in:

 ```
 torch/csrc/jit/passes/onednn_graph_fuser.h
 ```

 CMake for the integration code is in:

 ```
 caffe2/CMakeLists.txt
 cmake/public/mkldnn.cmake
 cmake/Modules/FindMKLDNN.cmake
 ```

 ## Limitations
 * In this PR, we only support Pytorch-oneDNN-Graph integration on Linux platform. Support on Windows and MacOS will be enabled as a next step.
 * We have only optimized the inference use-case.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/76622
Approved by: https://github.com/eellison"
pytorch/pytorch,33fabe9a2ebceb27e42cb089e025342676df3452,"`functional.max_unpool`: OpInfo tests + simpler backward + forward ad + fwad over backward ad

Resolves https://github.com/pytorch/pytorch/issues/67657, https://github.com/pytorch/pytorch/issues/67658, https://github.com/pytorch/pytorch/issues/67660.

These are not necessarily bugs because we cannot produce arbitrary samples coming from `max_pool` to the gradcheck's eternal satisfaction.

This PR also replaces low-level complicated backward kernels with much simpler high-level and well-tested counterparts. The replacement is also faster (before: parallel for loop, after: memory layout optimized TensorIterator's parallelization coming from `gather`).

cc @albanD @mruberry @jbschlosser @walterddr
Pull Request resolved: https://github.com/pytorch/pytorch/pull/68625
Approved by: https://github.com/albanD"
pytorch/pytorch,7cb7cd580247bce5ad477800e14f981ea26668fc,"Add linalg.lu

This PR modifies `lu_unpack` by:
- Using less memory when unpacking `L` and `U`
- Fuse the subtraction by `-1` with `unpack_pivots_stub`
- Define tensors of the correct types to avoid copies
- Port `lu_unpack` to be a strucutred kernel so that its `_out` version
does not incur on extra copies

Then we implement `linalg.lu` as a structured kernel, as we want to
compute its derivative manually. We do so because composing the
derivatives of `torch.lu_factor` and `torch.lu_unpack` would be less efficient.

This new function and `lu_unpack` comes with all the things it can come:
forward and backward ad, decent docs, correctness tests, OpInfo, complex support,
support for metatensors and support for vmap and vmap over the gradients.

I really hope we don't continue adding more features.

This PR also avoids saving some of the tensors that were previously
saved unnecessarily for the backward in `lu_factor_ex_backward` and
`lu_backward` and does some other general improvements here and there
to the forward and backward AD formulae of other related functions.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/67833

Approved by: https://github.com/IvanYashchuk, https://github.com/nikitaved, https://github.com/mruberry"
pytorch/pytorch,1a4eea57be447970445f207aaf649f665748fc88,"Improve derivative of QR decomposition

We derive and implement a more concise rule for the forward and backward
derivatives of the QR decomposition. While doing this we:
- Fix the composite compliance of `linalg.qr` and we make it support batches
- Improve the performance and simplify the implementation of both foward and backward
- Avoid saving the input matrix for the backward computation.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/76115

Approved by: https://github.com/nikitaved, https://github.com/albanD"
pytorch/pytorch,da15d76e8314c854f82847ef9fdf906a100f7741,"Revert ""Add ZT fastpath for remainder.Tensor JVP""

This reverts commit 7471f614ae00e31ff9cd47ac0b2d98702c905c76.

Reverted https://github.com/pytorch/pytorch/pull/76479 on behalf of https://github.com/anjali411"
pytorch/pytorch,7471f614ae00e31ff9cd47ac0b2d98702c905c76,"Add ZT fastpath for remainder.Tensor JVP

See #69687.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/76479
Approved by: https://github.com/anjali411"
pytorch/pytorch,4baf7c0899a2fa9c3630613f37d5fc65971db21c,"Dispatch to mv rather than mm in the case that tensor1.ndim == 1 and tensor2.ndim == 2

This should hopefully be faster, it makes the calling code simpler, and
it solves a bug when using matmul with the out= parameter (before it
would throw an incorrect error).

Pull Request resolved: https://github.com/pytorch/pytorch/pull/75195

Approved by: https://github.com/ezyang"
pytorch/pytorch,1fed6b75594894a27f5781e59e8edce2cf133714,"[SR] Eliminate extra permutes around softmax calls (#76391)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/76391

I've seen this pattern in many important internal models:

```
x = torch.permute(a, [0, 2, 1])
y = torch.softmax(x, 2)
z = torch.permute(y, [0, 2, 1])
```

This is equivalent to
```
z = torch.softmax(x, 1)
```

The `permute` ops can degrade performance, especially if copy variants are on. Add another pattern to our `EliminateExtraPermuteOpsPass` to handle this.
ghstack-source-id: 155466506

Test Plan: New unit tests

Reviewed By: navahgar, huiguoo

Differential Revision: D35938289

fbshipit-source-id: 398b5528077b0b3f1c6fc5544e483803e96d68e9
(cherry picked from commit d742abd094d1fef23ca6a34703d97a6da2d14bd1)"
pytorch/pytorch,ff94c9dee42679fd24f571c8aa3c0f257a14a9a6,"DOC fix momentum equation for nesterov

Fix https://github.com/pytorch/pytorch/issues/72395

This is a small fix in the doc for an indice in this equation:

![image](https://user-images.githubusercontent.com/3321081/166165461-140855b5-96b5-4417-85fc-2a170f95700a.png)

I think teh indice should not be `t-1` but `t`. This is coherent with [the implementation)[https://github.com/pytorch/pytorch/blob/master/torch/optim/sgd.py#L236] and with what is done for instance in [keras](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/SGD).
Pull Request resolved: https://github.com/pytorch/pytorch/pull/76639
Approved by: https://github.com/albanD"
pytorch/pytorch,68e012b0234c686b76d387ea2e4642237517567e,"Optimize half conversion for SYCL kernel

## Motivation:
Add support for SYCL half implicit/explicit conversion in SYCL kernels.

## Additional Context:
Macro `SYCL_LANGUAGE_VERSION` is suggested by SYCL compiler to instead of `__SYCL_DEVICE_ONLY__` in current version unless device and host specific implementation of the same function is necessary.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/76515
Approved by: https://github.com/ezyang"
pytorch/pytorch,56ea57de611683feee251dc1aeacfe1e04dc909e,"shard `pull / linux-xenial-cuda11.3-py3.7-gcc7 / test (distributed` 1->2

Fixes #ISSUE_NUMBER

shard `pull / linux-xenial-cuda11.3-py3.7-gcc7 / test (distributed ...` from 1 shard to 2

Pros:
- It currently takes about 2.6 hours and is 3rd longest running job on pull
- Theoretically minimal overhead

Cons:
- Requires changes to the run_test.py which might have correctness issues

Notes:
- Cannot shard further as one of the test files is responsible for about half of the total run time

spreadsheet regarding sharding: https://docs.google.com/spreadsheets/d/1BdtVsjRr0Is9LXMNilR02FEdPXNq7zEWl8AmR3ArsLQ/edit#gid=1153012347

Test Plan:
<details><summary>expand to see test plan (its long)</summary>

tests from a commit ran on master (90 tests ran)
```
2022-05-03T12:45:34.7974184Z Selected tests:
2022-05-03T12:45:34.7974495Z  distributed/_shard/sharded_optim/test_sharded_optim
2022-05-03T12:45:34.7974839Z  distributed/_shard/sharded_tensor/ops/test_binary_cmp
2022-05-03T12:45:34.7975209Z  distributed/_shard/sharded_tensor/ops/test_elementwise_ops
2022-05-03T12:45:34.7975575Z  distributed/_shard/sharded_tensor/ops/test_embedding
2022-05-03T12:45:34.7976180Z  distributed/_shard/sharded_tensor/ops/test_embedding_bag
2022-05-03T12:45:34.7976802Z  distributed/_shard/sharded_tensor/ops/test_init
2022-05-03T12:45:34.7977361Z  distributed/_shard/sharded_tensor/ops/test_linear
2022-05-03T12:45:34.7978157Z  distributed/_shard/sharded_tensor/ops/test_math_ops
2022-05-03T12:45:34.7978879Z  distributed/_shard/sharded_tensor/test_megatron_prototype
2022-05-03T12:45:34.7979594Z  distributed/_shard/sharded_tensor/test_sharded_tensor
2022-05-03T12:45:34.7980366Z  distributed/_shard/sharded_tensor/test_sharded_tensor_reshard
2022-05-03T12:45:34.7981066Z  distributed/_shard/sharding_plan/test_sharding_plan
2022-05-03T12:45:34.7981877Z  distributed/_shard/sharding_spec/test_sharding_spec
2022-05-03T12:45:34.7982387Z  distributed/_shard/test_partial_tensor
2022-05-03T12:45:34.7982691Z  distributed/_shard/test_replicated_tensor
2022-05-03T12:45:34.7982994Z  distributed/_shard/test_sharder
2022-05-03T12:45:34.7983280Z  distributed/algorithms/test_join
2022-05-03T12:45:34.7983695Z  distributed/elastic/events/lib_test
2022-05-03T12:45:34.7983984Z  distributed/elastic/metrics/api_test
2022-05-03T12:45:34.7984308Z  distributed/elastic/multiprocessing/api_test
2022-05-03T12:45:34.7984624Z  distributed/elastic/timer/api_test
2022-05-03T12:45:34.7984924Z  distributed/elastic/timer/local_timer_example
2022-05-03T12:45:34.7985254Z  distributed/elastic/timer/local_timer_test
2022-05-03T12:45:34.7985575Z  distributed/elastic/utils/distributed_test
2022-05-03T12:45:34.7985889Z  distributed/elastic/utils/logging_test
2022-05-03T12:45:34.7986176Z  distributed/elastic/utils/util_test
2022-05-03T12:45:34.7986492Z  distributed/fsdp/test_flatten_params_wrapper
2022-05-03T12:45:34.7986799Z  distributed/fsdp/test_fsdp_apply
2022-05-03T12:45:34.7987078Z  distributed/fsdp/test_fsdp_checkpoint
2022-05-03T12:45:34.7987388Z  distributed/fsdp/test_fsdp_clip_grad_norm
2022-05-03T12:45:34.7987691Z  distributed/fsdp/test_fsdp_comm
2022-05-03T12:45:34.7987961Z  distributed/fsdp/test_fsdp_core
2022-05-03T12:45:34.7988251Z  distributed/fsdp/test_fsdp_exec_order
2022-05-03T12:45:34.7988570Z  distributed/fsdp/test_fsdp_freezing_weights
2022-05-03T12:45:34.7988865Z  distributed/fsdp/test_fsdp_grad_acc
2022-05-03T12:45:34.7989176Z  distributed/fsdp/test_fsdp_ignored_modules
2022-05-03T12:45:34.7989478Z  distributed/fsdp/test_fsdp_input
2022-05-03T12:45:34.7989950Z  distributed/fsdp/test_fsdp_memory
2022-05-03T12:45:34.7990241Z  distributed/fsdp/test_fsdp_meta
2022-05-03T12:45:34.7990640Z  distributed/fsdp/test_fsdp_mixed_precision
2022-05-03T12:45:34.7990964Z  distributed/fsdp/test_fsdp_multiple_forward
2022-05-03T12:45:34.7991293Z  distributed/fsdp/test_fsdp_multiple_wrapping
2022-05-03T12:45:34.7991610Z  distributed/fsdp/test_fsdp_optim_state
2022-05-03T12:45:34.7991895Z  distributed/fsdp/test_fsdp_overlap
2022-05-03T12:45:34.7992195Z  distributed/fsdp/test_fsdp_pure_fp16
2022-05-03T12:45:34.7992500Z  distributed/fsdp/test_fsdp_state_dict
2022-05-03T12:45:34.7992818Z  distributed/fsdp/test_fsdp_summon_full_params
2022-05-03T12:45:34.7993117Z  distributed/fsdp/test_fsdp_traversal
2022-05-03T12:45:34.7993861Z  distributed/fsdp/test_fsdp_uneven
2022-05-03T12:45:34.7994181Z  distributed/fsdp/test_shard_utils
2022-05-03T12:45:34.7994447Z  distributed/fsdp/test_utils
2022-05-03T12:45:34.7994721Z  distributed/fsdp/test_wrap
2022-05-03T12:45:34.7995015Z  distributed/nn/jit/test_instantiator
2022-05-03T12:45:34.7995328Z  distributed/optim/test_zero_redundancy_optimizer
2022-05-03T12:45:34.7995664Z  distributed/pipeline/sync/skip/test_api
2022-05-03T12:45:34.7995983Z  distributed/pipeline/sync/skip/test_gpipe
2022-05-03T12:45:34.7996315Z  distributed/pipeline/sync/skip/test_inspect_skip_layout
2022-05-03T12:45:34.7996652Z  distributed/pipeline/sync/skip/test_leak
2022-05-03T12:45:34.7996977Z  distributed/pipeline/sync/skip/test_portal
2022-05-03T12:45:34.7997292Z  distributed/pipeline/sync/skip/test_stash_pop
2022-05-03T12:45:34.7997623Z  distributed/pipeline/sync/skip/test_tracker
2022-05-03T12:45:34.7997968Z  distributed/pipeline/sync/skip/test_verify_skippables
2022-05-03T12:45:34.7998301Z  distributed/pipeline/sync/test_balance
2022-05-03T12:45:34.7998591Z  distributed/pipeline/sync/test_bugs
2022-05-03T12:45:34.7998927Z  distributed/pipeline/sync/test_checkpoint
2022-05-03T12:45:34.7999243Z  distributed/pipeline/sync/test_copy
2022-05-03T12:45:34.7999557Z  distributed/pipeline/sync/test_deferred_batch_norm
2022-05-03T12:45:34.7999896Z  distributed/pipeline/sync/test_dependency
2022-05-03T12:45:34.8000215Z  distributed/pipeline/sync/test_inplace
2022-05-03T12:45:34.8000516Z  distributed/pipeline/sync/test_microbatch
2022-05-03T12:45:34.8000826Z  distributed/pipeline/sync/test_phony
2022-05-03T12:45:34.8001130Z  distributed/pipeline/sync/test_pipe
2022-05-03T12:45:34.8001424Z  distributed/pipeline/sync/test_pipeline
2022-05-03T12:45:34.8001733Z  distributed/pipeline/sync/test_stream
2022-05-03T12:45:34.8002055Z  distributed/pipeline/sync/test_transparency
2022-05-03T12:45:34.8002353Z  distributed/pipeline/sync/test_worker
2022-05-03T12:45:34.8002672Z  distributed/rpc/cuda/test_tensorpipe_agent
2022-05-03T12:45:34.8002982Z  distributed/rpc/test_faulty_agent
2022-05-03T12:45:34.8003270Z  distributed/rpc/test_tensorpipe_agent
2022-05-03T12:45:34.8003568Z  distributed/test_c10d_common
2022-05-03T12:45:34.8003839Z  distributed/test_c10d_gloo
2022-05-03T12:45:34.8004088Z  distributed/test_c10d_nccl
2022-05-03T12:45:34.8004369Z  distributed/test_c10d_spawn_gloo
2022-05-03T12:45:34.8004656Z  distributed/test_c10d_spawn_nccl
2022-05-03T12:45:34.8004938Z  distributed/test_data_parallel
2022-05-03T12:45:34.8005212Z  distributed/test_distributed_spawn
2022-05-03T12:45:34.8005496Z  distributed/test_launcher
2022-05-03T12:45:34.8005767Z  distributed/test_nccl
2022-05-03T12:45:34.8006019Z  distributed/test_pg_wrapper
2022-05-03T12:45:34.8006285Z  distributed/test_store
```

tests ran on first shard for distributed on this PR (34 tests)
```
2022-05-02T21:26:00.1385256Z Selected tests:
2022-05-02T21:26:00.1385767Z  distributed/test_distributed_spawn
2022-05-02T21:26:00.1386403Z  distributed/elastic/multiprocessing/api_test
2022-05-02T21:26:00.1387051Z  distributed/fsdp/test_fsdp_memory
2022-05-02T21:26:00.1387607Z  distributed/fsdp/test_fsdp_ignored_modules
2022-05-02T21:26:00.1388179Z  distributed/fsdp/test_fsdp_apply
2022-05-02T21:26:00.1388600Z  distributed/_shard/sharded_tensor/ops/test_binary_cmp
2022-05-02T21:26:00.1389181Z  distributed/_shard/sharding_spec/test_sharding_spec
2022-05-02T21:26:00.1389545Z  distributed/_shard/sharded_tensor/ops/test_linear
2022-05-02T21:26:00.1389878Z  distributed/fsdp/test_fsdp_uneven
2022-05-02T21:26:00.1390186Z  distributed/fsdp/test_fsdp_multiple_wrapping
2022-05-02T21:26:00.1390526Z  distributed/fsdp/test_fsdp_multiple_forward
2022-05-02T21:26:00.1390877Z  distributed/_shard/sharded_tensor/ops/test_embedding
2022-05-02T21:26:00.1391219Z  distributed/_shard/test_partial_tensor
2022-05-02T21:26:00.1391542Z  distributed/_shard/sharded_optim/test_sharded_optim
2022-05-02T21:26:00.1391915Z  distributed/_shard/sharded_tensor/ops/test_elementwise_ops
2022-05-02T21:26:00.1392297Z  distributed/fsdp/test_flatten_params_wrapper
2022-05-02T21:26:00.1392585Z  distributed/fsdp/test_utils
2022-05-02T21:26:00.1392883Z  distributed/nn/jit/test_instantiator
2022-05-02T21:26:00.1393167Z  distributed/test_nccl
2022-05-02T21:26:00.1393466Z  distributed/_shard/sharding_plan/test_sharding_plan
2022-05-02T21:26:00.1393787Z  distributed/_shard/test_sharder
2022-05-02T21:26:00.1394085Z  distributed/elastic/timer/api_test
2022-05-02T21:26:00.1394383Z  distributed/pipeline/sync/skip/test_api
2022-05-02T21:26:00.1394738Z  distributed/pipeline/sync/skip/test_inspect_skip_layout
2022-05-02T21:26:00.1395090Z  distributed/pipeline/sync/skip/test_portal
2022-05-02T21:26:00.1395424Z  distributed/pipeline/sync/skip/test_tracker
2022-05-02T21:26:00.1395935Z  distributed/pipeline/sync/test_balance
2022-05-02T21:26:00.1396288Z  distributed/pipeline/sync/test_checkpoint
2022-05-02T21:26:00.1396635Z  distributed/pipeline/sync/test_deferred_batch_norm
2022-05-02T21:26:00.1396953Z  distributed/pipeline/sync/test_inplace
2022-05-02T21:26:00.1397269Z  distributed/pipeline/sync/test_phony
2022-05-02T21:26:00.1397587Z  distributed/pipeline/sync/test_pipeline
2022-05-02T21:26:00.1397903Z  distributed/pipeline/sync/test_transparency
2022-05-02T21:26:00.1398221Z  distributed/rpc/test_faulty_agent
```

tests ran on second shard for distributed on this PR (56 tests)
```
2022-05-02T21:26:55.1342892Z Selected tests:
2022-05-02T21:26:55.1343201Z  distributed/rpc/cuda/test_tensorpipe_agent
2022-05-02T21:26:55.1343526Z  distributed/fsdp/test_fsdp_core
2022-05-02T21:26:55.1343829Z  distributed/test_c10d_nccl
2022-05-02T21:26:55.1344089Z  distributed/test_c10d_gloo
2022-05-02T21:26:55.1344408Z  distributed/fsdp/test_fsdp_summon_full_params
2022-05-02T21:26:55.1344749Z  distributed/fsdp/test_fsdp_mixed_precision
2022-05-02T21:26:55.1345085Z  distributed/optim/test_zero_redundancy_optimizer
2022-05-02T21:26:55.1345423Z  distributed/fsdp/test_fsdp_optim_state
2022-05-02T21:26:55.1345773Z  distributed/_shard/sharded_tensor/test_sharded_tensor
2022-05-02T21:26:55.1346088Z  distributed/fsdp/test_fsdp_state_dict
2022-05-02T21:26:55.1346379Z  distributed/test_store
2022-05-02T21:26:55.1346661Z  distributed/test_c10d_spawn_gloo
2022-05-02T21:26:55.1346966Z  distributed/test_pg_wrapper
2022-05-02T21:26:55.1347252Z  distributed/test_c10d_spawn_nccl
2022-05-02T21:26:55.1347565Z  distributed/fsdp/test_fsdp_clip_grad_norm
2022-05-02T21:26:55.1347871Z  distributed/fsdp/test_wrap
2022-05-02T21:26:55.1348369Z  distributed/fsdp/test_fsdp_grad_acc
2022-05-02T21:26:55.1348679Z  distributed/algorithms/test_join
2022-05-02T21:26:55.1349004Z  distributed/fsdp/test_fsdp_freezing_weights
2022-05-02T21:26:55.1349305Z  distributed/fsdp/test_fsdp_comm
2022-05-02T21:26:55.1349593Z  distributed/test_c10d_common
2022-05-02T21:26:55.1349885Z  distributed/fsdp/test_fsdp_meta
2022-05-02T21:26:55.1350171Z  distributed/fsdp/test_fsdp_exec_order
2022-05-02T21:26:55.1350486Z  distributed/fsdp/test_fsdp_checkpoint
2022-05-02T21:26:55.1350798Z  distributed/fsdp/test_fsdp_overlap
2022-05-02T21:26:55.1351105Z  distributed/elastic/timer/local_timer_example
2022-05-02T21:26:55.1351423Z  distributed/fsdp/test_fsdp_input
2022-05-02T21:26:55.1351749Z  distributed/_shard/sharded_tensor/ops/test_init
2022-05-02T21:26:55.1352190Z  distributed/elastic/timer/local_timer_test
2022-05-02T21:26:55.1352520Z  distributed/elastic/utils/distributed_test
2022-05-02T21:26:55.1352841Z  distributed/fsdp/test_fsdp_pure_fp16
2022-05-02T21:26:55.1353150Z  distributed/test_data_parallel
2022-05-02T21:26:55.1353437Z  distributed/fsdp/test_fsdp_traversal
2022-05-02T21:26:55.1353792Z  distributed/_shard/sharded_tensor/test_sharded_tensor_reshard
2022-05-02T21:26:55.1354174Z  distributed/_shard/sharded_tensor/ops/test_embedding_bag
2022-05-02T21:26:55.1354534Z  distributed/_shard/sharded_tensor/test_megatron_prototype
2022-05-02T21:26:55.1354858Z  distributed/test_launcher
2022-05-02T21:26:55.1355149Z  distributed/elastic/utils/util_test
2022-05-02T21:26:55.1355441Z  distributed/elastic/utils/logging_test
2022-05-02T21:26:55.1355755Z  distributed/elastic/metrics/api_test
2022-05-02T21:26:55.1356095Z  distributed/_shard/sharded_tensor/ops/test_math_ops
2022-05-02T21:26:55.1356455Z  distributed/_shard/test_replicated_tensor
2022-05-02T21:26:55.1356754Z  distributed/elastic/events/lib_test
2022-05-02T21:26:55.1357065Z  distributed/fsdp/test_shard_utils
2022-05-02T21:26:55.1357387Z  distributed/pipeline/sync/skip/test_gpipe
2022-05-02T21:26:55.1357702Z  distributed/pipeline/sync/skip/test_leak
2022-05-02T21:26:55.1358040Z  distributed/pipeline/sync/skip/test_stash_pop
2022-05-02T21:26:55.1358396Z  distributed/pipeline/sync/skip/test_verify_skippables
2022-05-02T21:26:55.1358716Z  distributed/pipeline/sync/test_bugs
2022-05-02T21:26:55.1359027Z  distributed/pipeline/sync/test_copy
2022-05-02T21:26:55.1359350Z  distributed/pipeline/sync/test_dependency
2022-05-02T21:26:55.1359662Z  distributed/pipeline/sync/test_microbatch
2022-05-02T21:26:55.1359983Z  distributed/pipeline/sync/test_pipe
2022-05-02T21:26:55.1360299Z  distributed/pipeline/sync/test_stream
2022-05-02T21:26:55.1360593Z  distributed/pipeline/sync/test_worker
2022-05-02T21:26:55.1360912Z  distributed/rpc/test_tensorpipe_agent
```
</details>
Pull Request resolved: https://github.com/pytorch/pytorch/pull/76564
Approved by: https://github.com/jeffdaily, https://github.com/janeyx99"
pytorch/pytorch,4fbbbed674437f01c2ffd4c13ea96f9815b735ae,"Support no sharding config

supporting no sharding config to make it similar to DDP algorithm

Benchmarked this version of ddp vs PyTorch C++ ddp, the perf gap is small and depends on the wrapping strategy. For 1GB bert model on 32 gpus with slow network connected.

if the bucket size is small like 25MB for PT DDP and wrapping min size is like 28MB for this version of ddp, this version of ddp has around 6% perf regression. The difference is because this version of ddp wrapping needs one more all reduce, not due to the python context switch;
if the bucket size is large like 40MB for PT DDP and wrapping min size is like 40MB for this version of ddp, this version of ddp has around 17% perf regression. because the last FSDP unit in this version of DDP wrapping has large delay to kick off the first all reduce; The difference is not due to the python context switch
if the bucket size larger than the model size, both PT DDP and this version of DDP will have single all reduce, they have the similar performance. That means python context switch is not a big concern again.
Overall, I think if the wrapping can be done well in this version of DDP and aligned with PT DDP bucketing orders, then the performance will be comparable. As you can see, we still need to improve our auto wrapping policy for both no_shard and fsdp strategy.

Once the auto wrapping issues is resolved for this API overall, it is promising to make this API back compatible with PT C++ DDP and merge them in the long run.

Also this API provides flexibility to mix DDP with other data parallelisms for a module, e.g FSDP(FSDP(submodule1, sharding_strategy=FULL_SHARDED), FSDP(submodule2, sharding_strategy=NO_SHARD), FSDP(submodule3, sharding_strategy=HSDP), submodule4, sharding_strategy=NO_SHARD)
Pull Request resolved: https://github.com/pytorch/pytorch/pull/76736
Approved by: https://github.com/rohan-varma"
pytorch/pytorch,aca55948187ed01d18342d20aaaee45a54b09547,"Turn on memory efficient format for jit pickle files.

Summary:
This enables previous change made at D35196883 (https://github.com/pytorch/pytorch/commit/b34b192d6b97325c9f78e5995c48c8498ede34bd)
Previous change is landed for 2 weeks to make sure that the format change introduced here will be handed in code.

Test Plan: existing tests

Differential Revision: D36074453

Pull Request resolved: https://github.com/pytorch/pytorch/pull/76688
Approved by: https://github.com/gmagogsfm"
pytorch/pytorch,76abbbe3179a83b05629ac3010817c39ad102242,"Adding output_size to to_padded_tensor (#76640)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/76640

- Adding output_size argument to to_padded_tensor
- Modified add_padding_kernelLauncher and kernels to iterate over padded tensor batch size instead of nested tensor batch size
- No fast path for CPU version

Test Plan:
buck test mode/dev-nosan  //caffe2/test:nested

Performance test using N1763981:

{F728168808}

Reviewed By: cpuhrsch

Differential Revision: D36056902

fbshipit-source-id: d6df2939d6649128a7f43a2ef32d227870a8e583
(cherry picked from commit 09465f36f09d4d74c9b3303981d8cce0c7c1092a)"
pytorch/pytorch,ccd5fa506fa1173380aa26610c6138ab6e0144a0,"[iOS][coreml] Add CoreML memory observer Round 2

Summary:
Add an observer to `PTMCoreMLExecutor` so we can inspect OOMs in production to help with T115554493.

The behaviour of the logger is as such:

1. Each time a model is compiled, there is a chance we publish all logs to QPL. This is determined by the randomly generated `_model_load_id` and `_sample_thresh`.
2. If we are publishing all logs, then every `_sample_every` inferences will be logged via QPL.
3. Every QPL log will collect memory metrics before and after model compilation/inference
4. If memory pressure is not normal (remaining mem < 400 MB) before or after compilation/inference, then that compilation/inference will be logged to QPL no matter what.

Previous diff got reverted due to OSS CI failures. Fixed those failures in this iteration.

Test Plan:
We can test in pytorch playground and inspect the QPL logs through Flipper:

```
arc focus2 -b pp-ios -a ModelRunner -a //xplat/caffe2/c10:c10Apple -a //xplat/caffe2:torch_mobile_coreApple  -a //xplat/caffe2/fb/dynamic_pytorch:dynamic_pytorch_implApple -a //xplat/caffe2:coreml_delegateApple  -a ModelRunnerDevOps -a //xplat/caffe2:torch_mobile_all_opsApple -a coreml_memory_observer -a //xplat/perflogger:perfloggerApple -fd --force-with-wrong-xcode
```

To check results in Hive/Scuba, test in instagram:

```
arc focus2 -b igios-no-extensions -a //fbobjc/Apps/Instagram/AppLibraries/Core/QPL/IGPerformanceLogging:IGPerformanceLogging -a //xplat/caffe2/c10:c10Apple -a //xplat/caffe2:torch_mobile_coreApple  -a //xplat/caffe2/fb/dynamic_pytorch:dynamic_pytorch_implApple -a //xplat/caffe2:coreml_delegateApple -a //xplat/caffe2:torch_mobile_all_opsApple -a //xplat/perflogger:perfloggerApple -a coreml_memory_observerApple -c pt.enable_qpl=1 --force-with-wrong-xcode
```

Note that we need to change `_sample_thresh` to ensure logs show up.

Reviewed By: kimishpatel

Differential Revision: D35970823

fbshipit-source-id: 2cb4d73931f35a0fa7f362b9fb44b9f0a00aeb82
(cherry picked from commit 1e965acf72958fc59d0cd0b4958e54955cc1adf2)"
pytorch/pytorch,a4c14caffcce3170fe048f73a02ef444a153d129,"[FSDP] Faster dict inversion

Pull Request resolved: https://github.com/pytorch/pytorch/pull/76665

Approved by: https://github.com/zhaojuanmao"
pytorch/pytorch,58d773ad291fb4723b34e1fa420b81531e3f4512,"Upgrade oneDNN to v2.6.0 (#75398)

Summary:
This PR upgrades oneDNN to v2.6.0.

v2.6.0 changes:

- Improved performance for future Intel Xeon® Scalable processors (code name Sapphire Rapids). The functionality requires Linux kernel 5.16 or later.
- Improved performance of matmul primitive for processors with Intel AVX-512 support.
- Introduced bfloat16 destination support for int8 convolution, matmul and inner product primitives for processors with Intel AVX-512 support and or future Intel Xeon® Scalable processors (code name Sapphire Rapids)
- Extended RNN primitive with support for [AUGRU cell](https://oneapi-src.github.io/oneDNN/dev_guide_rnn.html#augru).
- Added support for non-zero negative slope in ReLU post-op for batch normalization primitive.
- Introduced support for mixed source and destination data types in softmax primitive.
- Introduced [persistent cache API](https://oneapi-src.github.io/oneDNN/dev_guide_persistent_cache.html). This functionality allows to serialize and reuse JIT kernels.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/75398

Reviewed By: dagitses, bigfootjon

Differential Revision: D35976827

Pulled By: frank-wei

fbshipit-source-id: a77ae15cd77fc7c114ab9722453c28dc64aac679
(cherry picked from commit e376698d3c772aaa2dfbe51a4d1a75c8d17d0eee)"
pytorch/pytorch,b182c22e152bdf045d938d06d9154b357a3417ff,"[PyTorch] Exercise MHA fast path in JIT

Tests previously did not exercise this; now they do.

Differential Revision: [D35945821](https://our.internmc.facebook.com/intern/diff/D35945821/)

Pull Request resolved: https://github.com/pytorch/pytorch/pull/76416

Approved by: https://github.com/ezyang"
pytorch/pytorch,1ea49c68d05a1eb1fe81d6d7e3c49dc3e2e2caa1,"Add linalg.vander

This PR adds `linalg.vander`, the linalg version of `torch.vander`.

We add autograd support and support for batched inputs.

We also take this chance to improve the docs (TODO: Check that they
render correctly!) and add an OpInfo.

**Discussion**: The current default for the `increasing` kwargs is extremely
odd as it is the opposite of the classical definition (see
[wiki](https://en.wikipedia.org/wiki/Vandermonde_matrix)). This is
reflected in the docs, where I explicit both the odd defaults that we
use and the classical definition. See also [this stackoverflow
post](https://stackoverflow.com/a/71758047/5280578), which shows how
people are confused by this defaults.

My take on this would be to correct the default to be `increasing=True`
and document the divergence with NumPy (as we do for other `linalg`
functions) as:

- It is what people expect
- It gives the correct determinant called ""the Vandermonde determinant"" rather than (-1)^{n-1} times the Vandermonde det (ugh).
- [Minor] It is more efficient (no `flip` needed)
- Since it's under `linalg.vander`, it's strictly not a drop-in replacement for `np.vander`.

We will deprecate `torch.vander` in a PR after this one in this stack
(once we settle on what's the correct default).

Thoughts? mruberry

cc kgryte rgommers as they might have some context for the defaults of
NumPy.

Fixes https://github.com/pytorch/pytorch/issues/60197

Pull Request resolved: https://github.com/pytorch/pytorch/pull/76303

Approved by: https://github.com/albanD"
pytorch/pytorch,10bf20da8b44cd62c3c9ea228b0fc993a2e9fa32,"Change mixed precision API

Change the API such that if a precision type is not explicitly speciifed in `MixedPrecision` config, we don't cast it at all.

Previously, we would default to fp16 reduced precision, but this doesn't account for the case where user might want to, for example, use only reduced gradient comm precision. Trying to do this via:
```
MixedPrecision(reduce_dtype=torch.float16, param_dtype=torch.float32, buffer_dtype=torch.float32)
```

does not work for all use cases because the code will still attempt to cast params to fp32, but user's model may be assuming double type/fp64 somewhere.

Now, specifying
```
MixedPrecision(reduce_dtype=torch.float16)
```

only affects gradient comm precision, and does not touch casting of params / buffers.

Note that if user specifies reduced precision for only parameters, gradients will be of this reduced type in _post_backward_hook and are therefore communicated in this precision. Therefore, the priority of precision in which grads are communicated is:

reduce_dtype, if specified -> param_dtype, if specified -> full precision param type.

We take additional care to make sure grads are cast back to the full precision param type for optimizer step: either if parameter was in reduced precision or if parameter was not in reduced precision but reduced gradient precision for comm was configured.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/76423
Approved by: https://github.com/zhaojuanmao"
THUDM/ChatGLM-6B,fe8ff9d6612ea6542ff2ffab1f7a17e94c892e11,"Merge pull request #739 from hiyouga/patch-1

添加较成熟的开源微调项目ChatGLM-Efficient-Tuning"
THUDM/ChatGLM-6B,afe08a19ccadc8b238c218b245bb4c1c62598588,添加较成熟的开源微调项目ChatGLM-Efficient-Tuning
THUDM/ChatGLM-6B,afee32f2b14bb6bae60d998429e0a413c8ed4af5,"Merge pull request #19 from OedoSoldier/main

Fix `clear` command performance in Windows"
THUDM/ChatGLM-6B,fdc3e5646a38b10fabd7f23384cdd283b9ec95bf,Fix `clear` command performance in Windows
alpaka-group/alpaka,f27d78c23c22f4eb5044a39cbfa60360ad1cd129,"HIP: use emulated `atomicAdd(float*,float)`

fix #1657

Use emulated atomic for hierarchy threads. In PIConGPU builtin float atomics
e.g. to shared memory are a factor 3+ slower."
tensor-compiler/taco,7a05d638eff301a07af4b5976ddd7c8e82f9d4df,"Add mergeby scheduling directive.

This commit addes a new scheduling directive called mergeby. This directive specifies if the iterators of a given variable is merged by Two Finger merge or Galloping. The default strategy is Two Finger merge which is the same as the old behavior. Galloping merges the iterators with exponential search, which can be more efficient if the iterator sizes are skewed."
OpenSYCL/OpenSYCL,e092e65855376372685aeaed026077e4c4d2b353,"Add generic SSCP compilation flow: Single pass compiler to generic LLVM IR + runtime JIT (#862)


* [SSCP] Add kernel outlining and HCF embedding support

* [SSCP] Enable LLVM SSCP flow in syclcc

* [SSCP] Add LLVM flag to enable/disable SSCP

* [SSCP] Hook up SSCP kernel launcher prototype

* [SSCP] Generalize HCF generation and make __hipsycl_sscp_is_host work in host pass

* [SSCP] Add -mllvm -hipsycl-sscp-emit-hcf flag to emit HCF

* [SSCP] Make SSCP compiler support configurable in cmake and disable for old LLVM

* [SSCP] Code cleanup

* [SSCP] Support device-side __hipsycl_sscp_is_host/device

* [SSCP] Make all kernel code noexcept

* [SSCP] Require LLVM 14+

* [SSCP] Add SSCP libkernel detection and __hipsycl_if_target support

* [SSCP] Ensure IR constants have internal linkage to allow linking multiple compilation units together

* [SSCP] Support selection of SSCP kernel launcher

* [SSCP] Add llvm-to-backend infrastructure

* [SSCP] llvm-to-backend tools: Provide HCF as argument when constructin tool to gain access to kernel names

* [SSCP] Add some stage2 spir-v logic

* [SSCP] Correctly set llvm-to-backend include dirs for LLVM

* [SSCP] Add support for S2 IR constants

* [SSCP] Add libkernel bitcode generation infrastructure

* [SSCP] Map thread hierarchy to SSCP builtins

* [SSCP] Enable libkernel bitcode libraries to cmake

* [SSCP] Update IR constant and llvm-to-backend infrastructure

* [SSCP] Add support for basic parallel for

* [SSCP] Improve code generation quality

* [SSCP] Remove LLVM dependency from llvm-to-backend tools

* [SSCP] Use shared LLVM libraries for llvm-to-backend

* [SSCP] Add kernel_configuration header

* [SSCP] Make optimizations backend-configurable and fix error handling when parsing bitcode

* [SSCP] More error handling fixes

* [SSCP][llvm-to-spirv] Fix temp file discard on error

* [SSCP][llvm-to-spirv] Fix file handling

* [SSCP][llvm-to-spirv] Revert to using TempFile::create()

* [SSCP] Add linking methods to LLVMToBackend

* [SSCP] Initial support to obtain mangled kernel name in host code

* [SSCP] Correctly handle setting string IR constant when string length changes

* [SSCP][llvm-to-spirv] Add llvm-spirv translator as external dependency and use our own llvm-spirv translator in llvm-to-spirv

* [SSCP] Add hipSYCL-common

* [SSCP][llvm-to-spirv] Add more functionality to hipSYCL-common and rely on it in llvm-to-spirv for linking

* [SSCP][llvm-to-spirv] Add more diagnostic output, remove unneeded optimization in llvm-to-spirv

* [SSCP][llvm-to-backend-tool] Start adding infrastructure to dump partial transformation results

* [SSCP] Finalize support for partial transformations

* [SSCP][llvm-to-spirv] Mark functions as SPIR_FUNC

* [SSCP][llvm-to-backend] Link only needed symbols

* [SSCP][llvm-to-spirv] Apply spir_func to all functions and call sites as calling convention

* [SSCP] Add jit helper functions

* [SSCP] Enable SSCP device selection, refactor obtaining code object invokers

* [SSCP] Add missing include

* [SSCP][L0] Add Level Zero SSCP kernel invocation support and kernel launch infrastructure

* [SSCP] Fix jit::compile() constness

* [SSCP] Fix constness of arguments in kernel launcher

* [SSCP] Fix assertion from incorrect detection of IRConstant initialization

* [SSCP][L0] Only expose SSCP support when HIPSYCL_WITH_SSCP_COMPILER is set

* [SSCP] Fix segfaults in unoptimized code due to __hipsycl_sscp_extract_kernel_name calls to undef value

* [SSCP] Extend cmake rpath settings to compiler components and include llvm-to-backend libraries

* [SSCP][L0] Fix kernel name list extraction from code object

* [SSCP] Report error in case of launch failure

* [SSCP][llvm-to-ptx] Add initial llvm-to-ptx infrastructure

* [L0] Add build log to error message in case module compilation fails

* [SSCP][llvm-to-ptx] Add support for ptx codegen from flavored IR

* [L0] Add diagnostic output about kernel local size and num groups

* [SSCP][llvm-to-spirv] Use functions instead of global variables for SPIR-V builtins

* [SSCP][llvm-to-spirv] Add address space handling, assuming default address space is generic

* [SSCP][CUDA] Add initial SSCP kernel launch code to CUDA backend

* [SSCP][CUDA] Add cuda_sscp_executable_object::contains() implementation

* [SSCP][CUDA] Advertise SSCP support for CUDA backend

* [SSCP][CUDA] Fix potential incorrect module build due to module not initialized to nullptr

* [SSCP] Add some documentation

* [SSCP][llvm-to-ptx] Correctly set S2 IR constant for current backend to ptx

* [SSCP][llvm-to-spirv] Map math and native builtins

* [SSCP][libkernel] Avoid using stdint.h to prevent cross-compilation issues

* [SSCP][llvm-to-spirv] Automatically apply patch to llvm-spirv translator to fix address space handling

* [SSCP][llvm-to-spirv] Bad workaround for cmake interpreting already applied llvm-spirv patch as failure

* [SSCP] Map mooaar builtins

* [SSCP] Add group functions stub

* [SSCP] Avoid warning due to redefinition of HIPSYCL_LIBKERNEL_IS_UNIFIED_HOST_DEVICE_PASS

* [SSCP] Introduce __hipsycl_backend_switch to avoid warnings due to clang not recognizing that SSCP host and device paths are mutually exclusive

* [SSCP] Suppress warnings due to undefined __hipsycl_sscp_extract_kernel_name

* [SSCP] Use __hipsycl_backend_switch() to suppress warnings in thread_hierarchy.hpp

* [SSCP] Add dynamic local mem infrastructure to avoid warnings

* [SSCP] Fix semicolons in __hipsycl_backend_switch for non-SSCP flows

* [SSCP][llvm-to-spirv] Add dynamic local mem implementation for SPIR-V

* [SSCP] Fix codegen when not enabling optimizations

* [SSCP][llvm-to-spirv] Add initial support for __hipsycl_sscp_get_dynamic_local_memory() and revised build option infrastructure

* [SSCP][llvm-to-ptx] Automatically compile to compute capability of available hardware

* [SSCP] Make backend translation invocations more apparent when outputting debug messages

* [SSCP][llvm-to-ptx] Find and link with libdevice

* [SSCP][llvm-to-ptx] Map builtins to libdevice

* [SSCP][llvm-to-ptx] Map native builtins

* [SSCP][syclcc] Don't print directories as runtime backends when using --hipsycl-version

* [SSCP][llvm-to-amdgpu] Add initial llvm-to-amdgpu infrastructure

* [SSCP][llvm-to-amdgpu] Translate byval kernel arguments to byref

* [SSCP] Enable nd_range kernels

* [SSCP] Add subgroup support

* [SSCP] Silence compiler warning about some paths not returning return value

* [SSCP] Fix nd_range compilation issues

* [SSCP] Avoid LLVM aliases in device code

* [SSCP] Actually set user-specified build options and flags

* [SSCP] Remove target-specific attributes

* [SSCP][llvm-to-spirv] Remove dummy dynamic local memory array if no local memory was requested

* [SSCP] Fix bug in index flipping, causing group sizes of 0 to be launched

* [SSCP] Fix kernel outlining when intrinsics are encountered: Don't throw them away

* [SSCP][llvm-to-ptx][llvm-to-spirv] Don't change linkage of intrinsics

* [SSCP] Reenable llvm-spirv build

* [SSCP][llvm-to-amdgpu] Fix IR generation

* [SSCP][llvm-to-amdgpu] Generate correct HIP fat binaries and enable ROCm workarounds

* [SSCP][HIP] Add SSCP support to HIP runtime backend

* [SSCP][llvm-to-amdgpu] Compile builtins with -nogpulib to avoid ROCm dependency

* [SSCP][llvm-to-amdgpu][HIP] Add SSCP JIT support for AMD GPUs (#878)

* [SSCP][llvm-to-amdgpu] Add initial llvm-to-amdgpu infrastructure

* [SSCP][llvm-to-amdgpu] Translate byval kernel arguments to byref

* [SSCP][llvm-to-amdgpu] Fix IR generation

* [SSCP][llvm-to-amdgpu] Generate correct HIP fat binaries and enable ROCm workarounds

* [SSCP][HIP] Add SSCP support to HIP runtime backend

* [SSCP][llvm-to-amdgpu] Compile builtins with -nogpulib to avoid ROCm dependency

* [SSCP][llvm-to-amdgpu] Use hipRTC

* [SSCP][llvm-to-amdgpu] Don't just assume the latest code object model

* [SSCP][llvm-to-amdgpu] Use constant for target triple

* [SSCP][llvm-to-amdgpu] Add amdgpu local memory support

* [SSCP][llvm-to-amdgpu] Fix support for local memory

* [SSCP][llvm-to-amdgpu] Map math builtins to ocml

* [SSCP][llvm-to-amdgpu] Map subgroup builtins

* [SSCP][llvm-to-amdgpu] Map native builtins

* [SSCP][llvm-to-amdgpu] Map relational builtins

* [SSCP][llvm-to-amdgpu] Map integer builtins

* [SSCP] Only build libkernel-sscp builtins for those backends that are actually enabled

* [SSCP] Fix HCF collisions when compiling multiple TUs due to non-internal linkage of HCF registration

* [SSCP][llvm-to-spirv] Fix debug output format

* [SSCP] Pass kernel lambda by reference to avoid clang potentially decomposing the struct, changing number of kernel arguments

* [SSCP] Account for the kernel lambda now being passed as reference; add logic to infer kernel argument type to attach ByRef or ByVal attributes

* [SSCP] Allow inlining kernels for JIT even when compiling with -O0

* [SSCP] Add support for SYCL_EXTERNAL

* [SSCP][llvm-to-backend] Make LLVMToBackendTranslator have setBuildOption interface, and backends a separate applyBuildOption interface

* [SSCP] Print build configuration for debug purposes

* [SSCP] Add barrier builtins

* [SSCP] Map nd_item::barrier() to __hipsycl_sscp_work_group_barrier()

* [SSCP][llvm-to-spirv] Fix code generation for barriers

* [SSCP] Map group_barrier() to SSCP builtins

* [SSCP] Handle linkage changes inside LLVMToBackendTranslator rather than inside each backend

* [SSCP] Add alloca address space handling to AddressSpaceInferencePass

* [SSCP][llvm-to-amdgpu] Fix handling of alloca through AddressSpaceInferencePass and inlining

* [SSCP][llvm-to-ptx][llvm-to-spirv] Enable/Update AddressSpaceInferencePass for alloca

* [SSCP] Add some support for timing individual SSCP compilation phases

* [SSCP] Disable reduction tests for SSCP; reductions are not yet supported and prevent tests from compiling

* [SSCP] Add HIPSYCL_SSCP_FAILED_IR_DUMP_DIRECTORY to simplify debugging when JIT fails

* [SSCP] Add atomic builtin stubs to enable running more unit tests

* [SSCP] Emit bitcode instead of text IR in case of error

* [SSCP] Take alignment into account when modifying global variables

* [SSCP][llvm-to-spirv] Handle initializer when creating dynamic local memory allocation

* [SSCP] Fix mobile_shared_ptr pulling type definitions of shared_ptr<T> into device IR, which can break SPIR-V due to the occurence of function pointers

* [SSCP] Run GlobalDCE after IR constant application to eliminate unneeded functions on the host side

* [SSCP][llvm-to-spirv] Use default alloca AS 4, avoid non-standard integer widths and remove llvm.lifetime.start/end calls

* [SSCP] More diagnostics around alloca address spaces, and introduce interface to obtain addressspacemap for all backends

* [SSCP] Update readme

* [SSCP][llvm-to-amdgpu] Don't pollute global namespace with hipSYCL builtin internals

* [SSCP] Emit kernel parameter information to HCF

* [SSCP] Pass kernel args by value, and add support for passing in arguments based on parameter information from HCF

* [SSCP] Add support for full kernel argument decomposition

* [SSCP] Attempt to handle case when neither ByVal nor ByRef attributes are present

* [SSCP] Leverage alloca to determine kernel argument type

* [SSCP] Add hotfix for cg properties extension test for SSCP

* [SSCP] Revert to passing kernel argument by reference for more predictable behavior across multiple ABIs

* [SSCP] Only expose SSCP support if hipSYCL was built with SSCP enabled

* [SSCP][NFC] Update documentation"
OpenSYCL/OpenSYCL,4dfa0ad2b7af66f3f8c674c355e732a7374c9f4e,"WIP: Fix MacOS build CI (#882)

* add OpenMP path for macos build

* add path to openmp_c_incl_dir

* look for libomp version

* add c flags and openmp lib name

* fix typo in cmake var

* add path to omp library

* add libomp path for CXX

* fix libomp lib path

* add libomp path for building tests

* remove hipsycl targe and other cmake variables for libomp

* test installation

* add var for accelerated cpu backend

* fix typo in macos ci

* remove accelerated cpu backend

* provide absolute path to bin

* library-only with apple clang

* remove self test of hipSYCL build

* fix cpu test: add path to libomp

* change version of libomp

* add libomp paths for test

* debug omp_root

* debug build tests omp

* remove omp.library-only from apple clang version

* add all libomp cmake flags

* fix typo

* set omp_root env var

* add OMP_CXX_FLAGS to build

* add OMP_CXX_FLAGS to build tests

* add cmake_cxx_flags for building tests

* clean-up: remove flags that are not needed

* fix flag name from OpenMP_ to OMP_

* add both OpenMP and OMP flags

* add cmake variable for openmp

* add paths to libomp

* debug

* debug boost

* debug tests: add cmake_cxx_flags because setting omp_cxx_flags in build removes boost

* fix ld flags in cmake_cxx in tests

* remove OMP_CXX_FLAGS

* Reduce hipsycl build cmake options .github/workflows/macos.yml

Co-authored-by: fodinabor <5982050+fodinabor@users.noreply.github.com>

* Update build tests options .github/workflows/macos.yml

Co-authored-by: fodinabor <5982050+fodinabor@users.noreply.github.com>

Co-authored-by: fodinabor <5982050+fodinabor@users.noreply.github.com>"
OpenSYCL/OpenSYCL,51507bad524c33afe8b124804091b10fa25618dc,README: Add section on performance guidelines and add new paper for citation (#818)
OpenSYCL/OpenSYCL,cd684899baac064dd29474d0fd745407f3ec3806,Optimize inorder queue::wait() (#788)
OpenSYCL/OpenSYCL,17dae921cbae9cde0d2920c209e90d986cb308e9,Optimize queue::wait() by waiting on nodes in reverse submission order (#787)
OpenSYCL/OpenSYCL,e934e2ec19a08fbe8ec51b1bc1942e2b4be7ab63,"dag_submitted_ops: Manage node lifetime by asynchronously waiting instead of event queries (#761)

* dag_submitted_ops: Manage node lifetime by asynchronously waiting on them

* dag_manager: It's unnecessary to wait for the requirements of a node for garbage collection

* Optimize node purging by waiting on newest nodes first

* Add documentation node on best performance for node purging"
SMRT-AIST/fast_gicp,aff82ee0778178b8942e2d4d31855c0d8421ac3b,PointNormal in other registration algorithms to speed up compilation (#98)
SMRT-AIST/fast_gicp,0793d7b94724c6dc3fa9f2168a42ac13151a6744,added pcl::PointNormal to FastGICPSingleThread (#95)
kokkos/kokkos,52ea295327b856d234eb1c88d17442faab54c45a,"Merge branch release-candidate-4.0.0 for 4.0.0

Part of Kokkos C++ Performance Portability Programming EcoSystem 4.0"
kokkos/kokkos,537e36d2d72a61a971fcb034bf0f5bc096a36622,"Use local variable in the parallel_reduce(RangePolicy<OpenACC>, ...)

Easier for the compiler to analyze and optimize.

Co-Authored-By: Seyong Lee <lees2@ornl.gov>"
kokkos/kokkos,d63d81e5cc782ec568d4c112f4a63521e284f792,"SIMD AVX2 backend (#5512)

* copy AVX512 backend as AVX2

* AVX2 might need T-dependent mask types

* a little bit more thought on simd_mask::reference

* drafted a lot more of AVX2 backend

* finish drafting most of simd<double, AVX2>

also add a fallback for hmin in SIMD_Common,
since AVX2 has no min reduction intrinsic

* fix a lot of errors in AVX2 double

also switch to a fallback implementation
for horizontal sum reductions since the
_mm256_hadd_pd instruction is not quite
what we need and it doesn't look like the
intrinsics version would be much faster
than the scalar version

* draft AVX2 int32 mask

* fix fallback reduction return types

* AVX2 convert int32 mask to double mask

* draft simd<std::int64_t, AVX2>

* draft AVX2 int64 mask and int32 simd

* lots more work on AVX2

* a couple more int64 support methods

* condition for int64

* draft fallback hmax

* add where expressions for int32

but they don't have any of the usual methods

* convert between AVX2 masks

* add AVX2 int32

* add fallback multiplication for SIMD

this will get called for the AVX2
backend for int64, since no such
intrinsic seems to exist

* naive constructors between AVX2 mask types

this is an uncommon operation and expressing
it as intrinsics seems complicated, we'll
just do minimum viable product for now

* condition for int32

* convert int32 simd to int64

* generator constructor for int64 simd

* generator constructor for int32 SIMD

* AVX2 gather doubles using int32 indices

* masked store for AVX2 int32

* naive scatter for AVX2

AVX2 seems to only have gather but no scatter
intrinsic. add a scatter method that does the
most naive thing and just does scalar code

* masked load for AVX2 int32

* avoid ambiguity b/w fallback and scalar hsum

* add mask reduction test that fails on AVX2

* replace testc with movemask

testc was the wrong way to
compare masks because it
doesn't work if the second
mask has any false entries.
instead, use the movemask intrinsic
to convert a wide vector mask into
an int bitmask and compare that.
this should make the simd_mask::reference::operator bool
implementation a lot faster too

* formatting

* forgot to remove an unused template parameter

* add a comment about why this fallback exists

see review https://github.com/kokkos/kokkos/pull/5512#discussion_r999814722"
kokkos/kokkos,d5575d4611ae5c8eaf674c5fc313ab1299145574,"Merge pull request #5343 from cz4rs/port-sample-perf-test

Tracking performance testing: Port sample Kokkos Core performance test"
kokkos/kokkos,0018e5fb8bfd442912f7253db22c6f170d5d231a,"Merge branch 'release-candidate-3.7.00' for 3.7.00

Part of Kokkos C++ Performance Portability Programming EcoSystem 3.7"
kokkos/kokkos,2d6cbad7e079eb45ae69ac6a59929d9fcf10409a,"Tracking performance testing: Integrate google benchmark (#5177)

* Integrate google-benchmark library

- add embedded copy of google-benchmark
- skip non-vital parts of the library
- add sample benchmark

* Only link benchmark lib to performance tests

* Fix clang-tidy warnings in benchmark lib

* Remove deprecation for `benchmark::MemoryManager::Stop(...)`

Deprecated version is used by `Stop(Result &result)` overload.
BENCHMARK_DISABLE_DEPRECATED_WARNING does not work for CUDA-11.6-NVHPC,
so the easies way to silence the warning is to remove the deprecation
macro.

* Suppress `offsetof` warning in benchmark lib

* Add back benchmark install target

* Do not assert offset when using nvcc

* Remove `benchmark_main` target

* Try crafting custom target for benchmark lib

* Add a fixed copy of `export.h`

* Use custom benchmark target

* Remove CMake files from benchmark lib

* Fix formatting

* Move kokkos_benchmark target into perf_test directory

* Search for installed `benchmark` lib by default.

Search for an installed `benchmark` package and use it by default. When
not found, embedded copy from `tpls/benchmark` will be used.
One can force the use of the embedded copy by setting CMake variable
`CMAKE_DISABLE_FIND_PACKAGE_benchmark` to `TRUE`.

* Mark `Threads` package as required for `benchmark`

* Remove duplicated code

* Remove benchmark library code

* Use `FetchContent_Declare` for obtaining benchmark lib

* Disable clang-tidy for benchmark lib

* Disable warnings for benchmark lib targets

* Add git to container images

* Use fixed version of benchmark lib

* Add Kokkos_ENABLE_PERF_TESTS option

Add Kokkos_ENABLE_PERF_TESTS option (default OFF). Only look for
`benchmark` lib when the performance tests are enabled.

* Enable performance tests for all CI targets

* Disable performance tests for NVHPC builds

* Rename Kokkos_ENABLE_PERF_TESTS to Kokkos_ENABLE_BENCHMARKS

* Use pure CMake instead of `KOKKOS_ADD_EXECUTABLE_AND_TEST`

* Add KOKKOS_ADD_BENCHMARK function

* Remove incorret FAIL_REGULAR_EXPRESSION

* Improve CMake messages and formatting

* Use .tar.gz archive instead of git repository

* Revert ""Add git to container images""

This reverts commit f5b813e7a3b87089cb95a3b2964b1c847191ce86."
kokkos/kokkos,eb027ea5e41890c6f30f1121b28ed84187674d58,"Adding Changelog for Release 3.7.00

Part of Kokkos C++ Performance Portability Programming EcoSystem 3.7"
kokkos/kokkos,fe00acc24a831dcc6acd77c0503beb33f2a05a25,"use _mm512_cmp_pd_mask

GCC 7.2.0 and older don't provide the special
case _mm512_cmplt_pd_mask and friends, but all
the way down to GCC 5.5.0 do support the
three-argument _mm512_cmp_pd_mask intrinsic.
Instead of cluttering with #ifdef __GNUC__ < 8
everywhere, just use the three-argument instrinsic
always.
The Intel documentation suggests performance will
not suffer..."
kokkos/kokkos,dbb94f895ec4d28cb7e0578194edf1372b21e6e7,"remove first_n

this was a non-standard helper method that
can now be emulated (although less efficiently)
using the access operator for simd_mask"
kokkos/kokkos,32c6d9017c747f57b2d8da5f9ae494bbc26c9303,"draft operator[] for AVX-512 SIMD

Their implementation is copied from
the STK project... I'm almost certain
it violates C++ aliasing rules but
it seems to have ""worked"" until now
and is likely faster than things that
obey the rules of C++"
kokkos/kokkos,179be8c4389c707daa8c82efb47a333868728e58,"Adding Changelog for Release 3.6.01

Part of Kokkos C++ Performance Portability Programming EcoSystem 3.6"
kokkos/kokkos,1f905e1dd2ecf90a11d82b21f85c17f2282fec45,"In vector::insert(iterator, InputIterator, InputIterator):

Changed the out of range check on it to use std::less, as one
cannot portably perform ordered pointer comparisons on pointers
that aren't pointing inside the same array (to a first approximation).

Removed the special case when inserting into an empty vector,
as it was incorrect (see issue #4975) and has no real performance
gain (unlike this special case in insert(it, count, val)).

Removed the special case when inserting zero elements, as there
is no need to hand optimize for this rare case, as it falls out
of the general algorithm anyway.

In vector::insert(it, count, val):

Changed the out of range check to use std::less

In TestVector.hpp:
Added tests for the above"
benfred/implicit,734f5c7154e04939ce3b9168c4fe7494a62973fc,"Allow GPU models to train on sparse matrices that exceed the size of available GPU memory (#605)

Use CUDA Unified Virtual Memory for sparse matrices on the GPU. This allows GPU models to train on input sparse matrices that exceed the size of GPU memory, by letting cuda page data to/from host memory using UVM.

This has been tested on a ALS model with around 2B entries in the spare matrix, on a GPU with 16GB of memory. Previously this OOM'ed since we need around 32GB of GPU memory to store the sparse matrix and its transpose, but with this change training succeeded - and was around 20x faster on the GPU than on the CPU."
benfred/implicit,16bbd58d5f7623b8b9b48c639df4cfa7402ae9f0,Add performance section to release drafts
